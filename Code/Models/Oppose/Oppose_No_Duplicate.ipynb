{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oppose_No_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e14daa362d864e2991cbc406db22023d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2334e8bfb1184e319cf1391d093a15ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61919019147d49c0b20f9724b5497418",
              "IPY_MODEL_9b3b8b91629247f8ad113307f1f4b8cd"
            ]
          }
        },
        "2334e8bfb1184e319cf1391d093a15ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61919019147d49c0b20f9724b5497418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_318834fb6730457caa4f43ac1915b5a2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca41f74453dc420286f4d93d15a23542"
          }
        },
        "9b3b8b91629247f8ad113307f1f4b8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_91f5e1d660124a77b04d9704cc6a26c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:29&lt;00:00,  1.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_120d42321cf44d30ba476a1356325cc0"
          }
        },
        "318834fb6730457caa4f43ac1915b5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca41f74453dc420286f4d93d15a23542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91f5e1d660124a77b04d9704cc6a26c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "120d42321cf44d30ba476a1356325cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aecfae2449394d1c8b45d38db9068e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47bd37e5f03645da824b47fb8acc79a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1621a49a2c184253a4356fee9f38fced",
              "IPY_MODEL_2ad7771293024b74b0f827579ec79cf7"
            ]
          }
        },
        "47bd37e5f03645da824b47fb8acc79a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1621a49a2c184253a4356fee9f38fced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1280b834489e43f58a0769704b158e5b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c76bae016874a7d8a64dabc47626f80"
          }
        },
        "2ad7771293024b74b0f827579ec79cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_773509667ace4730a1f36af11609ea6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:55&lt;00:00,  1.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e845a9bc7804517b0946a19e8f64573"
          }
        },
        "1280b834489e43f58a0769704b158e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c76bae016874a7d8a64dabc47626f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "773509667ace4730a1f36af11609ea6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e845a9bc7804517b0946a19e8f64573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a936e9403824caaaf779752bcd1c573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e0eab9a6ab674687b65525883434f8a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68e05514d0434aa1932044160f81d6ca",
              "IPY_MODEL_258f5228ee864ebb88624e2ea9190352"
            ]
          }
        },
        "e0eab9a6ab674687b65525883434f8a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68e05514d0434aa1932044160f81d6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89b28838e18c4e0a8da73e24276b5a4c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd3ccbdc5bc3482a9e6dba4b2872e9e4"
          }
        },
        "258f5228ee864ebb88624e2ea9190352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d96ffc31f4f54dd2b9a8408ff18cdff8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:57&lt;00:00,  1.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56aad74dfa714943a0107b5fd180a4b6"
          }
        },
        "89b28838e18c4e0a8da73e24276b5a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd3ccbdc5bc3482a9e6dba4b2872e9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d96ffc31f4f54dd2b9a8408ff18cdff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56aad74dfa714943a0107b5fd180a4b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a5908790-b8a3-4a41-bcac-7db3cba54fc0"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 10.06 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccf81400-0023-4e57-ba02-3939befb0f7d"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01abc134-5134-4619-e310-07bf4e66172a"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1de4b79b-14c7-4c86-8feb-81d322e69a42"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4507c19f-e307-457f-f10b-144f34cd687a"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "aa54c023-4ce9-4776-c79d-0d2417bd47b5"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 22.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "963474ac-acc9-4371-e2b1-c016df1c56e7"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "585eb498-0f43-43d2-f63d-627dc45041c6"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c2597567-f400-4c79-cc71-7508eebf9ab8"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1b46e815-70ee-4a57-a913-aaac5fb2638c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    #MeToo movement in China: Powerful yet fragile...\n",
              "1    From the Economist #AccordingToKeyhole  After ...\n",
              "2    #WETOO | for #METOO men for women Dinamalar Ha...\n",
              "3    Bollywood is a place for #hypocrite.  He appea...\n",
              "4    Alyssa Milano, one of the first proponents of ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b566df3-99d8-4561-f1ed-dbea6460a11a"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:42<00:00, 9668421.94B/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb867cde-dc0f-44ac-f062-678a534b7d69"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 263058.00B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "746a259a-b596-4730-c96d-44b6752b2b3f"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5879f2f6-dad8-4527-abc0-510516e48b42"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "            random.shuffle(text)\n",
        "            text4 = ' '.join(text)\n",
        "            df4['text'][i]=text4\n",
        "            random.shuffle(text)\n",
        "            text5 = ' '.join(text)\n",
        "            df5['text'][i]=text5\n",
        "        #self.data = self.data.append(df2, ignore_index=True)\n",
        "        #self.data = self.data.append(df3, ignore_index=True)\n",
        "        #self.data = self.data.append(df4, ignore_index=True)\n",
        "        #self.data = self.data.append(df5, ignore_index=True)\n",
        "        #self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6fabe45-c565-48b9-ee4c-04ac3c215411"
      },
      "source": [
        "col_name = \"Oppose\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898,
          "referenced_widgets": [
            "e14daa362d864e2991cbc406db22023d",
            "2334e8bfb1184e319cf1391d093a15ca",
            "61919019147d49c0b20f9724b5497418",
            "9b3b8b91629247f8ad113307f1f4b8cd",
            "318834fb6730457caa4f43ac1915b5a2",
            "ca41f74453dc420286f4d93d15a23542",
            "91f5e1d660124a77b04d9704cc6a26c2",
            "120d42321cf44d30ba476a1356325cc0",
            "aecfae2449394d1c8b45d38db9068e7b",
            "47bd37e5f03645da824b47fb8acc79a5",
            "1621a49a2c184253a4356fee9f38fced",
            "2ad7771293024b74b0f827579ec79cf7",
            "1280b834489e43f58a0769704b158e5b",
            "3c76bae016874a7d8a64dabc47626f80",
            "773509667ace4730a1f36af11609ea6b",
            "7e845a9bc7804517b0946a19e8f64573",
            "0a936e9403824caaaf779752bcd1c573",
            "e0eab9a6ab674687b65525883434f8a7",
            "68e05514d0434aa1932044160f81d6ca",
            "258f5228ee864ebb88624e2ea9190352",
            "89b28838e18c4e0a8da73e24276b5a4c",
            "dd3ccbdc5bc3482a9e6dba4b2872e9e4",
            "d96ffc31f4f54dd2b9a8408ff18cdff8",
            "56aad74dfa714943a0107b5fd180a4b6"
          ]
        },
        "outputId": "18658fab-ea38-4243-bf5b-dc338ce8b9a7"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6382\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 1596\n",
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e14daa362d864e2991cbc406db22023d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0451\n",
            "Train Losses : [0.00708181643858552, 0.11913187801837921, 0.1122686043381691, 0.009443534538149834, 0.01192593201994896, 0.012171893380582333, 0.01070486195385456, 0.008361070416867733, 0.005737831816077232, 0.0036499295383691788, 0.00225647515617311, 0.0012922602472826838, 0.0007714815437793732, 0.000426995859015733, 0.00024886970641091466, 0.00013574224431067705, 0.2450391799211502, 0.00014145171735435724, 0.00024094835680443794, 0.00037798055564053357, 0.000540098175406456, 0.1397998183965683, 0.14719118177890778, 0.002332812873646617, 0.09883685410022736, 0.016289496794342995, 0.010906985960900784, 0.013704168610274792, 0.23802143335342407, 0.01783427782356739, 0.08424175530672073, 0.01793678291141987, 0.017939820885658264, 0.21623946726322174, 0.11383635550737381, 0.022513974457979202, 0.1346600353717804, 0.023044349625706673, 0.023959800601005554, 0.023497246205806732, 0.14559248089790344, 0.019115595147013664, 0.14215171337127686, 0.014613229781389236, 0.012190984562039375, 0.010991846211254597, 0.008442208170890808, 0.007313854061067104, 0.005923000164330006, 0.004724383354187012, 0.07881547510623932, 0.00322736450470984, 0.15120163559913635, 0.11624185740947723, 0.0028238401282578707, 0.0029897026252001524, 0.003229031804949045, 0.0033264881931245327, 0.0034426990896463394, 0.12664452195167542, 0.0036462952848523855, 0.003629373386502266, 0.12533456087112427, 0.004062911495566368, 0.004743508063256741, 0.08649273961782455, 0.005039095412939787, 0.005862744525074959, 0.005791656207293272, 0.005940447095781565, 0.006090402137488127, 0.0059382617473602295, 0.005395531188696623, 0.005597440991550684, 0.00484505807980895, 0.00473178643733263, 0.004468818660825491, 0.003773181000724435, 0.11816520243883133, 0.003557416144758463, 0.003442139131948352, 0.003112229285761714, 0.003329069819301367, 0.480325311422348, 0.0037290179170668125, 0.0050580971874296665, 0.006124037317931652, 0.006973228882998228, 0.008786655031144619, 0.16870412230491638, 0.1420205980539322, 0.01091731246560812, 0.012527763843536377, 0.012887714430689812, 0.17953136563301086, 0.4195091426372528, 0.015354307368397713, 0.09067600220441818, 0.01870725117623806, 0.019673356786370277, 0.020792484283447266, 0.020919622853398323, 0.020457347854971886, 0.01979842223227024, 0.018517712131142616, 0.1053999736905098, 0.0163409560918808, 0.015287317335605621, 0.013967406004667282, 0.012702985666692257, 0.09862774610519409, 0.010317559354007244, 0.009522461332380772, 0.12446470558643341, 0.007930241525173187, 0.007206018082797527, 0.0065956758335232735, 0.006088503170758486, 0.005482315551489592, 0.1283394694328308, 0.004666340537369251, 0.004420682787895203, 0.004144786391407251, 0.12674282491207123, 0.003858063602820039, 0.12704668939113617, 0.0037496613804250956, 0.1279410570859909, 0.003947969060391188, 0.0040939380414783955, 0.004205978009849787, 0.004235674161463976, 0.004257182590663433, 0.00426034489646554, 0.004183650016784668, 0.004086832981556654, 0.003951733931899071, 0.003821284743025899, 0.003702393267303705, 0.0035126253496855497, 0.11905483156442642, 0.12186215072870255, 0.12040365487337112, 0.0038049032445997, 0.00406030286103487, 0.13274303078651428, 0.0046219895593822, 0.004913098644465208, 0.10611983388662338, 0.005604824982583523, 0.1063903197646141, 0.00619468092918396, 0.12196578085422516, 0.09337799996137619, 0.0076697696931660175, 0.007893153466284275, 0.00823457632213831, 0.008547008037567139, 0.1048097014427185, 0.008749817498028278, 0.10874476283788681, 0.0941169261932373, 0.11699172854423523, 0.09888731688261032, 0.010032369755208492, 0.010166972875595093, 0.01030963659286499, 0.01027530524879694, 0.11344563215970993, 0.010161157697439194, 0.010029264725744724, 0.009656962007284164, 0.009335154667496681, 0.00889506470412016, 0.10573485493659973, 0.008235949091613293, 0.007593636866658926, 0.14754514396190643, 0.007041349541395903, 0.00684805354103446, 0.006594830192625523, 0.006176582537591457, 0.10919728875160217, 0.00566243939101696, 0.005532121751457453, 0.0052913944236934185, 0.11153534054756165, 0.004950325470417738, 0.3919105529785156, 0.11790505796670914, 0.0066622537560760975, 0.007632465101778507, 0.1288115531206131, 0.11457251757383347, 0.010465594939887524, 0.11625660955905914, 0.012226089835166931, 0.012900426052510738, 0.013489784672856331, 0.013703130185604095, 0.01378447376191616, 0.01354274246841669, 0.01326896995306015, 0.08364473283290863, 0.012539257295429707, 0.012054165825247765, 0.1155674085021019, 0.011037168093025684, 0.10718490928411484, 0.010191645473241806, 0.009742051362991333, 0.009274453856050968, 0.008840224705636501, 0.00824823509901762, 0.11090511083602905, 0.007372954394668341, 0.13045085966587067, 0.006821735762059689, 0.10178926587104797, 0.006483846809715033, 0.00634258147329092, 0.11744221299886703, 0.09937310218811035, 0.00620222557336092, 0.006192420143634081, 0.006155882030725479, 0.006059896666556597, 0.0059446110390126705, 0.0057866680435836315, 0.005571677815169096, 0.0053732977248728275, 0.005143018905073404, 0.00491422088816762, 0.004654957912862301, 0.0044244760647416115, 0.004181454423815012, 0.003934002481400967, 0.111808642745018, 0.003618562128394842, 0.003509861184284091, 0.12711918354034424, 0.0034517343156039715, 0.0034543585497885942, 0.0034334624651819468, 0.0033930642530322075, 0.1302909404039383, 0.0034198672510683537, 0.0034596077166497707, 0.0034907804802060127, 0.13602150976657867, 0.0036205528303980827, 0.003710680641233921, 0.003781600622460246, 0.0038010242860764265, 0.0038419675547629595, 0.003824374405667186, 0.0037395074032247066, 0.12922990322113037, 0.003760483581572771, 0.003800255013629794, 0.0038055391050875187, 0.11568187177181244, 0.003908313345164061, 0.003977584652602673, 0.004010602366179228, 0.004016535356640816, 0.003999271895736456, 0.12986613810062408, 0.004048881120979786, 0.004098177421838045, 0.004112664144486189, 0.11329076439142227, 0.42011621594429016, 0.005083146505057812, 0.00597023218870163, 0.006780717056244612, 0.0075057935900986195, 0.008163279853761196, 0.008732868358492851, 0.009162718430161476, 0.009466031566262245, 0.10640909522771835, 0.009937699884176254, 0.10813097655773163, 0.010285487398505211, 0.11364848166704178, 0.10555270314216614, 0.010753569193184376, 0.010816480964422226, 0.010799065232276917, 0.010662541724741459, 0.010431820526719093, 0.10760970413684845, 0.11029716581106186, 0.11158803105354309, 0.009809164330363274, 0.009704306721687317, 0.10038765519857407, 0.009481023997068405, 0.0993104949593544, 0.009205549024045467, 0.00909684132784605, 0.008833878673613071, 0.008602568879723549, 0.09817401319742203, 0.008087466470897198, 0.10859967768192291, 0.007726621814072132, 0.007576188538223505, 0.007339548785239458, 0.007086973171681166, 0.006836646236479282, 0.0065087522380054, 0.006237025372684002, 0.005891764536499977, 0.10039229691028595, 0.11923974752426147, 0.0053686704486608505, 0.005307464394718409, 0.005194875877350569, 0.005099854432046413, 0.00494020851328969, 0.00478068133816123, 0.004587715957313776, 0.004414383787661791, 0.004222715739160776, 0.00404278514906764, 0.003856913186609745, 0.0037041152827441692, 0.003523716703057289, 0.8859143853187561, 0.004246827680617571, 0.00523220282047987, 0.7219542264938354, 0.00837701465934515, 0.1019846722483635, 0.10697675496339798, 0.1188606470823288, 0.019013315439224243, 0.021608393639326096, 0.2435450255870819, 0.12046239525079727, 0.02933962456882, 0.12549971044063568, 0.03358945995569229, 0.034896038472652435, 0.0360100083053112, 0.03639046847820282, 0.036363549530506134, 0.03619883954524994, 0.03547988459467888, 0.10582446306943893, 0.03372432291507721, 0.03233962878584862, 0.11381841450929642, 0.029807180166244507, 0.028287161141633987, 0.026710841804742813, 0.025088481605052948, 0.023494815453886986, 0.02174959145486355, 0.10454364120960236, 0.01862536370754242, 0.017211243510246277, 0.015849409624934196, 0.014581781812012196, 0.10462367534637451, 0.012159670703113079, 0.011176965199410915, 0.10427030920982361, 0.009563815779983997, 0.008783180266618729, 0.008058164268732071, 0.007447300013154745, 0.006836684886366129, 0.006276327650994062, 0.005723533686250448, 0.368276447057724, 0.0054827784188091755, 0.005703052971512079, 0.005918133072555065, 0.11802694201469421, 0.12693572044372559, 0.006488010287284851, 0.006686229724436998, 0.10346218198537827, 0.007046648301184177, 0.007191245909780264, 0.328604519367218, 0.007955861277878284, 0.008549127727746964, 0.10121041536331177, 0.11452113837003708, 0.010205790400505066, 0.11406351625919342, 0.011140458285808563, 0.0114429397508502, 0.011704166419804096, 0.011850043199956417, 0.011780609376728535, 0.011806292459368706, 0.09755678474903107, 0.12015832215547562, 0.011635839007794857, 0.01153647806495428, 0.011392499320209026, 0.011100661009550095, 0.010835971683263779, 0.010500294156372547, 0.010134914889931679, 0.009830824099481106, 0.00938656460493803, 0.10918159037828445, 0.008757200092077255, 0.008447579108178616, 0.09545817971229553, 0.007878802716732025, 0.007697645574808121, 0.007384339347481728, 0.007156203966587782, 0.006888268515467644, 0.006587819661945105, 0.006372591946274042, 0.006091166287660599, 0.005808020010590553, 0.3843250274658203, 0.00581635907292366, 0.006105105392634869, 0.006310049444437027, 0.00645748944953084, 0.006557796150445938, 0.006577479653060436, 0.12364329397678375, 0.006687017157673836, 0.00682127894833684, 0.006754844915121794, 0.006736365146934986, 0.1078491136431694, 0.006681710947304964, 0.10466388612985611, 0.11359865218400955, 0.11861445009708405, 0.007002207450568676, 0.007080284878611565, 0.10233063995838165, 0.0073110321536660194, 0.007345188409090042, 0.007385844364762306, 0.007365948986262083, 0.10707823187112808, 0.1274379938840866, 0.007387903053313494, 0.10376449674367905, 0.12389667332172394, 0.007686129771173, 0.09918083250522614, 0.3359854817390442, 0.11242662370204926, 0.10712108016014099, 0.009967094287276268, 0.010575673542916775, 0.011071802116930485, 0.011542554013431072, 0.1082163155078888, 0.29375743865966797, 0.013049358502030373, 0.09833179414272308, 0.10464413464069366, 0.015093029476702213, 0.09525426477193832, 0.01609070599079132, 0.01639120653271675, 0.016618959605693817, 0.016595784574747086, 0.016546009108424187, 0.016305848956108093, 0.10673271864652634, 0.015865599736571312, 0.015527074225246906, 0.015050848014652729, 0.11243715137243271, 0.0920298844575882, 0.013969129882752895, 0.01368202269077301, 0.09405908733606339, 0.012826410122215748, 0.012509685941040516, 0.012149342335760593, 0.011666742153465748, 0.10000868886709213, 0.01090541947633028, 0.01045057363808155, 0.010024303570389748, 0.009746596217155457, 0.009241643361747265, 0.09656799584627151, 0.008583911694586277, 0.00830022431910038, 0.007918208837509155, 0.007591370493173599, 0.00735221104696393, 0.09570761770009995, 0.006808131467550993, 0.0065686702728271484, 0.1251697540283203, 0.006242205388844013, 0.006043161731213331, 0.005948859732598066, 0.10809779912233353, 0.005629133433103561, 0.005540910642594099, 0.12823018431663513, 0.005474690813571215, 0.0053983209654688835, 0.005331405438482761, 0.005238895770162344, 0.005118590779602528, 0.005041135009378195, 0.36562907695770264, 0.005212598945945501, 0.00558829540386796, 0.005746542476117611, 0.006028168369084597, 0.13470181822776794, 0.00641887029632926, 0.006506761536002159, 0.006610819138586521, 0.35773828625679016, 0.007210393436253071, 0.007793216034770012, 0.008212634362280369, 0.008544663898646832, 0.009116269648075104, 0.0090189753100276, 0.009145530872046947, 0.009176344610750675, 0.009237637743353844, 0.00925714336335659, 0.009097795002162457, 0.008999284356832504, 0.1294514685869217, 0.10126031935214996, 0.00875233206897974, 0.008763598278164864, 0.008632566779851913, 0.12557630240917206, 0.008449671790003777, 0.09115325659513474, 0.008321638219058514, 0.008266933262348175, 0.10606087744235992, 0.008060815744102001, 0.12028051167726517, 0.00798784103244543, 0.11402612924575806, 0.007948131300508976, 0.007889930158853531, 0.0078034293837845325, 0.09703268110752106, 0.007692377548664808, 0.007609093561768532, 0.34701666235923767, 0.007896202616393566, 0.008219758979976177, 0.008470985107123852, 0.00864511076360941, 0.11715111881494522, 0.09220361709594727, 0.3124091923236847, 0.09835151582956314, 0.010485824197530746, 0.011040563695132732, 0.011505305767059326, 0.09334120899438858, 0.012285320088267326, 0.11442887037992477, 0.01278340071439743, 0.013011116534471512, 0.013019289821386337, 0.01306853536516428, 0.013138968497514725, 0.012826530262827873, 0.012655014172196388, 0.01253291592001915, 0.12915490567684174, 0.10599519312381744, 0.011841528117656708, 0.011493833735585213, 0.01127066183835268, 0.010937255807220936, 0.12058082222938538, 0.01045320276170969, 0.010205842554569244, 0.009918439202010632, 0.09669195115566254, 0.10661164671182632, 0.10369155555963516, 0.09483551234006882, 0.009100357070565224, 0.009002456441521645, 0.1103152483701706, 0.008808349259197712, 0.008705584332346916, 0.00866760965436697, 0.00840736273676157, 0.008283267728984356, 0.008085677400231361, 0.007900457829236984, 0.09833600372076035, 0.007500171661376953, 0.11375455558300018, 0.10454786568880081, 0.0072159552946686745, 0.0071504670195281506, 0.007100061047822237, 0.006975555792450905, 0.00685762707144022, 0.35706663131713867, 0.3503406047821045, 0.11281926929950714, 0.008532945066690445, 0.1154276579618454, 0.009869830682873726, 0.010435624979436398, 0.011015444062650204, 0.3483903706073761, 0.012317759916186333, 0.013029864057898521, 0.013742010109126568, 0.3051014542579651, 0.09607725590467453, 0.01623653434216976, 0.10539282858371735, 0.08926306664943695, 0.018362868577241898, 0.10826997458934784, 0.01934378780424595, 0.019633106887340546, 0.10664613544940948, 0.019912978634238243, 0.019863858819007874, 0.09250626713037491, 0.019596539437770844, 0.11219710111618042, 0.0191225316375494, 0.11223730444908142, 0.018473541364073753, 0.09742852300405502, 0.01768697425723076, 0.0173056498169899, 0.01688215509057045, 0.016374392434954643, 0.27925628423690796, 0.015866564586758614, 0.09925048053264618, 0.10413909703493118, 0.015652555972337723, 0.015426377765834332, 0.11334729194641113, 0.08776020258665085, 0.01494411751627922, 0.10276882350444794, 0.01457875408232212, 0.014332527294754982, 0.09581178426742554, 0.09963987022638321, 0.10594414174556732, 0.11457905918359756, 0.013347127474844456, 0.013192658312618732, 0.012973573990166187, 0.012741231359541416, 0.012386304326355457, 0.01209753006696701, 0.11276499927043915, 0.011466358788311481, 0.1032453179359436, 0.010929406620562077, 0.0106511190533638, 0.010356971994042397, 0.010046779178082943, 0.09402836114168167, 0.009539300575852394, 0.009299186989665031, 0.009031730704009533, 0.008729532361030579, 0.008470923639833927, 0.008185471408069134, 0.007922878488898277, 0.007632732857018709, 0.007384403143078089, 0.007088458631187677, 0.11628638207912445, 0.006651884876191616, 0.10124360769987106, 0.006336575839668512, 0.11431446671485901, 0.006154290400445461, 0.006087395828217268, 0.005988071206957102, 0.005870346911251545, 0.0057777599431574345, 0.005666546523571014, 0.005530442576855421, 0.005411842837929726, 0.005272167269140482, 0.005132690537720919, 0.1242421343922615, 0.004922718740999699, 0.004861644469201565, 0.0047744205221533775, 0.0046945312060415745, 0.004589036572724581, 0.004489797167479992, 0.004414239898324013, 0.0042996215634047985, 0.0041848341934382915, 0.0040925596840679646, 0.003994290716946125, 0.11792321503162384, 0.4224328100681305, 0.00414196215569973, 0.004413684364408255, 0.004638554062694311, 0.004872763063758612, 0.005031268578022718, 0.005197451449930668, 0.4128287732601166, 0.10891573876142502, 0.006299471948295832, 0.09785740822553635, 0.007227305788546801, 0.00765828974545002, 0.008023478090763092, 0.1216004341840744, 0.008679522201418877, 0.00895894318819046, 0.009152074344456196, 0.009326290339231491, 0.009450875222682953, 0.009478029794991016, 0.009492249228060246, 0.00942313764244318, 0.009344460442662239, 0.009245945140719414, 0.09600043296813965, 0.09512338787317276, 0.008998465724289417, 0.008945326320827007, 0.00882626697421074, 0.00870527420192957, 0.008525480516254902, 0.10205147415399551, 0.3511951267719269, 0.008606753312051296, 0.008790440857410431, 0.008982481434941292, 0.0091966912150383, 0.009199649095535278, 0.009233796037733555, 0.00923579465597868, 0.11263777315616608, 0.009202779270708561, 0.009203523397445679, 0.3351249396800995, 0.009461605921387672, 0.009687338955700397, 0.11939293146133423, 0.010170362889766693, 0.010291976854205132, 0.010407712310552597, 0.11368627846240997, 0.010585407726466656, 0.11083091795444489, 0.010641592554748058, 0.09716913104057312, 0.01067610364407301, 0.01065231952816248, 0.010610164143145084, 0.32226812839508057, 0.010793225839734077, 0.011030275374650955, 0.011217889375984669, 0.011342263780534267, 0.01135844923555851, 0.011390000581741333, 0.011342236772179604, 0.011192027479410172, 0.01106458343565464, 0.010869404301047325, 0.01066390611231327, 0.010451354086399078, 0.010184120386838913, 0.11299178749322891, 0.009726587682962418, 0.009513018652796745, 0.1118263304233551, 0.009106796234846115, 0.008922213688492775, 0.32649949193000793, 0.008935281075537205, 0.009071694687008858, 0.009196029044687748, 0.00924704223871231, 0.009237515740096569, 0.11476600915193558, 0.009243309497833252, 0.009220133535563946, 0.0091846389696002, 0.009063167497515678, 0.33579692244529724, 0.11068344116210938, 0.009538567624986172, 0.10281110554933548, 0.010057579725980759, 0.09916283935308456, 0.010463150218129158, 0.010571729391813278, 0.2998943030834198, 0.011104128323495388, 0.011561165563762188, 0.10675038397312164, 0.01209357287734747, 0.012364450842142105, 0.09065520763397217, 0.09692931175231934, 0.012808701954782009, 0.11032576858997345, 0.10324598848819733, 0.01315458957105875, 0.09029913693666458, 0.013250521384179592, 0.013219019398093224, 0.013232522644102573, 0.25547242164611816, 0.013367642648518085, 0.09455503523349762, 0.013808325864374638, 0.10927386581897736, 0.10773366689682007, 0.014402639120817184, 0.014295374974608421, 0.013908250257372856, 0.08209531009197235, 0.013689656741917133, 0.014078361913561821, 0.013624276965856552, 0.013376479968428612, 0.013018597848713398, 0.1455080658197403, 0.12953482568264008, 0.0993187427520752, 0.012232089415192604, 0.012147383764386177, 0.011722341179847717, 0.011660723015666008, 0.01131418440490961, 0.011153093539178371, 0.14381720125675201, 0.30251288414001465, 0.11358063668012619, 0.010995674878358841, 0.35876017808914185, 0.35168418288230896, 0.012546734884381294, 0.013154415413737297, 0.013863485306501389, 0.12390267848968506, 0.014963625930249691, 0.015502344816923141, 0.015837565064430237, 0.09951668977737427, 0.016130052506923676, 0.016346324235200882, 0.01626012846827507, 0.01629120111465454, 0.016089744865894318, 0.015889493748545647, 0.015602508559823036, 0.015257046557962894, 0.014967129565775394, 0.10496731102466583, 0.0962669774889946, 0.014002439565956593, 0.013654184527695179, 0.2765609920024872, 0.12396378070116043, 0.013616901822388172, 0.11048728972673416, 0.0135237080976367, 0.32446303963661194, 0.08356782793998718, 0.01436901930719614, 0.11115085333585739, 0.11803006380796432, 0.015082009136676788, 0.01524340733885765, 0.015320450067520142, 0.015318315476179123, 0.015262731350958347, 0.015128986909985542, 0.01487636286765337, 0.014648057520389557, 0.014341684058308601, 0.01402282901108265, 0.013666457496583462, 0.013285969384014606, 0.012899138033390045, 0.012513452209532261, 0.012103413231670856, 0.011696061119437218, 0.011289499700069427, 0.010880800895392895, 0.01048465259373188, 0.01010267436504364, 0.009730187244713306, 0.009351657703518867, 0.10134802013635635, 0.008711612783372402, 0.3341817557811737, 0.09867867082357407, 0.008666851557791233, 0.10076098144054413, 0.008848745375871658, 0.09801923483610153, 0.009033136069774628, 0.00907868705689907, 0.009095236659049988, 0.009074773639440536, 0.009032273665070534, 0.008924179710447788, 0.10028205811977386, 0.008768158964812756, 0.00868930947035551, 0.10799741744995117, 0.008504876866936684, 0.008446292020380497, 0.008313002064824104, 0.11541321128606796, 0.10373245924711227, 0.11769960075616837, 0.008107032626867294, 0.0080976951867342, 0.008078123442828655, 0.11510584503412247, 0.007957457564771175, 0.11021808534860611, 0.007940948940813541, 0.007900061085820198, 0.007840009406208992, 0.007766044232994318, 0.11150500178337097, 0.11855777353048325, 0.3206237852573395, 0.007950306870043278, 0.008204738609492779, 0.09687653928995132, 0.008704138919711113, 0.00887469481676817, 0.009040829725563526, 0.009112577885389328, 0.00915355235338211, 0.00918575655668974, 0.009137648157775402, 0.0091187609359622, 0.10588894784450531, 0.008982039988040924, 0.09844035655260086, 0.008877979591488838, 0.008825228549540043, 0.00871773436665535, 0.11444340646266937, 0.00858436431735754, 0.00853023398667574, 0.008388204500079155, 0.008249768055975437, 0.11794965714216232, 0.10504411906003952, 0.008021042682230473, 0.007969595491886139, 0.007869953289628029, 0.007751005701720715, 0.007648003753274679, 0.12107284367084503, 0.0074441093020141125, 0.007366545498371124, 0.00724940886721015, 0.10861854255199432, 0.007088063284754753, 0.006996653042733669, 0.006894547026604414, 0.00679862080141902, 0.006664836313575506, 0.11553878337144852, 0.00647464394569397, 0.006421858910471201, 0.006323196459561586, 0.006225524470210075, 0.0060990313068032265, 0.11733318120241165, 0.005946709308773279, 0.3756978213787079, 0.09803936630487442, 0.1005546823143959, 0.006642644293606281, 0.006921945605427027, 0.007106282748281956, 0.1233786791563034, 0.007459343411028385, 0.11843441426753998, 0.007811340503394604, 0.007928138598799706, 0.10545442998409271, 0.00817142054438591, 0.00822971947491169, 0.008291639387607574, 0.10366376489400864, 0.008392678573727608, 0.008382736705243587, 0.00836860854178667, 0.008307985961437225, 0.008266431279480457, 0.008179479278624058, 0.00805075466632843, 0.11462627351284027, 0.00785905122756958, 0.1078479215502739, 0.11173123121261597, 0.007754881866276264, 0.007699915207922459, 0.007662492338567972, 0.007582174614071846, 0.007501897867769003, 0.12074827402830124, 0.0073328479193151, 0.11236875504255295, 0.007243976928293705, 0.007189243100583553, 0.007143537048250437, 0.11027047783136368, 0.007031804416328669, 0.0980978012084961, 0.11935742199420929, 0.006999386940151453, 0.007007654290646315, 0.10626161098480225, 0.11421741545200348, 0.007110272068530321, 0.007106374949216843, 0.10213038325309753, 0.007153868675231934, 0.007195411715656519, 0.0071822525933384895, 0.11107999086380005, 0.007140365429222584, 0.00713689997792244, 0.00711261248216033, 0.007040604017674923, 0.0069800992496311665, 0.006879838183522224, 0.006795117165893316, 0.006694556679576635, 0.0065648979507386684, 0.0064553930424153805, 0.006322415079921484, 0.006196298636496067, 0.006056914106011391, 0.005923756863921881, 0.005790943745523691, 0.005655377171933651, 0.395565927028656, 0.005678493529558182, 0.0058277626521885395, 0.11066105961799622, 0.006109847221523523, 0.1177675649523735, 0.006376683246344328, 0.006490794476121664, 0.006584716960787773, 0.006656541023403406, 0.006679307669401169, 0.0066913943737745285, 0.006678769364953041, 0.11708095669746399, 0.006665852852165699, 0.006650725845247507, 0.006626364309340715, 0.006582084111869335, 0.00653827702626586, 0.006453908514231443, 0.37150388956069946, 0.006606592331081629, 0.11422862112522125, 0.00704111997038126, 0.007213934790343046, 0.007355592679232359, 0.0074768164195120335, 0.007537032011896372, 0.007579850032925606, 0.0076351892203092575, 0.00759349437430501, 0.007568064145743847, 0.0075106085278093815, 0.00743996212258935, 0.007341912481933832, 0.007247454021126032, 0.10523594170808792, 0.007067505270242691, 0.007006506901234388, 0.0068982443772256374, 0.10946176946163177, 0.006756090559065342, 0.10939204692840576, 0.006676225923001766, 0.0066048698499798775, 0.1057049110531807, 0.006544196046888828, 0.006529695354402065, 0.3670061528682709, 0.11745509505271912, 0.007009310647845268, 0.007252312730997801, 0.0074627585709095, 0.007639991119503975, 0.0077621350064873695, 0.00785899255424738, 0.10457398742437363, 0.007975441403687, 0.008026137948036194, 0.008032151497900486, 0.008027381263673306, 0.09979353100061417, 0.10456227511167526, 0.10478033870458603, 0.00815136544406414, 0.008123863488435745, 0.008114254102110863, 0.008097377605736256, 0.008099761791527271, 0.10561072081327438, 0.007973656058311462, 0.007934079505503178, 0.00785104464739561, 0.10197499394416809, 0.12572993338108063, 0.007711788639426231, 0.007648793049156666, 0.10850273817777634, 0.11570505052804947, 0.007655358407646418, 0.007624947465956211, 0.007608023937791586, 0.10473442822694778, 0.007604964077472687, 0.11280050873756409, 0.007574660703539848, 0.007548745721578598, 0.007497854996472597, 0.007478422950953245, 0.007363057229667902, 0.11430152505636215, 0.3526565134525299, 0.007513546850532293, 0.0077818394638597965, 0.1088089868426323, 0.10152873396873474, 0.10801789164543152, 0.10485758632421494, 0.11493522673845291, 0.108546182513237, 0.009443010203540325, 0.009664497338235378, 0.009824671782553196, 0.009940044954419136, 0.01003208290785551, 0.10222232341766357, 0.010116267949342728, 0.0101160304620862, 0.10295573621988297, 0.11235646158456802, 0.010149341076612473, 0.31852707266807556, 0.010461135767400265, 0.010703586041927338, 0.010904429480433464, 0.11127056926488876, 0.011227340437471867, 0.011326864361763, 0.011390690691769123, 0.011382638476788998, 0.10774236917495728, 0.1048416867852211, 0.011361167766153812, 0.011293316259980202, 0.011229891330003738, 0.10324753820896149, 0.011083679273724556, 0.010963676497340202, 0.10938124358654022, 0.09802349656820297, 0.010637728497385979, 0.010574337095022202, 0.1075562983751297, 0.010383708402514458, 0.010257575660943985, 0.010117282159626484, 0.009979844093322754, 0.009774534963071346, 0.009581498801708221, 0.10150270909070969, 0.31936123967170715, 0.11162771284580231, 0.009641085751354694, 0.009794394485652447, 0.009913215413689613, 0.009970552287995815, 0.00999361090362072, 0.11239372193813324, 0.11673465371131897, 0.010030493140220642, 0.01003558374941349, 0.01002895925194025, 0.009926428087055683, 0.009822712279856205, 0.00970035046339035, 0.00958395004272461, 0.09920500218868256, 0.11404605954885483, 0.10077287256717682, 0.0091946329921484, 0.009157105349004269, 0.009048812091350555, 0.008950889110565186, 0.008803118951618671, 0.008683876134455204, 0.008542198687791824, 0.008365759626030922, 0.008176878094673157, 0.008002553135156631, 0.007841171696782112, 0.10122215747833252, 0.10084032267332077, 0.0074053192511200905, 0.1063322126865387, 0.00729057751595974, 0.0071867527440190315, 0.11993171274662018, 0.007057827897369862, 0.007046298123896122, 0.006939735263586044, 0.006872571539133787, 0.006796323228627443, 0.006650703493505716, 0.10842158645391464, 0.00649997778236866, 0.006408968940377235, 0.11117416620254517, 0.006290927529335022, 0.006254887208342552, 0.006229879334568977, 0.006111747585237026, 0.006014978513121605, 0.09672247618436813, 0.34680241346359253, 0.00613983953371644, 0.10081871598958969, 0.006584336049854755, 0.0068328967317938805, 0.006949682720005512, 0.007017624564468861, 0.007164390292018652, 0.007157881278544664, 0.007229982875287533, 0.13411125540733337, 0.007249739952385426, 0.00728218350559473, 0.007335173897445202, 0.007228316273540258, 0.007196290884166956, 0.007171919569373131, 0.007093892432749271, 0.007029231637716293, 0.00692094536498189, 0.006784046534448862, 0.006644095294177532, 0.006550975143909454, 0.006514353211969137, 0.006269869860261679, 0.006180115044116974, 0.006036704406142235, 0.1306803971529007, 0.00589140085503459, 0.0057340650819242, 0.005607345141470432, 0.3469274342060089, 0.005769896320998669, 0.005859251134097576, 0.006015174090862274, 0.13306215405464172, 0.006204406265169382, 0.006294217426329851, 0.0063881599344313145, 0.006440861616283655, 0.006515374407172203, 0.006493416614830494, 0.11320770531892776, 0.006474186200648546, 0.006494607776403427, 0.006484305486083031, 0.006441598292440176, 0.12351728230714798, 0.006394603755325079, 0.006392686162143946, 0.006336581893265247, 0.10282585024833679, 0.006323819048702717, 0.006280933041125536, 0.006252152379602194, 0.006186037324368954, 0.006166932173073292, 0.006054368801414967, 0.005953487008810043, 0.005893071182072163, 0.10979034006595612, 0.005770424380898476, 0.005671280901879072, 0.36342930793762207, 0.005858988501131535, 0.0059913028962910175, 0.13071878254413605, 0.006383887957781553, 0.0064818658865988255, 0.11823490262031555, 0.006764545571058989, 0.0069692134857177734, 0.006911656819283962, 0.10065723955631256, 0.007047460414469242, 0.007123603485524654, 0.007216106168925762, 0.007166729774326086, 0.007152238395065069, 0.0071234977804124355, 0.007068575359880924, 0.007057103328406811, 0.1018853560090065, 0.006923064589500427, 0.006928818300366402, 0.006755766924470663, 0.0066865552216768265, 0.1095685362815857, 0.006609788164496422, 0.11843915283679962, 0.11069898307323456, 0.006508849561214447, 0.006478553172200918, 0.12196636199951172, 0.006495374720543623, 0.006545822601765394, 0.0064784386195242405, 0.006520669907331467, 0.006412433460354805, 0.33453330397605896, 0.006656907964497805, 0.006803115829825401, 0.006918419152498245, 0.007058710791170597, 0.0976913645863533, 0.10761969536542892, 0.00745972478762269, 0.11380057036876678, 0.007564394734799862, 0.007740864064544439, 0.11820168793201447, 0.007822614163160324, 0.00796249508857727, 0.007898506708443165, 0.007928322069346905, 0.007895936258137226, 0.007963833399116993, 0.007826688699424267, 0.10506834834814072, 0.1072482019662857, 0.09387710690498352, 0.007745677139610052, 0.0077687897719442844, 0.0076919011771678925, 0.007759875152260065, 0.007591721136122942, 0.007549342233687639, 0.007607087027281523, 0.007365235593169928, 0.007193675730377436, 0.007169239688664675, 0.12323019653558731, 0.11895602941513062, 0.006980505771934986, 0.13292904198169708, 0.006784051191061735, 0.12366463989019394, 0.006932431366294622, 0.006868970580399036, 0.006849600933492184, 0.006788316182792187, 0.006701407488435507, 0.006693543866276741, 0.006568925455212593, 0.006531488616019487, 0.12432707101106644, 0.006341678090393543, 0.006229651626199484, 0.006245673168450594, 0.006094083655625582, 0.00604481203481555, 0.005950515158474445, 0.12544505298137665, 0.1148674264550209, 0.005771760828793049, 0.10628887265920639, 0.1048927754163742, 0.005827047862112522, 0.005832325667142868, 0.005889436695724726, 0.005861719138920307, 0.005887942388653755, 0.13251572847366333, 0.10923662781715393, 0.11975450068712234, 0.005948464386165142, 0.005998175125569105, 0.11690806597471237, 0.006102333776652813, 0.0061772847548127174, 0.006175667978823185, 0.006178086157888174, 0.36101260781288147, 0.006445577833801508, 0.006676068529486656, 0.11892557889223099, 0.007080580107867718, 0.007248826790601015, 0.36035603284835815, 0.007823757827281952, 0.008206776343286037, 0.008536403998732567, 0.008794629015028477, 0.009046007879078388, 0.00924921128898859, 0.00934220477938652, 0.009454131126403809, 0.009481146931648254, 0.009537134319543839, 0.00950666330754757, 0.009436571970582008, 0.009413151070475578, 0.009269033558666706, 0.00918601918965578, 0.009029990062117577, 0.10206566005945206, 0.008780475705862045, 0.008666129782795906, 0.008593060076236725, 0.008401678875088692, 0.00824911892414093, 0.008090288378298283, 0.007937162183225155, 0.007749907206743956, 0.007593672256916761, 0.007443196140229702, 0.00724455900490284, 0.10296449810266495, 0.00694317976012826, 0.0068232095800340176, 0.006689096800982952, 0.10354907065629959, 0.006467589642852545, 0.09616168588399887, 0.006333462428301573, 0.006256630178540945, 0.00619775103405118, 0.006101415492594242, 0.006044534966349602, 0.005935907829552889, 0.11894895136356354, 0.0058008176274597645, 0.1145540326833725, 0.005714419297873974, 0.005665587726980448, 0.0996667891740799, 0.11502298712730408, 0.005665098782628775, 0.0056856353767216206, 0.005699813365936279, 0.005657943896949291, 0.005636648740619421, 0.10300647467374802, 0.005615300498902798, 0.1116810068488121, 0.0056156315840780735, 0.005622572731226683, 0.005612524226307869, 0.005585276987403631, 0.00556135643273592, 0.005527665838599205, 0.1029873862862587, 0.12422690540552139, 0.10391128808259964, 0.005535957869142294, 0.005596233531832695, 0.005605865735560656, 0.38210374116897583, 0.005894528701901436, 0.11668405681848526, 0.00635913060978055, 0.006581574212759733, 0.0067906160838902, 0.006920639891177416, 0.007061602082103491, 0.007166080176830292, 0.00721046281978488, 0.0072594513185322285, 0.007280395831912756, 0.007232068572193384, 0.007206477224826813, 0.11488725244998932, 0.0071778553538024426, 0.007158874999731779, 0.007101954892277718, 0.0070633250288665295, 0.12633933126926422, 0.006941493134945631, 0.0069099669344723225, 0.0068592787720263, 0.006776477675884962, 0.00670671695843339, 0.0066147176548838615, 0.006554674822837114, 0.0064168707467615604, 0.006312482059001923, 0.006206808611750603, 0.10782875865697861, 0.0060121119022369385, 0.005940339528024197, 0.005875709466636181, 0.10787927359342575, 0.005726444534957409, 0.0056656901724636555, 0.10987641662359238, 0.00559866102412343, 0.005550484638661146, 0.11354804784059525, 0.005514038726687431, 0.12444131821393967, 0.005543336737900972, 0.0055312588810920715, 0.005531704518944025, 0.005511421710252762, 0.005488847382366657, 0.005442880094051361, 0.005393181461840868, 0.00536374794319272, 0.0052899024449288845, 0.10944949835538864, 0.005198286380618811, 0.005143348593264818, 0.10532243549823761, 0.0051115588285028934, 0.1255389004945755, 0.1171695739030838, 0.00521169463172555, 0.0052320449613034725, 0.005231647752225399, 0.005230350885540247, 0.0052264295518398285, 0.005229577887803316, 0.005198673345148563, 0.005151838064193726, 0.005118517205119133, 0.00505357701331377, 0.005003101192414761, 0.004955681040883064, 0.00488243019208312, 0.12201011180877686, 0.004778228234499693, 0.0047467537224292755, 0.004764823243021965, 0.3753792941570282, 0.11452843248844147, 0.005134932231158018, 0.10051189363002777, 0.0054569547064602375, 0.09309030324220657, 0.0058154272846877575, 0.006047005765140057, 0.31239575147628784]\n",
            "Val loss 0.044494396547102986\n",
            "Val auc roc 0.4968683817866768\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aecfae2449394d1c8b45d38db9068e7b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0443\n",
            "Train Losses : [0.10435203462839127, 0.0981622040271759, 0.007276337593793869, 0.007757817395031452, 0.12571939826011658, 0.34441566467285156, 0.008984141051769257, 0.36676225066185, 0.010159862227737904, 0.3438590168952942, 0.1371879279613495, 0.012996844947338104, 0.08958804607391357, 0.014788370579481125, 0.015362245962023735, 0.015783417969942093, 0.016367750242352486, 0.01704317145049572, 0.017541732639074326, 0.26038578152656555, 0.01791013777256012, 0.28323283791542053, 0.019071757793426514, 0.019315866753458977, 0.019893638789653778, 0.10106680542230606, 0.020327609032392502, 0.10613222420215607, 0.020598653703927994, 0.09728226065635681, 0.10001616925001144, 0.02059229277074337, 0.02058141678571701, 0.102364681661129, 0.020290445536375046, 0.02001217007637024, 0.2439049929380417, 0.10492298007011414, 0.112746462225914, 0.09634421020746231, 0.01998015120625496, 0.019879015162587166, 0.01968489959836006, 0.019475487992167473, 0.09388727694749832, 0.01904732547700405, 0.11069463193416595, 0.018503429368138313, 0.018138611689209938, 0.27316781878471375, 0.017840148881077766, 0.017750302329659462, 0.2687910795211792, 0.017759107053279877, 0.017912767827510834, 0.017889106646180153, 0.01783798821270466, 0.017701642587780952, 0.01755836047232151, 0.01738031394779682, 0.017090722918510437, 0.016769764944911003, 0.11173266172409058, 0.016186518594622612, 0.015830421820282936, 0.096437007188797, 0.015248781070113182, 0.014901969581842422, 0.01466099638491869, 0.0142677528783679, 0.10270945727825165, 0.01360099297016859, 0.013267750851809978, 0.012977907434105873, 0.012645957060158253, 0.10178222507238388, 0.012020478956401348, 0.011807987466454506, 0.011493202298879623, 0.011169317178428173, 0.010871147736907005, 0.10735971480607986, 0.010359533131122589, 0.010130716487765312, 0.009904400445520878, 0.10138619691133499, 0.009436934255063534, 0.009239275939762592, 0.10901491343975067, 0.008911611512303352, 0.008744613267481327, 0.008632362820208073, 0.3216591775417328, 0.008551269769668579, 0.008580565452575684, 0.008647854439914227, 0.11798038333654404, 0.008721379563212395, 0.008741604164242744, 0.008739395998418331, 0.008684564381837845, 0.00864940695464611, 0.008548552170395851, 0.008468902669847012, 0.10141546279191971, 0.10802958160638809, 0.008269524201750755, 0.10977011919021606, 0.008189180865883827, 0.10573267936706543, 0.008147393353283405, 0.008178848773241043, 0.00808658916503191, 0.11020465195178986, 0.007999943569302559, 0.007997888140380383, 0.11173728853464127, 0.00793776661157608, 0.10957300662994385, 0.007876480929553509, 0.0077852122485637665, 0.007793714292347431, 0.007695529144257307, 0.1095111295580864, 0.10168209671974182, 0.007547639310359955, 0.007538155186921358, 0.11704105138778687, 0.11373769491910934, 0.0074999574571847916, 0.007498624734580517, 0.007506298832595348, 0.007452696096152067, 0.007388516794890165, 0.0073386733420193195, 0.007247798144817352, 0.10208140313625336, 0.007118389010429382, 0.00705000851303339, 0.007003506179898977, 0.006917910650372505, 0.006873426027595997, 0.006808391772210598, 0.006635804194957018, 0.006598085165023804, 0.006395490374416113, 0.006310599856078625, 0.11363324522972107, 0.3491044044494629, 0.006275806576013565, 0.0064215222373604774, 0.006594102364033461, 0.11849015951156616, 0.006750951986759901, 0.0068478877656161785, 0.006889421492815018, 0.007111378479748964, 0.006987445056438446, 0.006961623206734657, 0.006945998407900333, 0.0069395615719258785, 0.006874679587781429, 0.10611456632614136, 0.006777534261345863, 0.12143830955028534, 0.11046577244997025, 0.006770086940377951, 0.0067772469483315945, 0.006781136151403189, 0.10342907905578613, 0.006821436807513237, 0.006764607969671488, 0.0067848628386855125, 0.0067191096022725105, 0.006674344651401043, 0.006605732254683971, 0.0065424093045294285, 0.0064554596319794655, 0.006405366584658623, 0.00628606928512454, 0.006272106897085905, 0.006095086224377155, 0.005985305644571781, 0.005905659403651953, 0.005820740945637226, 0.11713434010744095, 0.005631639156490564, 0.1301603615283966, 0.005551628302782774, 0.12110432982444763, 0.005494209472090006, 0.00551284896209836, 0.005509958602488041, 0.005464745685458183, 0.005405779462307692, 0.005348799284547567, 0.0053092255257070065, 0.12496015429496765, 0.10603556036949158, 0.005225573666393757, 0.005229076836258173, 0.005244726315140724, 0.005291238892823458, 0.0051885987631976604, 0.005151033867150545, 0.005109245888888836, 0.005091873928904533, 0.0050225937739014626, 0.004931892268359661, 0.0048925625160336494, 0.004806928802281618, 0.004758043680340052, 0.004691289737820625, 0.004659655969589949, 0.004552257712930441, 0.004459140356630087, 0.1290779560804367, 0.004352706950157881, 0.004342072177678347, 0.004287680145353079, 0.004243004135787487, 0.004204197321087122, 0.004181982483714819, 0.004095695912837982, 0.00404069758951664, 0.003996315877884626, 0.003940511494874954, 0.12414770573377609, 0.0039004762656986713, 0.0038493380416184664, 0.0038014212623238564, 0.4315636456012726, 0.12180592119693756, 0.0041444445960223675, 0.004336469806730747, 0.004482480231672525, 0.11648829281330109, 0.004776617977768183, 0.004914411809295416, 0.005023682955652475, 0.00511455861851573, 0.005191736854612827, 0.005263268016278744, 0.12121647596359253, 0.005356970243155956, 0.11023323982954025, 0.0055043380707502365, 0.005537902005016804, 0.005596102215349674, 0.005625106394290924, 0.005658306181430817, 0.005625791382044554, 0.005617334507405758, 0.005598897114396095, 0.005555585026741028, 0.005527082365006208, 0.10843541473150253, 0.005474671255797148, 0.005439222324639559, 0.005439972039312124, 0.005398636683821678, 0.1218106672167778, 0.0053152889013290405, 0.005329721141606569, 0.0052839843556284904, 0.0052383048459887505, 0.005208055954426527, 0.005193348042666912, 0.0050882031209766865, 0.005063426680862904, 0.0049618203192949295, 0.0049021379090845585, 0.004831008613109589, 0.004800326656550169, 0.004716698545962572, 0.0046332827769219875, 0.13584360480308533, 0.004539477173238993, 0.004493915010243654, 0.42910224199295044, 0.004643550608307123, 0.004786219447851181, 0.004912808071821928, 0.3705054223537445, 0.005322296638041735, 0.005607586354017258, 0.1137656718492508, 0.006141126621514559, 0.006364618893712759, 0.006544698961079121, 0.11543703079223633, 0.006908677518367767, 0.007046745158731937, 0.007168304640799761, 0.10933541506528854, 0.007380197290331125, 0.1158578023314476, 0.09617423266172409, 0.1148255243897438, 0.007836884818971157, 0.007955831475555897, 0.008095618337392807, 0.09982258826494217, 0.11190806329250336, 0.00819618534296751, 0.09446556866168976, 0.008337445557117462, 0.10372400283813477, 0.008453421294689178, 0.008556127548217773, 0.1079176515340805, 0.31917399168014526, 0.008870279416441917, 0.009122342802584171, 0.09567349404096603, 0.00955236330628395, 0.09296055883169174, 0.009737333282828331, 0.009845834225416183, 0.01003182865679264, 0.010105983354151249, 0.0101566631346941, 0.010038645006716251, 0.009982526302337646, 0.11636807024478912, 0.10959036648273468, 0.009893257170915604, 0.10244348645210266, 0.12094037979841232, 0.00992473866790533, 0.009942212142050266, 0.009758991189301014, 0.009726078249514103, 0.009761949069797993, 0.00959040317684412, 0.009516960941255093, 0.009338702075183392, 0.009194782935082912, 0.00900049414485693, 0.08234849572181702, 0.008763359859585762, 0.10590507090091705, 0.008651047013700008, 0.008518796414136887, 0.008388728834688663, 0.008249253034591675, 0.008139415644109249, 0.008136480115354061, 0.1101817712187767, 0.007750056684017181, 0.12851987779140472, 0.007579027209430933, 0.00753004290163517, 0.007374319713562727, 0.0900772362947464, 0.0072962394915521145, 0.00726764090359211, 0.00721677066758275, 0.007027952000498772, 0.0987461656332016, 0.00688584428280592, 0.006842694245278835, 0.006778777111321688, 0.0066935718059539795, 0.006557777523994446, 0.006522927898913622, 0.1121942475438118, 0.006423511076718569, 0.1086672693490982, 0.7726139426231384, 0.006822051480412483, 0.007062477059662342, 0.09390022605657578, 0.12729544937610626, 0.008134309202432632, 0.008518008515238762, 0.10128109157085419, 0.0090412562713027, 0.10669919103384018, 0.009424067102372646, 0.00973272230476141, 0.00995547603815794, 0.10576608031988144, 0.010103034786880016, 0.09661006182432175, 0.08466746658086777, 0.010375717654824257, 0.010445376858115196, 0.01043999008834362, 0.01045201625674963, 0.010437371209263802, 0.1181965321302414, 0.010332728736102581, 0.01037407573312521, 0.01017529983073473, 0.010117290541529655, 0.010181535966694355, 0.12665575742721558, 0.009774448350071907, 0.009703675284981728, 0.009611020796000957, 0.009572058916091919, 0.11593849956989288, 0.00920113269239664, 0.09535995125770569, 0.009085729718208313, 0.00896687526255846, 0.008918014355003834, 0.00877701211720705, 0.10784496366977692, 0.10712053626775742, 0.008495665155351162, 0.008437030017375946, 0.00835647713392973, 0.008292697370052338, 0.008187427185475826, 0.10578029602766037, 0.09314831346273422, 0.007982531562447548, 0.007905088365077972, 0.007767432369291782, 0.007849374786019325, 0.007586520165205002, 0.09015856683254242, 0.007637429516762495, 0.09694115072488785, 0.12374438345432281, 0.0073652369901537895, 0.007282507140189409, 0.007309297099709511, 0.007374017033725977, 0.0072015332989394665, 0.007110426668077707, 0.1437038779258728, 0.007042295765131712, 0.006896667648106813, 0.1052183136343956, 0.006831674370914698, 0.006839676294475794, 0.0068641286343336105, 0.11137121170759201, 0.09628470987081528, 0.006634985096752644, 0.40474680066108704, 0.006937846541404724, 0.0071620037779212, 0.39912986755371094, 0.00768287293612957, 0.09790825843811035, 0.008280470035970211, 0.008518341928720474, 0.008894393220543861, 0.009078544564545155, 0.11323636770248413, 0.3383525311946869, 0.10799622535705566, 0.010089294984936714, 0.010409964248538017, 0.010687991045415401, 0.08633638918399811, 0.011143608018755913, 0.011302266269922256, 0.011452116072177887, 0.11534538865089417, 0.09769117087125778, 0.11943836510181427, 0.011844263412058353, 0.10547969490289688, 0.09910762310028076, 0.012143054977059364, 0.012042437680065632, 0.01206835638731718, 0.10256355255842209, 0.011995721608400345, 0.011967074126005173, 0.012095156125724316, 0.011780445463955402, 0.09744236618280411, 0.011714611202478409, 0.09065186232328415, 0.011410005390644073, 0.011287928558886051, 0.6268073320388794, 0.08641447871923447, 0.012136321514844894, 0.012527215294539928, 0.01287874486297369, 0.10533688962459564, 0.013214939273893833, 0.1056935265660286, 0.08614538609981537, 0.013716853223741055, 0.013896116986870766, 0.013933309353888035, 0.11106464266777039, 0.01397672202438116, 0.10378290712833405, 0.10919342190027237, 0.013934352435171604, 0.013916614465415478, 0.013918234966695309, 0.013777624815702438, 0.013657434843480587, 0.013617818243801594, 0.013324478641152382, 0.013137358240783215, 0.012900542467832565, 0.10373794287443161, 0.0125771164894104, 0.012638361193239689, 0.11265344172716141, 0.011969557031989098, 0.012036764994263649, 0.011660931631922722, 0.3219745457172394, 0.011647502891719341, 0.011601949110627174, 0.011536448262631893, 0.011429892852902412, 0.09207355976104736, 0.011474695056676865, 0.011365470476448536, 0.09996414929628372, 0.011287188157439232, 0.011124247685074806, 0.10574573278427124, 0.010942083783447742, 0.01084312703460455, 0.010844684205949306, 0.12420119345188141, 0.010551873594522476, 0.010425075888633728, 0.010378536768257618, 0.010131362825632095, 0.11088579148054123, 0.009930879808962345, 0.009747550822794437, 0.009674680419266224, 0.009493598714470863, 0.009363570250570774, 0.009201203472912312, 0.009015204384922981, 0.008832304738461971, 0.008697925135493279, 0.008489337749779224, 0.008382623083889484, 0.008209416642785072, 0.008027872070670128, 0.007963458076119423, 0.007705818861722946, 0.007529622875154018, 0.007342202588915825, 0.007206404581665993, 0.006991418078541756, 0.006847458425909281, 0.006754830479621887, 0.0065227379091084, 0.00635369960218668, 0.006239305716007948, 0.006063720677047968, 0.00591284641996026, 0.0057374839670956135, 0.09846336394548416, 0.005488983821123838, 0.005452325101941824, 0.005321713164448738, 0.39325150847435, 0.005408373195677996, 0.12236826121807098, 0.005603982135653496, 0.005690018180757761, 0.005824167747050524, 0.005852542817592621, 0.005944748874753714, 0.0959518551826477, 0.006034737918525934, 0.006041685584932566, 0.006150857545435429, 0.006096092984080315, 0.006111053749918938, 0.0060798656195402145, 0.006051570642739534, 0.11626647412776947, 0.36262431740760803, 0.0061941915191709995, 0.006425685249269009, 0.006521843373775482, 0.1013106182217598, 0.006765197962522507, 0.11091501265764236, 0.007055961061269045, 0.11869055032730103, 0.007267460692673922, 0.11811040341854095, 0.007425068877637386, 0.007509760558605194, 0.007533660624176264, 0.007589131593704224, 0.007590719033032656, 0.007586290128529072, 0.09988784790039062, 0.11320478469133377, 0.007659445516765118, 0.3616271913051605, 0.09559247642755508, 0.008174225687980652, 0.1270761340856552, 0.12474196404218674, 0.008699852041900158, 0.11553164571523666, 0.009048143401741982, 0.0091850021854043, 0.10179747641086578, 0.00940423458814621, 0.009538760408759117, 0.009511294774711132, 0.009523837827146053, 0.31162962317466736, 0.009809226728975773, 0.009950640611350536, 0.010042384266853333, 0.010175525210797787, 0.010199721902608871, 0.3188464343547821, 0.010535276494920254, 0.010716864839196205, 0.01087657455354929, 0.10631032288074493, 0.011142168194055557, 0.011218160390853882, 0.1193818673491478, 0.12158109247684479, 0.28696590662002563, 0.01160121988505125, 0.011944357305765152, 0.012056796811521053, 0.012126327492296696, 0.012407380156219006, 0.01232114713639021, 0.01234697550535202, 0.01231160294264555, 0.012291261926293373, 0.10812235623598099, 0.12638995051383972, 0.01208890788257122, 0.012063154019415379, 0.011934371665120125, 0.011865580454468727, 0.10442765802145004, 0.1130031943321228, 0.30573514103889465, 0.011809099465608597, 0.011799406260251999, 0.01192306075245142, 0.26931023597717285, 0.012198423035442829, 0.012411111034452915, 0.012516198679804802, 0.012718466110527515, 0.01267760805785656, 0.012658045627176762, 0.012600730173289776, 0.012733145616948605, 0.012499912641942501, 0.012524508871138096, 0.10711698979139328, 0.012286514043807983, 0.012208142317831516, 0.01195999700576067, 0.1222226470708847, 0.011768692173063755, 0.01166762225329876, 0.11557669192552567, 0.10046061873435974, 0.011246779002249241, 0.011194667778909206, 0.011051005683839321, 0.01097599696367979, 0.01098080724477768, 0.010595876723527908, 0.09984954446554184, 0.11107426881790161, 0.08842852711677551, 0.010196937248110771, 0.11729177832603455, 0.11505655944347382, 0.09954354166984558, 0.010069380514323711, 0.009993432089686394, 0.12320831418037415, 0.11163990199565887, 0.11006122827529907, 0.009906915947794914, 0.009886935353279114, 0.009911950677633286, 0.009815040975809097, 0.009752552956342697, 0.009638114832341671, 0.009543844498693943, 0.009530602023005486, 0.11570530384778976, 0.009241938591003418, 0.009143942035734653, 0.009046786464750767, 0.11873625963926315, 0.008902736008167267, 0.008817787282168865, 0.09719005227088928, 0.008683276362717152, 0.008542296476662159, 0.008464240469038486, 0.008369546383619308, 0.008257943205535412, 0.11987096071243286, 0.008070580661296844, 0.10533157736063004, 0.10324125736951828, 0.007958421483635902, 0.11727068573236465, 0.007908742874860764, 0.11584922671318054, 0.007847491651773453, 0.007839835248887539, 0.007776881102472544, 0.12036125361919403, 0.007752854377031326, 0.00779525563120842, 0.0076729729771614075, 0.00766341807320714, 0.007598672527819872, 0.007549123372882605, 0.007432727608829737, 0.007358801085501909, 0.007271816488355398, 0.11631545424461365, 0.00709416251629591, 0.007080108858644962, 0.00695155980065465, 0.10988563299179077, 0.006872766651213169, 0.006888549774885178, 0.006715899333357811, 0.006663509178906679, 0.3659609258174896, 0.00671596173197031, 0.00681167421862483, 0.006884241010993719, 0.006970892660319805, 0.12197805196046829, 0.007075774949043989, 0.0071534342132508755, 0.007102675270289183, 0.007105778902769089, 0.007173205725848675, 0.007117187604308128, 0.09993819892406464, 0.00706785824149847, 0.007067044731229544, 0.12614870071411133, 0.007047183811664581, 0.007062988355755806, 0.007003929000347853, 0.00696652103215456, 0.12675741314888, 0.006912540644407272, 0.006897428538650274, 0.006863008718937635, 0.006781656760722399, 0.006742897443473339, 0.00668301759287715, 0.12488984316587448, 0.00655886298045516, 0.006541271228343248, 0.11633360385894775, 0.11259875446557999, 0.33951112627983093, 0.006689573638141155, 0.006804673001170158, 0.0069356998428702354, 0.00702527305111289, 0.007138651330024004, 0.007209374103695154, 0.6903221607208252, 0.007659283932298422, 0.00807018019258976, 0.008417784236371517, 0.09978768229484558, 0.009046083316206932, 0.009336808696389198, 0.009568494744598866, 0.009753497317433357, 0.009917162358760834, 0.1081652119755745, 0.010205816477537155, 0.01026124320924282, 0.10295940935611725, 0.010409023612737656, 0.010473005473613739, 0.010457438416779041, 0.010460340417921543, 0.01045637484639883, 0.09563011676073074, 0.010359273292124271, 0.010296206921339035, 0.010243776254355907, 0.10947629809379578, 0.010157461278140545, 0.010056442581117153, 0.010001853108406067, 0.00985938124358654, 0.10996194183826447, 0.009694376960396767, 0.009646973572671413, 0.11698082834482193, 0.1152428463101387, 0.00948507059365511, 0.009412919171154499, 0.009282511658966541, 0.11303272098302841, 0.009149361401796341, 0.009080074727535248, 0.009008633904159069, 0.008958308957517147, 0.10493811219930649, 0.00879684742540121, 0.00869013462215662, 0.09592211246490479, 0.008583658374845982, 0.008489379659295082, 0.008417082950472832, 0.11684639751911163, 0.10668792575597763, 0.00823038537055254, 0.3471599817276001, 0.008373722434043884, 0.008477131836116314, 0.6507596969604492, 0.09927387535572052, 0.009519356302917004, 0.1001756489276886, 0.010320707224309444, 0.010697556659579277, 0.011049147695302963, 0.11294828355312347, 0.10736458748579025, 0.011721014976501465, 0.011953884735703468, 0.012157232500612736, 0.012278696522116661, 0.012300975620746613, 0.11610503494739532, 0.01245793141424656, 0.012393688783049583, 0.012400585226714611, 0.09981755167245865, 0.11821062117815018, 0.10308439284563065, 0.012354174628853798, 0.11810363829135895, 0.01229121070355177, 0.09993955492973328, 0.1044793501496315, 0.012269600294530392, 0.10996533930301666, 0.012235188856720924, 0.012163583189249039, 0.012140853330492973, 0.012044819071888924, 0.011928449384868145, 0.011868689209222794, 0.09643954038619995, 0.10757266730070114, 0.28254517912864685, 0.011671675369143486, 0.011719156056642532, 0.11069424450397491, 0.011811506934463978, 0.011863521300256252, 0.01183691993355751, 0.011880028992891312, 0.011788555420935154, 0.01170263346284628, 0.011638524010777473, 0.01149233803153038, 0.011413739994168282, 0.11320257931947708, 0.11484863609075546, 0.011109278537333012, 0.011039519682526588, 0.01092451624572277, 0.010822090320289135, 0.1108524426817894, 0.10109758377075195, 0.010592659935355186, 0.10931222140789032, 0.010364890098571777, 0.11327313631772995, 0.010262588039040565, 0.010211546905338764, 0.10802586376667023, 0.11861559003591537, 0.10439188778400421, 0.010040851309895515, 0.009987111203372478, 0.009956922382116318, 0.00986297708004713, 0.009809518232941628, 0.009749331511557102, 0.009637049399316311, 0.009534257464110851, 0.009424928575754166, 0.009305519051849842, 0.009189981035888195, 0.009047113358974457, 0.008924754336476326, 0.008802871219813824, 0.008664640597999096, 0.008529632352292538, 0.008398697711527348, 0.11706118285655975, 0.3492722511291504, 0.008254206739366055, 0.008326539769768715, 0.008385326713323593, 0.117633156478405, 0.008447571657598019, 0.00843812245875597, 0.00844697654247284, 0.10748740285634995, 0.6565967798233032, 0.008833267726004124, 0.10832028090953827, 0.009532500989735126, 0.11188475787639618, 0.010118195787072182, 0.01033245399594307, 0.010597167536616325, 0.010718870908021927, 0.01085883378982544, 0.010942003689706326, 0.01099028903990984, 0.011084175668656826, 0.011062169447541237, 0.011098179034888744, 0.011051132343709469, 0.01099244225770235, 0.01090635359287262, 0.11121585220098495, 0.09042012691497803, 0.010826191864907742, 0.010714547708630562, 0.01070716604590416, 0.010554329492151737, 0.010480565018951893, 0.010374417528510094, 0.10762562602758408, 0.01021280512213707, 0.010108808055520058, 0.10822678357362747, 0.09934282302856445, 0.1084025576710701, 0.009825030341744423, 0.009773348458111286, 0.009714152663946152, 0.009639755822718143, 0.009590045548975468, 0.009487154893577099, 0.009373676963150501, 0.10221898555755615, 0.009219550527632236, 0.009140348061919212, 0.009029637090861797, 0.00897603016346693, 0.008834405802190304, 0.008795220404863358, 0.008632272481918335, 0.00858970358967781, 0.09353756904602051, 0.00834004394710064, 0.008231625892221928, 0.008131826296448708, 0.008078283630311489, 0.007960421033203602, 0.007822892628610134, 0.007750101387500763, 0.11096225678920746, 0.0075629944913089275, 0.00747301522642374, 0.007394226733595133, 0.10174855589866638, 0.007247333414852619, 0.007185723166912794, 0.007122233044356108, 0.007050325628370047, 0.006969239097088575, 0.006900278385728598, 0.00680320430546999, 0.0067330473102629185, 0.006640643812716007, 0.0065793003886938095, 0.006472212262451649, 0.006404159124940634, 0.0063301497139036655, 0.00629674457013607, 0.006120763253420591, 0.38209956884384155, 0.006108938716351986, 0.006201640237122774, 0.006220139097422361, 0.11157412827014923, 0.11204000562429428, 0.006403268780559301, 0.10895808786153793, 0.006506134290248156, 0.10714539885520935, 0.006626870017498732, 0.006653353571891785, 0.006677074823528528, 0.006706484593451023, 0.006730665452778339, 0.1202203631401062, 0.00674207229167223, 0.006742993835359812, 0.006731872912496328, 0.006715802941471338, 0.11193162947893143, 0.11068973690271378, 0.006699202116578817, 0.3809359073638916, 0.11141687631607056, 0.11801757663488388, 0.11243066191673279, 0.007376641966402531, 0.11156021803617477, 0.0076620737090706825, 0.007808275055140257, 0.007903167977929115, 0.11063922941684723, 0.008065094240009785, 0.11178401857614517, 0.008241090923547745, 0.008281244896352291, 0.008328725583851337, 0.3442988097667694, 0.008515387773513794, 0.008676535449922085, 0.00879109837114811, 0.008915609680116177, 0.008961314335465431, 0.009044555947184563, 0.3276691436767578, 0.009218413382768631, 0.0093768872320652, 0.09858234971761703, 0.009652392938733101, 0.00975651852786541, 0.009797417558729649, 0.009871367365121841, 0.10791771113872528, 0.009921755641698837, 0.009930047206580639, 0.11973153799772263, 0.32938212156295776, 0.10724721848964691, 0.010278359986841679, 0.010452370159327984, 0.010546661913394928, 0.010629488155245781, 0.010747095569968224, 0.010708493180572987, 0.10911570489406586, 0.010734738782048225, 0.010714970529079437, 0.010699245147407055, 0.010687408037483692, 0.010615763254463673, 0.010553068481385708, 0.010471411049365997, 0.010391808114945889, 0.010296336375176907, 0.11216936260461807, 0.010142016224563122, 0.010042778216302395, 0.10154395550489426, 0.009885122068226337, 0.009802709333598614, 0.009697281755506992, 0.009596644900739193, 0.10375923663377762, 0.108348049223423, 0.1092032939195633, 0.009378391318023205, 0.009290609508752823, 0.10179752856492996, 0.009197928942739964, 0.009159254841506481, 0.00909709557890892, 0.009039733558893204, 0.008978215046226978, 0.008899850770831108, 0.008791680447757244, 0.10010532289743423, 0.008656863123178482, 0.008569716475903988, 0.008518286980688572, 0.10524100810289383, 0.09932424128055573, 0.11226161569356918, 0.008297061547636986, 0.10295233875513077, 0.0082587581127882, 0.008232231251895428, 0.008205177262425423, 0.10640162229537964, 0.008152222260832787, 0.00812386255711317, 0.11169733852148056, 0.008039961569011211, 0.00800909474492073, 0.007968299090862274, 0.10825522989034653, 0.007906048558652401, 0.00787243153899908, 0.007836292497813702, 0.10577721893787384, 0.007748843636363745, 0.0077028945088386536, 0.007659133989363909, 0.09513664245605469, 0.007584654726088047, 0.11716616153717041, 0.007538020145148039, 0.007505231536924839, 0.007454962469637394, 0.007411935832351446, 0.0073641082271933556, 0.11219031363725662, 0.007292680907994509, 0.1109904870390892, 0.007240446750074625, 0.007220970466732979, 0.007171492092311382, 0.11675362288951874, 0.007112755905836821, 0.007104780059307814, 0.10831040889024734, 0.00703350268304348, 0.11000483483076096, 0.00701508205384016, 0.12055034935474396, 0.12079303711652756, 0.0070509943179786205, 0.36233851313591003, 0.007203308865427971, 0.007341545075178146, 0.007457474246621132, 0.11544739454984665, 0.007666586432605982, 0.007721332833170891, 0.007770152296870947, 0.00784075353294611, 0.007857724092900753, 0.09893954545259476, 0.007881917990744114, 0.007898421958088875, 0.007903287187218666, 0.007895209826529026, 0.007861416786909103, 0.007846892811357975, 0.10376375168561935, 0.007768172770738602, 0.007748740259557962, 0.007726263254880905, 0.007662861607968807, 0.007615471258759499, 0.007573486305773258, 0.10360371321439743, 0.007446889765560627, 0.007412564009428024, 0.007347023580223322, 0.007302429527044296, 0.007238973397761583, 0.007170428987592459, 0.007111721206456423, 0.007030358072370291, 0.006957185920327902, 0.006879216525703669, 0.1193014457821846, 0.006765422876924276, 0.006703010760247707, 0.10337080806493759, 0.006642268039286137, 0.11303804814815521, 0.006553501356393099, 0.006536710076034069, 0.006496524903923273, 0.006457761395722628, 0.006420515943318605, 0.006376942619681358, 0.006322205066680908, 0.006287203636020422, 0.11615219712257385, 0.11735222488641739, 0.006184072233736515, 0.006169169209897518, 0.006155604496598244, 0.0061133368872106075, 0.11915561556816101, 0.0060774413868784904, 0.0060555050149559975, 0.006022502202540636, 0.006005522795021534, 0.005960649810731411, 0.11022376269102097, 0.005915528628975153, 0.10646934062242508, 0.005886460188776255, 0.11079312115907669, 0.11820715665817261, 0.005904599092900753, 0.005927701480686665, 0.11324869841337204, 0.10303841531276703, 0.005985874682664871, 0.00602356530725956, 0.006030223798006773, 0.006046620197594166, 0.0060554747469723225, 0.00604384345933795, 0.006017137784510851, 0.006002009846270084, 0.10907107591629028, 0.005968285258859396, 0.005967287812381983, 0.005932491272687912, 0.005900528747588396, 0.0058809188194572926, 0.1110253781080246, 0.11118797957897186, 0.005836116150021553, 0.0058253672905266285, 0.005846961867064238, 0.005799689330160618, 0.12190890312194824, 0.005780961364507675, 0.005777350161224604, 0.38194867968559265, 0.005886996630579233, 0.006004365626722574, 0.11748054623603821, 0.11034878343343735, 0.006304773036390543, 0.1042243167757988, 0.00646717194467783, 0.11743295192718506, 0.00662987120449543, 0.0066977329552173615, 0.006750930100679398, 0.006801949348300695, 0.006831164006143808, 0.0068586221896111965, 0.006863057147711515, 0.006870385259389877, 0.006842309143394232, 0.11101040989160538, 0.11614200472831726, 0.357781320810318, 0.006996787153184414, 0.007115841377526522, 0.10918080061674118, 0.007350184489041567, 0.007436865940690041, 0.10891902446746826, 0.0075798360630869865, 0.007651084568351507, 0.007713539991527796, 0.007731213700026274, 0.00774778425693512, 0.11095724254846573, 0.10373779386281967, 0.007833867333829403, 0.1124233528971672, 0.00787308719009161, 0.09775848686695099, 0.00791732594370842, 0.007934876717627048, 0.1077851727604866, 0.3465287983417511, 0.10733987390995026, 0.10866996645927429, 0.008435217663645744, 0.008582983165979385, 0.008685695007443428, 0.10600364208221436, 0.008856445550918579, 0.008947771042585373, 0.008977038785815239, 0.009018532000482082, 0.009019874967634678, 0.10392144322395325, 0.009041884914040565, 0.009075840935111046, 0.009035520255565643, 0.008997999131679535, 0.008956687524914742, 0.11353940516710281, 0.008885477669537067, 0.008845106698572636, 0.008804689161479473, 0.008750581182539463, 0.008678928948938847, 0.008610662072896957, 0.00856707151979208, 0.008465095423161983, 0.10585403442382812, 0.008333598263561726, 0.008260645903646946, 0.008200239390134811, 0.10237009823322296, 0.008058527484536171, 0.00799959059804678, 0.10996851325035095, 0.007907191291451454, 0.00785458367317915, 0.3413366973400116, 0.007898008450865746, 0.007962839677929878, 0.10970064252614975, 0.008081653155386448, 0.008130798116326332, 0.008184188976883888, 0.00816906988620758, 0.00817100889980793, 0.008163684979081154, 0.3322610557079315, 0.008256864733994007, 0.008356699720025063, 0.008428060449659824, 0.008477014489471912, 0.008540033362805843, 0.1166606917977333, 0.008603774942457676, 0.10448674857616425, 0.11216659098863602, 0.10161500424146652, 0.008722695522010326, 0.008753345347940922, 0.008781869895756245, 0.00878696609288454, 0.008788750506937504, 0.008754855021834373, 0.008751963265240192, 0.008679533377289772, 0.008648804388940334, 0.008575948886573315, 0.008523719385266304, 0.00844576582312584, 0.00840527843683958, 0.008305388502776623, 0.11025474220514297, 0.008166457526385784, 0.1022806316614151, 0.008071914315223694, 0.008006960153579712, 0.10402508080005646, 0.007924062199890614, 0.007875841110944748, 0.007847827859222889, 0.007779664825648069, 0.0077234539203345776, 0.007659588474780321, 0.007587232161313295, 0.007526650559157133, 0.007465082686394453, 0.007374213542789221, 0.007293029222637415, 0.11212862282991409, 0.10943444073200226, 0.0071418010629713535, 0.007109186612069607, 0.35202932357788086, 0.007157143671065569, 0.10781851410865784, 0.007319450378417969, 0.007363701704889536, 0.10774100571870804, 0.0074732424691319466, 0.007522692438215017, 0.007551243528723717, 0.10628867149353027, 0.007590660359710455, 0.007614936679601669, 0.007616848684847355, 0.007616856601089239, 0.11066016554832458, 0.00760000990703702, 0.10528408735990524, 0.007604518439620733, 0.11233874410390854, 0.0076119424775242805, 0.007604222744703293, 0.007593243382871151, 0.007582616060972214, 0.007550866808742285, 0.007532008457928896, 0.007475268095731735, 0.007437326479703188, 0.007383821997791529, 0.0073273153975605965, 0.007265790831297636, 0.007210779003798962, 0.007152588106691837, 0.007076608948409557, 0.10796801745891571, 0.1001020297408104, 0.10664545744657516, 0.006936238147318363, 0.11118298023939133, 0.10825587809085846, 0.0069239153526723385, 0.006927243899554014, 0.006935994140803814, 0.11412398517131805, 0.006917471531778574, 0.006928488612174988, 0.00691875908523798, 0.00690397247672081, 0.006854349747300148, 0.00681663304567337, 0.10641899704933167, 0.006793459877371788, 0.0067381164990365505, 0.11781809478998184, 0.0067215231247246265, 0.006683805491775274, 0.10579442232847214, 0.10482844710350037, 0.006668993271887302, 0.11019182205200195, 0.10535925626754761, 0.006719029042869806, 0.006720205768942833, 0.00673843827098608, 0.006745010148733854, 0.006711694411933422, 0.006739269942045212, 0.00667265010997653, 0.006659541744738817, 0.11594017595052719, 0.10797332227230072, 0.00661925645545125, 0.006628207862377167, 0.006599718239158392, 0.006592966616153717, 0.00657991785556078, 0.006500696763396263, 0.0064779892563819885, 0.006449035368859768, 0.006411820184439421, 0.006356157828122377, 0.0062972167506814, 0.006251037120819092, 0.006196189671754837, 0.006139021832495928, 0.006079502869397402, 0.0060165259055793285, 0.005971422418951988, 0.11500661075115204, 0.005896588321775198, 0.11850032955408096, 0.005823667626827955, 0.00580366188660264, 0.005765422713011503, 0.005728795193135738, 0.1215980052947998, 0.12206010520458221, 0.005690654274076223, 0.11215951293706894, 0.10944987088441849, 0.11600003391504288, 0.005777670070528984, 0.0058136568404734135, 0.1061074361205101, 0.005868099629878998, 0.12246091663837433, 0.005921769421547651, 0.005950829014182091, 0.005973173305392265, 0.11792386323213577, 0.10516709089279175, 0.006041273940354586, 0.006083943881094456, 0.006115236785262823, 0.39277389645576477, 0.006232524756342173, 0.0063505759462714195, 0.11800628155469894, 0.006548020057380199, 0.0066566248424351215, 0.006709059234708548, 0.0067870523780584335, 0.006803654599934816, 0.0068441033363342285, 0.10566647350788116, 0.006898865569382906, 0.006910509429872036, 0.0069259339943528175, 0.006920261774212122, 0.11391821503639221, 0.11963412910699844, 0.006956026889383793, 0.006956035271286964, 0.006954401731491089, 0.006948802620172501, 0.11439257860183716, 0.006962483283132315, 0.006955922115594149, 0.006936955731362104, 0.006898418068885803, 0.006878627464175224, 0.006843226030468941, 0.006809533573687077, 0.006777515169233084, 0.006717658136039972, 0.006667398847639561, 0.0066200802102684975, 0.1116149052977562, 0.006546037271618843, 0.0065106842666864395, 0.006478925235569477, 0.006411160342395306, 0.006365907844156027, 0.006322401110082865, 0.006266303826123476, 0.006217074580490589, 0.006163237616419792, 0.11701025068759918, 0.10899749398231506, 0.006072104908525944, 0.11401460319757462, 0.006027383264154196, 0.006026357877999544, 0.11559196561574936, 0.11065542697906494, 0.006040696520358324, 0.006046853493899107, 0.006046568974852562, 0.00603331346064806, 0.11369670927524567, 0.006025131791830063, 0.00601705489680171, 0.0060087027959525585, 0.006006115581840277, 0.0059881960041821, 0.107558473944664, 0.10621637850999832, 0.10986064374446869, 0.005984961986541748, 0.11054977774620056, 0.006031248718500137, 0.006039384752511978, 0.006056511774659157, 0.11565040796995163, 0.006081258878111839, 0.113901287317276, 0.0061105103231966496, 0.006127457600086927, 0.006125502288341522, 0.006121518090367317, 0.006115748547017574, 0.006103482097387314, 0.10800068080425262, 0.00607936130836606, 0.11584757268428802, 0.37525564432144165, 0.00621462007984519, 0.006311058532446623, 0.1071372702717781, 0.10507237166166306, 0.006609146948903799, 0.0066904849372804165, 0.11089636385440826, 0.0068367524072527885, 0.11111815273761749, 0.10892292112112045, 0.0070481207221746445, 0.007137398701161146, 0.0071711367927491665, 0.10605525970458984, 0.10576225072145462, 0.007305099628865719, 0.007349309511482716, 0.0073806350119411945, 0.0073861489072442055, 0.10923192650079727]\n",
            "Val loss 0.039470072849688675\n",
            "Val auc roc 0.49974730458221023\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a936e9403824caaaf779752bcd1c573",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0438\n",
            "Train Losses : [0.007427095901221037, 0.007429840508848429, 0.007426604628562927, 0.007411300204694271, 0.10776369273662567, 0.007403789088129997, 0.0074003892950713634, 0.007376144174486399, 0.007362015545368195, 0.10926355421543121, 0.007293324451893568, 0.007262816186994314, 0.007251477800309658, 0.007207270245999098, 0.10507716983556747, 0.007145868148654699, 0.007118246518075466, 0.007084769196808338, 0.007041745353490114, 0.0070175412110984325, 0.11038892716169357, 0.006927982904016972, 0.0068942224606871605, 0.10753857344388962, 0.11600324511528015, 0.006862958427518606, 0.006826476193964481, 0.006809886544942856, 0.10602779686450958, 0.00678594782948494, 0.10570426285266876, 0.006773791741579771, 0.006770357955247164, 0.006763514131307602, 0.006728142965584993, 0.0067233797162771225, 0.006675357930362225, 0.006638276390731335, 0.0066007873974740505, 0.006562954746186733, 0.006512171123176813, 0.006481577642261982, 0.006440234836190939, 0.006365027744323015, 0.006331190932542086, 0.11436396837234497, 0.006229302380234003, 0.10977362096309662, 0.1102239266037941, 0.006171735934913158, 0.006155862007290125, 0.0061416830867528915, 0.00612575002014637, 0.006111428141593933, 0.006082289386540651, 0.11534962058067322, 0.006026581861078739, 0.006015365477651358, 0.005995231680572033, 0.005961466580629349, 0.1248912438750267, 0.005910391453653574, 0.005901218857616186, 0.005881117191165686, 0.005862047895789146, 0.005837669596076012, 0.11038656532764435, 0.005795229692012072, 0.005754987243562937, 0.00573388347402215, 0.11495362967252731, 0.0057004415430128574, 0.005693511106073856, 0.3708881735801697, 0.005778376013040543, 0.005848441272974014, 0.10783739387989044, 0.11172950267791748, 0.006079145707190037, 0.006139320787042379, 0.00619456497952342, 0.0062260557897388935, 0.11855963617563248, 0.11368314921855927, 0.12094216793775558, 0.006420636083930731, 0.006479269824922085, 0.10904936492443085, 0.11559969186782837, 0.006642170716077089, 0.006684210617095232, 0.006710597313940525, 0.1124258041381836, 0.006785123143345118, 0.006797513458877802, 0.00681433267891407, 0.0068300636485219, 0.3398555517196655, 0.11550946533679962, 0.007031166460365057, 0.007131742313504219, 0.007217754609882832, 0.007280927617102861, 0.007328184321522713, 0.11285024881362915, 0.007400200702250004, 0.007452061865478754, 0.111908458173275, 0.007489463314414024, 0.007519373204559088, 0.0075183045119047165, 0.007535185664892197, 0.007526444736868143, 0.007495764177292585, 0.35368531942367554, 0.007573729380965233, 0.007662411779165268, 0.007716053631156683, 0.007766534574329853, 0.007799201179295778, 0.007809583563357592, 0.007836900651454926, 0.10616842657327652, 0.1150413230061531, 0.007878628559410572, 0.007890668697655201, 0.007870342582464218, 0.007882524281740189, 0.007856217212975025, 0.09966926276683807, 0.007823385298252106, 0.007803776301443577, 0.00777300214394927, 0.007754014804959297, 0.007716339081525803, 0.007675859145820141, 0.0076200529001653194, 0.0075661856681108475, 0.10863279551267624, 0.0075092613697052, 0.11745192855596542, 0.0074250828474760056, 0.007395696360617876, 0.007361256051808596, 0.0073179686442017555, 0.007279496174305677, 0.007248008158057928, 0.007200203370302916, 0.0071304370649158955, 0.34444135427474976, 0.007155136205255985, 0.0071899741888046265, 0.007223782129585743, 0.007241458632051945, 0.1034221276640892, 0.007278750650584698, 0.007293166127055883, 0.00731602031737566, 0.007306978572160006, 0.007292021065950394, 0.007283889688551426, 0.007249235641211271, 0.1014210507273674, 0.007198648992925882, 0.10847598314285278, 0.0071958997286856174, 0.10915577411651611, 0.007152687292546034, 0.007158875931054354, 0.11557058244943619, 0.007153528742492199, 0.0071351174265146255, 0.007125782314687967, 0.0071233538910746574, 0.007076933979988098, 0.007054752204567194, 0.11450746655464172, 0.007010945584625006, 0.006970834918320179, 0.7254753112792969, 0.007161055691540241, 0.10369250923395157, 0.1068587526679039, 0.007716227322816849, 0.007865658961236477, 0.008017796091735363, 0.10379655659198761, 0.008248688653111458, 0.008337914012372494, 0.008434492163360119, 0.00847428385168314, 0.11179088801145554, 0.008570719510316849, 0.008609374053776264, 0.008642131462693214, 0.00863837543874979, 0.008640093728899956, 0.11210360378026962, 0.00864779856055975, 0.008641254156827927, 0.008619489148259163, 0.008600223809480667, 0.10524392873048782, 0.10536041110754013, 0.10199392586946487, 0.008543947711586952, 0.00854770839214325, 0.008536710403859615, 0.11813417077064514, 0.008517034351825714, 0.008497380651533604, 0.11676432937383652, 0.008467670530080795, 0.008449121378362179, 0.11831965297460556, 0.00840806495398283, 0.008407754823565483, 0.00836863461881876, 0.00834340788424015, 0.008292538113892078, 0.0082411402836442, 0.10225250571966171, 0.11000755429267883, 0.1065823882818222, 0.11007574200630188, 0.008154403418302536, 0.008147499524056911, 0.008146378211677074, 0.008126741275191307, 0.008095717057585716, 0.008068900555372238, 0.008034900762140751, 0.10932452976703644, 0.007975366897881031, 0.007928152568638325, 0.007890104316174984, 0.007847249507904053, 0.007810490671545267, 0.007753992453217506, 0.0077013797126710415, 0.0076620010659098625, 0.10817458480596542, 0.10402017086744308, 0.007529107853770256, 0.007510931696742773, 0.007474751677364111, 0.11014614254236221, 0.10720065236091614, 0.3555341362953186, 0.007500082720071077, 0.007587752304971218, 0.007641054689884186, 0.007711621001362801, 0.007739787921309471, 0.1110777035355568, 0.10551922768354416, 0.1062297448515892, 0.007890958338975906, 0.007929347455501556, 0.3217718303203583, 0.008063366636633873, 0.008190764114260674, 0.008259586058557034, 0.10746695846319199, 0.008390518836677074, 0.008452869020402431, 0.00850062258541584, 0.1097562238574028, 0.33209943771362305, 0.008696400560438633, 0.008822733536362648, 0.008902616798877716, 0.008989986963570118, 0.009046126157045364, 0.00907429400831461, 0.009112377651035786, 0.09944944083690643, 0.009163487702608109, 0.009163446724414825, 0.09932880103588104, 0.009170633740723133, 0.009196784347295761, 0.10581734031438828, 0.009187142364680767, 0.009150072000920773, 0.009156879037618637, 0.00911879725754261, 0.009065103717148304, 0.009037388488650322, 0.009016326628625393, 0.00895957462489605, 0.008905901573598385, 0.008835881017148495, 0.008752880617976189, 0.008700133301317692, 0.008624391630291939, 0.1089048683643341, 0.10679484158754349, 0.00848038773983717, 0.008432632312178612, 0.00841858983039856, 0.008332752622663975, 0.10621784627437592, 0.008250556886196136, 0.008229735307395458, 0.008203568868339062, 0.008116128854453564, 0.008060506545007229, 0.008003320544958115, 0.00795457512140274, 0.10745352506637573, 0.00786597654223442, 0.007842117920517921, 0.10054992884397507, 0.11098192632198334, 0.007695875596255064, 0.007683442905545235, 0.1158561185002327, 0.007625091355293989, 0.007625684142112732, 0.007577232550829649, 0.007564779836684465, 0.007540145888924599, 0.1036730483174324, 0.007454202976077795, 0.10846777260303497, 0.00741007411852479, 0.007408937904983759, 0.12124675512313843, 0.007378909736871719, 0.10974197089672089, 0.007336860988289118, 0.00733639020472765, 0.007341478019952774, 0.1237831711769104, 0.007299387827515602, 0.3439788818359375, 0.007343796547502279, 0.007412547245621681, 0.007472684141248465, 0.007510732859373093, 0.0075683025643229485, 0.11095884442329407, 0.007592137437313795, 0.33856964111328125, 0.0077319880947470665, 0.007818946614861488, 0.007915294729173183, 0.007959374226629734, 0.008009793236851692, 0.008039621636271477, 0.008107900619506836, 0.008089992217719555, 0.00809389352798462, 0.10932232439517975, 0.008096578530967236, 0.008104037493467331, 0.12057268619537354, 0.008103536441922188, 0.008094631135463715, 0.008082102984189987, 0.008071355521678925, 0.11022136360406876, 0.008041737601161003, 0.008007516153156757, 0.007974990643560886, 0.007948952727019787, 0.007916789501905441, 0.007875866256654263, 0.007843228057026863, 0.00781267136335373, 0.007751720957458019, 0.007692747283726931, 0.007643797434866428, 0.00760887935757637, 0.00753435306251049, 0.00748897809535265, 0.007434980012476444, 0.00737999239936471, 0.00732596917077899, 0.007268037647008896, 0.10060075670480728, 0.007149093318730593, 0.10568427294492722, 0.007079534698277712, 0.1160474345088005, 0.007017407566308975, 0.007013634778559208, 0.00697499280795455, 0.006965779699385166, 0.006908543407917023, 0.006879561115056276, 0.0068458495661616325, 0.00679450947791338, 0.006757691968232393, 0.3660629689693451, 0.006788266357034445, 0.0068052359856665134, 0.35783514380455017, 0.0069536189548671246, 0.007050622254610062, 0.007132288534194231, 0.0072087449952960014, 0.10815399885177612, 0.11770589649677277, 0.007391879800707102, 0.10037542134523392, 0.007520375307649374, 0.007556868717074394, 0.007605225313454866, 0.007623572368174791, 0.09873949736356735, 0.00766490725800395, 0.007662553805857897, 0.34941941499710083, 0.0077836718410253525, 0.007844165898859501, 0.00791230145841837, 0.10608982294797897, 0.11089722812175751, 0.008104556240141392, 0.008132184855639935, 0.10383553057909012, 0.008212517946958542, 0.008237863890826702, 0.008259087800979614, 0.008262433111667633, 0.008293699473142624, 0.10893239080905914, 0.11415859311819077, 0.008287242613732815, 0.008289187215268612, 0.008273208513855934, 0.008283055387437344, 0.008250322192907333, 0.008228791877627373, 0.11150500178337097, 0.10876251012086868, 0.1132344976067543, 0.008181972429156303, 0.10186689347028732, 0.008187825791537762, 0.1059567853808403, 0.00820346549153328, 0.008197094313800335, 0.008183831349015236, 0.008168945088982582, 0.008147336542606354, 0.10281524062156677, 0.008117551915347576, 0.008091017603874207, 0.00806803721934557, 0.008053349331021309, 0.35256892442703247, 0.008073791861534119, 0.00811320822685957, 0.008151163347065449, 0.0081646628677845, 0.008184846490621567, 0.0993344634771347, 0.10412369668483734, 0.008216810412704945, 0.008262763731181622, 0.008236374706029892, 0.008232057094573975, 0.008232622407376766, 0.008204882964491844, 0.34905728697776794, 0.008250294253230095, 0.10566649585962296, 0.10468169301748276, 0.33300143480300903, 0.11376774311065674, 0.008693600073456764, 0.00881295558065176, 0.10747480392456055, 0.009012152440845966, 0.11347189545631409, 0.009179998189210892, 0.10918949544429779, 0.009329249151051044, 0.009371279738843441, 0.0094025619328022, 0.009430231526494026, 0.10953837633132935, 0.00948263518512249, 0.009485560469329357, 0.009479036554694176, 0.009472372010350227, 0.11354270577430725, 0.009461795911192894, 0.10617753118276596, 0.0094327786937356, 0.009439541958272457, 0.00939109455794096, 0.009381099604070187, 0.009329358115792274, 0.009308313019573689, 0.009260958060622215, 0.009213644079864025, 0.10788146406412125, 0.10238457471132278, 0.10464509576559067, 0.009115482680499554, 0.11770468950271606, 0.11080815643072128, 0.10838278383016586, 0.12048681825399399, 0.1188570186495781, 0.3504813611507416, 0.0092137036845088, 0.009367212653160095, 0.10633815079927444, 0.09215543419122696, 0.11676854640245438, 0.10926981270313263, 0.009736515581607819, 0.009742874652147293, 0.009859372861683369, 0.009860514663159847, 0.00989371258765459, 0.009870273992419243, 0.11535997688770294, 0.009885621257126331, 0.009906521067023277, 0.0098543306812644, 0.00988833885639906, 0.09564657509326935, 0.009772446006536484, 0.00979337003082037, 0.00975559838116169, 0.009739179164171219, 0.009674095548689365, 0.09094948321580887, 0.009641548618674278, 0.009571753442287445, 0.009516534395515919, 0.009451063349843025, 0.10514858365058899, 0.009363768622279167, 0.009362073615193367, 0.009299665689468384, 0.009248556569218636, 0.10753490030765533, 0.009172730147838593, 0.009074026718735695, 0.09944411367177963, 0.11309021711349487, 0.008999007754027843, 0.008984201587736607, 0.00893340352922678, 0.008897416293621063, 0.008871803991496563, 0.008853538893163204, 0.008754437789320946, 0.008695805445313454, 0.008702727034687996, 0.008631786331534386, 0.095059834420681, 0.008557709865272045, 0.008508752100169659, 0.008401171304285526, 0.008406591601669788, 0.1217285543680191, 0.00830896571278572, 0.008271987549960613, 0.008193529210984707, 0.00817829929292202, 0.008088321425020695, 0.00804639607667923, 0.00800275057554245, 0.007946179248392582, 0.007877449505031109, 0.00786601658910513, 0.007803435903042555, 0.007782110944390297, 0.11444102227687836, 0.007705126889050007, 0.0076681943610310555, 0.10269681364297867, 0.007524364627897739, 0.007533608935773373, 0.007453363388776779, 0.12256808578968048, 0.1019524335861206, 0.10093270242214203, 0.0074243927374482155, 0.007382560521364212, 0.007354735396802425, 0.007353981491178274, 0.12990936636924744, 0.007331513799726963, 0.007307695224881172, 0.007306877989321947, 0.007252095267176628, 0.0072207264602184296, 0.11524096876382828, 0.007221252657473087, 0.10380200296640396, 0.12188757956027985, 0.007229364477097988, 0.007210549432784319, 0.00714935502037406, 0.10527892410755157, 0.007138705346733332, 0.007137107662856579, 0.007120135240256786, 0.10416748374700546, 0.007111697923392057, 0.00710912374779582, 0.12155011296272278, 0.0945473238825798, 0.007114605978131294, 0.007114051375538111, 0.0070464275777339935, 0.007090321276336908, 0.11158762872219086, 0.006989798508584499, 0.12251251935958862, 0.0070433951914310455, 0.10313709825277328, 0.007078034803271294, 0.007038997020572424, 0.0070286402478814125, 0.1044330894947052, 0.006984887644648552, 0.12050275504589081, 0.007045709062367678, 0.006967041175812483, 0.006978566292673349, 0.006974209100008011, 0.006990437395870686, 0.1215292438864708, 0.0069400854408741, 0.119991734623909, 0.36244359612464905, 0.007034451700747013, 0.00713184243068099, 0.11697644740343094, 0.09483394026756287, 0.007223936263471842, 0.007280227728188038, 0.1193801537156105, 0.007379631511867046, 0.007367079146206379, 0.007423735223710537, 0.007471289485692978, 0.007487216964364052, 0.007426450960338116, 0.0074270786717534065, 0.1022152230143547, 0.11500536650419235, 0.007427179254591465, 0.007455785293132067, 0.007459923624992371, 0.007418008055537939, 0.007423290051519871, 0.007403504569083452, 0.0073765600100159645, 0.007350550033152103, 0.007357659749686718, 0.007314777933061123, 0.36644211411476135, 0.007353575434535742, 0.007328819949179888, 0.007400404196232557, 0.0074094198644161224, 0.11148055642843246, 0.007437532301992178, 0.10359203815460205, 0.0074812727980315685, 0.007476458325982094, 0.007496426813304424, 0.1099466010928154, 0.007515019737184048, 0.007495624478906393, 0.11058947443962097, 0.007471885532140732, 0.007502510212361813, 0.0075162434950470924, 0.007473520934581757, 0.007438544183969498, 0.11981762945652008, 0.007443056907504797, 0.00745052145794034, 0.11058633774518967, 0.10846424102783203, 0.007389002945274115, 0.007413686718791723, 0.007401885464787483, 0.007375234737992287, 0.007362425792962313, 0.0073738982900977135, 0.11975713074207306, 0.34831926226615906, 0.007370978593826294, 0.3177361488342285, 0.37256187200546265, 0.10149282962083817, 0.007845330983400345, 0.33726099133491516, 0.008226258680224419, 0.008396396413445473, 0.008546959608793259, 0.008679244667291641, 0.008764602243900299, 0.09257829934358597, 0.00900172907859087, 0.11443441361188889, 0.11513381451368332, 0.1124870628118515, 0.009306561201810837, 0.12042612582445145, 0.3442358076572418, 0.009584113955497742, 0.10845773667097092, 0.009752713143825531, 0.009846588596701622, 0.009965784847736359, 0.010050549171864986, 0.01004247460514307, 0.010052124038338661, 0.010175899602472782, 0.010095382109284401, 0.01012489851564169, 0.010141552425920963, 0.11543630808591843, 0.010121536441147327, 0.08808187395334244, 0.010072479955852032, 0.010077716782689095, 0.09944175183773041, 0.010010356083512306, 0.12341327220201492, 0.009984373115003109, 0.10636095702648163, 0.11902736872434616, 0.00998566672205925, 0.10844644904136658, 0.00999794714152813, 0.009980406612157822, 0.3025616407394409, 0.01003261934965849, 0.010056651197373867, 0.10196913033723831, 0.010141418315470219, 0.1033741682767868, 0.010176843963563442, 0.1112288162112236, 0.010191365145146847, 0.010194877162575722, 0.010202472098171711, 0.010216104798018932, 0.1039755642414093, 0.010237248614430428, 0.010186892934143543, 0.010123902931809425, 0.01010646391659975, 0.01010427437722683, 0.12010069191455841, 0.010038751177489758, 0.10008025914430618, 0.0099722845479846, 0.009939479641616344, 0.009956478141248226, 0.009907091967761517, 0.009857059456408024, 0.009792422875761986, 0.009769469499588013, 0.009732830338180065, 0.00966924149543047, 0.009617643430829048, 0.11524409800767899, 0.09573479741811752, 0.009509282186627388, 0.009484364651143551, 0.00944770872592926, 0.10797145962715149, 0.6611179709434509, 0.009464934468269348, 0.1073075607419014, 0.009736252948641777, 0.009771909564733505, 0.00986181665211916, 0.009943017736077309, 0.009975888766348362, 0.010013800114393234, 0.01003976073116064, 0.010064435191452503, 0.010043887421488762, 0.010054010897874832, 0.09981334209442139, 0.010115036740899086, 0.09467606991529465, 0.10316576808691025, 0.01001978013664484, 0.0944008156657219, 0.01003641914576292, 0.010018862783908844, 0.01003635860979557, 0.10806800425052643, 0.10901077836751938, 0.010009149089455605, 0.009979458525776863, 0.009980830363929272, 0.10779710859060287, 0.009902656078338623, 0.10772525519132614, 0.009871850721538067, 0.009871689602732658, 0.009908421896398067, 0.009840982966125011, 0.009815545752644539, 0.009745452553033829, 0.009717983193695545, 0.009665003046393394, 0.009619392454624176, 0.11170835793018341, 0.11614535003900528, 0.009568650275468826, 0.009493866935372353, 0.009484411217272282, 0.009443743154406548, 0.009432406164705753, 0.11371312290430069, 0.009325970895588398, 0.009323164820671082, 0.009284719824790955, 0.00924509298056364, 0.12634390592575073, 0.009222385473549366, 0.00916148442775011, 0.009118843823671341, 0.009061682969331741, 0.009018715471029282, 0.00898123811930418, 0.008981714025139809, 0.008897284977138042, 0.10337713360786438, 0.12392961978912354, 0.008813687600195408, 0.008813638240098953, 0.008750203996896744, 0.008699280209839344, 0.008685417473316193, 0.10375067591667175, 0.008612534962594509, 0.008612146601080894, 0.008564917370676994, 0.008565056137740612, 0.008530682884156704, 0.10611730068922043, 0.11890864372253418, 0.10490023344755173, 0.008436526171863079, 0.008433103561401367, 0.008359473198652267, 0.10386903584003448, 0.10817254334688187, 0.008339371532201767, 0.10801079124212265, 0.008317953906953335, 0.008315538987517357, 0.00830842088907957, 0.008315536193549633, 0.10495337098836899, 0.008289777673780918, 0.00829592440277338, 0.00822005607187748, 0.008236766792833805, 0.008191742934286594, 0.0996645912528038, 0.10958918184041977, 0.008159670047461987, 0.10138276219367981, 0.11494068801403046, 0.008115110918879509, 0.12226153910160065, 0.008112135343253613, 0.008153339847922325, 0.008143807761371136, 0.008096834644675255, 0.008127842098474503, 0.10510143637657166, 0.008052282966673374, 0.008060462772846222, 0.00803283229470253, 0.008019236847758293, 0.00801511574536562, 0.007969594560563564, 0.007963943295180798, 0.007943999953567982, 0.11192307621240616, 0.3248634934425354, 0.1088576391339302, 0.007947452366352081, 0.007968270219862461, 0.007993827573955059, 0.008015349507331848, 0.09708517789840698, 0.008022365160286427, 0.00805331114679575, 0.008053348399698734, 0.10737248510122299, 0.1150251030921936, 0.008093113079667091, 0.008075627498328686, 0.00807549711316824, 0.00803875271230936, 0.10197863727807999, 0.09885989129543304, 0.12200548499822617, 0.11314066499471664, 0.008062667213380337, 0.008087418973445892, 0.11286894232034683, 0.00809694267809391, 0.008090964518487453, 0.008095997385680676, 0.008143077604472637, 0.008071099407970905, 0.008055536076426506, 0.008046187460422516, 0.008043368346989155, 0.008009232580661774, 0.008050314150750637, 0.007980157621204853, 0.10740964859724045, 0.007924942299723625, 0.007945535704493523, 0.007889935746788979, 0.007865361869335175, 0.007852321490645409, 0.10981868207454681, 0.10312982648611069, 0.007757133338600397, 0.007760621141642332, 0.11740642040967941, 0.10348287224769592, 0.007715161889791489, 0.11632690578699112, 0.007761416491121054, 0.007722417823970318, 0.007709799334406853, 0.11161120235919952, 0.10702698677778244, 0.0077011859975755215, 0.1084580272436142, 0.007771465927362442, 0.0076864720322191715, 0.00775060523301363, 0.0077119311317801476, 0.007678880821913481, 0.3379230797290802, 0.0077111851423978806, 0.007726673502475023, 0.11696960777044296, 0.007783202454447746, 0.007827235385775566, 0.007808215916156769, 0.00784829631447792, 0.00783609040081501, 0.007841717451810837, 0.007835223339498043, 0.10480493307113647, 0.007820907980203629, 0.007792459335178137, 0.007805827539414167, 0.007793618366122246, 0.007792165502905846, 0.007776697166264057, 0.007745555602014065, 0.007727464195340872, 0.007704334333539009, 0.007678806781768799, 0.0076398118399083614, 0.007636077702045441, 0.1107708066701889, 0.007570989895612001, 0.00754515640437603, 0.11081939935684204, 0.0075268931686878204, 0.10988017171621323, 0.0074705760926008224, 0.007519264239817858, 0.007487870752811432, 0.00747937336564064, 0.10882235318422318, 0.007416489068418741, 0.333185076713562, 0.1128569170832634, 0.3344702124595642, 0.0075599257834255695, 0.007615228183567524, 0.0076810638420283794, 0.1135176420211792, 0.0077729420736432076, 0.007853318005800247, 0.36659738421440125, 0.007950744591653347, 0.10473813861608505, 0.11681979894638062, 0.00813433900475502, 0.09993366152048111, 0.008229623548686504, 0.10301516950130463, 0.008307975716888905, 0.008367466740310192, 0.0977533757686615, 0.008393627591431141, 0.008428078144788742, 0.008448558859527111, 0.008473171852529049, 0.008470320142805576, 0.008489562198519707, 0.008474275469779968, 0.00847844872623682, 0.11702610552310944, 0.008462576195597649, 0.10717388987541199, 0.00842683482915163, 0.008463072590529919, 0.09959197789430618, 0.008446398191154003, 0.10009220242500305, 0.10476183146238327, 0.10893339663743973, 0.008450262248516083, 0.008426337502896786, 0.008413423784077168, 0.10962806642055511, 0.008426178246736526, 0.008399211801588535, 0.008428836241364479, 0.10568919777870178, 0.008411578834056854, 0.3106272220611572, 0.008449751883745193, 0.008437900803983212, 0.10951954871416092, 0.008507677353918552, 0.00856779608875513, 0.008588564582169056, 0.09687869995832443, 0.008584744296967983, 0.008586431853473186, 0.008541516959667206, 0.10727369785308838, 0.00855621974915266, 0.008552035316824913, 0.008586173877120018, 0.008537860587239265, 0.008528633043169975, 0.008496792055666447, 0.11010697484016418, 0.00848689116537571, 0.008457704447209835, 0.12146884202957153, 0.10241670161485672, 0.10441164672374725, 0.09945334494113922, 0.008452670648694038, 0.008465293794870377, 0.008431484922766685, 0.008432726375758648, 0.11075583100318909, 0.11375543475151062, 0.12010949850082397, 0.008416334167122841, 0.008435296826064587, 0.008406761102378368, 0.10015291720628738, 0.008414333686232567, 0.008447284810245037, 0.11666294932365417, 0.3301582336425781, 0.1143508329987526, 0.008484551683068275, 0.008522178046405315, 0.008549134247004986, 0.008568314835429192, 0.008574303239583969, 0.008592995814979076, 0.008605582639575005, 0.008604802191257477, 0.0950944796204567, 0.00859774835407734, 0.10228100419044495, 0.008575309999287128, 0.008561368100345135, 0.008595985360443592, 0.008595509454607964, 0.008570910431444645, 0.008524267002940178, 0.008575956337153912, 0.008492007851600647, 0.008496643044054508, 0.0966600701212883, 0.008463689126074314, 0.008436827920377254, 0.008425084874033928, 0.008397836238145828, 0.008360623382031918, 0.008362812921404839, 0.008364282548427582, 0.008300184272229671, 0.008302898146212101, 0.008259207010269165, 0.008242820389568806, 0.00820503756403923, 0.008199051022529602, 0.008152400143444538, 0.008138339966535568, 0.008068283088505268, 0.008051525801420212, 0.008009480312466621, 0.008029916323721409, 0.11487801373004913, 0.6648980975151062, 0.008006898686289787, 0.10061965137720108, 0.008156641386449337, 0.00820484384894371, 0.008217988535761833, 0.12325511127710342, 0.008282785303890705, 0.008279786445200443, 0.1099214032292366, 0.008371464908123016, 0.0083692017942667, 0.34723609685897827, 0.00848634634166956, 0.008483639918267727, 0.008480081334710121, 0.09436645358800888, 0.008542869240045547, 0.00858335942029953, 0.0984949916601181, 0.09664568305015564, 0.008631009608507156, 0.008677161298692226, 0.008664584718644619, 0.10411415249109268, 0.008662218227982521, 0.008684384636580944, 0.008658936247229576, 0.008648188784718513, 0.008669275790452957, 0.008652922697365284, 0.008622870780527592, 0.008597133681178093, 0.11236738413572311, 0.008613706566393375, 0.1169695258140564, 0.10396790504455566, 0.11183534562587738, 0.00856862124055624, 0.008564393036067486, 0.1021435558795929, 0.008566895499825478, 0.00859478022903204, 0.008590416051447392, 0.008551503531634808, 0.00852159783244133, 0.11176133900880814, 0.11047983169555664, 0.008513683453202248, 0.00850763637572527, 0.008533700369298458, 0.0085233673453331, 0.008512534201145172, 0.008471217937767506, 0.008444894105196, 0.00842137262225151, 0.11647025495767593, 0.10417182743549347, 0.10156449675559998, 0.11819802969694138, 0.008416428230702877, 0.008387942798435688, 0.008386892266571522, 0.008388228714466095, 0.10230451077222824, 0.10532308369874954, 0.008403671905398369, 0.008381043560802937, 0.00835409015417099, 0.008354425430297852, 0.008337358944118023, 0.11364325135946274, 0.00830709096044302, 0.008298574015498161, 0.09914296120405197, 0.008309800177812576, 0.008288714103400707, 0.11729398369789124, 0.008276386186480522, 0.00826601404696703, 0.008282158523797989, 0.008251496590673923, 0.008219616487622261, 0.008230426348745823, 0.008191992528736591, 0.008173440583050251, 0.008200788870453835, 0.008157211355865002, 0.008141371421515942, 0.008133016526699066, 0.008081640116870403, 0.008043862879276276, 0.008085358887910843, 0.008006522431969643, 0.008005366660654545, 0.11391303688287735, 0.007955081760883331, 0.007947742007672787, 0.11496063321828842, 0.007911957800388336, 0.007914132438600063, 0.00789165310561657, 0.12133963406085968, 0.007871938869357109, 0.007846887223422527, 0.10188796371221542, 0.007860216312110424, 0.007817843928933144, 0.3353631794452667, 0.007866180501878262, 0.10564976930618286, 0.007870438508689404, 0.007873240858316422, 0.00789414532482624, 0.007894884794950485, 0.007871885783970356, 0.007877860218286514, 0.12390784174203873, 0.007922048680484295, 0.00794085580855608, 0.09801272302865982, 0.007917464710772038, 0.007905601523816586, 0.007884511724114418, 0.11286931484937668, 0.007896224968135357, 0.10412201285362244, 0.007906134240329266, 0.007880724035203457, 0.007851783186197281, 0.007870716974139214, 0.007863674312829971, 0.007854958064854145, 0.11995691061019897, 0.007865223102271557, 0.09919552505016327, 0.0078123221173882484, 0.007821962237358093, 0.00779327005147934, 0.007781052961945534, 0.007803057320415974, 0.007777017075568438, 0.3706077039241791, 0.11216971278190613, 0.007788579910993576, 0.007792915683239698, 0.007840093225240707, 0.007827808149158955, 0.007836065255105495, 0.007850795984268188, 0.007811755407601595, 0.007811854127794504, 0.11198346316814423, 0.007842125371098518, 0.00788491778075695, 0.10291706770658493, 0.00782662071287632, 0.34193527698516846, 0.00784418173134327, 0.007878173142671585, 0.007843978703022003, 0.007877548225224018, 0.007897649891674519, 0.0993882492184639, 0.00793845672160387, 0.00789502914994955, 0.36383360624313354, 0.10979364812374115, 0.007968651130795479, 0.008003602735698223, 0.35602712631225586, 0.008057165890932083, 0.008059810847043991, 0.11141184717416763, 0.09661555290222168, 0.00815337710082531, 0.008195348083972931, 0.008198520168662071, 0.11647181957960129, 0.10986953228712082, 0.008237565867602825, 0.008287656120955944, 0.008305796422064304, 0.10389626026153564, 0.008279708214104176, 0.34621912240982056, 0.008301973342895508, 0.10607227683067322, 0.10044366866350174, 0.0083621172234416, 0.008390136063098907, 0.008419042453169823, 0.008415848016738892, 0.008448256179690361, 0.1168646514415741, 0.3468831777572632, 0.008452747948467731, 0.008537271991372108, 0.008540020324289799, 0.00854038167744875, 0.008534207940101624, 0.008561708964407444, 0.00856670830398798, 0.09637540578842163, 0.008581883274018764, 0.008584151975810528, 0.008556192740797997, 0.008585067465901375, 0.008562861010432243, 0.008572607301175594, 0.008550631813704967, 0.008575526997447014, 0.1144745722413063, 0.008593015372753143, 0.008529123850166798, 0.008511362597346306, 0.008527682162821293, 0.008537353947758675, 0.008520015515387058, 0.008482909761369228, 0.10327006131410599, 0.008463206700980663, 0.008446307852864265, 0.008440383709967136, 0.008453580550849438, 0.12355650216341019, 0.008456929586827755, 0.008427837863564491, 0.11629954725503922, 0.008393009193241596, 0.008416594937443733, 0.0083632105961442, 0.10021154582500458, 0.008367243222892284, 0.008354178629815578, 0.008372838608920574, 0.008374174125492573, 0.008333882316946983, 0.008349456824362278, 0.008312653750181198, 0.00835205614566803, 0.008307250216603279, 0.008273263461887836, 0.09680306166410446, 0.008250742219388485, 0.008239205926656723, 0.1186412051320076, 0.10188132524490356, 0.008212213404476643, 0.11025703698396683, 0.008233201690018177, 0.10162793099880219, 0.11381053179502487, 0.00822506658732891, 0.008218919858336449, 0.11018689721822739, 0.008196552284061909, 0.008183558471500874, 0.008197600953280926, 0.008210859261453152, 0.008204828016459942, 0.3127979040145874, 0.008222399279475212, 0.008235431276261806, 0.00824486929923296, 0.10679343342781067, 0.10737820714712143, 0.00823132786899805, 0.0082198865711689, 0.10816211998462677, 0.008268256671726704, 0.1213207021355629, 0.00823616236448288, 0.00822929572314024, 0.008241359144449234, 0.11610245704650879, 0.008252563886344433, 0.008263756521046162, 0.11367063224315643, 0.11053691059350967, 0.008285104297101498, 0.00826675072312355, 0.008242638781666756, 0.10159094631671906, 0.12072385847568512, 0.008254049345850945, 0.10406701266765594, 0.008231923915445805, 0.1171727403998375, 0.00825319904834032, 0.008251234889030457, 0.008243110962212086, 0.3606148362159729, 0.00826151855289936, 0.008261796087026596, 0.008310212753713131, 0.008273433893918991, 0.10955476015806198, 0.008275832049548626, 0.008306904695928097, 0.00833168439567089, 0.008308641612529755, 0.1075037494301796, 0.008289189077913761, 0.11529875546693802, 0.35469144582748413, 0.11146635562181473, 0.008307458832859993, 0.008337722159922123, 0.008352434262633324, 0.008367671631276608, 0.008360751904547215, 0.008368114940822124, 0.008386056870222092, 0.008372664451599121, 0.10541471838951111, 0.008364592678844929, 0.008391295559704304, 0.00837688148021698, 0.008388396352529526, 0.008359905332326889, 0.1088489219546318, 0.008378971368074417, 0.3414755165576935, 0.00837584026157856, 0.008371113799512386, 0.09738919883966446, 0.00836498849093914, 0.008422010578215122, 0.008413096889853477, 0.008423603139817715, 0.008408230729401112, 0.11111845821142197, 0.11134970933198929, 0.008412670344114304, 0.10555992275476456, 0.008447173051536083, 0.10876902937889099, 0.11846363544464111, 0.09661643207073212, 0.12009598314762115, 0.008399310521781445, 0.10685595124959946, 0.10550116747617722, 0.09809167683124542, 0.10166827589273453, 0.008426843211054802, 0.10600867867469788, 0.00843583233654499, 0.00846875924617052, 0.008432138711214066, 0.008449847809970379, 0.11431440711021423, 0.008460598066449165, 0.008469417691230774, 0.008443265222012997, 0.008443975821137428, 0.00847539771348238, 0.00845931377261877, 0.008474420756101608, 0.00842353142797947, 0.008443791419267654, 0.008455275557935238, 0.008415448479354382, 0.008409291505813599, 0.008419872261583805, 0.008454075083136559, 0.10531344264745712, 0.10636341571807861, 0.008379334583878517, 0.008432727307081223, 0.008386814035475254, 0.008398846723139286, 0.008380073122680187, 0.008375377394258976, 0.008371900767087936, 0.0083689009770751, 0.10803071409463882, 0.008351047523319721, 0.008360933512449265, 0.008351006545126438, 0.008345420472323895, 0.008349604904651642, 0.008324063383042812, 0.09685106575489044, 0.008348282426595688, 0.008311199024319649, 0.008315732702612877, 0.12542256712913513, 0.008307978510856628, 0.008320738561451435, 0.008281661197543144, 0.008343551307916641, 0.008297517895698547, 0.008308079093694687, 0.11229479312896729, 0.008294501341879368, 0.008284150622785091, 0.008277412503957748, 0.11627143621444702, 0.008254426531493664, 0.008299068547785282, 0.00828576646745205, 0.00825237575918436, 0.008267792873084545, 0.00825309008359909, 0.11474744975566864, 0.00826333649456501, 0.10789903253316879, 0.00823658425360918, 0.10094470530748367, 0.008254214189946651, 0.11438973993062973, 0.008240669034421444, 0.008226455189287663, 0.1253102719783783, 0.008264501579105854, 0.10777375847101212, 0.1143772080540657, 0.10706496238708496, 0.008246087469160557, 0.008228730410337448, 0.008222512900829315, 0.008226134814321995, 0.008237488567829132, 0.008219319395720959, 0.008219082839787006, 0.008230714127421379, 0.008219162002205849, 0.008221223950386047, 0.008258789777755737, 0.008221028372645378, 0.12347722798585892, 0.354247123003006, 0.12453682720661163, 0.008225374855101109, 0.008231041021645069, 0.008219271898269653, 0.008215769194066525, 0.008235172368586063, 0.008238384500145912, 0.008217744529247284, 0.008273026905953884, 0.008245347999036312, 0.008220471441745758, 0.008220197632908821, 0.008217562921345234, 0.3382149338722229, 0.00827116146683693, 0.008262903429567814, 0.3180553913116455, 0.008241509087383747, 0.09659384191036224, 0.008247235789895058, 0.3476181924343109, 0.008252963423728943, 0.008261104114353657, 0.11909249424934387, 0.008268973790109158, 0.008242851123213768, 0.00824410654604435, 0.11660460382699966, 0.00824234914034605, 0.00829728040844202, 0.09982313960790634, 0.008243190124630928, 0.008292502723634243, 0.00826372392475605, 0.008276445791125298, 0.008263691328465939, 0.008267924189567566, 0.00826991070061922, 0.008265133015811443, 0.008265169337391853, 0.10545482486486435, 0.008279746398329735, 0.008254819549620152]\n",
            "Val loss 0.04142839873774458\n",
            "Val auc roc 0.45506834809395463\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}