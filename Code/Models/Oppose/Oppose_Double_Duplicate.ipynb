{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oppose_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77b4ae6f005e4049a3a35ccec5f3c24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8183da60bc7443e2a68bd0f587771b12",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c47bdcdda8934e54992e9fff0c4e7b1b",
              "IPY_MODEL_113ac2dbde6a42da87565a0f5bfc3096"
            ]
          }
        },
        "8183da60bc7443e2a68bd0f587771b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c47bdcdda8934e54992e9fff0c4e7b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3231f81ef81b4d67907b555971b569ae",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1839,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1839,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_047a18d06f3341ab8fd2b4c2f852e7c7"
          }
        },
        "113ac2dbde6a42da87565a0f5bfc3096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f65b9fb20e84355b1957c9ebc95d456",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1839/1839 [17:47&lt;00:00,  1.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af9e56611ad843819e33b146220d1f78"
          }
        },
        "3231f81ef81b4d67907b555971b569ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "047a18d06f3341ab8fd2b4c2f852e7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f65b9fb20e84355b1957c9ebc95d456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af9e56611ad843819e33b146220d1f78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4960ce30ac2c4c489d2b7a87aaa2c299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed8f10a89e144f70acf0e58e92498e8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e5097841e1445168c018f841bb99f8a",
              "IPY_MODEL_d272bb5be8f24a1aab8c8c24acb03546"
            ]
          }
        },
        "ed8f10a89e144f70acf0e58e92498e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e5097841e1445168c018f841bb99f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7ff48c19bb143f1a2decc973604e947",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1839,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1839,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dce7745b96d4e57b317fb3416aad7dd"
          }
        },
        "d272bb5be8f24a1aab8c8c24acb03546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_80423066aecb466eae7c95302868c8cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1839/1839 [17:36&lt;00:00,  1.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d69272d63c14652a1a0dd4f9d44a24b"
          }
        },
        "c7ff48c19bb143f1a2decc973604e947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dce7745b96d4e57b317fb3416aad7dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80423066aecb466eae7c95302868c8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d69272d63c14652a1a0dd4f9d44a24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3d8edc773734c1f8c6328839d0ddf9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_102ccdf113ac401399f22a6e10a2f861",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_447544f6552a4e9aac9be0ff89281de4",
              "IPY_MODEL_5aa7f83117ae4634b6b720046a2354de"
            ]
          }
        },
        "102ccdf113ac401399f22a6e10a2f861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "447544f6552a4e9aac9be0ff89281de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5a490d1d16234448aaa8568a9083b6f3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1839,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1839,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d009f0f572b44d69b76256b9f52edbc2"
          }
        },
        "5aa7f83117ae4634b6b720046a2354de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_123a0c983ff74a22a159b3bd896c6207",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1839/1839 [17:46&lt;00:00,  1.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7157adb0a1d47e5a94460d62ce8793c"
          }
        },
        "5a490d1d16234448aaa8568a9083b6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d009f0f572b44d69b76256b9f52edbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "123a0c983ff74a22a159b3bd896c6207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7157adb0a1d47e5a94460d62ce8793c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "facd0024-d8aa-4ddf-b858-f31580f9d50c"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/33)\u001b[K\rremote: Counting objects:   6% (2/33)\u001b[K\rremote: Counting objects:   9% (3/33)\u001b[K\rremote: Counting objects:  12% (4/33)\u001b[K\rremote: Counting objects:  15% (5/33)\u001b[K\rremote: Counting objects:  18% (6/33)\u001b[K\rremote: Counting objects:  21% (7/33)\u001b[K\rremote: Counting objects:  24% (8/33)\u001b[K\rremote: Counting objects:  27% (9/33)\u001b[K\rremote: Counting objects:  30% (10/33)\u001b[K\rremote: Counting objects:  33% (11/33)\u001b[K\rremote: Counting objects:  36% (12/33)\u001b[K\rremote: Counting objects:  39% (13/33)\u001b[K\rremote: Counting objects:  42% (14/33)\u001b[K\rremote: Counting objects:  45% (15/33)\u001b[K\rremote: Counting objects:  48% (16/33)\u001b[K\rremote: Counting objects:  51% (17/33)\u001b[K\rremote: Counting objects:  54% (18/33)\u001b[K\rremote: Counting objects:  57% (19/33)\u001b[K\rremote: Counting objects:  60% (20/33)\u001b[K\rremote: Counting objects:  63% (21/33)\u001b[K\rremote: Counting objects:  66% (22/33)\u001b[K\rremote: Counting objects:  69% (23/33)\u001b[K\rremote: Counting objects:  72% (24/33)\u001b[K\rremote: Counting objects:  75% (25/33)\u001b[K\rremote: Counting objects:  78% (26/33)\u001b[K\rremote: Counting objects:  81% (27/33)\u001b[K\rremote: Counting objects:  84% (28/33)\u001b[K\rremote: Counting objects:  87% (29/33)\u001b[K\rremote: Counting objects:  90% (30/33)\u001b[K\rremote: Counting objects:  93% (31/33)\u001b[K\rremote: Counting objects:  96% (32/33)\u001b[K\rremote: Counting objects: 100% (33/33)\u001b[K\rremote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 44.84 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "4f830707-b352-4311-90ce-8d9cc200cf10"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " clean_datav5.csv\n",
            " clean_datav6.csv\n",
            " Data_without-invalid_cells.csv\n",
            " final_dataset.csv\n",
            " final_test2.csv\n",
            " final_test3_unpreprocessed.csv\n",
            " \u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/\n",
            " model_state_dict.pth\n",
            " README.md\n",
            " test_data_cleaned.csv\n",
            " \u001b[01;34mtest_images\u001b[0m/\n",
            "'test_labels_actual_2020-08-05 09:25:41.780430_.txt'\n",
            "'test_labels_actual_2020-08-05 09:42:13.492534_.txt'\n",
            "'test_labels_actual_2020-08-05 09:58:42.164284_.txt'\n",
            "'test_labels_actual_2020-08-05 10:18:08.183174_.txt'\n",
            "'test_labels_actual_2020-08-05 10:35:06.808663_.txt'\n",
            "'test_labels_actual_2020-08-05 10:52:04.703076_.txt'\n",
            "'test_labels_actual_2020-08-05 11:11:52.841732_.txt'\n",
            "'test_labels_actual_2020-08-05 11:27:55.324284_.txt'\n",
            "'test_labels_actual_2020-08-05 11:43:58.976716_.txt'\n",
            "'test_labels_pred_2020-08-05 09:25:41.780430_.txt'\n",
            "'test_labels_pred_2020-08-05 09:42:13.492534_.txt'\n",
            "'test_labels_pred_2020-08-05 09:58:42.164284_.txt'\n",
            "'test_labels_pred_2020-08-05 10:18:08.183174_.txt'\n",
            "'test_labels_pred_2020-08-05 10:35:06.808663_.txt'\n",
            "'test_labels_pred_2020-08-05 10:52:04.703076_.txt'\n",
            "'test_labels_pred_2020-08-05 11:11:52.841732_.txt'\n",
            "'test_labels_pred_2020-08-05 11:27:55.324284_.txt'\n",
            "'test_labels_pred_2020-08-05 11:43:58.976716_.txt'\n",
            " test_tweet_2.csv\n",
            " \u001b[01;34mtrain_images\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d83fcc00-fa99-4457-f8e6-d21b4c7d23d5"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b3c57cd1-318d-4c2c-ec8a-297b254d0df4"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "45ff47d4-36a3-47b6-ed29-d2f658ae695a"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "e30b3d77-75b4-41dd-b69b-343e0f09a697"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9c71af4f-46c4-491a-affe-3d63e60f78f6"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1869b1d5-4e58-422b-f360-9c7364e91842"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "4b30e373-25f6-4d85-d0fb-05d8dc6a9895"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5b3a788b-8872-4ca7-98eb-9509d2aafb1f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    #MeToo firestorm consumes Bollywood and Indian...\n",
              "1    🌿MODERN-DAY PARENTING REQUISITES 🌿   🐚#METOO R...\n",
              "2    One more #MeToo moment  #MeTooIndia #MeToo #Me...\n",
              "3    Amidst all the #metoo stories that give women ...\n",
              "4    ICYMI... #AsiaBibi was jailed over 9 years ago...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ed0ee5b1-f351-495c-a4d8-e96dacb19e35"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "365f6dcb-be49-4f89-9e0b-573a65ac7f57"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b1c2824-b9d4-4291-b8a7-def66c6a1cbf"
      },
      "source": [
        "col_name = \"Oppose\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762,
          "referenced_widgets": [
            "77b4ae6f005e4049a3a35ccec5f3c24a",
            "8183da60bc7443e2a68bd0f587771b12",
            "c47bdcdda8934e54992e9fff0c4e7b1b",
            "113ac2dbde6a42da87565a0f5bfc3096",
            "3231f81ef81b4d67907b555971b569ae",
            "047a18d06f3341ab8fd2b4c2f852e7c7",
            "2f65b9fb20e84355b1957c9ebc95d456",
            "af9e56611ad843819e33b146220d1f78",
            "4960ce30ac2c4c489d2b7a87aaa2c299",
            "ed8f10a89e144f70acf0e58e92498e8c",
            "9e5097841e1445168c018f841bb99f8a",
            "d272bb5be8f24a1aab8c8c24acb03546",
            "c7ff48c19bb143f1a2decc973604e947",
            "4dce7745b96d4e57b317fb3416aad7dd",
            "80423066aecb466eae7c95302868c8cf",
            "0d69272d63c14652a1a0dd4f9d44a24b",
            "c3d8edc773734c1f8c6328839d0ddf9c",
            "102ccdf113ac401399f22a6e10a2f861",
            "447544f6552a4e9aac9be0ff89281de4",
            "5aa7f83117ae4634b6b720046a2354de",
            "5a490d1d16234448aaa8568a9083b6f3",
            "d009f0f572b44d69b76256b9f52edbc2",
            "123a0c983ff74a22a159b3bd896c6207",
            "d7157adb0a1d47e5a94460d62ce8793c"
          ]
        },
        "outputId": "cf7bf2ca-3b7c-4389-a04d-40f1ab27df92"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 7358\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77b4ae6f005e4049a3a35ccec5f3c24a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1839.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1185\n",
            "Train Losses : [0.20030198991298676, 0.2894008755683899, 0.19283778965473175, 6.127965927124023, 1.238962173461914, 2.565951347351074, 0.0517415888607502, 0.6931618452072144, 0.5874079465866089, 1.0194578170776367, 0.6844848990440369, 0.20225146412849426, 0.05663127452135086, 0.26755619049072266, 0.5593166351318359, 0.6372483372688293, 0.3228079676628113, 0.4293789863586426, 0.15517178177833557, 0.4031321406364441, 0.017664393410086632, 0.4361806809902191, 0.10703485459089279, 0.11141874641180038, 0.07726117968559265, 0.0881163477897644, 0.19087523221969604, 0.09932585060596466, 0.1914532482624054, 0.04403705149888992, 0.0384589321911335, 0.06628484278917313, 0.3602348864078522, 0.0415019616484642, 0.024838758632540703, 0.08380663394927979, 0.005214669741690159, 1.0801359415054321, 0.009744689799845219, 0.011203610338270664, 0.010704231448471546, 0.514423668384552, 1.4819592237472534, 0.48390135169029236, 0.3299810290336609, 0.007680876180529594, 0.5734031200408936, 0.012194626033306122, 0.0146105345338583, 0.24879521131515503, 0.08935918658971786, 0.38679444789886475, 0.07199589163064957, 0.04635867476463318, 0.004305246286094189, 0.08898485451936722, 0.1618705540895462, 0.04730277881026268, 0.4264977276325226, 0.3189694881439209, 0.025299077853560448, 0.03853410482406616, 0.35571640729904175, 0.0993926152586937, 0.2563149929046631, 0.01877308078110218, 0.055991582572460175, 0.049627725034952164, 0.019402045756578445, 0.20110400021076202, 0.09611254930496216, 0.0798969492316246, 0.042950551956892014, 0.016555437818169594, 0.013408047147095203, 0.02069338969886303, 0.27154067158699036, 0.033997152000665665, 0.18677875399589539, 0.704606831073761, 0.33379924297332764, 0.013682917691767216, 0.5008460879325867, 0.009926464408636093, 0.011827844195067883, 0.415147602558136, 0.011268855072557926, 0.12100020796060562, 0.01279908511787653, 0.17943067848682404, 0.5715984106063843, 0.01535166148096323, 0.05375971272587776, 0.015671707689762115, 0.0885743647813797, 0.01774291880428791, 0.022905664518475533, 0.06931758671998978, 0.09563332796096802, 0.35837218165397644, 0.03040977381169796, 0.7224318981170654, 0.3699054718017578, 0.10540776699781418, 0.036375727504491806, 0.07877145707607269, 0.1891210973262787, 0.22309686243534088, 0.04316185414791107, 0.5354282259941101, 0.03784177079796791, 0.04496070742607117, 0.17449326813220978, 0.33235445618629456, 0.2837007939815521, 0.13625727593898773, 0.1527612954378128, 0.05324399098753929, 0.3312462568283081, 0.13660569489002228, 0.05202450975775719, 0.05261677876114845, 0.12721528112888336, 0.050072915852069855, 0.04900360107421875, 0.405311644077301, 0.05076288431882858, 0.04476064071059227, 0.04479019716382027, 0.12779150903224945, 0.05777771398425102, 0.033870115876197815, 0.03495396301150322, 0.056060705333948135, 0.09348519146442413, 0.025782156735658646, 0.04348205402493477, 0.23260797560214996, 0.07604435086250305, 0.24390004575252533, 0.024785416200757027, 0.20379158854484558, 0.01891370490193367, 0.12764278054237366, 0.06308456510305405, 0.020226506516337395, 0.11784259974956512, 0.018310891464352608, 0.019816456362605095, 0.020636826753616333, 0.02143939398229122, 0.020490022376179695, 0.02092345617711544, 0.014256265945732594, 0.24445973336696625, 0.7604807019233704, 0.021860718727111816, 0.1509278118610382, 0.238817498087883, 0.2861608564853668, 0.15668292343616486, 0.09281127899885178, 0.16659928858280182, 0.012094314210116863, 0.013016301207244396, 0.06512359529733658, 0.015348697081208229, 0.01638764701783657, 0.013180251233279705, 0.06328755617141724, 0.09452608972787857, 0.3713415861129761, 0.1313643753528595, 0.4832157790660858, 0.012481656856834888, 0.255953848361969, 0.3013599216938019, 0.15980121493339539, 0.017881540581583977, 0.015134823508560658, 0.017221933230757713, 0.1595233678817749, 0.017731603235006332, 0.15464124083518982, 0.0229497067630291, 0.018522843718528748, 0.023698629811406136, 0.09411515295505524, 0.01793164387345314, 0.0724308043718338, 0.27270713448524475, 0.20132353901863098, 0.017834316939115524, 0.8141067028045654, 0.09417162090539932, 0.20402438938617706, 0.026925155892968178, 0.030517134815454483, 0.24867959320545197, 0.3073357045650482, 0.45783254504203796, 0.037996988743543625, 0.07311295717954636, 0.3309899568557739, 0.24952425062656403, 0.12436579167842865, 0.11718633770942688, 0.2609077990055084, 0.0548790879547596, 0.44082626700401306, 0.061012737452983856, 0.06499659270048141, 0.12249966710805893, 0.09158991277217865, 0.07782737910747528, 0.12257885932922363, 0.07352012395858765, 0.10662999749183655, 0.1313253492116928, 0.38251781463623047, 0.26121535897254944, 0.3386767506599426, 0.16023989021778107, 0.198081374168396, 0.10018745064735413, 0.11966437101364136, 0.3380464017391205, 0.10493879020214081, 0.1299576610326767, 0.10913663357496262, 0.12210576236248016, 0.12121647596359253, 0.11839941889047623, 0.11098482459783554, 0.09910716861486435, 0.2697530686855316, 0.09235459566116333, 0.10067392885684967, 0.10590136051177979, 0.1577185094356537, 0.4083017408847809, 0.08641757071018219, 0.07922384142875671, 0.06922316551208496, 0.06705529242753983, 0.3172041177749634, 0.06272079050540924, 0.15061067044734955, 0.20115120708942413, 0.10108138620853424, 0.05663410574197769, 0.05915284901857376, 0.14437294006347656, 0.20479018986225128, 0.37109681963920593, 0.12468384951353073, 0.04362749680876732, 0.04485699534416199, 0.09703397005796432, 0.28025874495506287, 0.3157305419445038, 0.13636872172355652, 0.0418083518743515, 0.03981276601552963, 0.18143223226070404, 0.04738494008779526, 0.04026291146874428, 0.29572048783302307, 0.30912867188453674, 0.03733467310667038, 0.1416671872138977, 0.23831038177013397, 0.04047877714037895, 0.04567964747548103, 0.10954183340072632, 0.21542207896709442, 0.04143154248595238, 0.1029328778386116, 0.03481798246502876, 0.029322026297450066, 0.030457541346549988, 0.03064749576151371, 0.02651004120707512, 0.029671253636479378, 0.025204481557011604, 0.022780608385801315, 0.0226841252297163, 0.15709206461906433, 0.10514064133167267, 0.13080385327339172, 0.15741083025932312, 0.01696152053773403, 0.016036445274949074, 0.015116449445486069, 0.11538518965244293, 0.015505854971706867, 0.013203843496739864, 0.012669900432229042, 0.01275578048080206, 0.3411251902580261, 0.1547773778438568, 0.011021817103028297, 0.011527391150593758, 0.012005697935819626, 0.1112508550286293, 0.33204469084739685, 0.34308677911758423, 0.25671330094337463, 0.01274681743234396, 0.012767117470502853, 0.10458554327487946, 0.15230432152748108, 0.12131305783987045, 0.09328536689281464, 0.01729261875152588, 0.1436447650194168, 0.10923681408166885, 0.49263888597488403, 0.018103381618857384, 0.022052297368645668, 0.2599201500415802, 0.02035444602370262, 0.13158515095710754, 0.024568693712353706, 0.11835040152072906, 0.453708678483963, 0.13269281387329102, 0.21046234667301178, 0.12179935723543167, 0.030752401798963547, 0.20712505280971527, 0.03423475846648216, 0.2738284766674042, 0.15779860317707062, 0.03404321148991585, 0.03598904237151146, 0.12712238729000092, 0.07895100861787796, 0.10866640508174896, 0.03430451452732086, 0.03541458770632744, 0.24555854499340057, 0.03250329568982124, 0.26861318945884705, 0.0334056131541729, 0.03191124647855759, 0.032269466668367386, 0.2007511705160141, 0.09541043639183044, 0.030128709971904755, 0.03044215217232704, 0.3607918322086334, 0.21060718595981598, 0.16493569314479828, 0.14912770688533783, 0.12308573722839355, 0.13977016508579254, 0.028963500633835793, 0.255698025226593, 0.10714388638734818, 0.2532272934913635, 0.02959733083844185, 0.10630165040493011, 0.02996118552982807, 0.031289611011743546, 0.1394226849079132, 0.07758544385433197, 0.10111308097839355, 0.029392113909125328, 0.029053768143057823, 0.02985338307917118, 0.24809755384922028, 0.02713555470108986, 0.07454662770032883, 0.10947869718074799, 0.10831014811992645, 0.12305551767349243, 0.025470802560448647, 0.024575145915150642, 0.023526985198259354, 0.0708509236574173, 0.022607047110795975, 0.09157906472682953, 0.08022603392601013, 0.12262735515832901, 0.1034984141588211, 0.08522944897413254, 0.01852123811841011, 0.08061297982931137, 0.0664898008108139, 0.09779238700866699, 0.01688108965754509, 0.12126314640045166, 0.016047513112425804, 0.07890012860298157, 0.016326462849974632, 0.07550519704818726, 0.04554784670472145, 0.09329945594072342, 0.014152363874018192, 0.05789472162723541, 0.07092244923114777, 0.13526004552841187, 0.013459025882184505, 0.015100116841495037, 0.014018111862242222, 0.013758152723312378, 0.0486854687333107, 0.012300350703299046, 0.10479728877544403, 0.11961624026298523, 0.01209004782140255, 0.1427045464515686, 0.011047090403735638, 0.2773025333881378, 0.11831127107143402, 0.012024071998894215, 0.5630325078964233, 0.01175863016396761, 0.2937884032726288, 0.015496719628572464, 0.33410847187042236, 0.11577891558408737, 0.17813818156719208, 0.018275806680321693, 0.017288126051425934, 0.018970707431435585, 0.018088387325406075, 0.020323878154158592, 0.09516837447881699, 0.14563505351543427, 0.01883123256266117, 0.019292259588837624, 0.08131163567304611, 0.10751138627529144, 0.11328151077032089, 0.27740806341171265, 0.2924722135066986, 0.15659847855567932, 0.12410097569227219, 0.5206660628318787, 0.10816286504268646, 0.11887779086828232, 0.23752911388874054, 0.19751504063606262, 0.03091627173125744, 0.1361956000328064, 0.1052779108285904, 0.1792498528957367, 0.03410756587982178, 0.03289160504937172, 0.2714270353317261, 0.03281529247760773, 0.034811489284038544, 0.096329465508461, 0.17471040785312653, 0.2270244061946869, 0.1431901901960373, 0.2279742956161499, 0.10468775033950806, 0.0350087434053421, 0.10656099021434784, 0.038099147379398346, 0.07947009801864624, 0.08851499110460281, 0.10886197537183762, 0.07138440757989883, 0.03483659774065018, 0.034811533987522125, 0.12642596662044525, 0.12240386009216309, 0.03105139173567295, 0.10283048450946808, 0.029184570536017418, 0.23149332404136658, 0.027051126584410667, 0.15365347266197205, 0.03155665099620819, 0.09661529958248138, 0.08930642902851105, 0.33343204855918884, 0.08437871932983398, 0.10475321114063263, 0.28662922978401184, 0.06068940460681915, 0.026203475892543793, 0.027919990941882133, 0.14893510937690735, 0.10556666553020477, 0.19787020981311798, 0.026213180273771286, 0.023347439244389534, 0.1576310098171234, 0.027672380208969116, 0.02516716532409191, 0.20076869428157806, 0.36631494760513306, 0.5385191440582275, 0.025914467871189117, 0.02747117355465889, 0.11430158466100693, 0.06769001483917236, 0.07142594456672668, 0.029451996088027954, 0.030548933893442154, 0.08397386968135834, 0.03509422764182091, 0.22149571776390076, 0.29630905389785767, 0.030865058302879333, 0.09835659712553024, 0.03192644566297531, 0.0803094282746315, 0.03041353076696396, 0.15382695198059082, 0.12686903774738312, 0.03224462643265724, 0.035434067249298096, 0.03197237476706505, 0.10173079371452332, 0.02806910127401352, 0.02967783994972706, 0.08081429451704025, 0.2257179319858551, 0.025494983419775963, 0.02610812522470951, 0.11335401237010956, 0.14868910610675812, 0.2146216332912445, 0.023826466873288155, 0.16838456690311432, 0.08997847884893417, 0.022050783038139343, 0.023284099996089935, 0.10583086311817169, 0.09915284067392349, 0.1493373066186905, 0.020203422755002975, 0.291680246591568, 0.023189133033156395, 0.15007707476615906, 0.06363973021507263, 0.021285144612193108, 0.22185297310352325, 0.23258699476718903, 0.10749629139900208, 0.02100999280810356, 0.13092108070850372, 0.02043413370847702, 0.021428020671010017, 0.020794078707695007, 0.02460688166320324, 0.1027921810746193, 0.1876453310251236, 0.14079643785953522, 0.07049884647130966, 0.02656879648566246, 0.020317867398262024, 0.021295001730322838, 0.27961456775665283, 0.07844725251197815, 0.0738435685634613, 0.020342126488685608, 0.24397562444210052, 0.01843111962080002, 0.01992953196167946, 0.07034821063280106, 0.07386818528175354, 0.02334832027554512, 0.1390492469072342, 0.019993193447589874, 0.0192096009850502, 0.11833913624286652, 0.06567103415727615, 0.5334324836730957, 0.06530453264713287, 0.40192002058029175, 0.12683138251304626, 0.12285555899143219, 0.29436594247817993, 0.024676019325852394, 0.12459506094455719, 0.1807493418455124, 0.1248052567243576, 0.027445267885923386, 0.029155222699046135, 0.19465506076812744, 0.19059571623802185, 0.05420835316181183, 0.11117082089185715, 0.25401586294174194, 0.03313751146197319, 0.03251243382692337, 0.18276745080947876, 0.32520437240600586, 0.03167412430047989, 0.033749524503946304, 0.03331269696354866, 0.3226954936981201, 0.17253130674362183, 0.11595511436462402, 0.0325741283595562, 0.03103410080075264, 0.2560712397098541, 0.22257928550243378, 0.033352091908454895, 0.03161529079079628, 0.09427044540643692, 0.10346444696187973, 0.07571762800216675, 0.14339298009872437, 0.1708381325006485, 0.14901547133922577, 0.030214037746191025, 0.17205633223056793, 0.12054700404405594, 0.16762419044971466, 0.09318826347589493, 0.07346709817647934, 0.13836626708507538, 0.13280640542507172, 0.28465625643730164, 0.12918229401111603, 0.339991956949234, 0.03356103599071503, 0.08884915709495544, 0.13669858872890472, 0.13693106174468994, 0.03197723627090454, 0.031597886234521866, 0.031615499407052994, 0.030351657420396805, 0.029260708019137383, 0.0858786329627037, 0.07833496481180191, 0.02808493562042713, 0.12142287194728851, 0.1471497267484665, 0.10356611758470535, 0.0237866323441267, 0.2759421169757843, 0.02257736772298813, 0.2808079421520233, 0.10072058439254761, 0.07417004555463791, 0.08588693290948868, 0.11324843764305115, 0.458260178565979, 0.02363729476928711, 0.15007318556308746, 0.2678239643573761, 0.02671578899025917, 0.10583310574293137, 0.11552788317203522, 0.08918873965740204, 0.21195891499519348, 0.12364965677261353, 0.0861373320221901, 0.10715320706367493, 0.12834608554840088, 0.034964803606271744, 0.03243261203169823, 0.13175146281719208, 0.03249339014291763, 0.2153126299381256, 0.11784692853689194, 0.11169488728046417, 0.08824696391820908, 0.12915846705436707, 0.03169441595673561, 0.03161751106381416, 0.21981073915958405, 0.12741777300834656, 0.02908993698656559, 0.10526464134454727, 0.10856794565916061, 0.10635675489902496, 0.15155620872974396, 0.027692176401615143, 0.25532621145248413, 0.11929953098297119, 0.11393053829669952, 0.11702261865139008, 0.4973078668117523, 0.026673637330532074, 0.027497893199324608, 0.09945601224899292, 0.029363015666604042, 0.02848333865404129, 0.029074694961309433, 0.22763124108314514, 0.028206564486026764, 0.12756642699241638, 0.028102649375796318, 0.027848346158862114, 0.19492891430854797, 0.027673516422510147, 0.08962509781122208, 0.13603998720645905, 0.25345006585121155, 0.20545198023319244, 0.06930404156446457, 0.027994336560368538, 0.028328290209174156, 0.02839406579732895, 0.027800127863883972, 0.02759426087141037, 0.027496103197336197, 0.026285139843821526, 0.14646503329277039, 0.1499536633491516, 0.02417435497045517, 0.09501975029706955, 0.1644919365644455, 0.022564392536878586, 0.021463975310325623, 0.29388222098350525, 0.11669378727674484, 0.2509748935699463, 0.02052747830748558, 0.09417541325092316, 0.09872983396053314, 0.02105863206088543, 0.02077360451221466, 0.08864818513393402, 0.02131034806370735, 0.16051194071769714, 0.10059855133295059, 0.09365321695804596, 0.12596026062965393, 0.019271139055490494, 0.01872261054813862, 0.09539946168661118, 0.09680886566638947, 0.017805442214012146, 0.07882387936115265, 0.26509079337120056, 0.10870807617902756, 0.01735652983188629, 0.32529956102371216, 0.10247420519590378, 0.11099588871002197, 0.5039958953857422, 0.10043256729841232, 0.20962610840797424, 0.022941553965210915, 0.02411617338657379, 0.025113575160503387, 0.31173044443130493, 0.11068662256002426, 0.028558403253555298, 0.029324132949113846, 0.03031657636165619, 0.11140551418066025, 0.030255300924181938, 0.030435118824243546, 0.09961705654859543, 0.029376430436968803, 0.028827501460909843, 0.028516653925180435, 0.027899011969566345, 0.09476055204868317, 0.026138965040445328, 0.0872817412018776, 0.44218024611473083, 0.02468096651136875, 0.025229036808013916, 0.024742770940065384, 0.2898637652397156, 0.02494918927550316, 0.024852540343999863, 0.1062895655632019, 0.10422378033399582, 0.024552658200263977, 0.11868851631879807, 0.1240672841668129, 0.02375246211886406, 0.2649432122707367, 0.023894095793366432, 0.08742295205593109, 0.08586003631353378, 0.09548420459032059, 0.2186189442873001, 0.12138126790523529, 0.07587111741304398, 0.1098100021481514, 0.07562929391860962, 0.07847299426794052, 0.24849088490009308, 0.02503734454512596, 0.09154126048088074, 0.10690875351428986, 0.12490925192832947, 0.024920666590332985, 0.12237565219402313, 0.02514694631099701, 0.2615610659122467, 0.0947956070303917, 0.08510143309831619, 0.10192686319351196, 0.26340252161026, 0.025534383952617645, 0.13934318721294403, 0.025786934420466423, 0.08125180006027222, 0.08437428623437881, 0.025655632838606834, 0.026171473786234856, 0.16403836011886597, 0.024970151484012604, 0.12052319943904877, 0.0256720669567585, 0.02675582841038704, 0.025964640080928802, 0.16910295188426971, 0.025703221559524536, 0.023776356130838394, 0.022969160228967667, 0.25866618752479553, 0.09654711931943893, 0.2717302739620209, 0.022340022027492523, 0.02045074850320816, 0.02018677070736885, 0.020421039313077927, 0.020278003066778183, 0.12451574951410294, 0.019778797402977943, 0.1745785027742386, 0.019034171476960182, 0.31716057658195496, 0.019297095015645027, 0.019442472606897354, 0.020625606179237366, 0.154229074716568, 0.32162243127822876, 0.10862905532121658, 0.020122172310948372, 0.1485190987586975, 0.10295244306325912, 0.11248527467250824, 0.1421668380498886, 0.020187387242913246, 0.020544445142149925, 0.06660354882478714, 0.15747179090976715, 0.16340050101280212, 0.26697129011154175, 0.02017255127429962, 0.20034390687942505, 0.0815005823969841, 0.11107749491930008, 0.022059131413698196, 0.2152991145849228, 0.022328771650791168, 0.09744663536548615, 0.02279750071465969, 0.12782150506973267, 0.09050992876291275, 0.2823774814605713, 0.14148715138435364, 0.023814823478460312, 0.1899455338716507, 0.025462450459599495, 0.10248010605573654, 0.026088111102581024, 0.02560276724398136, 0.129966139793396, 0.13408824801445007, 0.3166328966617584, 0.026006009429693222, 0.0265427827835083, 0.15881575644016266, 0.025669943541288376, 0.12157291173934937, 0.025113113224506378, 0.025530407205224037, 0.0727686733007431, 0.024425683543086052, 0.10951483249664307, 0.14844688773155212, 0.30048373341560364, 0.23677165806293488, 0.2315623164176941, 0.0237947516143322, 0.024067115038633347, 0.024700969457626343, 0.10820318758487701, 0.13118860125541687, 0.09787332266569138, 0.025186607614159584, 0.11218858510255814, 0.12758883833885193, 0.2042594999074936, 0.025109440088272095, 0.024585869163274765, 0.024623163044452667, 0.0790904238820076, 0.024145713075995445, 0.0236543919891119, 0.023256544023752213, 0.14129357039928436, 0.29338863492012024, 0.1090681254863739, 0.08961324393749237, 0.10250987857580185, 0.08436988294124603, 0.021931996569037437, 0.14165057241916656, 0.10413265973329544, 0.12325876951217651, 0.0909433513879776, 0.020809821784496307, 0.11630283296108246, 0.12136389315128326, 0.01998906396329403, 0.27472859621047974, 0.09495809674263, 0.020088884979486465, 0.01989653892815113, 0.26815298199653625, 0.01989130489528179, 0.019968941807746887, 0.02019190974533558, 0.1185021698474884, 0.13616327941417694, 0.08439692109823227, 0.10104479640722275, 0.01944155991077423, 0.11173123866319656, 0.019848715513944626, 0.11680403351783752, 0.10931142419576645, 0.018322335556149483, 0.01783614046871662, 0.11407440155744553, 0.1302090436220169, 0.08977179229259491, 0.10971473902463913, 0.11088594049215317, 0.1156170591711998, 0.01647445373237133, 0.09858720004558563, 0.07992566376924515, 0.2617926001548767, 0.24960771203041077, 0.22826921939849854, 0.11477723717689514, 0.10963251441717148, 0.27430832386016846, 0.12224473059177399, 0.02161579020321369, 0.022228987887501717, 0.10756626725196838, 0.02390991896390915, 0.02396666631102562, 0.02400832436978817, 0.02383761666715145, 0.023812226951122284, 0.023548100143671036, 0.023223336786031723, 0.022961052134633064, 0.022112812846899033, 0.1015050858259201, 0.1010720506310463, 0.21813514828681946, 0.13401015102863312, 0.12091556191444397, 0.2729150354862213, 0.09221246838569641, 0.09442190825939178, 0.0966508761048317, 0.10039348155260086, 0.021628566086292267, 0.08887644112110138, 0.1209319606423378, 0.28744232654571533, 0.022633733227849007, 0.02230389416217804, 0.022528700530529022, 0.022400062531232834, 0.22256658971309662, 0.02255301922559738, 0.20116113126277924, 0.11858334392309189, 0.02356581576168537, 0.0922364741563797, 0.2257695347070694, 0.2488856166601181, 0.08970841765403748, 0.10970466583967209, 0.08368479460477829, 0.22173483669757843, 0.27867335081100464, 0.10711219906806946, 0.03073195181787014, 0.03195195645093918, 0.14733704924583435, 0.0329136960208416, 0.07298330962657928, 0.23481273651123047, 0.21492482721805573, 0.09694527834653854, 0.09455470740795135, 0.035944387316703796, 0.036161623895168304, 0.036585550755262375, 0.0350932739675045, 0.03490694612264633, 0.033769991248846054, 0.03394811972975731, 0.03188952058553696, 0.031651582568883896, 0.02959969826042652, 0.02786317467689514, 0.02716730907559395, 0.10361223667860031, 0.23808066546916962, 0.44861236214637756, 0.024277476593852043, 0.11740604788064957, 0.02457222156226635, 0.025047950446605682, 0.2596694231033325, 0.09829937666654587, 0.06147417798638344, 0.07900100946426392, 0.11526060104370117, 0.16078154742717743, 0.11916464567184448, 0.25889354944229126, 0.27829983830451965, 0.027518387883901596, 0.0279147457331419, 0.028330454602837563, 0.4549059569835663, 0.029698792845010757, 0.07626305520534515, 0.3748517334461212, 0.03368210420012474, 0.11389423906803131, 0.036451708525419235, 0.03714334964752197, 0.16537705063819885, 0.18119879066944122, 0.10293441265821457, 0.09215067327022552, 0.24988971650600433, 0.1389145404100418, 0.044196031987667084, 0.04479698836803436, 0.10916560888290405, 0.04511550813913345, 0.044588588178157806, 0.1081290990114212, 0.04254574701189995, 0.041609782725572586, 0.11171888560056686, 0.10674764215946198, 0.12611670792102814, 0.03592800348997116, 0.03508702665567398, 0.4092909097671509, 0.17040270566940308, 0.03349348530173302, 0.11267358809709549, 0.12965472042560577, 0.11486315727233887, 0.10575226694345474, 0.1380338817834854, 0.030377881601452827, 0.10248368978500366, 0.15287014842033386, 0.029153259471058846, 0.23544920980930328, 0.028184501454234123, 0.027369627729058266, 0.25267139077186584, 0.09776104241609573, 0.12418637424707413, 0.027655502781271935, 0.0936802476644516, 0.12763634324073792, 0.10057815909385681, 0.266743540763855, 0.025450924411416054, 0.025528689846396446, 0.10141800343990326, 0.025450285524129868, 0.10268010199069977, 0.07581786811351776, 0.09112231433391571, 0.06959138810634613, 0.023359673097729683, 0.11368762701749802, 0.09676641970872879, 0.24960987269878387, 0.022750496864318848, 0.02273690700531006, 0.12209168821573257, 0.022431612014770508, 0.0841401070356369, 0.23457038402557373, 0.022060146555304527, 0.02140527218580246, 0.02125997468829155, 0.08311125636100769, 0.09161089360713959, 0.10106281191110611, 0.020446065813302994, 0.02057081274688244, 0.02024856023490429, 0.2897607684135437, 0.08874427527189255, 0.10741255432367325, 0.22770772874355316, 0.30003294348716736, 0.27205729484558105, 0.02135438472032547, 0.28065910935401917, 0.09409184753894806, 0.08143488317728043, 0.08194179087877274, 0.026459544897079468, 0.02799648977816105, 0.0981254056096077, 0.24987950921058655, 0.16592925786972046, 0.029907429590821266, 0.030621802434325218, 0.03051847033202648, 0.03287070244550705, 0.3032675087451935, 0.11734414845705032, 0.03242890164256096, 0.2339496612548828, 0.11784947663545609, 0.13386550545692444, 0.13510632514953613, 0.033263497054576874, 0.10077899694442749, 0.1298210769891739, 0.033494822680950165, 0.12726818025112152, 0.090830497443676, 0.12051790207624435, 0.08639178425073624, 0.029826615005731583, 0.028994252905249596, 0.028478533029556274, 0.2381795197725296, 0.026712562888860703, 0.15737761557102203, 0.025891361758112907, 0.13385802507400513, 0.21560423076152802, 0.02602972835302353, 0.10434625297784805, 0.024090366438031197, 0.26740455627441406, 0.09426485002040863, 0.269484281539917, 0.09182213246822357, 0.13671717047691345, 0.10151071101427078, 0.02536873333156109, 0.11910033971071243, 0.02500045858323574, 0.2525125741958618, 0.025260355323553085, 0.025334035977721214, 0.10809557884931564, 0.1237969920039177, 0.02504374086856842, 0.024661092087626457, 0.024832550436258316, 0.08921575546264648, 0.0970393493771553, 0.2883550226688385, 0.02307458035647869, 0.02341809868812561, 0.0232298094779253, 0.022858822718262672, 0.25848570466041565, 0.08947820216417313, 0.10684646666049957, 0.12136513739824295, 0.23671017587184906, 0.41146326065063477, 0.08524835109710693, 0.024402998387813568, 0.16115900874137878, 0.02743552252650261, 0.09933917224407196, 0.09638474136590958, 0.027450203895568848, 0.02783934772014618, 0.027711689472198486, 0.08982760459184647, 0.027692260220646858, 0.246477872133255, 0.028359094634652138, 0.10643486678600311, 0.0973019152879715, 0.026891088113188744, 0.0836353600025177, 0.08583678305149078, 0.08991523832082748, 0.08477017283439636, 0.026157861575484276, 0.025445973500609398, 0.025047043338418007, 0.26655077934265137, 0.09950190037488937, 0.0232878215610981, 0.22836533188819885, 0.10921099036931992, 0.10611389577388763, 0.1315714567899704, 0.024332229048013687, 0.3020161986351013, 0.10449864715337753, 0.11283986270427704, 0.2270335704088211, 0.02461758255958557, 0.025065774098038673, 0.22511383891105652, 0.025618622079491615, 0.026896320283412933, 0.20970837771892548, 0.09659474343061447, 0.026751957833766937, 0.027495937421917915, 0.02834307961165905, 0.09585826843976974, 0.026629623025655746, 0.1956903636455536, 0.02770095504820347, 0.21940898895263672, 0.08647950738668442, 0.026557747274637222, 0.19289618730545044, 0.027590947225689888, 0.2586565315723419, 0.11516787111759186, 0.02867821604013443, 0.030236808583140373, 0.07259208709001541, 0.028626825660467148, 0.097666896879673, 0.029153086245059967, 0.09259898215532303, 0.029444951564073563, 0.026566149666905403, 0.026349833235144615, 0.025165000930428505, 0.14557220041751862, 0.02519851364195347, 0.2132595181465149, 0.1690034121274948, 0.022828061133623123, 0.10549551993608475, 0.022841326892375946, 0.023169158026576042, 0.1272554099559784, 0.02162141725420952, 0.0997297391295433, 0.17126740515232086, 0.10942493379116058, 0.021006396040320396, 0.01976269669830799, 0.13143940269947052, 0.3329446017742157, 0.22738362848758698, 0.07178989797830582, 0.6142596006393433, 0.021999111399054527, 0.021771900355815887, 0.11689479649066925, 0.024219831451773643, 0.1042298674583435, 0.09838898479938507, 0.11323872953653336, 0.10061015188694, 0.026138538494706154, 0.026254117488861084, 0.23325863480567932, 0.13013306260108948, 0.18228192627429962, 0.026992179453372955, 0.02785855531692505, 0.13246247172355652, 0.026920994743704796, 0.19158044457435608, 0.09978049993515015, 0.10598795115947723, 0.026376349851489067, 0.11153416335582733, 0.02460589073598385, 0.2703443467617035, 0.12369807809591293, 0.023857003077864647, 0.02545236051082611, 0.33300870656967163, 0.23169420659542084, 0.023912936449050903, 0.1327713578939438, 0.12628169357776642, 0.024550234898924828, 0.025738900527358055, 0.1251581311225891, 0.09059759229421616, 0.24748516082763672, 0.10204977542161942, 0.10608923435211182, 0.11002493649721146, 0.09978335350751877, 0.2672813832759857, 0.02590927481651306, 0.11111128330230713, 0.10741056501865387, 0.26519620418548584, 0.10825110971927643, 0.026925692334771156, 0.15843527019023895, 0.41988691687583923, 0.028659868985414505, 0.029770378023386, 0.28002506494522095, 0.1900710165500641, 0.0331367552280426, 0.03400193899869919, 0.14606550335884094, 0.03519584238529205, 0.03524429723620415, 0.256453275680542, 0.11123469471931458, 0.1034831628203392, 0.12524424493312836, 0.20867498219013214, 0.11620347946882248, 0.10844992101192474, 0.11916831880807877, 0.038094744086265564, 0.03697800636291504, 0.14029502868652344, 0.03582536801695824, 0.20620302855968475, 0.23603598773479462, 0.03497808426618576, 0.034957222640514374, 0.03576885908842087, 0.25146228075027466, 0.03388752043247223, 0.09860816597938538, 0.033803459256887436, 0.09969688206911087, 0.12096908688545227, 0.03164400905370712, 0.031014636158943176, 0.03038565255701542, 0.029518622905015945, 0.028691133484244347, 0.11342258751392365, 0.48632776737213135, 0.20904816687107086, 0.10369183868169785, 0.028141461312770844, 0.24178986251354218, 0.028594130650162697, 0.11878277361392975, 0.09883188456296921, 0.12045325338840485, 0.02962404489517212, 0.029611913487315178, 0.08406433463096619, 0.21969181299209595, 0.1119251698255539, 0.10389196872711182, 0.07844526320695877, 0.12059924751520157, 0.10428197681903839, 0.10796302556991577, 0.02931760624051094, 0.1094861552119255, 0.028149647638201714, 0.02778693661093712, 0.027312278747558594, 0.10360442847013474, 0.08236096054315567, 0.025745511054992676, 0.10341440886259079, 0.21751955151557922, 0.024129044264554977, 0.24723868072032928, 0.11805909872055054, 0.10785748809576035, 0.023797787725925446, 0.023704206570982933, 0.023265743628144264, 0.13397012650966644, 0.023379305377602577, 0.16334913671016693, 0.02191016636788845, 0.12187649309635162, 0.021135520190000534, 0.09454676508903503, 0.08953353762626648, 0.021104663610458374, 0.12676140666007996, 0.08640159666538239, 0.09945158660411835, 0.018353497609496117, 0.08986459672451019, 0.2897823452949524, 0.11301738768815994, 0.017929431051015854, 0.09451641887426376, 0.017698731273412704, 0.018307600170373917, 0.01766633801162243, 0.28950977325439453, 0.11773975938558578, 0.10228153318166733, 0.11743409186601639, 0.11841870099306107, 0.017662296071648598, 0.09709274023771286, 0.1170690581202507, 0.01779869757592678, 0.017615724354982376, 0.10531150549650192, 0.0990690141916275, 0.25207459926605225, 0.01786595582962036, 0.017870966345071793, 0.2843749523162842, 0.01775522716343403, 0.0756559818983078, 0.10601625591516495, 0.018711693584918976, 0.10041683167219162, 0.09085037559270859, 0.5149298906326294, 0.249955952167511, 0.020778074860572815, 0.2890477776527405, 0.1288405805826187, 0.024358831346035004, 0.1119309663772583, 0.1109689250588417, 0.1123017966747284, 0.24026234447956085, 0.1037866622209549, 0.029628073796629906, 0.11086467653512955, 0.030956240370869637, 0.1309421807527542, 0.2502869665622711, 0.137870192527771, 0.11995650082826614, 0.03269943967461586, 0.032933611422777176, 0.09948405623435974, 0.10390561074018478, 0.032475803047418594, 0.23020431399345398, 0.2307800054550171, 0.03243700787425041, 0.10657095909118652, 0.11635854095220566, 0.2146621197462082, 0.3868674337863922, 0.11616157740354538, 0.09840984642505646, 0.03658592700958252, 0.03736778721213341, 0.12067023664712906, 0.038115404546260834, 0.038577526807785034, 0.037784550338983536, 0.20910634100437164, 0.09977130591869354, 0.22894418239593506, 0.036914534866809845, 0.03701069578528404, 0.12620943784713745, 0.0364811047911644, 0.03670405596494675, 0.10970013588666916, 0.236959770321846, 0.034504856914281845, 0.03360844776034355, 0.032952938228845596, 0.03212905675172806, 0.2282898873090744, 0.03147382661700249, 0.09931091964244843, 0.13322991132736206, 0.02924879640340805, 0.08915352076292038, 0.11201335489749908, 0.027992935851216316, 0.027177028357982635, 0.10102712363004684, 0.12343146651983261, 0.025028536096215248, 0.09518694877624512, 0.13177180290222168, 0.2492443472146988, 0.022740760818123817, 0.022561706602573395, 0.1372814178466797, 0.0957719162106514, 0.021300185471773148, 0.4694834053516388, 0.31072747707366943, 0.02253333479166031, 0.022937066853046417, 0.10782016813755035, 0.02387635037302971, 0.1270071119070053, 0.024246154353022575, 0.024670332670211792, 0.02459147572517395, 0.024294503033161163, 0.02393738552927971, 0.4610181450843811, 0.1036309078335762, 0.025025006383657455, 0.11231408268213272, 0.02525920420885086, 0.025400899350643158, 0.025517020374536514, 0.10813530534505844, 0.11301498115062714, 0.11326044052839279, 0.11051526665687561, 0.2311246544122696, 0.09671885520219803, 0.11473598331212997, 0.24974849820137024, 0.09525251388549805, 0.23310627043247223, 0.1061071902513504, 0.10891039669513702, 0.12926585972309113, 0.02763696201145649, 0.027948319911956787, 0.027928363531827927, 0.02789798192679882, 0.11083006858825684, 0.027244729921221733, 0.02703923173248768, 0.02647216059267521, 0.23555566370487213, 0.10272933542728424, 0.025656448677182198, 0.02527749538421631, 0.11283861845731735, 0.024445651099085808, 0.10657188296318054, 0.10178471356630325, 0.023181520402431488, 0.10906399041414261, 0.022433212026953697, 0.10676266252994537, 0.12406855821609497, 0.11381056904792786, 0.020725660026073456, 0.020721979439258575, 0.1261570006608963, 0.019495880231261253, 0.09980207681655884, 0.018680451437830925, 0.018175940960645676, 0.11504006385803223, 0.017346996814012527, 0.1126544401049614, 0.24596400558948517, 0.2745271623134613, 0.01681889221072197, 0.016978004947304726, 0.09472497552633286, 0.017281701788306236, 0.017435487359762192, 0.01710803434252739, 0.017076406627893448, 0.01687883399426937, 0.2750217318534851, 0.2804070711135864, 0.017339350655674934, 0.01740599051117897, 0.09719476103782654, 0.017749829217791557, 0.10478115826845169, 0.017876548692584038, 0.09314941614866257, 0.11335005611181259, 0.10455382615327835, 0.10384887456893921, 0.10363302379846573, 0.26221632957458496, 0.09693758189678192, 0.018519621342420578, 0.01864127814769745, 0.018804475665092468, 0.1039958968758583, 0.018773693591356277, 0.018632739782333374, 0.01862211525440216, 0.10727005451917648, 0.10864563286304474, 0.018191950395703316, 0.2853381037712097, 0.08671695739030838, 0.018126972019672394, 0.2712167799472809, 0.0182981938123703, 0.10888335853815079, 0.1066838800907135, 0.11295586079359055, 0.019036270678043365, 0.26277804374694824, 0.019564511254429817, 0.01993691362440586, 0.1005125492811203, 0.10100501775741577, 0.020212873816490173, 0.020174095407128334, 0.02004402130842209, 0.019968386739492416, 0.32358258962631226, 0.10973606258630753, 0.020048294216394424, 0.019951248541474342, 0.12224496155977249, 0.01977752149105072, 0.019715623930096626, 0.01920229382812977, 0.01898913085460663, 0.2621956467628479, 0.28118616342544556, 0.01889660209417343, 0.10859669744968414, 0.11024387180805206, 0.3058045208454132, 0.4615306258201599, 0.23091557621955872, 0.1108342856168747, 0.11437904089689255, 0.025386041030287743, 0.12251567840576172, 0.09838423132896423, 0.028284957632422447, 0.09604018181562424, 0.08412297070026398, 0.02973494865000248, 0.23640309274196625, 0.03035833314061165, 0.030689215287566185, 0.03077877126634121, 0.10074042528867722, 0.11010263860225677, 0.11232385784387589, 0.11564791202545166, 0.030507439747452736, 0.12017828226089478, 0.0294853076338768, 0.028962383046746254, 0.02864004112780094, 0.10953431576490402, 0.10607314109802246, 0.026371996849775314, 0.10430321842432022, 0.1074092909693718, 0.024457711726427078, 0.02396089769899845, 0.023252811282873154, 0.02251647599041462, 0.021887337788939476, 0.10463924705982208, 0.11394334584474564, 0.019872166216373444, 0.1206217035651207, 0.26454752683639526, 0.10008082538843155, 0.018595410510897636, 0.2684168219566345, 0.0185348242521286, 0.11329784989356995, 0.018646709620952606, 0.09800751507282257, 0.018513070419430733, 0.018679151311516762, 0.11530795693397522, 0.11076629161834717, 0.12269345670938492, 0.09428450465202332, 0.09448067098855972, 0.296412855386734, 0.10811594873666763, 0.2371324598789215, 0.018589699640870094, 0.10730698704719543, 0.01910521648824215, 0.019215883687138557, 0.10782377421855927, 0.01941179856657982, 0.019628655165433884, 0.25397390127182007, 0.12562412023544312, 0.0937751829624176, 0.02002132497727871, 0.10943862795829773, 0.020212450996041298, 0.020022526383399963, 0.020056243985891342, 0.019759466871619225, 0.10032162070274353, 0.26506608724594116, 0.019351299852132797, 0.09119407087564468, 0.09006042033433914, 0.019493140280246735, 0.01924777403473854, 0.10066420584917068, 0.0906137079000473, 0.01893988624215126, 0.09901273995637894, 0.09739282727241516, 0.01823573186993599, 0.10206053406000137, 0.01787913776934147, 0.01761120930314064, 0.11972709745168686, 0.01721022091805935, 0.016797054558992386, 0.09300090372562408, 0.016167160123586655, 0.30746331810951233, 0.27269962430000305, 0.016201362013816833, 0.08562464267015457, 0.01689685694873333, 0.09340209513902664, 0.24300450086593628, 0.27242499589920044, 0.11644086241722107, 0.28261539340019226, 0.23984582722187042, 0.020591052249073982, 0.0940183475613594, 0.022552350535988808, 0.09275184571743011, 0.24073588848114014, 0.266079843044281, 0.02645399421453476, 0.027086859568953514, 0.1372513324022293, 0.02859978936612606, 0.028949100524187088, 0.11043602973222733, 0.3849802017211914, 0.4062254726886749, 0.03331686928868294, 0.034156784415245056, 0.09969636797904968, 0.03704569861292839, 0.339788019657135, 0.04007123410701752, 0.09809601306915283, 0.20942220091819763, 0.22111840546131134, 0.13172434270381927, 0.11828706413507462, 0.048208486288785934, 0.3599705696105957, 0.10334175080060959, 0.11546101421117783, 0.12863090634346008, 0.3080563247203827, 0.09956985712051392, 0.05785883218050003, 0.05868770554661751, 0.18925413489341736, 0.06059296801686287, 0.09125334024429321, 0.1861671656370163, 0.13965626060962677, 0.11109450459480286, 0.06052670627832413, 0.1141514703631401, 0.10282713919878006, 0.09615331888198853, 0.18615616858005524, 0.21692399680614471, 0.23129594326019287, 0.05615232139825821, 0.11041028797626495, 0.14750170707702637, 0.10565366595983505, 0.054403796792030334, 0.05429740250110626, 0.12989473342895508, 0.10878631472587585, 0.13222889602184296, 0.11429325491189957, 0.1276816874742508, 0.045761559158563614, 0.19759683310985565, 0.10374923795461655, 0.20300549268722534, 0.04165682941675186, 0.2190779447555542, 0.09260405600070953, 0.11468125879764557, 0.11924482882022858, 0.03946232795715332, 0.23632988333702087, 0.11503321677446365, 0.09216362237930298, 0.18686997890472412, 0.11204259097576141, 0.25079652667045593, 0.03739830106496811, 0.03735804185271263, 0.1248745247721672, 0.03795353323221207, 0.13239584863185883, 0.27561357617378235, 0.12892821431159973, 0.03611089289188385, 0.09311389178037643, 0.22693784534931183, 0.12573690712451935, 0.035375501960515976, 0.10033856332302094, 0.034390345215797424]\n",
            "Val loss 0.09740294264057618\n",
            "Val auc roc 0.49604414770619754\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4960ce30ac2c4c489d2b7a87aaa2c299",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1839.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1043\n",
            "Train Losses : [0.2334212362766266, 0.09694411605596542, 0.03377486765384674, 0.03292591869831085, 0.12482438236474991, 0.10374549776315689, 0.10391031950712204, 0.031104007735848427, 0.030695881694555283, 0.2143344134092331, 0.1048748716711998, 0.09245088696479797, 0.02923821471631527, 0.10895518958568573, 0.13369446992874146, 0.02786177210509777, 0.027193501591682434, 0.026934778317809105, 0.4494079053401947, 0.11314456164836884, 0.026628609746694565, 0.026676058769226074, 0.232626274228096, 0.10630612075328827, 0.026906529441475868, 0.11335181444883347, 0.1020040363073349, 0.11497404426336288, 0.23901383578777313, 0.23086118698120117, 0.028039993718266487, 0.12032193690538406, 0.1125372052192688, 0.02894427999854088, 0.240897074341774, 0.11459103226661682, 0.029306849464774132, 0.23297399282455444, 0.23748309910297394, 0.2386714071035385, 0.03153501823544502, 0.09786836057901382, 0.10477335751056671, 0.03316584229469299, 0.033682119101285934, 0.21393825113773346, 0.10325592011213303, 0.03430603817105293, 0.09587150812149048, 0.08799545466899872, 0.034317683428525925, 0.23937486112117767, 0.10439173132181168, 0.249540776014328, 0.3293153941631317, 0.036099810153245926, 0.03674513101577759, 0.037361349910497665, 0.12203661352396011, 0.03810584545135498, 0.03803135082125664, 0.2081092894077301, 0.22265537083148956, 0.11085401475429535, 0.12495938688516617, 0.03851618617773056, 0.09840252250432968, 0.1035335510969162, 0.25263550877571106, 0.03848130255937576, 0.2569548785686493, 0.03914206475019455, 0.03864699974656105, 0.038170281797647476, 0.22559262812137604, 0.10387206822633743, 0.037976834923028946, 0.11140957474708557, 0.11782169342041016, 0.03726030886173248, 0.12455475330352783, 0.035471245646476746, 0.09293202310800552, 0.12065747380256653, 0.0785527378320694, 0.032695673406124115, 0.09959327429533005, 0.031067529693245888, 0.284401535987854, 0.11690489202737808, 0.2489945888519287, 0.029745405539870262, 0.1119198203086853, 0.029238829389214516, 0.2744136452674866, 0.028970763087272644, 0.25530877709388733, 0.1131211593747139, 0.029237354174256325, 0.22023680806159973, 0.24351012706756592, 0.030077215284109116, 0.030450355261564255, 0.11737608164548874, 0.2141760289669037, 0.20684929192066193, 0.1012793481349945, 0.03238717094063759, 0.03259854391217232, 0.03291573002934456, 0.11240173876285553, 0.03294003754854202, 0.10616935789585114, 0.12421650439500809, 0.03188587725162506, 0.031413815915584564, 0.03123551607131958, 0.2303374707698822, 0.030299212783575058, 0.22296811640262604, 0.23385082185268402, 0.11608252674341202, 0.09144649654626846, 0.03019300289452076, 0.11492359638214111, 0.1141509935259819, 0.029892142862081528, 0.029466809704899788, 0.02927379496395588, 0.0287619736045599, 0.2222219556570053, 0.09860225766897202, 0.11936437338590622, 0.12274612486362457, 0.10566433519124985, 0.0973498672246933, 0.026525327935814857, 0.08726772665977478, 0.11477411538362503, 0.02547326497733593, 0.025691771879792213, 0.22582149505615234, 0.02485504560172558, 0.09371345490217209, 0.023959767073392868, 0.023781396448612213, 0.09232977032661438, 0.09439373016357422, 0.2958405613899231, 0.2487405240535736, 0.023401182144880295, 0.09556471556425095, 0.10000424832105637, 0.09045331180095673, 0.02329929918050766, 0.10752192884683609, 0.11313224583864212, 0.023501815274357796, 0.02285657823085785, 0.023262152448296547, 0.02250002697110176, 0.022078389301896095, 0.02177836373448372, 0.02167176641523838, 0.07469970732927322, 0.32445743680000305, 0.09233144670724869, 0.2869187295436859, 0.02106444351375103, 0.0930762067437172, 0.02052190527319908, 0.06972279399633408, 0.10932936519384384, 0.020817430689930916, 0.1109529510140419, 0.02031317725777626, 0.02103779837489128, 0.020278222858905792, 0.11383311450481415, 0.26685598492622375, 0.08023722469806671, 0.5238491296768188, 0.2613714337348938, 0.02282712608575821, 0.022052323445677757, 0.0224810428917408, 0.11639042943716049, 0.0236333180218935, 0.024206528440117836, 0.33700332045555115, 0.07838401943445206, 0.026061905547976494, 0.07870309799909592, 0.026044290512800217, 0.24835459887981415, 0.07706772536039352, 0.026406552642583847, 0.0932866632938385, 0.27177393436431885, 0.26260870695114136, 0.02797996625304222, 0.028280574828386307, 0.09679759293794632, 0.028659148141741753, 0.07626623660326004, 0.22077424824237823, 0.09547001868486404, 0.09367959946393967, 0.23991930484771729, 0.12931399047374725, 0.25943416357040405, 0.26767292618751526, 0.25014346837997437, 0.08674094080924988, 0.0341227687895298, 0.14782893657684326, 0.11705721914768219, 0.03774881362915039, 0.09254207462072372, 0.13266068696975708, 0.1917889416217804, 0.0398377850651741, 0.03967747092247009, 0.2523571252822876, 0.21813733875751495, 0.2621648609638214, 0.041323471814394, 0.07711783051490784, 0.2271319031715393, 0.04250645264983177, 0.19379426538944244, 0.044009510427713394, 0.044936731457710266, 0.17159956693649292, 0.04408356547355652, 0.13235017657279968, 0.20858599245548248, 0.33830726146698, 0.11754152923822403, 0.1576901078224182, 0.09141349792480469, 0.049074672162532806, 0.04723315313458443, 0.047582414001226425, 0.14509452879428864, 0.11609922349452972, 0.04790114611387253, 0.13286316394805908, 0.04417910799384117, 0.07660658657550812, 0.04214398190379143, 0.04153665900230408, 0.03993215784430504, 0.13665468990802765, 0.03905424848198891, 0.13537944853305817, 0.03559764474630356, 0.10104814916849136, 0.11900892853736877, 0.14117135107517242, 0.031151827424764633, 0.12951242923736572, 0.02928944304585457, 0.15043456852436066, 0.02724573202431202, 0.026578178629279137, 0.08938554674386978, 0.025135481730103493, 0.08608508855104446, 0.26649585366249084, 0.02293316274881363, 0.02244677022099495, 0.08129102736711502, 0.4495713710784912, 0.02243819274008274, 0.10479816794395447, 0.022508414462208748, 0.09563051909208298, 0.023002559319138527, 0.09407524019479752, 0.022267337888479233, 0.022312508895993233, 0.12647101283073425, 0.022051669657230377, 0.022465432062745094, 0.021261120215058327, 0.02136065810918808, 0.02096126601099968, 0.08941161632537842, 0.09574578702449799, 0.019811149686574936, 0.019022244960069656, 0.10297739505767822, 0.09371380507946014, 0.08033319562673569, 0.01812582090497017, 0.0849539041519165, 0.017169993370771408, 0.016970809549093246, 0.016765138134360313, 0.3082350194454193, 0.540818452835083, 0.08919166028499603, 0.3130013048648834, 0.32861194014549255, 0.359638512134552, 0.08895695209503174, 0.021603863686323166, 0.10940420627593994, 0.26409199833869934, 0.09102217108011246, 0.30198848247528076, 0.026848433539271355, 0.027043083682656288, 0.30828073620796204, 0.029191190376877785, 0.11787345260381699, 0.10369601845741272, 0.031406328082084656, 0.03194393590092659, 0.10497306287288666, 0.26707229018211365, 0.03319927304983139, 0.033139195293188095, 0.2060767561197281, 0.033364929258823395, 0.0331125371158123, 0.033399246633052826, 0.03335215151309967, 0.033109281212091446, 0.2126971036195755, 0.03225261718034744, 0.10928761959075928, 0.1232294961810112, 0.031515222042798996, 0.11642224341630936, 0.030509784817695618, 0.24249134957790375, 0.24405236542224884, 0.029731430113315582, 0.02984725683927536, 0.11716726422309875, 0.13733117282390594, 0.27284660935401917, 0.12216530740261078, 0.10072919726371765, 0.10788214206695557, 0.10601907968521118, 0.10879608243703842, 0.02943825162947178, 0.11825639009475708, 0.11605961620807648, 0.028683563694357872, 0.028654176741838455, 0.11059850454330444, 0.027582334354519844, 0.027308104559779167, 0.026746314018964767, 0.10213860869407654, 0.08116509020328522, 0.025049986317753792, 0.22801710665225983, 0.10261815786361694, 0.024138325825333595, 0.09535165131092072, 0.11988898366689682, 0.0235761571675539, 0.25091105699539185, 0.022817207500338554, 0.23586401343345642, 0.023075545206665993, 0.022825418040156364, 0.08223824948072433, 0.022827543318271637, 0.02274913713335991, 0.02237951196730137, 0.09071574360132217, 0.23815034329891205, 0.13409852981567383, 0.26256415247917175, 0.2783018946647644, 0.29557448625564575, 0.023716092109680176, 0.024042179808020592, 0.0970412865281105, 0.02495417557656765, 0.025324983522295952, 0.10213123261928558, 0.11290618777275085, 0.08626510947942734, 0.10882316529750824, 0.2491682469844818, 0.2660087049007416, 0.1305839717388153, 0.026720838621258736, 0.027445636689662933, 0.027173401787877083, 0.477628231048584, 0.0277719646692276, 0.02845083735883236, 0.029022498056292534, 0.07439366728067398, 0.029433701187372208, 0.029136423021554947, 0.10085774213075638, 0.02916370891034603, 0.38492369651794434, 0.10908620804548264, 0.029938526451587677, 0.10679775476455688, 0.10840698331594467, 0.10176552832126617, 0.10492166131734848, 0.11524276435375214, 0.13175374269485474, 0.10443402826786041, 0.20966815948486328, 0.09953998029232025, 0.030662430450320244, 0.030753489583730698, 0.03038276545703411, 0.029966125264763832, 0.08291816711425781, 0.2250615507364273, 0.029163338243961334, 0.028849676251411438, 0.11861765384674072, 0.028855599462985992, 0.24340227246284485, 0.12035191059112549, 0.11360141634941101, 0.027934320271015167, 0.2147776484489441, 0.027777740731835365, 0.1168627142906189, 0.02781732752919197, 0.027365785092115402, 0.199561208486557, 0.10004941374063492, 0.027359914034605026, 0.11190468817949295, 0.41023683547973633, 0.13356123864650726, 0.027613384649157524, 0.09634260833263397, 0.12626147270202637, 0.21334582567214966, 0.10807008296251297, 0.028968719765543938, 0.10149173438549042, 0.09480094909667969, 0.029426855966448784, 0.029595287516713142, 0.21905110776424408, 0.22379092872142792, 0.03019578568637371, 0.029723118990659714, 0.07214011996984482, 0.030012957751750946, 0.09899254143238068, 0.11630237102508545, 0.10792578011751175, 0.4559061527252197, 0.030910400673747063, 0.030485335737466812, 0.11176163703203201, 0.031151212751865387, 0.0308337714523077, 0.030580420047044754, 0.18645095825195312, 0.0861545205116272, 0.030488863587379456, 0.030685776844620705, 0.09231113642454147, 0.03057974763214588, 0.08669344335794449, 0.1379871815443039, 0.2086239755153656, 0.09795103967189789, 0.028477540239691734, 0.18069441616535187, 0.1386762112379074, 0.13156235218048096, 0.11233104020357132, 0.029355866834521294, 0.02858860231935978, 0.11108221113681793, 0.07734175771474838, 0.026918809860944748, 0.10827763378620148, 0.026376664638519287, 0.09794685244560242, 0.10198290646076202, 0.2025548815727234, 0.025333933532238007, 0.13627901673316956, 0.1224336251616478, 0.12860606610774994, 0.026288818567991257, 0.14534518122673035, 0.1256604939699173, 0.023967182263731956, 0.11793037503957748, 0.023537954315543175, 0.28425461053848267, 0.14737431704998016, 0.022839277982711792, 0.1500396728515625, 0.022976180538535118, 0.022460827603936195, 0.1264069676399231, 0.022304531186819077, 0.07165143638849258, 0.4750906825065613, 0.0220793467015028, 0.022280912846326828, 0.06584065407514572, 0.022876987233757973, 0.02273152582347393, 0.10852208733558655, 0.11307923495769501, 0.14573799073696136, 0.07818981260061264, 0.10470197349786758, 0.12745843827724457, 0.022278880700469017, 0.23315086960792542, 0.11968883872032166, 0.11672577261924744, 0.022434081882238388, 0.02206180989742279, 0.11702494323253632, 0.02191063016653061, 0.2430274486541748, 0.1253345012664795, 0.4339052736759186, 0.022782431915402412, 0.02305559068918228, 0.023543963208794594, 0.27456629276275635, 0.08404876291751862, 0.08154499530792236, 0.025581156834959984, 0.02541862241923809, 0.23673175275325775, 0.02689281478524208, 0.10546549409627914, 0.13591508567333221, 0.027253560721874237, 0.09688349068164825, 0.14865542948246002, 0.10890347510576248, 0.02656979113817215, 0.22098366916179657, 0.441623330116272, 0.11062343418598175, 0.09630326926708221, 0.25967147946357727, 0.12402956187725067, 0.03003348596394062, 0.0961695984005928, 0.4214882552623749, 0.09091793745756149, 0.03299656882882118, 0.2049175500869751, 0.034562189131975174, 0.03517580032348633, 0.12459230422973633, 0.12461365759372711, 0.11505521088838577, 0.12375158816576004, 0.11799190938472748, 0.03750619664788246, 0.03659886494278908, 0.08266133069992065, 0.03585970401763916, 0.22058726847171783, 0.13114094734191895, 0.09381550550460815, 0.10031505674123764, 0.35353025794029236, 0.08077294379472733, 0.11629854142665863, 0.20561060309410095, 0.036528341472148895, 0.03679867461323738, 0.13094079494476318, 0.12140257656574249, 0.13542330265045166, 0.036913856863975525, 0.09585568308830261, 0.03615359961986542, 0.03642469644546509, 0.10290654003620148, 0.1039959043264389, 0.03470204025506973, 0.37516218423843384, 0.41096770763397217, 0.20079071819782257, 0.09532997757196426, 0.10325063019990921, 0.11924134939908981, 0.03803432360291481, 0.21340075135231018, 0.039004672318696976, 0.21184977889060974, 0.19685043394565582, 0.21980775892734528, 0.04229327663779259, 0.12778496742248535, 0.04286302626132965, 0.09479464590549469, 0.04353993386030197, 0.103165403008461, 0.33427003026008606, 0.044421929866075516, 0.11324584484100342, 0.10413194447755814, 0.04483749344944954, 0.09776420891284943, 0.20687051117420197, 0.09075615555047989, 0.11066849529743195, 0.12539513409137726, 0.12537920475006104, 0.18870799243450165, 0.11131330579519272, 0.11722907423973083, 0.08222470432519913, 0.04315950348973274, 0.0430549792945385, 0.042598359286785126, 0.041665270924568176, 0.04068782925605774, 0.23488037288188934, 0.10950792580842972, 0.11225704103708267, 0.038264695554971695, 0.08413521200418472, 0.3670700192451477, 0.0993974506855011, 0.13020704686641693, 0.037175409495830536, 0.13560456037521362, 0.03697168081998825, 0.11243340373039246, 0.036130666732788086, 0.11703115701675415, 0.03530440106987953, 0.03517410159111023, 0.13122057914733887, 0.10897943377494812, 0.12052928656339645, 0.03228144720196724, 0.09857628494501114, 0.08731932193040848, 0.2671995759010315, 0.10493233054876328, 0.029815688729286194, 0.02954207919538021, 0.029862940311431885, 0.029154518619179726, 0.028280384838581085, 0.12378200143575668, 0.026928143575787544, 0.026605913415551186, 0.025869693607091904, 0.025390859693288803, 0.024881741032004356, 0.11537640541791916, 0.10872428119182587, 0.11339394748210907, 0.022507132962346077, 0.1159147173166275, 0.2136913686990738, 0.021583348512649536, 0.13068673014640808, 0.11091628670692444, 0.020643165335059166, 0.09911172091960907, 0.7439229488372803, 0.1036941409111023, 0.08669634908437729, 0.021867413073778152, 0.11052325367927551, 0.11688528954982758, 0.022525591775774956, 0.022823000326752663, 0.105800561606884, 0.108610138297081, 0.11432728916406631, 0.023107746616005898, 0.02297467179596424, 0.0230354405939579, 0.022585859522223473, 0.022307811304926872, 0.11233357340097427, 0.022057220339775085, 0.11382806301116943, 0.11343657225370407, 0.02104533277451992, 0.02118242159485817, 0.11051282286643982, 0.020345836877822876, 0.11503508687019348, 0.019665751606225967, 0.0193633995950222, 0.44751158356666565, 0.019372044131159782, 0.10314082354307175, 0.019653858616948128, 0.09938550740480423, 0.2502841651439667, 0.020254958420991898, 0.26214852929115295, 0.24680793285369873, 0.02111642248928547, 0.02142704650759697, 0.021724827587604523, 0.10631277412176132, 0.02220325544476509, 0.09621470421552658, 0.13076522946357727, 0.022283222526311874, 0.0946204885840416, 0.022742852568626404, 0.0925489068031311, 0.09950362145900726, 0.23900894820690155, 0.02234482206404209, 0.022405456751585007, 0.02224777266383171, 0.021953249350190163, 0.08817902207374573, 0.10249454528093338, 0.021661853417754173, 0.25440698862075806, 0.02169383130967617, 0.10320155322551727, 0.09070003777742386, 0.021353289484977722, 0.1248307004570961, 0.10003384202718735, 0.10164890438318253, 0.020886998623609543, 0.02088436670601368, 0.020580271258950233, 0.020315537229180336, 0.020706772804260254, 0.1006205826997757, 0.10831770300865173, 0.01939620077610016, 0.10412389785051346, 0.27024537324905396, 0.11187140643596649, 0.01904893107712269, 0.10499631613492966, 0.26231691241264343, 0.10868591070175171, 0.019239505752921104, 0.07344083487987518, 0.1038176491856575, 0.01934579759836197, 0.06908424198627472, 0.10834046453237534, 0.019626963883638382, 0.3091776967048645, 0.1108701303601265, 0.3090110123157501, 0.26116517186164856, 0.020360980182886124, 0.09690172225236893, 0.11464781314134598, 0.021335702389478683, 0.1206328496336937, 0.2242002636194229, 0.10541832447052002, 0.09629093110561371, 0.24944043159484863, 0.09615514427423477, 0.49382534623146057, 0.11334286630153656, 0.02596551738679409, 0.43237024545669556, 0.02776019088923931, 0.11367484927177429, 0.0293678380548954, 0.030156316235661507, 0.03081970475614071, 0.03159693628549576, 0.03164546191692352, 0.031566739082336426, 0.031485870480537415, 0.031418051570653915, 0.10713911056518555, 0.03136499971151352, 0.09049604833126068, 0.21941988170146942, 0.2113988995552063, 0.030505826696753502, 0.09280460327863693, 0.031063873320817947, 0.2634286880493164, 0.030522022396326065, 0.12404538691043854, 0.10498757660388947, 0.030725140124559402, 0.22411228716373444, 0.0306475181132555, 0.03036721795797348, 0.03027697280049324, 0.13024182617664337, 0.2574716806411743, 0.029860997572541237, 0.26126646995544434, 0.09246738255023956, 0.030096029862761497, 0.6150532364845276, 0.2240847796201706, 0.11221455037593842, 0.1063905581831932, 0.1190040186047554, 0.10048578679561615, 0.24555404484272003, 0.03559999540448189, 0.09281057864427567, 0.3757849931716919, 0.037513669580221176, 0.11179225891828537, 0.20676475763320923, 0.03960384801030159, 0.04101664200425148, 0.040746014565229416, 0.10768599063158035, 0.040749311447143555, 0.11829891055822372, 0.04067964851856232, 0.2166193276643753, 0.12126784771680832, 0.04007222503423691, 0.11622082442045212, 0.10748270899057388, 0.09893939644098282, 0.04002879932522774, 0.10111625492572784, 0.10546106100082397, 0.2269650399684906, 0.03736509755253792, 0.03687189519405365, 0.22336620092391968, 0.2164197713136673, 0.036262042820453644, 0.12883956730365753, 0.03615104779601097, 0.10812097042798996, 0.23289383947849274, 0.035446662455797195, 0.035366903990507126, 0.1284448802471161, 0.10374670475721359, 0.13729751110076904, 0.11480943858623505, 0.03423649072647095, 0.11305835098028183, 0.07781607657670975, 0.09712940454483032, 0.033162690699100494, 0.13191545009613037, 0.03130152076482773, 0.09262746572494507, 0.10482124239206314, 0.42216894030570984, 0.029922230169177055, 0.22873881459236145, 0.030242780223488808, 0.03119811974465847, 0.13932833075523376, 0.09239387512207031, 0.23377636075019836, 0.23753871023654938, 0.11821616441011429, 0.10576406121253967, 0.2338993102312088, 0.03189258649945259, 0.032069310545921326, 0.032589081674814224, 0.4135434925556183, 0.033001020550727844, 0.03378617763519287, 0.3841171860694885, 0.08385162800550461, 0.09976331889629364, 0.09121856838464737, 0.03650115802884102, 0.11470960080623627, 0.03679277002811432, 0.03718995302915573, 0.11407098919153214, 0.03659440949559212, 0.0366973802447319, 0.03645649179816246, 0.03618313744664192, 0.12127763777971268, 0.12054388225078583, 0.1051442101597786, 0.03359493240714073, 0.19856718182563782, 0.10207336395978928, 0.09726374596357346, 0.032156527042388916, 0.10004863888025284, 0.03150678053498268, 0.12330269813537598, 0.09787848591804504, 0.030203422531485558, 0.09419017285108566, 0.08942045271396637, 0.02895193174481392, 0.45362141728401184, 0.23411118984222412, 0.028755836188793182, 0.02918028086423874, 0.029027782380580902, 0.09575784206390381, 0.41118839383125305, 0.12499113380908966, 0.029845837503671646, 0.10372807830572128, 0.030382784083485603, 0.03062754124403, 0.03067913092672825, 0.22080744802951813, 0.0935673862695694, 0.030926281586289406, 0.10770025849342346, 0.10119442641735077, 0.11575588583946228, 0.030302584171295166, 0.030097302049398422, 0.09116087853908539, 0.029599184170365334, 0.25135159492492676, 0.029358666390180588, 0.028913043439388275, 0.028716688975691795, 0.028849409893155098, 0.13104097545146942, 0.11160677671432495, 0.10529672354459763, 0.027071190997958183, 0.026729371398687363, 0.24550354480743408, 0.026217084378004074, 0.025923265144228935, 0.1076042652130127, 0.10774201899766922, 0.11338289082050323, 0.025028998032212257, 0.024949055165052414, 0.024238385260105133, 0.02414146065711975, 0.023703137412667274, 0.09576545655727386, 0.09099886566400528, 0.022914385423064232, 0.23274002969264984, 0.511339008808136, 0.02239171974360943, 0.2697626054286957, 0.10357888787984848, 0.10674042254686356, 0.024017123505473137, 0.10709890723228455, 0.08751726150512695, 0.11444617062807083, 0.0960843414068222, 0.1008668839931488, 0.02475590631365776, 0.02470514550805092, 0.10268963873386383, 0.024513352662324905, 0.024374717846512794, 0.10603146255016327, 0.10868228226900101, 0.24979795515537262, 0.024059515446424484, 0.024197205901145935, 0.09347180277109146, 0.02407952770590782, 0.09084155410528183, 0.09864320605993271, 0.2234213501214981, 0.02322731725871563, 0.023239649832248688, 0.10194147378206253, 0.023076623678207397, 0.23694613575935364, 0.023267962038517, 0.023241499438881874, 0.117091603577137, 0.2048034965991974, 0.02295253798365593, 0.10673913359642029, 0.022906435653567314, 0.022868037223815918, 0.023397359997034073, 0.022640490904450417, 0.02236299216747284, 0.4618585407733917, 0.02344943955540657, 0.02281743474304676, 0.11833164840936661, 0.09136824309825897, 0.023028526455163956, 0.13069994747638702, 0.11124011129140854, 0.023017514497041702, 0.10420115292072296, 0.2476402223110199, 0.09260590374469757, 0.0229337215423584, 0.2501002252101898, 0.10207118839025497, 0.023792222142219543, 0.023762650787830353, 0.28541699051856995, 0.023469291627407074, 0.09749871492385864, 0.11776730418205261, 0.023915842175483704, 0.024131804704666138, 0.11495980620384216, 0.11757390946149826, 0.11193687468767166, 0.137254998087883, 0.023432886227965355, 0.02369583211839199, 0.02313041500747204, 0.11143365502357483, 0.022878140211105347, 0.10563495755195618, 0.12757597863674164, 0.11392610520124435, 0.02206793799996376, 0.021616609767079353, 0.021295685321092606, 0.09774808585643768, 0.02077629044651985, 0.020574267953634262, 0.02033616229891777, 0.2497863918542862, 0.01972603239119053, 0.019702322781085968, 0.019671231508255005, 0.01943448930978775, 0.30246689915657043, 0.0190273504704237, 0.01912817917764187, 0.019149940460920334, 0.01856929250061512, 0.268044114112854, 0.018568968400359154, 0.09703070670366287, 0.2959108054637909, 0.09955969452857971, 0.26061540842056274, 0.10293067991733551, 0.11082711815834045, 0.11930864304304123, 0.23250219225883484, 0.02017267420887947, 0.12105477601289749, 0.02059425786137581, 0.021154237911105156, 0.24971064925193787, 0.09205561876296997, 0.09915070980787277, 0.02190493419766426, 0.09168224781751633, 0.2591906487941742, 0.29984408617019653, 0.26760247349739075, 0.10578805953264236, 0.11111625283956528, 0.02433253824710846, 0.0980726033449173, 0.106838159263134, 0.025215623900294304, 0.10043284296989441, 0.27771931886672974, 0.0260152705013752, 0.4306699335575104, 0.02681116573512554, 0.027372580021619797, 0.027977006509900093, 0.0286269411444664, 0.02831469289958477, 0.02860739268362522, 0.02862507849931717, 0.028537411242723465, 0.028407692909240723, 0.2180279642343521, 0.13035856187343597, 0.027962662279605865, 0.027854880318045616, 0.02793770283460617, 0.02743551880121231, 0.22770678997039795, 0.02718830667436123, 0.10082757472991943, 0.026883546262979507, 0.026613472029566765, 0.026414984837174416, 0.10327700525522232, 0.025794081389904022, 0.025467289611697197, 0.02501104213297367, 0.11007066816091537, 0.21510489284992218, 0.450280100107193, 0.024611294269561768, 0.2268202304840088, 0.09709182381629944, 0.025401486083865166, 0.10617981106042862, 0.02587234415113926, 0.025924911722540855, 0.09872031211853027, 0.0259505994617939, 0.088200144469738, 0.098125159740448, 0.09577179700136185, 0.11385634541511536, 0.12187003344297409, 0.11374232918024063, 0.22877517342567444, 0.09183046966791153, 0.025437498465180397, 0.10494518280029297, 0.025448225438594818, 0.025296803563833237, 0.025136888027191162, 0.11206073313951492, 0.26843389868736267, 0.024785196408629417, 0.09315913915634155, 0.02461159974336624, 0.024810396134853363, 0.10878419131040573, 0.08069014549255371, 0.23449036478996277, 0.08779037743806839, 0.09418167918920517, 0.024129705503582954, 0.023951638489961624, 0.6884188652038574, 0.09626876562833786, 0.024859365075826645, 0.4216957092285156, 0.09168484807014465, 0.1142926886677742, 0.027693865820765495, 0.13021224737167358, 0.028506310656666756, 0.02872585505247116, 0.029017196968197823, 0.10851187258958817, 0.02899770811200142, 0.02918010763823986, 0.09922904521226883, 0.0990327000617981, 0.22113049030303955, 0.028577633202075958, 0.10014773905277252, 0.0287166740745306, 0.12244254350662231, 0.11472032964229584, 0.10015115141868591, 0.43759289383888245, 0.09846377372741699, 0.11702042818069458, 0.028816089034080505, 0.029459547251462936, 0.029186246916651726, 0.029068531468510628, 0.028958704322576523, 0.11357490718364716, 0.11171609908342361, 0.23493458330631256, 0.23932823538780212, 0.10891393572092056, 0.10723470896482468, 0.41414016485214233, 0.02960684336721897, 0.02979959174990654, 0.24736003577709198, 0.11785190552473068, 0.10906009376049042, 0.031781505793333054, 0.25051072239875793, 0.11346248537302017, 0.03243761137127876, 0.03267846256494522, 0.135506734251976, 0.10831306874752045, 0.0326157808303833, 0.22770105302333832, 0.03259875997900963, 0.22611002624034882, 0.09504802525043488, 0.03273022174835205, 0.11005298793315887, 0.09782160073518753, 0.10648870468139648, 0.09259748458862305, 0.03237141668796539, 0.10111204534769058, 0.13685622811317444, 0.11288043111562729, 0.10582910478115082, 0.11203601956367493, 0.030957559123635292, 0.11987055093050003, 0.11918854713439941, 0.030359292402863503, 0.02980721928179264, 0.029364734888076782, 0.11768455058336258, 0.10486500710248947, 0.028246881440281868, 0.02776271104812622, 0.23040923476219177, 0.11483492702245712, 0.10089779645204544, 0.026653369888663292, 0.09342214465141296, 0.10448881983757019, 0.11908016353845596, 0.025786930695176125, 0.10686208307743073, 0.11259900033473969, 0.02487013302743435, 0.11157996207475662, 0.0242615919560194, 0.02405184879899025, 0.24637268483638763, 0.09758172184228897, 0.10119938105344772, 0.11160464584827423, 0.1120610162615776, 0.12572146952152252, 0.10613628476858139, 0.42892998456954956, 0.11957422643899918, 0.02363273873925209, 0.11798381805419922, 0.0239879060536623, 0.024229051545262337, 0.024030771106481552, 0.09923110157251358, 0.439430832862854, 0.02439124323427677, 0.09898284822702408, 0.24836330115795135, 0.02529468573629856, 0.025688914582133293, 0.02619602158665657, 0.02604535222053528, 0.026210032403469086, 0.24249325692653656, 0.27538150548934937, 0.10420496016740799, 0.02665204554796219, 0.10704771429300308, 0.10020525008440018, 0.027203569188714027, 0.12125974893569946, 0.027063170447945595, 0.10866519808769226, 0.6471177339553833, 0.02773735672235489, 0.0279293991625309, 0.028363211080431938, 0.09946300089359283, 0.40455663204193115, 0.2275286614894867, 0.030100533738732338, 0.1036357581615448, 0.03133729100227356, 0.10963983833789825, 0.09629000723361969, 0.0323190800845623, 0.11428532749414444, 0.03258148953318596, 0.03266124054789543, 0.10369425266981125, 0.03234061226248741, 0.24482405185699463, 0.22007188200950623, 0.11557449400424957, 0.03261710703372955, 0.11046668887138367, 0.10241871327161789, 0.39486163854599, 0.1066468209028244, 0.03325393795967102, 0.033431652933359146, 0.03379981592297554, 0.03362088277935982, 0.03347397595643997, 0.03339676931500435, 0.03304946795105934, 0.10942954570055008, 0.03250731900334358, 0.11208774149417877, 0.03166293725371361, 0.12036336958408356, 0.11014766246080399, 0.19818635284900665, 0.10034572333097458, 0.030393335968255997, 0.029994284734129906, 0.09987328201532364, 0.02925632707774639, 0.10454452782869339, 0.028716932982206345, 0.02823016420006752, 0.028287969529628754, 0.11354342103004456, 0.09880198538303375, 0.26425808668136597, 0.026580609381198883, 0.02636340633034706, 0.02609912119805813, 0.025816095992922783, 0.28180161118507385, 0.025476915761828423, 0.22660166025161743, 0.02519446611404419, 0.25704309344291687, 0.09016692638397217, 0.025262149050831795, 0.11072321981191635, 0.11492765694856644, 0.1375565081834793, 0.07765430212020874, 0.2753775119781494, 0.09525192528963089, 0.2342897206544876, 0.025844914838671684, 0.10421808809041977, 0.22567839920520782, 0.10417361557483673, 0.026568399742245674, 0.09970187395811081, 0.13566023111343384, 0.11380799859762192, 0.02711094729602337, 0.0274664219468832, 0.11668990552425385, 0.10007934272289276, 0.02666519209742546, 0.10405254364013672, 0.22680920362472534, 0.10281815379858017, 0.10535932332277298, 0.2403256595134735, 0.02654144912958145, 0.11495517939329147, 0.10869556665420532, 0.026591461151838303, 0.10478146374225616, 0.02650599367916584, 0.10939700901508331, 0.11467285454273224, 0.13129587471485138, 0.10744824260473251, 0.09747172147035599, 0.11445724219083786, 0.1053248718380928, 0.02549685910344124, 0.025381755083799362, 0.11561576277017593, 0.025275083258748055, 0.22815456986427307, 0.10872704535722733, 0.10073487460613251, 0.26478272676467896, 0.024733196943998337, 0.11208370327949524, 0.10394483804702759, 0.08443603664636612, 0.26788023114204407, 0.20898860692977905, 0.24681055545806885, 0.025447480380535126, 0.025627249851822853, 0.13346362113952637, 0.026123235002160072, 0.09825896471738815, 0.21052996814250946, 0.026454897597432137, 0.02677382528781891, 0.02673548460006714, 0.026721183210611343, 0.09295996278524399, 0.22056032717227936, 0.20419780910015106, 0.10668468475341797, 0.10272800922393799, 0.027171390131115913, 0.28497979044914246, 0.027781523764133453, 0.4573785066604614, 0.11568796634674072, 0.02901303395628929, 0.029869964346289635, 0.11400634050369263, 0.26298993825912476, 0.20024387538433075, 0.1163615956902504, 0.1060195341706276, 0.09573828428983688, 0.09006491303443909, 0.03178201615810394, 0.08929051458835602, 0.2047383338212967, 0.09797149151563644, 0.24187994003295898, 0.032703232020139694, 0.11612905561923981, 0.11540436744689941, 0.2100449800491333, 0.03372536227107048, 0.0334571972489357, 0.10689994692802429, 0.03344857692718506, 0.09258624911308289, 0.3597818613052368, 0.03449079394340515, 0.10967512428760529, 0.13259756565093994, 0.034243352711200714, 0.10272639244794846, 0.0340626984834671, 0.03434431180357933, 0.0342181995511055, 0.11268797516822815, 0.033776070922613144, 0.03307664021849632, 0.03301083296537399, 0.032319530844688416, 0.42911678552627563, 0.13119123876094818, 0.032003164291381836, 0.1074199229478836, 0.031428270041942596, 0.08320650458335876, 0.03126141056418419, 0.031073248013854027, 0.12119270116090775, 0.22932271659374237, 0.030585139989852905, 0.1270160973072052, 0.11298045516014099, 0.11024701595306396, 0.13913051784038544, 0.029325630515813828, 0.13546371459960938, 0.02953026443719864, 0.11965546011924744, 0.09405288100242615, 0.10966996103525162, 0.11122415214776993, 0.10755643248558044, 0.24482008814811707, 0.09804310649633408, 0.22966121137142181, 0.02737378515303135, 0.10123629868030548, 0.027356982231140137, 0.09223365038633347, 0.09135337918996811, 0.02713630348443985, 0.027203930541872978, 0.09279239177703857, 0.027038492262363434, 0.11283779889345169, 0.4174230396747589, 0.10945580899715424, 0.026527218520641327, 0.6508325338363647, 0.08826340734958649, 0.0279841311275959, 0.02838851511478424, 0.12249830365180969, 0.22537361085414886, 0.2481907606124878, 0.12749555706977844, 0.24715793132781982, 0.031085824593901634, 0.08531727641820908, 0.03209663927555084, 0.03225028142333031, 0.03253656625747681, 0.4021311104297638, 0.03333130478858948, 0.10724091529846191, 0.034086890518665314, 0.21920029819011688, 0.03503634035587311, 0.034354642033576965, 0.03449060767889023, 0.11004936695098877, 0.22943001985549927, 0.09866367280483246, 0.03490147367119789, 0.1958552896976471, 0.23660920560359955, 0.03521928936243057, 0.10754423588514328, 0.2269512265920639, 0.0356585830450058, 0.03589702770113945, 0.09439510107040405, 0.035982754081487656, 0.10590013116598129, 0.13006369769573212, 0.03517398610711098, 0.034939587116241455, 0.11810947209596634, 0.03455035015940666, 0.034174274653196335, 0.03383722901344299, 0.08474433422088623, 0.03291415050625801, 0.032906416803598404, 0.1275537610054016, 0.031410977244377136, 0.2198638916015625, 0.10465928167104721, 0.10460639744997025, 0.08736951649188995, 0.1022493839263916, 0.09939157962799072, 0.23458899557590485, 0.02930736355483532, 0.02958769164979458, 0.028943030163645744, 0.09565857797861099, 0.02844667248427868, 0.10144075751304626, 0.028100362047553062, 0.10148663073778152, 0.10774794965982437, 0.0274263396859169, 0.24301981925964355, 0.026752691715955734, 0.026452068239450455, 0.11195642501115799, 0.10490751266479492, 0.10423236340284348, 0.09761783480644226, 0.10357759892940521, 0.02537735551595688, 0.24668017029762268, 0.09371082484722137, 0.10619954764842987, 0.09814131259918213, 0.025352247059345245, 0.10107210278511047, 0.09395775198936462, 0.2317487746477127, 0.024618009105324745, 0.11931923776865005, 0.09329017251729965, 0.024853646755218506, 0.25190985202789307, 0.11440547555685043, 0.024759074673056602, 0.02473590336740017, 0.10046874731779099, 0.02469533123075962, 0.02443934604525566, 0.09384853392839432, 0.0241367369890213, 0.023996902629733086, 0.11549248546361923, 0.24298851191997528, 0.09950269758701324, 0.023682208731770515, 0.10442082583904266, 0.023481816053390503, 0.10115157067775726, 0.023544125258922577, 0.02293563075363636, 0.2411605566740036, 0.25938329100608826, 0.09886359423398972, 0.02299647219479084, 0.09806495159864426, 0.023063212633132935, 0.4402175843715668, 0.023423975333571434, 0.09558805078268051, 0.024057036265730858, 0.10332171618938446, 0.024088697507977486, 0.11359699815511703, 0.024202752858400345, 0.12521804869174957, 0.07852255553007126, 0.024038396775722504, 0.10948530584573746, 0.02404092438519001, 0.0238534864038229, 0.023713013157248497, 0.08827819675207138, 0.08729609102010727, 0.09727960079908371, 0.4449869990348816, 0.11241979151964188, 0.023525962606072426, 0.023573508486151695, 0.09528318792581558, 0.023739080876111984, 0.11580713093280792, 0.024012207984924316, 0.02360333874821663, 0.11962036043405533, 0.09347118437290192, 0.10962168127298355, 0.11454150080680847, 0.11349469423294067, 0.09665573388338089, 0.10004151612520218, 0.02280702255666256, 0.1095762550830841, 0.022538118064403534, 0.11509782820940018, 0.02237406186759472, 0.10107842832803726, 0.021998021751642227, 0.021790048107504845, 0.10625828057527542, 0.021366147324442863, 0.08964143693447113, 0.2758112847805023, 0.09015272557735443, 0.02098386362195015, 0.020794693380594254, 0.09985300898551941, 0.28921881318092346, 0.021052580326795578, 0.10446370393037796, 0.2993665039539337, 0.10143796354532242, 0.1098460778594017, 0.09106305986642838, 0.11250659078359604, 0.1149381548166275, 0.021553751081228256, 0.09845951944589615, 0.11378949880599976, 0.02151433937251568, 0.46506908535957336, 0.021712681278586388, 0.021937508136034012, 0.09183311462402344, 0.25184106826782227, 0.02261274866759777, 0.10424451529979706, 0.10781658440828323, 0.11839257180690765, 0.023059789091348648, 0.09037379920482635, 0.10698967427015305, 0.2781318724155426, 0.10115648806095123, 0.023607822135090828, 0.023703401908278465, 0.023708701133728027, 0.11691094189882278, 0.26559025049209595, 0.023720091208815575, 0.023848526179790497, 0.09636373072862625, 0.023897498846054077, 0.28024280071258545, 0.12263524532318115, 0.024162955582141876, 0.02389703318476677, 0.11134635657072067, 0.13196326792240143, 0.4652315676212311, 0.02435437962412834, 0.024801334366202354, 0.24624082446098328, 0.24787704646587372, 0.025400588288903236, 0.025672154501080513, 0.45056217908859253, 0.02653108723461628, 0.026932774111628532, 0.09772049635648727, 0.09573374688625336, 0.027775824069976807, 0.10588813573122025, 0.028165603056550026, 0.251534640789032, 0.13227076828479767, 0.028661765158176422, 0.09676039963960648, 0.10340062528848648, 0.09195386618375778, 0.028857173398137093, 0.028945820406079292, 0.11990952491760254, 0.10217135399580002, 0.028413619846105576, 0.02825910784304142, 0.11245778203010559, 0.027980970218777657, 0.1063402071595192, 0.093015655875206, 0.027141988277435303, 0.026888905093073845, 0.02665308490395546, 0.02630465291440487, 0.1373182088136673, 0.09616105258464813, 0.11089424043893814, 0.08186232298612595, 0.024779032915830612, 0.10037800669670105, 0.024304307997226715, 0.0839526429772377, 0.024128921329975128, 0.10296615213155746, 0.10015738010406494, 0.09598042815923691, 0.10388313233852386, 0.10427536070346832, 0.1039339080452919, 0.02223885990679264, 0.10594047605991364, 0.021774938330054283, 0.2578364908695221, 0.021817076951265335, 0.2549653947353363, 0.021622857078909874, 0.021596599370241165, 0.11036252230405807, 0.12495638430118561, 0.24660196900367737, 0.10084313899278641, 0.25244081020355225, 0.12129678577184677, 0.09914535284042358, 0.2639160752296448, 0.11391928791999817, 0.02283508889377117, 0.023052506148815155, 0.5025641918182373, 0.10464747995138168, 0.10520975291728973, 0.11670013517141342, 0.0998644009232521, 0.11279410868883133, 0.025075742974877357, 0.12061654031276703, 0.10269589722156525, 0.23775427043437958, 0.2518931031227112, 0.2544054687023163, 0.026558926329016685, 0.026915300637483597, 0.40858784317970276, 0.24402639269828796, 0.10675717145204544, 0.11111162602901459, 0.11290711164474487, 0.030142545700073242, 0.030553916469216347, 0.030690500512719154, 0.09855763614177704, 0.10623487085103989, 0.03114117495715618, 0.031139297410845757, 0.09298034757375717, 0.0967184528708458, 0.10816525667905807, 0.2346818745136261, 0.031178735196590424, 0.1075667291879654, 0.031190279871225357, 0.24445456266403198, 0.1113361269235611, 0.2158866673707962, 0.08596329391002655, 0.26150044798851013, 0.10974577069282532, 0.03136659041047096, 0.0316455215215683, 0.03144180402159691, 0.09744538366794586, 0.10466618835926056, 0.031273528933525085, 0.22417326271533966, 0.03117320127785206, 0.031139694154262543, 0.24383167922496796, 0.03169400617480278, 0.031125715002417564, 0.11051976680755615, 0.10732241719961166, 0.2266690880060196, 0.23137213289737701, 0.03072395920753479, 0.2339181900024414, 0.10913557559251785, 0.09576930105686188, 0.03127225115895271, 0.10844819247722626, 0.11075776070356369, 0.12918752431869507, 0.20898877084255219, 0.22116659581661224, 0.1151202991604805, 0.11583642661571503, 0.03182335942983627, 0.24070090055465698, 0.032128408551216125, 0.03237176313996315, 0.03264486417174339, 0.1229407787322998, 0.11324252188205719, 0.03208376094698906, 0.21152377128601074]\n",
            "Val loss 0.09773076975538411\n",
            "Val auc roc 0.4956337650243468\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3d8edc773734c1f8c6328839d0ddf9c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1839.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1024\n",
            "Train Losses : [0.23370730876922607, 0.25188642740249634, 0.03201372176408768, 0.10885743051767349, 0.031902968883514404, 0.22861798107624054, 0.2614220082759857, 0.031953562051057816, 0.0318978913128376, 0.11867175996303558, 0.23843951523303986, 0.03200170025229454, 0.0323667898774147, 0.39347073435783386, 0.03204303979873657, 0.03200911730527878, 0.032096341252326965, 0.2566269040107727, 0.03226587176322937, 0.11263413727283478, 0.032239727675914764, 0.032393503934144974, 0.03220410272479057, 0.10979676246643066, 0.106166310608387, 0.23368392884731293, 0.10552036762237549, 0.03204338625073433, 0.10658804327249527, 0.03222762420773506, 0.09555023908615112, 0.24601790308952332, 0.10740204900503159, 0.24487951397895813, 0.22484906017780304, 0.10839832574129105, 0.10554900020360947, 0.23870021104812622, 0.39969727396965027, 0.11285573244094849, 0.03237094730138779, 0.0324445441365242, 0.11505576968193054, 0.23230451345443726, 0.03237435221672058, 0.26980721950531006, 0.03243333101272583, 0.09927011281251907, 0.10634368658065796, 0.12125855684280396, 0.11471046507358551, 0.11017683148384094, 0.38319939374923706, 0.03271050751209259, 0.03263598307967186, 0.10605213046073914, 0.09921092540025711, 0.03260605037212372, 0.21110643446445465, 0.03271011635661125, 0.09414713829755783, 0.23747004568576813, 0.032661281526088715, 0.0326436348259449, 0.23867006599903107, 0.03264840692281723, 0.10535965114831924, 0.11041705310344696, 0.10581476241350174, 0.21828174591064453, 0.09038396179676056, 0.03276113420724869, 0.09037722647190094, 0.03274359554052353, 0.1955050826072693, 0.03265582025051117, 0.09810639172792435, 0.03262786567211151, 0.22170817852020264, 0.23155836760997772, 0.1091918870806694, 0.1087636947631836, 0.12428967654705048, 0.10120224207639694, 0.10689578205347061, 0.4071454703807831, 0.11637941002845764, 0.09261462092399597, 0.21332307159900665, 0.2288263440132141, 0.10592488944530487, 0.11631651222705841, 0.033068668097257614, 0.03299710899591446, 0.03322599455714226, 0.10605841875076294, 0.21276940405368805, 0.11258281022310257, 0.11339877545833588, 0.03290974348783493, 0.03320341929793358, 0.11321843415498734, 0.22915266454219818, 0.03298861160874367, 0.03300929069519043, 0.38184645771980286, 0.20001399517059326, 0.032924942672252655, 0.03293469548225403, 0.3731435239315033, 0.03306913748383522, 0.10297203063964844, 0.03319118916988373, 0.2194153368473053, 0.2595558166503906, 0.09689491242170334, 0.11560674011707306, 0.12172671407461166, 0.03341973200440407, 0.03334978222846985, 0.11826907098293304, 0.23885412514209747, 0.033455364406108856, 0.11006594449281693, 0.11789896339178085, 0.033303577452898026, 0.03333134204149246, 0.10977113246917725, 0.24765387177467346, 0.08321481943130493, 0.09295320510864258, 0.09791840612888336, 0.09327511489391327, 0.1039256826043129, 0.03338378295302391, 0.033364176750183105, 0.2146671712398529, 0.033207036554813385, 0.03349216654896736, 0.03320545330643654, 0.03330843895673752, 0.11050797998905182, 0.03317832574248314, 0.1058092936873436, 0.1259888857603073, 0.03305671736598015, 0.09629399329423904, 0.37356188893318176, 0.2120567113161087, 0.033007849007844925, 0.11195347458124161, 0.097223199903965, 0.1033424660563469, 0.033243369311094284, 0.033099785447120667, 0.10639678686857224, 0.03297070413827896, 0.11811397224664688, 0.032925356179475784, 0.03291281685233116, 0.09464791417121887, 0.11657080799341202, 0.0329890213906765, 0.10221465677022934, 0.03285438194870949, 0.032707490026950836, 0.084662526845932, 0.0328202061355114, 0.12260214984416962, 0.03263656049966812, 0.10453829169273376, 0.23182103037834167, 0.10743572562932968, 0.10837015509605408, 0.10386334359645844, 0.10181203484535217, 0.09359057992696762, 0.2771439254283905, 0.10373389720916748, 0.10178875178098679, 0.1063341274857521, 0.10024990886449814, 0.22364242374897003, 0.03243744373321533, 0.11023811250925064, 0.1035289540886879, 0.03237239643931389, 0.10919583588838577, 0.03228788822889328, 0.1099102795124054, 0.032326001673936844, 0.1227702647447586, 0.032486267387866974, 0.09762787818908691, 0.24482573568820953, 0.11788368225097656, 0.24666807055473328, 0.12052957713603973, 0.1039392501115799, 0.12080138921737671, 0.23571622371673584, 0.0973614975810051, 0.1035180613398552, 0.09359848499298096, 0.03208479657769203, 0.1053871288895607, 0.1087169274687767, 0.03206139802932739, 0.10477709770202637, 0.03221427649259567, 0.1161051020026207, 0.032065249979496, 0.11261384189128876, 0.032069843262434006, 0.03200799599289894, 0.10699936002492905, 0.11188840121030807, 0.03207563981413841, 0.10683461278676987, 0.032298021018505096, 0.03188195824623108, 0.032033175230026245, 0.03165651485323906, 0.03170929476618767, 0.03166540712118149, 0.03160170093178749, 0.10781402885913849, 0.10868281871080399, 0.03142915666103363, 0.10495416074991226, 0.1179090365767479, 0.11463326960802078, 0.031404364854097366, 0.031472139060497284, 0.031245606020092964, 0.031123772263526917, 0.2226019650697708, 0.031103866174817085, 0.23484347760677338, 0.10550964623689651, 0.10197010636329651, 0.03096284531056881, 0.0943385437130928, 0.11197479814291, 0.031030775979161263, 0.10606199502944946, 0.03093765862286091, 0.2147848904132843, 0.030860982835292816, 0.03076188452541828, 0.03089478239417076, 0.03080574981868267, 0.031165841966867447, 0.39941126108169556, 0.09448269754648209, 0.09747099876403809, 0.11433252692222595, 0.10456696152687073, 0.0937579795718193, 0.030796285718679428, 0.23653943836688995, 0.030781704932451248, 0.2495485246181488, 0.03085639514029026, 0.11323702335357666, 0.03075503185391426, 0.10264227539300919, 0.10779036581516266, 0.03062715381383896, 0.4205525815486908, 0.21550193428993225, 0.030937086790800095, 0.11638811975717545, 0.03071516565978527, 0.030706938356161118, 0.10217772424221039, 0.09250250458717346, 0.22096852958202362, 0.030704468488693237, 0.21279920637607574, 0.41809409856796265, 0.2588917016983032, 0.11953328549861908, 0.030875669792294502, 0.10783936828374863, 0.030956577509641647, 0.23726235330104828, 0.031062282621860504, 0.031034346669912338, 0.10475770384073257, 0.10519547015428543, 0.03119533881545067, 0.031370505690574646, 0.22610196471214294, 0.1193179339170456, 0.03148931637406349, 0.13468503952026367, 0.09619826078414917, 0.09191146492958069, 0.10802213847637177, 0.031123517081141472, 0.10153340548276901, 0.03140236809849739, 0.03106379508972168, 0.09619003534317017, 0.09950736910104752, 0.25200188159942627, 0.031092924997210503, 0.20810550451278687, 0.10654553025960922, 0.031222816556692123, 0.2335294932126999, 0.03110714815557003, 0.03085012175142765, 0.08788154274225235, 0.030943792313337326, 0.09418516606092453, 0.031171413138508797, 0.1233135387301445, 0.03083164431154728, 0.08445597440004349, 0.03078300878405571, 0.09493473172187805, 0.1071031466126442, 0.030984845012426376, 0.030669936910271645, 0.24056638777256012, 0.03062010556459427, 0.11055082827806473, 0.11483301222324371, 0.030638381838798523, 0.09711498022079468, 0.23840124905109406, 0.030645493417978287, 0.2344166785478592, 0.03048761375248432, 0.10645134001970291, 0.0917559340596199, 0.2058945745229721, 0.030502619221806526, 0.11237872391939163, 0.25461286306381226, 0.030467823147773743, 0.23050396144390106, 0.09758975356817245, 0.030550310388207436, 0.2124064415693283, 0.10428855568170547, 0.10384664684534073, 0.030620122328400612, 0.030547259375452995, 0.030580934137105942, 0.2414601445198059, 0.09578124433755875, 0.10467448830604553, 0.1077975332736969, 0.03074486181139946, 0.1058734655380249, 0.03075338527560234, 0.21593762934207916, 0.030713465064764023, 0.3888688385486603, 0.10613678395748138, 0.030537858605384827, 0.11697592586278915, 0.0950772762298584, 0.030583888292312622, 0.11628056317567825, 0.03065103106200695, 0.11118566244840622, 0.10933277010917664, 0.11962806433439255, 0.11395562440156937, 0.09989093989133835, 0.12341015785932541, 0.11963535100221634, 0.11721286177635193, 0.3736291825771332, 0.030604980885982513, 0.10933341830968857, 0.24213816225528717, 0.03059130534529686, 0.235061377286911, 0.10395048558712006, 0.030635621398687363, 0.11085733026266098, 0.12029711157083511, 0.09461366385221481, 0.1115407645702362, 0.03060920350253582, 0.1016082763671875, 0.0306631401181221, 0.030670713633298874, 0.24943426251411438, 0.10673433542251587, 0.030750658363103867, 0.11485033482313156, 0.030542509630322456, 0.03059079870581627, 0.030512014403939247, 0.09651102870702744, 0.25657370686531067, 0.030830757692456245, 0.030540144070982933, 0.2503655254840851, 0.20927545428276062, 0.030431855469942093, 0.10953482985496521, 0.10197783261537552, 0.11299502849578857, 0.23951418697834015, 0.030518079176545143, 0.030545122921466827, 0.23430940508842468, 0.22507554292678833, 0.21402877569198608, 0.030513465404510498, 0.03053145296871662, 0.09990658611059189, 0.030585547909140587, 0.3836427330970764, 0.03066069446504116, 0.030550630763173103, 0.03098040632903576, 0.1189478188753128, 0.10969895124435425, 0.10906579345464706, 0.030674610286951065, 0.030876347795128822, 0.030679088085889816, 0.030532240867614746, 0.23372691869735718, 0.2521662414073944, 0.030634041875600815, 0.22534805536270142, 0.10285662859678268, 0.03059035912156105, 0.11138633638620377, 0.11286168545484543, 0.03070438653230667, 0.10508232563734055, 0.030525285750627518, 0.23889781534671783, 0.030493244528770447, 0.10747095942497253, 0.0925857350230217, 0.22996720671653748, 0.030558550730347633, 0.03059566393494606, 0.23815445601940155, 0.03045596554875374, 0.1278829127550125, 0.09836876392364502, 0.09084286540746689, 0.23925504088401794, 0.030480338260531425, 0.03068627044558525, 0.26060110330581665, 0.107411228120327, 0.0306133683770895, 0.030674584209918976, 0.19962595403194427, 0.2353675216436386, 0.03072662092745304, 0.11035138368606567, 0.03070814348757267, 0.10199127346277237, 0.10556166619062424, 0.030652962625026703, 0.2461850941181183, 0.030761118978261948, 0.11049038171768188, 0.22836531698703766, 0.27366191148757935, 0.030577754601836205, 0.03080892190337181, 0.030594507232308388, 0.2539008557796478, 0.10591699928045273, 0.11348091065883636, 0.03082091361284256, 0.030642081052064896, 0.030582370236516, 0.10882564634084702, 0.030537130311131477, 0.08484053611755371, 0.23331674933433533, 0.22069032490253448, 0.09964858740568161, 0.03061686083674431, 0.1130068302154541, 0.030562350526452065, 0.030485281720757484, 0.03059985674917698, 0.030467506498098373, 0.10929446667432785, 0.10653547942638397, 0.08655061572790146, 0.10679996758699417, 0.415038138628006, 0.030827483162283897, 0.4361911416053772, 0.21644122898578644, 0.03060942143201828, 0.03051423840224743, 0.11738930642604828, 0.24727164208889008, 0.11218059808015823, 0.21647478640079498, 0.0308645348995924, 0.09912417083978653, 0.09977204352617264, 0.030652210116386414, 0.030638625845313072, 0.24183066189289093, 0.22713914513587952, 0.031028229743242264, 0.24084413051605225, 0.22977018356323242, 0.11564063280820847, 0.13157904148101807, 0.21485155820846558, 0.03084861859679222, 0.10402768850326538, 0.030916662886738777, 0.030938997864723206, 0.1189446821808815, 0.0873156487941742, 0.1148470789194107, 0.031076204031705856, 0.031356848776340485, 0.12286971509456635, 0.030874842777848244, 0.10238908231258392, 0.030894024297595024, 0.0984988883137703, 0.10377074033021927, 0.21806854009628296, 0.21569083631038666, 0.100957952439785, 0.031098701059818268, 0.08787117898464203, 0.03084680251777172, 0.11293259263038635, 0.03080812469124794, 0.11177212744951248, 0.09851590543985367, 0.030811753123998642, 0.03071623481810093, 0.09747593849897385, 0.030781012028455734, 0.11280037462711334, 0.11761809140443802, 0.11466171592473984, 0.22403529286384583, 0.030705435201525688, 0.030674371868371964, 0.03066355548799038, 0.10841751843690872, 0.10538362711668015, 0.11280297487974167, 0.03100762516260147, 0.09732332080602646, 0.0873093456029892, 0.2559679448604584, 0.11211241036653519, 0.030864335596561432, 0.03049980290234089, 0.12549711763858795, 0.10305845737457275, 0.030411457642912865, 0.03049195557832718, 0.030307011678814888, 0.10569501668214798, 0.08770530670881271, 0.10415726900100708, 0.2620740234851837, 0.03035910241305828, 0.10750468075275421, 0.11492527276277542, 0.030138332396745682, 0.10583724826574326, 0.0945969745516777, 0.030111169442534447, 0.11591152846813202, 0.030188465490937233, 0.09460707008838654, 0.12252626568078995, 0.23985318839550018, 0.03028004989027977, 0.02999521605670452, 0.030013058334589005, 0.0300558190792799, 0.2282336950302124, 0.0965924933552742, 0.030317671597003937, 0.1081666499376297, 0.10353327542543411, 0.4171748459339142, 0.10960390418767929, 0.25358712673187256, 0.10409655421972275, 0.10418692231178284, 0.030076107010245323, 0.095460906624794, 0.030095743015408516, 0.09742558002471924, 0.030121956020593643, 0.11257431656122208, 0.09817405790090561, 0.11137035489082336, 0.103197380900383, 0.37458351254463196, 0.23343226313591003, 0.030086413025856018, 0.030047951266169548, 0.0299870315939188, 0.03015737049281597, 0.10765495151281357, 0.12025535106658936, 0.10650566220283508, 0.030389899387955666, 0.11406401544809341, 0.03012326918542385, 0.030018994584679604, 0.10361558198928833, 0.10823900997638702, 0.2693116366863251, 0.03007661923766136, 0.12184934318065643, 0.10156955569982529, 0.23917874693870544, 0.029955318197607994, 0.030002593994140625, 0.11120586842298508, 0.10895702987909317, 0.11291864514350891, 0.22025325894355774, 0.11265967041254044, 0.25491926074028015, 0.030032837763428688, 0.10530410706996918, 0.1081906259059906, 0.10209465026855469, 0.116243377327919, 0.029995054006576538, 0.09975224733352661, 0.029941651970148087, 0.10075501352548599, 0.0964634045958519, 0.02986348606646061, 0.10571791976690292, 0.10259658843278885, 0.02991032414138317, 0.10516048222780228, 0.1101783961057663, 0.10521145910024643, 0.029726441949605942, 0.11020450294017792, 0.1041051372885704, 0.11750952899456024, 0.029798738658428192, 0.09897303581237793, 0.2142007201910019, 0.21550358831882477, 0.11051639914512634, 0.029744748026132584, 0.13195782899856567, 0.02961215190589428, 0.22286033630371094, 0.029681256040930748, 0.029700007289648056, 0.23385705053806305, 0.030051829293370247, 0.02957269363105297, 0.029588153585791588, 0.11059778928756714, 0.42331624031066895, 0.09725961834192276, 0.029605768620967865, 0.029671991243958473, 0.10187748819589615, 0.10359934717416763, 0.02972135692834854, 0.20050424337387085, 0.02964138239622116, 0.029647905379533768, 0.4333815276622772, 0.029801975935697556, 0.11240880191326141, 0.24661888182163239, 0.10311315953731537, 0.029794614762067795, 0.09984954446554184, 0.029770979657769203, 0.02973197214305401, 0.09967466443777084, 0.24646224081516266, 0.22603543102741241, 0.029737768694758415, 0.40384441614151, 0.11378593742847443, 0.029773768037557602, 0.10472787916660309, 0.2742648422718048, 0.029869595542550087, 0.029964858666062355, 0.1042051613330841, 0.029861055314540863, 0.029967481270432472, 0.10146591067314148, 0.029862916097044945, 0.11359722167253494, 0.029906807467341423, 0.030008934438228607, 0.029874615371227264, 0.02990570291876793, 0.10932517051696777, 0.0299525149166584, 0.11250749975442886, 0.02989301271736622, 0.10358896106481552, 0.2391204535961151, 0.23203767836093903, 0.10234405100345612, 0.029716873541474342, 0.03001094050705433, 0.1178574338555336, 0.02978031523525715, 0.029788773506879807, 0.08711652457714081, 0.030110638588666916, 0.10422725230455399, 0.09620639681816101, 0.10274901986122131, 0.10905733704566956, 0.029629230499267578, 0.1071997657418251, 0.02959931269288063, 0.10196401178836823, 0.02954564429819584, 0.1124725341796875, 0.02951718494296074, 0.029559772461652756, 0.029625030234456062, 0.09649783372879028, 0.029438382014632225, 0.10588636994361877, 0.029418738558888435, 0.09587620943784714, 0.09586326777935028, 0.029434969648718834, 0.2154761403799057, 0.1257779449224472, 0.10294277966022491, 0.09179676324129105, 0.2552831470966339, 0.02921672910451889, 0.029447447508573532, 0.02929363213479519, 0.23346096277236938, 0.10884138196706772, 0.10410900413990021, 0.0291135311126709, 0.10990279912948608, 0.09878695011138916, 0.25417155027389526, 0.08991274982690811, 0.029094507917761803, 0.10725477337837219, 0.029180562123656273, 0.0988680049777031, 0.029076198115944862, 0.11402250826358795, 0.02915014699101448, 0.11456862837076187, 0.029346073046326637, 0.0290844589471817, 0.2446783483028412, 0.029081961140036583, 0.02907971478998661, 0.028982024639844894, 0.10077369213104248, 0.02907412499189377, 0.029200416058301926, 0.10944945365190506, 0.4111180305480957, 0.0979764312505722, 0.028945954516530037, 0.09850026667118073, 0.02890416793525219, 0.029106780886650085, 0.028970010578632355, 0.2655487358570099, 0.028850167989730835, 0.09582719206809998, 0.02893851324915886, 0.029005995020270348, 0.11480747908353806, 0.09677189588546753, 0.2312842756509781, 0.2271166890859604, 0.02885383926331997, 0.09841902554035187, 0.11659267544746399, 0.4191684126853943, 0.02913435362279415, 0.028836585581302643, 0.11989370733499527, 0.23626843094825745, 0.1005052700638771, 0.028885135427117348, 0.09848625212907791, 0.028966352343559265, 0.02897067368030548, 0.10802605003118515, 0.1003054529428482, 0.02898276410996914, 0.11322193592786789, 0.11688344925642014, 0.11987309902906418, 0.21007655560970306, 0.028839191421866417, 0.02901402860879898, 0.1022351086139679, 0.029059933498501778, 0.02898074872791767, 0.08829546719789505, 0.2590397894382477, 0.22992615401744843, 0.09664008766412735, 0.028845669701695442, 0.23342716693878174, 0.11274559050798416, 0.12476235628128052, 0.028816480189561844, 0.1135641559958458, 0.12082868069410324, 0.029030421748757362, 0.028819352388381958, 0.10926675796508789, 0.0290035642683506, 0.028819244354963303, 0.02875504083931446, 0.10757097601890564, 0.0956810936331749, 0.028850940987467766, 0.028713596984744072, 0.02894614078104496, 0.08963709324598312, 0.028726723045110703, 0.02892266772687435, 0.028742818161845207, 0.11592139303684235, 0.028619693592190742, 0.028756218031048775, 0.09971658140420914, 0.41486436128616333, 0.11756366491317749, 0.11217600107192993, 0.09885154664516449, 0.028572332113981247, 0.10837476700544357, 0.11350934952497482, 0.028768299147486687, 0.11437337845563889, 0.11243851482868195, 0.23601195216178894, 0.11248409748077393, 0.02846434712409973, 0.12188572436571121, 0.028518671169877052, 0.2148483693599701, 0.10969587415456772, 0.028586797416210175, 0.23853044211864471, 0.23581759631633759, 0.09469626098871231, 0.21960534155368805, 0.028556304052472115, 0.24627013504505157, 0.028577368706464767, 0.09905174374580383, 0.11690110713243484, 0.21445569396018982, 0.09224294871091843, 0.028703415766358376, 0.028625506907701492, 0.10459549725055695, 0.10870391130447388, 0.1258552074432373, 0.10447518527507782, 0.41023167967796326, 0.09572478383779526, 0.21580073237419128, 0.2661221921443939, 0.028717949986457825, 0.0967148020863533, 0.02881799452006817, 0.10063949227333069, 0.028758537024259567, 0.02889818139374256, 0.02867797203361988, 0.028717955574393272, 0.028740854933857918, 0.0998055711388588, 0.10922852158546448, 0.23839205503463745, 0.02894064225256443, 0.09276174753904343, 0.028699414804577827, 0.028750061988830566, 0.02884669229388237, 0.23260797560214996, 0.22552165389060974, 0.028796447440981865, 0.09776367247104645, 0.0899130329489708, 0.224208801984787, 0.11946456879377365, 0.1125270426273346, 0.22082774341106415, 0.09455656260251999, 0.10306224226951599, 0.02883872017264366, 0.028748195618391037, 0.25320157408714294, 0.02870911918580532, 0.02877628803253174, 0.028734328225255013, 0.22959929704666138, 0.02897322177886963, 0.23614071309566498, 0.028863755986094475, 0.028807446360588074, 0.2599228620529175, 0.028818506747484207, 0.10022047162055969, 0.09698914736509323, 0.10425422340631485, 0.02868131548166275, 0.028682580217719078, 0.11509012430906296, 0.10606113821268082, 0.11095248907804489, 0.08220290392637253, 0.11503639072179794, 0.028624268248677254, 0.10572781413793564, 0.1001390740275383, 0.02870004065334797, 0.10418305546045303, 0.25166091322898865, 0.11509834975004196, 0.10033217072486877, 0.10479473322629929, 0.028634127229452133, 0.21394942700862885, 0.02858465351164341, 0.028920166194438934, 0.11126920580863953, 0.028560491278767586, 0.23526151478290558, 0.2436067909002304, 0.09336861968040466, 0.028656121343374252, 0.028551552444696426, 0.2502982020378113, 0.02859928458929062, 0.0285808052867651, 0.0285577904433012, 0.10512018203735352, 0.0995449647307396, 0.028780195862054825, 0.11193055659532547, 0.10326430201530457, 0.028601108118891716, 0.09569136798381805, 0.2419864386320114, 0.4315471649169922, 0.028560247272253036, 0.0286564938724041, 0.028706537559628487, 0.02870451845228672, 0.1064385250210762, 0.24458177387714386, 0.02860301174223423, 0.10279165208339691, 0.10048277676105499, 0.028750497847795486, 0.11100604385137558, 0.02854858711361885, 0.10675505548715591, 0.11006307601928711, 0.1115013062953949, 0.028623342514038086, 0.028604641556739807, 0.10845045745372772, 0.12455679476261139, 0.02850698120892048, 0.028620673343539238, 0.10729213804006577, 0.10209023207426071, 0.1138552874326706, 0.4352682828903198, 0.21071971952915192, 0.23469585180282593, 0.10113110393285751, 0.02846796251833439, 0.10587459057569504, 0.10494614392518997, 0.028532808646559715, 0.028624869883060455, 0.028675954788923264, 0.10795356333255768, 0.028564641252160072, 0.10393323004245758, 0.09003770351409912, 0.24104265868663788, 0.028458982706069946, 0.11112149804830551, 0.02848489210009575, 0.02847999334335327, 0.1021379828453064, 0.02854073978960514, 0.09930282086133957, 0.2561427354812622, 0.028493480756878853, 0.0955570787191391, 0.0284588560461998, 0.028468245640397072, 0.02839466743171215, 0.0285307839512825, 0.11541321873664856, 0.09637846052646637, 0.02867073565721512, 0.22214290499687195, 0.02834082767367363, 0.22808842360973358, 0.028315313160419464, 0.028493760153651237, 0.10008799284696579, 0.2414083480834961, 0.028313182294368744, 0.11378474533557892, 0.09453121572732925, 0.028347646817564964, 0.10784168541431427, 0.25345665216445923, 0.10443507879972458, 0.10169782489538193, 0.028454642742872238, 0.088782399892807, 0.3936910033226013, 0.10826997458934784, 0.02844065986573696, 0.028478650376200676, 0.09478646516799927, 0.02834366261959076, 0.2568640112876892, 0.2445644587278366, 0.10306448489427567, 0.25027942657470703, 0.26610520482063293, 0.09260435402393341, 0.10995428264141083, 0.1181458979845047, 0.10081100463867188, 0.028404323384165764, 0.028625361621379852, 0.10559342801570892, 0.028550220653414726, 0.10143619030714035, 0.10042496025562286, 0.10954819619655609, 0.11505118757486343, 0.09634432941675186, 0.028515074402093887, 0.10185219347476959, 0.02856522612273693, 0.11238596588373184, 0.02841193787753582, 0.02852233499288559, 0.09166036546230316, 0.09384344518184662, 0.23132659494876862, 0.11695094406604767, 0.02842779830098152, 0.39886289834976196, 0.10487744957208633, 0.10838127881288528, 0.028346488252282143, 0.39850980043411255, 0.399524986743927, 0.2428583800792694, 0.1134001612663269, 0.11326290667057037, 0.0995418056845665, 0.09742327779531479, 0.028581557795405388, 0.12198728322982788, 0.10317099839448929, 0.09649747610092163, 0.11221831291913986, 0.4062861502170563, 0.028600512072443962, 0.11212708801031113, 0.10422218590974808, 0.08699699491262436, 0.02872941642999649, 0.028750933706760406, 0.02899288758635521, 0.09824900329113007, 0.10909293591976166, 0.10239060968160629, 0.09088940918445587, 0.1104060634970665, 0.028793765231966972, 0.13294295966625214, 0.39001449942588806, 0.02898269146680832, 0.11366613954305649, 0.21017923951148987, 0.26477813720703125, 0.12225888669490814, 0.21865521371364594, 0.12051686644554138, 0.02878379262983799, 0.028806352987885475, 0.02916431427001953, 0.21524479985237122, 0.1029079407453537, 0.23256659507751465, 0.028774170204997063, 0.10776518285274506, 0.028899656608700752, 0.43096327781677246, 0.11417429894208908, 0.09555666148662567, 0.2329956442117691, 0.02903905138373375, 0.11785618960857391, 0.1105315312743187, 0.028927525505423546, 0.028909437358379364, 0.02890237793326378, 0.11971031129360199, 0.029295125976204872, 0.02907908707857132, 0.10979550331830978, 0.10316591709852219, 0.02897709049284458, 0.02906738966703415, 0.02913978323340416, 0.028931235894560814, 0.028898263350129128, 0.10235967487096786, 0.11473148316144943, 0.02886824496090412, 0.11071141064167023, 0.028930235654115677, 0.028862927109003067, 0.23525983095169067, 0.02874455228447914, 0.10015352815389633, 0.0930921733379364, 0.02870091050863266, 0.09548982232809067, 0.028775492683053017, 0.09065273404121399, 0.028812367469072342, 0.23682448267936707, 0.23379825055599213, 0.029032668098807335, 0.10482743382453918, 0.028635669499635696, 0.10259301960468292, 0.11409009993076324, 0.028773801401257515, 0.1131591871380806, 0.10568992048501968, 0.028908776119351387, 0.10444872081279755, 0.1067880168557167, 0.0947318896651268, 0.10354172438383102, 0.12845101952552795, 0.11490050703287125, 0.23178362846374512, 0.02862083911895752, 0.22979822754859924, 0.10591292381286621, 0.028577590361237526, 0.10609964281320572, 0.02864263392984867, 0.10306234657764435, 0.028658978641033173, 0.028564022853970528, 0.028538145124912262, 0.10408942401409149, 0.09395454078912735, 0.22773560881614685, 0.028675120323896408, 0.10360385477542877, 0.02864699997007847, 0.028482044115662575, 0.02860654518008232, 0.028585750609636307, 0.09481339156627655, 0.028553655371069908, 0.22671693563461304, 0.028499331325292587, 0.028497042134404182, 0.02852664701640606, 0.1109984740614891, 0.028455350548028946, 0.10746026784181595, 0.23990735411643982, 0.10782832652330399, 0.2424069195985794, 0.028405703604221344, 0.028456028550863266, 0.10445098578929901, 0.10472489148378372, 0.2308991551399231, 0.10120304673910141, 0.1174316480755806, 0.028399944305419922, 0.11744263768196106, 0.10956080257892609, 0.11231810599565506, 0.028496986255049706, 0.10838073492050171, 0.08840104937553406, 0.24366143345832825, 0.11400841176509857, 0.028346890583634377, 0.0963907539844513, 0.028565863147377968, 0.02837364375591278, 0.10063759982585907, 0.02843860164284706, 0.10996618866920471, 0.23144643008708954, 0.0892535075545311, 0.108678437769413, 0.0284667257219553, 0.028455477207899094, 0.1138196513056755, 0.2321392148733139, 0.11387813836336136, 0.02844628319144249, 0.10178960114717484, 0.0976642444729805, 0.23081588745117188, 0.02831313945353031, 0.21795183420181274, 0.09943892061710358, 0.11117755621671677, 0.0283992700278759, 0.028292648494243622, 0.10474832355976105, 0.02853892557322979, 0.4436371922492981, 0.22505754232406616, 0.10089465230703354, 0.028509221971035004, 0.028309805318713188, 0.028323769569396973, 0.12458275258541107, 0.0283834058791399, 0.028340090066194534, 0.10219253599643707, 0.12457028031349182, 0.028348447754979134, 0.028624171391129494, 0.10348695516586304, 0.10597383230924606, 0.11174790561199188, 0.028331371024250984, 0.028305204585194588, 0.09440833330154419, 0.24496577680110931, 0.10933496803045273, 0.028295913711190224, 0.2464088648557663, 0.22904163599014282, 0.028271568939089775, 0.11444585025310516, 0.02824031561613083, 0.10221385210752487, 0.02828187867999077, 0.24362006783485413, 0.11456392705440521, 0.10940402746200562, 0.0282976683229208, 0.02825126051902771, 0.028373999521136284, 0.028223901987075806, 0.10546782612800598, 0.10957031697034836, 0.02842688001692295, 0.11049520969390869, 0.02867855317890644, 0.028510214760899544, 0.09878367185592651, 0.10587142407894135, 0.2389635145664215, 0.02833794243633747, 0.09572271257638931, 0.028178049251437187, 0.11882628500461578, 0.028314556926488876, 0.2585652470588684, 0.02829110249876976, 0.02823995053768158, 0.2408684939146042, 0.09638234972953796, 0.20150741934776306, 0.02859131619334221, 0.11605986952781677, 0.02816259302198887, 0.028197763487696648, 0.11196233332157135, 0.028141962364315987, 0.028209209442138672, 0.09923677146434784, 0.24185539782047272, 0.11471538990736008, 0.028197383508086205, 0.10601537674665451, 0.028107622638344765, 0.11115892231464386, 0.028098374605178833, 0.028271658346056938, 0.09822014719247818, 0.11453641951084137, 0.028335686773061752, 0.028203321620821953, 0.028112508356571198, 0.02815825678408146, 0.028056353330612183, 0.10238398611545563, 0.112543486058712, 0.02799420803785324, 0.1117425262928009, 0.02819572389125824, 0.09968190640211105, 0.254512757062912, 0.027942592278122902, 0.02806212566792965, 0.10388755053281784, 0.23778703808784485, 0.26429587602615356, 0.20562490820884705, 0.027974873781204224, 0.2315131574869156, 0.102951280772686, 0.2762993276119232, 0.09498027712106705, 0.09526369720697403, 0.02810131013393402, 0.028135444968938828, 0.028100304305553436, 0.1072569414973259, 0.02807331457734108, 0.10954068601131439, 0.25140616297721863, 0.2629086971282959, 0.028096776455640793, 0.028079144656658173, 0.027957722544670105, 0.028087444603443146, 0.0281758364289999, 0.028002746403217316, 0.42364028096199036, 0.11601866036653519, 0.10133574157953262, 0.10068593174219131, 0.02808460406959057, 0.24622079730033875, 0.2256203144788742, 0.02834727056324482, 0.10306200385093689, 0.027984321117401123, 0.11566872149705887, 0.1081831231713295, 0.028001483529806137, 0.10969417542219162, 0.11054699867963791, 0.22683465480804443, 0.028072385117411613, 0.24201872944831848, 0.028173714876174927, 0.11604303866624832, 0.09773047268390656, 0.028095534071326256, 0.10824435204267502, 0.10823516547679901, 0.11164132505655289, 0.10020383447408676, 0.09221366792917252, 0.09846438467502594, 0.22544093430042267, 0.028044095262885094, 0.027976946905255318, 0.0281693022698164, 0.028102997690439224, 0.028064927086234093, 0.028191981837153435, 0.11948476731777191, 0.02794460766017437, 0.1081405058503151, 0.11472446471452713, 0.02803797833621502, 0.2610955536365509, 0.42286303639411926, 0.22467418015003204, 0.10943523049354553, 0.12703047692775726, 0.23818151652812958, 0.23630335927009583, 0.09689660370349884, 0.09277496486902237, 0.11146146059036255, 0.10546981543302536, 0.412075936794281, 0.02815338410437107, 0.2357635796070099, 0.10033343732357025, 0.028106437996029854, 0.028150146827101707, 0.21089906990528107, 0.1028975173830986, 0.028097620233893394, 0.02828068658709526, 0.4309696853160858, 0.10097749531269073, 0.028159890323877335, 0.02813023515045643, 0.028264479711651802, 0.09892857819795609, 0.10792741179466248, 0.02828909642994404, 0.10576895624399185, 0.23245486617088318, 0.41726523637771606, 0.11615774780511856, 0.24499793350696564, 0.10065437853336334, 0.44066980481147766, 0.11040984094142914, 0.11037476360797882, 0.11103707551956177, 0.02835896983742714, 0.028371453285217285, 0.10801544785499573, 0.2243649661540985, 0.10080723464488983, 0.41836604475975037, 0.02841484174132347, 0.26431912183761597, 0.028390271589159966, 0.028376219794154167, 0.10439203679561615, 0.02844223380088806, 0.1030876487493515, 0.12869121134281158, 0.028453830629587173, 0.028595859184861183, 0.11237631738185883, 0.10512518137693405, 0.028378916904330254, 0.02838241681456566, 0.10313499718904495, 0.02835298888385296, 0.10817085951566696, 0.028325209394097328, 0.02839406579732895, 0.21804730594158173, 0.028605975210666656, 0.24807357788085938, 0.10608845204114914, 0.028340408578515053, 0.21645547449588776, 0.028402213007211685, 0.10252318531274796, 0.3962801992893219, 0.028396330773830414, 0.10351689904928207, 0.028368372470140457, 0.0888974592089653, 0.10304056107997894, 0.028477802872657776, 0.09822499006986618, 0.028523704037070274, 0.11575517058372498, 0.23262038826942444, 0.028491118922829628, 0.23869937658309937, 0.02855365350842476, 0.1243986263871193, 0.11411916464567184, 0.10296381264925003, 0.126107856631279, 0.028358599171042442, 0.10477990657091141, 0.10696429014205933, 0.08439292013645172, 0.12366285175085068, 0.028586314991116524, 0.09052696079015732, 0.11030904948711395, 0.028436878696084023, 0.09796199947595596, 0.02832699939608574, 0.11946402490139008, 0.028766246512532234, 0.24005255103111267, 0.11131051182746887, 0.028378304094076157, 0.028448568657040596, 0.41723906993865967, 0.10279620438814163, 0.11361326277256012, 0.02854044735431671, 0.02844882942736149, 0.10728628188371658, 0.028521981090307236, 0.028394147753715515, 0.4313352704048157, 0.23548704385757446, 0.02845507115125656, 0.028339121490716934, 0.028395188972353935, 0.10458776354789734, 0.24174679815769196, 0.11505545675754547, 0.02833363227546215, 0.1085442453622818, 0.028364261612296104, 0.028323840349912643, 0.029047364369034767, 0.1011156216263771, 0.028457745909690857, 0.12320677191019058, 0.10250865668058395, 0.02858581766486168, 0.09832079708576202, 0.028370071202516556, 0.028309639543294907, 0.24050208926200867, 0.2254304140806198, 0.0979790985584259, 0.24623391032218933, 0.09521030634641647, 0.22709275782108307, 0.09918510168790817, 0.02838325873017311, 0.11530951410531998, 0.10405442863702774, 0.0283349696546793, 0.10705573111772537, 0.11966945230960846, 0.1183946430683136, 0.10725709050893784, 0.10560234636068344, 0.02834455855190754, 0.028474951162934303, 0.02833622694015503, 0.21288307011127472, 0.09833525866270065, 0.02864094264805317, 0.028299350291490555, 0.2604837119579315, 0.028284138068556786, 0.11379319429397583, 0.10996770858764648, 0.11427474766969681, 0.23021069169044495, 0.028333090245723724, 0.23857730627059937, 0.028402933850884438, 0.10804969817399979, 0.0283780787140131, 0.028371866792440414, 0.028454847633838654, 0.028604457154870033, 0.0284767746925354, 0.13709446787834167, 0.09295248985290527, 0.02848920226097107, 0.028355391696095467, 0.09672339260578156, 0.0992177352309227, 0.028530631214380264, 0.0285264253616333, 0.11059895157814026, 0.028327245265245438, 0.02834402024745941, 0.10339896380901337, 0.11499445140361786, 0.10083354264497757, 0.02829507179558277, 0.22026926279067993, 0.2546994686126709, 0.10933846980333328, 0.0282510407269001, 0.028554672375321388, 0.02830011583864689, 0.11509459465742111, 0.2205037623643875, 0.11181874573230743, 0.2033836990594864, 0.028311481699347496, 0.1153629720211029, 0.02827332355082035, 0.22734491527080536, 0.028260285034775734, 0.028205465525388718, 0.11140576004981995, 0.10248619318008423, 0.10601244866847992, 0.23918955028057098, 0.1399017572402954, 0.23521816730499268, 0.10612083971500397, 0.24238073825836182, 0.028303975239396095, 0.02838733419775963, 0.028280014172196388, 0.028272660449147224, 0.028286641463637352, 0.028360668569803238, 0.1016150563955307, 0.09925882518291473, 0.11614633351564407, 0.028364036232233047, 0.09481792896986008, 0.11626767367124557, 0.09680946916341782, 0.028219787403941154, 0.0877717137336731, 0.08046884834766388, 0.11529037356376648, 0.028209801763296127, 0.2720057964324951, 0.028168873861432076, 0.02823612466454506, 0.028184613212943077, 0.2665747106075287, 0.24782386422157288, 0.11292382329702377, 0.02854715846478939, 0.11934301257133484, 0.028301848098635674, 0.2349158078432083, 0.028253065422177315, 0.09519750624895096, 0.12169396132230759, 0.1095777079463005, 0.028456605970859528, 0.10616578906774521, 0.02843378856778145, 0.028220996260643005, 0.11197212338447571, 0.028311854228377342, 0.262437641620636, 0.10990125685930252, 0.11321530491113663, 0.2594310939311981, 0.09812238812446594, 0.10159683227539062, 0.1053256019949913, 0.09824170917272568, 0.028202950954437256, 0.02842794731259346, 0.028164559975266457, 0.10805128514766693, 0.028374146670103073, 0.028180448338389397, 0.23484981060028076, 0.028250867500901222, 0.24598614871501923, 0.10590101033449173, 0.11697801947593689, 0.028260188177227974, 0.2580779194831848, 0.028421899303793907, 0.11374910175800323, 0.0281667597591877, 0.41515713930130005, 0.028148291632533073, 0.10251704603433609, 0.11102671921253204, 0.028284775093197823, 0.11572185158729553, 0.10215731710195541, 0.120001882314682, 0.028517454862594604, 0.02817734144628048, 0.028200848028063774, 0.028177151456475258, 0.1160476878285408, 0.24504400789737701, 0.25550198554992676, 0.028262630105018616, 0.10485575348138809, 0.028186291456222534, 0.028238292783498764, 0.2180788367986679, 0.028205610811710358, 0.10650232434272766, 0.26388588547706604, 0.028152260929346085, 0.10330675542354584, 0.09959719330072403, 0.0973082035779953, 0.028658583760261536, 0.028291955590248108, 0.10712780803442001, 0.02848224714398384, 0.028258441016077995, 0.11221801489591599, 0.23768429458141327, 0.11311238259077072, 0.10504256933927536, 0.028287481516599655, 0.10920698195695877, 0.11428352445363998, 0.028135085478425026, 0.028358612209558487, 0.028198465704917908, 0.09963776916265488, 0.028295185416936874, 0.09730338305234909, 0.10860651731491089, 0.028237394988536835, 0.02830149605870247, 0.028366178274154663, 0.11225137859582901, 0.028231879696249962, 0.028140179812908173, 0.02828841097652912, 0.11907003074884415, 0.10108879208564758, 0.02816956862807274, 0.10355459898710251, 0.09964293986558914, 0.11794403940439224, 0.21934086084365845, 0.028110258281230927, 0.13338743150234222, 0.028180278837680817, 0.10673948377370834, 0.02829824946820736, 0.02807135321199894, 0.2535433769226074, 0.028147779405117035, 0.02865498885512352, 0.10598615556955338, 0.028089428320527077, 0.028109189122915268, 0.028163477778434753, 0.12221065908670425, 0.09655822813510895, 0.24764341115951538, 0.23672688007354736, 0.028239835053682327, 0.028134770691394806, 0.028186121955513954, 0.11245563626289368, 0.028285523876547813, 0.02812674455344677, 0.028318054974079132, 0.028287045657634735, 0.10352698713541031, 0.02807197906076908, 0.11769814044237137, 0.09690874069929123, 0.23907232284545898, 0.028083840385079384, 0.02818206511437893, 0.02815929241478443, 0.22830535471439362, 0.23320025205612183, 0.11360987275838852, 0.1024656668305397, 0.12324772030115128, 0.2176443338394165, 0.23339888453483582, 0.11500505357980728, 0.028202634304761887, 0.4321127235889435, 0.10526014864444733, 0.02823195420205593, 0.028101831674575806, 0.12258130311965942, 0.0281999371945858, 0.028089741244912148, 0.11806090176105499, 0.02838483452796936, 0.40947678685188293, 0.11141908913850784, 0.028291847556829453, 0.02810218557715416, 0.19796885550022125, 0.02811247855424881, 0.02827589400112629, 0.02842736802995205, 0.24409466981887817, 0.10696984082460403, 0.028233852237462997, 0.02811037190258503, 0.02822032943367958, 0.11015984416007996, 0.02852928638458252, 0.21630986034870148, 0.09856016933917999, 0.028467172756791115, 0.028096651658415794, 0.10240737348794937, 0.08487176150083542, 0.028259243816137314, 0.028055764734745026, 0.09698028862476349, 0.09977476298809052, 0.11309339106082916, 0.028418080881237984, 0.11582458019256592, 0.10325945913791656, 0.028095759451389313, 0.09546640515327454, 0.10147616267204285, 0.24682369828224182, 0.12337584048509598, 0.1026880294084549, 0.09363625943660736, 0.24811029434204102]\n",
            "Val loss 0.09593702936463319\n",
            "Val auc roc 0.5184455177529969\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7fa2059-70e7-4dac-a63f-2a89d345a223"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}