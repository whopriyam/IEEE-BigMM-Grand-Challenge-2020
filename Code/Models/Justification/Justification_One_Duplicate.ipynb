{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Justification_One_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "836434fe3c65457b965c9ab7e1c9b47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4503e0375e064e2a9ac7b60748908b2d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6add90de50294f058b817ee3aa81b609",
              "IPY_MODEL_b202d7f056634bb59d984c28271e1580"
            ]
          }
        },
        "4503e0375e064e2a9ac7b60748908b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6add90de50294f058b817ee3aa81b609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43924d2ccdf74c9ba62449c1246c9ffb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1650,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1650,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc4d270213234641b7f27122ff86954a"
          }
        },
        "b202d7f056634bb59d984c28271e1580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_418e1b61561642cd9f1ae2926741d07d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1650/1650 [29:48&lt;00:00,  1.08s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fcaf0d9f1f4844e0a2033b9e8a0d6d78"
          }
        },
        "43924d2ccdf74c9ba62449c1246c9ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc4d270213234641b7f27122ff86954a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "418e1b61561642cd9f1ae2926741d07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fcaf0d9f1f4844e0a2033b9e8a0d6d78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18580be845f94fde903003fbaa6bcc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3adc801c754f4211b2b259fba8879dca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52e1a263b4994044b0a5d8b4133cf30d",
              "IPY_MODEL_8ebe56c3c1544948b28499d206645146"
            ]
          }
        },
        "3adc801c754f4211b2b259fba8879dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52e1a263b4994044b0a5d8b4133cf30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83ccb01915bd4fd6bfa44c19aacb4f59",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1650,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1650,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a611f89745d4a0982973df2f05ca493"
          }
        },
        "8ebe56c3c1544948b28499d206645146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6058f76edc504391aab84650cec0845f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1650/1650 [29:07&lt;00:00,  1.06s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f51a122b2368485a8b679064cbac3fdd"
          }
        },
        "83ccb01915bd4fd6bfa44c19aacb4f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a611f89745d4a0982973df2f05ca493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6058f76edc504391aab84650cec0845f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f51a122b2368485a8b679064cbac3fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec43ed6830ea47b48fa93a477bed2b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c85b296a75c4014b7c1f8f2249ce699",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d1ea12f92e2472daf8177f02199b678",
              "IPY_MODEL_a72568b3fe5b44bfa6cc8e44798ce201"
            ]
          }
        },
        "9c85b296a75c4014b7c1f8f2249ce699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d1ea12f92e2472daf8177f02199b678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b3f5c4914ff4b91880e94bd656689f1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1650,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1650,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_453ef40ecec74c36a99939f6bb0298af"
          }
        },
        "a72568b3fe5b44bfa6cc8e44798ce201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b89714e6ff9846fc819e709496df50ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1650/1650 [28:58&lt;00:00,  1.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9fc708e730324c82ae7019557d0b07e2"
          }
        },
        "7b3f5c4914ff4b91880e94bd656689f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "453ef40ecec74c36a99939f6bb0298af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b89714e6ff9846fc819e709496df50ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9fc708e730324c82ae7019557d0b07e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "43646e00-65d3-4c8b-cf65-ea45e2d9aaaf"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 5.51 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0537e886-7f1a-4ba7-abd4-f16f9d531faf"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78ac0066-157c-4f39-e7fb-5d119b8591f7"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c2e1b720-1393-4856-9bf1-f8bd5fff7c5e"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c2484525-c302-43c4-beb3-dd3ee3f150d2"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "878620a4-234c-4cfd-a0b3-cc349e6acc87"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3034db56-816b-4c55-d3de-8090b25fbfef"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f85be8ac-f179-4647-ed9c-001d745e9f2f"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "e85662f0-3728-4265-8cc1-3c785fc5d6fe"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fcc7d6bc-85b8-4d31-e5fb-982e28d722cd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    @kathyonair @TKras @JPPetersonSprts @CharleyBe...\n",
              "1    #BIGNEWS: Some are using #MeToo campaign for p...\n",
              "2                      Anonymous posts weaken #MeToo  \n",
              "3    Edit | Anonymous posts weaken #MeToo   Those j...\n",
              "4    Taxidermy DeathsHead Moth by Oddity Asylum #ta...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "512494ba-365c-49bf-ecd7-eb2fc90f3869"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:15<00:00, 26912134.64B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cd977d2-5451-4d9f-bf7f-86b402bc067c"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 699982.43B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "233d167a-9f9a-46a6-db63-3d3763cd51df"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47509607-6a5a-458c-dde0-1ede81e5824e"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d275ca9-2713-4f94-d2da-619ee23831dd"
      },
      "source": [
        "col_name = \"Justification\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728,
          "referenced_widgets": [
            "836434fe3c65457b965c9ab7e1c9b47e",
            "4503e0375e064e2a9ac7b60748908b2d",
            "6add90de50294f058b817ee3aa81b609",
            "b202d7f056634bb59d984c28271e1580",
            "43924d2ccdf74c9ba62449c1246c9ffb",
            "cc4d270213234641b7f27122ff86954a",
            "418e1b61561642cd9f1ae2926741d07d",
            "fcaf0d9f1f4844e0a2033b9e8a0d6d78",
            "18580be845f94fde903003fbaa6bcc38",
            "3adc801c754f4211b2b259fba8879dca",
            "52e1a263b4994044b0a5d8b4133cf30d",
            "8ebe56c3c1544948b28499d206645146",
            "83ccb01915bd4fd6bfa44c19aacb4f59",
            "5a611f89745d4a0982973df2f05ca493",
            "6058f76edc504391aab84650cec0845f",
            "f51a122b2368485a8b679064cbac3fdd",
            "ec43ed6830ea47b48fa93a477bed2b6d",
            "9c85b296a75c4014b7c1f8f2249ce699",
            "5d1ea12f92e2472daf8177f02199b678",
            "a72568b3fe5b44bfa6cc8e44798ce201",
            "7b3f5c4914ff4b91880e94bd656689f1",
            "453ef40ecec74c36a99939f6bb0298af",
            "b89714e6ff9846fc819e709496df50ab",
            "9fc708e730324c82ae7019557d0b07e2"
          ]
        },
        "outputId": "7b849709-92b4-4030-c5ac-5c1d455ccf4f"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6602\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "836434fe3c65457b965c9ab7e1c9b47e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1650.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0419\n",
            "Train Losses : [0.17825418710708618, 0.17342166602611542, 0.1676734983921051, 0.15747521817684174, 0.14265893399715424, 0.1547037661075592, 0.09937409311532974, 0.07221768796443939, 0.10535569489002228, 0.12100467085838318, 0.01647874154150486, 0.0072476826608181, 0.0031326799653470516, 0.0013838218292221427, 0.00043220524094067514, 0.00016241733101196587, 0.00011375738540664315, 2.990875145769678e-05, 1.1829559298348613e-05, 5.06679316458758e-06, 2.343070718779927e-06, 1.0509093044674955e-06, 0.5374290347099304, 1.1362315490259789e-05, 0.7675407528877258, 1.9478331523714587e-05, 3.3862375858007e-05, 0.0001843703939812258, 0.00012573259300552309, 0.013489097356796265, 0.0005049535539001226, 0.45056402683258057, 0.07720805704593658, 0.061816681176424026, 0.004828264936804771, 0.009740198962390423, 0.022404078394174576, 0.5956175327301025, 0.33240029215812683, 0.0056692142970860004, 0.007089872844517231, 0.012162749655544758, 0.013145296834409237, 0.008950000628829002, 0.009474828839302063, 0.01094027515500784, 0.012287394143640995, 0.011238772422075272, 0.00878275465220213, 0.009428792633116245, 0.22195887565612793, 0.0072172218933701515, 0.006347326096147299, 0.010410661809146404, 0.008678177371621132, 0.008895421400666237, 0.004526811186224222, 0.003837169613689184, 0.004247726872563362, 0.003168626455590129, 0.002706163562834263, 0.11048587411642075, 0.0026839724741876125, 0.28891146183013916, 0.0032381112687289715, 0.002600331325083971, 0.004012495279312134, 0.004405449144542217, 0.09873098134994507, 0.003594823181629181, 0.22783949971199036, 0.012325158342719078, 0.009323286823928356, 0.009762683883309364, 0.03705501928925514, 0.33350449800491333, 0.009707067161798477, 0.09402424097061157, 0.009861207567155361, 0.008705101907253265, 0.007688247133046389, 0.008453963324427605, 0.008744369260966778, 0.007571306545287371, 0.00804650317877531, 0.006014622747898102, 0.42270565032958984, 0.00665323669090867, 0.007591476663947105, 0.16820339858531952, 0.00893547385931015, 0.28416764736175537, 0.010693985968828201, 0.010642312467098236, 0.011479046195745468, 0.0138930669054389, 0.010680288076400757, 0.07296973466873169, 0.010612605139613152, 0.08701790869235992, 0.010729237459599972, 0.010423902422189713, 0.13326384127140045, 0.008767259307205677, 0.010179677046835423, 0.00858727004379034, 0.007857052609324455, 0.007639877963811159, 0.007160970475524664, 0.09902224689722061, 0.006081265863031149, 0.005752007011324167, 0.005229208618402481, 0.005461720284074545, 0.004692854825407267, 0.004484852310270071, 0.0043779658153653145, 0.0038869420532137156, 0.138808935880661, 0.0036032102070748806, 0.0034844467882066965, 0.003514222102239728, 0.13697196543216705, 0.0032935114577412605, 0.0036020821426063776, 0.0036457946989685297, 0.003142192028462887, 0.003008011495694518, 0.0030600011814385653, 0.003049212973564863, 0.0027646550443023443, 0.12001357972621918, 0.10460105538368225, 0.19256505370140076, 0.0031706274021416903, 0.19479501247406006, 0.15813826024532318, 0.004780470859259367, 0.005279859062284231, 0.005840204190462828, 0.11598476022481918, 0.007364044431596994, 0.0074200439266860485, 0.007669737096875906, 0.008211052045226097, 0.008591110818088055, 0.008103646337985992, 0.008046525530517101, 0.00795874372124672, 0.007793762721121311, 0.07927636057138443, 0.1389147788286209, 0.007284915074706078, 0.00714963348582387, 0.007143998984247446, 0.006828689947724342, 0.006485642399638891, 0.006261453032493591, 0.1098114624619484, 0.005835993215441704, 0.005824198946356773, 0.13841377198696136, 0.00537225604057312, 0.005411377642303705, 0.128919780254364, 0.005344515200704336, 0.005331285297870636, 0.005181376822292805, 0.005074474029242992, 0.005031216889619827, 0.004858165979385376, 0.004617750644683838, 0.004395811818540096, 0.1179826483130455, 0.004264240153133869, 0.004114165902137756, 0.00411936966702342, 0.004026270005851984, 0.0038577374070882797, 0.0037220974918454885, 0.0035066534765064716, 0.0033594220876693726, 0.0033182434272021055, 0.0030216737650334835, 0.002877676161006093, 0.002850655233487487, 0.0025721844285726547, 0.10541486740112305, 0.002493391279131174, 0.002420135075226426, 0.0023746832739561796, 0.0023525201249867678, 0.002308051800355315, 0.0022602607496082783, 0.002173501066863537, 0.002102731727063656, 0.1397378146648407, 0.0021428633481264114, 0.002172032603994012, 0.0022876050788909197, 0.0023012161254882812, 0.14787083864212036, 0.002390650101006031, 0.0025344127789139748, 0.002622175496071577, 0.0027140923775732517, 0.13578595221042633, 0.002926579909399152, 0.003030203515663743, 0.13940224051475525, 0.003376759821549058, 0.0035846829414367676, 0.003976020496338606, 0.003918747883290052, 0.11935974657535553, 0.004284399561583996, 0.004449029453098774, 0.00456928089261055, 0.004830561578273773, 0.0999252125620842, 0.09877853095531464, 0.11422401666641235, 0.13200049102306366, 0.0062021855264902115, 0.0065804594196379185, 0.00698285736143589, 0.09719289839267731, 0.0076215025037527084, 0.008012481965124607, 0.34522250294685364, 0.009352508001029491, 0.010728226974606514, 0.011824817396700382, 0.012642758898437023, 0.09607095271348953, 0.013956918381154537, 0.014064992778003216, 0.014270804822444916, 0.014116283506155014, 0.013846714980900288, 0.01348292175680399, 0.13070368766784668, 0.1167188435792923, 0.011745302937924862, 0.01139784324914217, 0.010649210773408413, 0.010596184991300106, 0.009478146210312843, 0.008966024965047836, 0.008112593553960323, 0.007718477863818407, 0.10879933089017868, 0.09855715185403824, 0.006389150395989418, 0.10131173580884933, 0.005918907467275858, 0.1336456686258316, 0.13587519526481628, 0.005869917571544647, 0.10874288529157639, 0.12933889031410217, 0.11141087859869003, 0.0068532503210008144, 0.0072298659943044186, 0.007430842611938715, 0.007605461869388819, 0.00762957101687789, 0.11196614801883698, 0.007682008668780327, 0.3756687343120575, 0.008847814053297043, 0.009819264523684978, 0.010555380024015903, 0.01132944319397211, 0.011761185713112354, 0.011963285505771637, 0.012187747284770012, 0.011992305517196655, 0.11063280701637268, 0.011681821197271347, 0.011465874500572681, 0.011032951064407825, 0.010699509643018246, 0.010136290453374386, 0.009651360101997852, 0.008985132910311222, 0.008336449973285198, 0.007770515978336334, 0.11157916486263275, 0.006823353469371796, 0.11528241634368896, 0.0983486995100975, 0.006167407147586346, 0.005998546257615089, 0.005840707570314407, 0.0057228426449000835, 0.1352388560771942, 0.1103544756770134, 0.005463459994643927, 0.005455058999359608, 0.005415588151663542, 0.005363032221794128, 0.11074508726596832, 0.00530271464958787, 0.005289614200592041, 0.0052223606035113335, 0.005093865562230349, 0.005014925263822079, 0.11498162895441055, 0.3752569854259491, 0.005638635717332363, 0.006381954066455364, 0.007098695263266563, 0.007698805071413517, 0.00820530578494072, 0.008613018319010735, 0.008959390223026276, 0.009096968919038773, 0.009063169360160828, 0.009051964618265629, 0.008970697410404682, 0.008685053326189518, 0.008454362861812115, 0.34971728920936584, 0.00890637282282114, 0.009510033763945103, 0.009871769696474075, 0.010047698393464088, 0.010330394841730595, 0.010382969863712788, 0.0102974409237504, 0.010032173246145248, 0.009821038693189621, 0.009561472572386265, 0.009034843184053898, 0.008652296848595142, 0.008323227986693382, 0.007744837552309036, 0.007331470958888531, 0.0068668401800096035, 0.006411160342395306, 0.0059946272522211075, 0.005581879988312721, 0.005269554443657398, 0.0048401840031147, 0.004556416068226099, 0.11184424161911011, 0.0040335566736757755, 0.1410190314054489, 0.0038336473517119884, 0.0037922777701169252, 0.0037209296133369207, 0.0036722952499985695, 0.003548021661117673, 0.003438015701249242, 0.42621049284935, 0.003859524382278323, 0.00432595144957304, 0.10836161673069, 0.005329939536750317, 0.005835595075041056, 0.006259514484554529, 0.006679232697933912, 0.006961524952203035, 0.007181952241808176, 0.007372576277703047, 0.007389549631625414, 0.007386601530015469, 0.007406107150018215, 0.007220305968075991, 0.10598854720592499, 0.0071073370054364204, 0.0069832224398851395, 0.0068758330307900906, 0.006660418584942818, 0.006486913189291954, 0.006326178554445505, 0.006016694474965334, 0.005851093679666519, 0.09386593848466873, 0.12449774891138077, 0.005312634631991386, 0.005276947282254696, 0.005186690017580986, 0.005139686167240143, 0.005038861650973558, 0.004850233439356089, 0.13486580550670624, 0.129604771733284, 0.37437692284584045, 0.005491040647029877, 0.006172365974634886, 0.0068330056965351105, 0.09450778365135193, 0.008102118037641048, 0.008630577474832535, 0.009155417792499065, 0.11825818568468094, 0.09284782409667969, 0.010257083922624588, 0.010575317777693272, 0.11805226653814316, 0.13551750779151917, 0.10450607538223267, 0.01157367043197155, 0.10784049332141876, 0.10919342190027237, 0.10573592782020569, 0.01250736229121685, 0.012793827801942825, 0.012698824517428875, 0.09456068277359009, 0.01257530227303505, 0.27915892004966736, 0.013105422258377075, 0.013478290289640427, 0.013828999362885952, 0.014058388769626617, 0.013984212651848793, 0.01386354025453329, 0.013510745950043201, 0.013200400397181511, 0.11665123701095581, 0.012358150444924831, 0.10650263726711273, 0.01169626135379076, 0.08768753707408905, 0.010867203585803509, 0.010565719567239285, 0.010076053440570831, 0.009688236750662327, 0.009181286208331585, 0.00872779916971922, 0.008377103134989738, 0.11830410361289978, 0.007461367174983025, 0.007072713691741228, 0.006784370169043541, 0.006414948496967554, 0.0060716671869158745, 0.005663916934281588, 0.005464962217956781, 0.10780644416809082, 0.0048923976719379425, 0.004752372391521931, 0.00454596895724535, 0.00434486661106348, 0.00417843833565712, 0.003970865625888109, 0.13207225501537323, 0.13716405630111694, 0.1213669553399086, 0.003962995018810034, 0.1057777851819992, 0.004200333263725042, 0.0043457807041704655, 0.004428524989634752, 0.10240107774734497, 0.004696267191320658, 0.0048097874969244, 0.004865784198045731, 0.004954909440129995, 0.09328827261924744, 0.12104971706867218, 0.005235049407929182, 0.005396842025220394, 0.10500922799110413, 0.005625654011964798, 0.0057206046767532825, 0.11202431470155716, 0.3368094861507416, 0.006809369195252657, 0.007558595389127731, 0.008158165030181408, 0.34072837233543396, 0.010240916162729263, 0.011450868099927902, 0.012695283629000187, 0.013551347889006138, 0.01428885105997324, 0.014803660102188587, 0.015241600573062897, 0.08959903568029404, 0.015754669904708862, 0.015782555565238, 0.10499244183301926, 0.015498317778110504, 0.08271647989749908, 0.014891977421939373, 0.014509838074445724, 0.014088665135204792, 0.013751216232776642, 0.09168404340744019, 0.09462035447359085, 0.01218340639024973, 0.11124568432569504, 0.011529317125678062, 0.09166426956653595, 0.010903919115662575, 0.010511448606848717, 0.010156272910535336, 0.009570203721523285, 0.009163202717900276, 0.008718478493392467, 0.00845011230558157, 0.11095405369997025, 0.10526842623949051, 0.007318385876715183, 0.007077323738485575, 0.0068082963116467, 0.00662789074704051, 0.006376501172780991, 0.006124414037913084, 0.00583536084741354, 0.08969482034444809, 0.005336469504982233, 0.005238775163888931, 0.004991420079022646, 0.004804965108633041, 0.00459532905369997, 0.0044152457267045975, 0.004314080346375704, 0.1327109932899475, 0.004023617133498192, 0.00405751820653677, 0.0038340354803949594, 0.003799616126343608, 0.0037217785138636827, 0.13818359375, 0.003651738865301013, 0.4518788158893585, 0.004097886849194765, 0.00452518742531538, 0.004939940758049488, 0.13540875911712646, 0.005917798262089491, 0.0063863713294267654, 0.006658890750259161, 0.006912890821695328, 0.0998271182179451, 0.0074720438569784164, 0.007682688534259796, 0.007881169207394123, 0.007956436835229397, 0.007894380949437618, 0.007929827086627483, 0.007830537855625153, 0.007888291962444782, 0.007588544860482216, 0.11942911893129349, 0.08167152106761932, 0.007375242654234171, 0.09855503588914871, 0.007325022481381893, 0.10397560894489288, 0.007286773528903723, 0.007249389309436083, 0.0072579337283968925, 0.007104744203388691, 0.007010249886661768, 0.006859858985990286, 0.006670036818832159, 0.006635904312133789, 0.006344155874103308, 0.005931898020207882, 0.005730149801820517, 0.0055060056038200855, 0.005424530245363712, 0.005086705554276705, 0.004784870892763138, 0.004662733059376478, 0.0043674660846591, 0.004228385630995035, 0.11018209904432297, 0.003957387059926987, 0.12934482097625732, 0.003814764553681016, 0.0037672033067792654, 0.12822385132312775, 0.0038190262857824564, 0.11875498294830322, 0.11675082892179489, 0.004095986485481262, 0.004347486421465874, 0.004465711303055286, 0.004511201288551092, 0.004654925316572189, 0.004681783262640238, 0.004652210511267185, 0.004587491508573294, 0.0045581115409731865, 0.004567363299429417, 0.0045502702705562115, 0.004342264030128717, 0.0041903164237737656, 0.004117046948522329, 0.004148378036916256, 0.003957373555749655, 0.0038205937016755342, 0.003710226621478796, 0.0036356106866151094, 0.0034315299708396196, 0.0032676439732313156, 0.003218557685613632, 0.003099613357335329, 0.0030732350423932076, 0.002859126776456833, 0.002804187824949622, 0.0027358252555131912, 0.17714789509773254, 0.002589901676401496, 0.002573483157902956, 0.002555190585553646, 0.002532983897253871, 0.002500404603779316, 0.002515394939109683, 0.002458300208672881, 0.00239183334633708, 0.0023472104221582413, 0.46234238147735596, 0.002666574204340577, 0.1297299712896347, 0.10626507550477982, 0.37219610810279846, 0.004957101307809353, 0.005912765394896269, 0.09778307378292084, 0.008057383820414543, 0.009051455184817314, 0.010193740017712116, 0.010854620486497879, 0.01172284223139286, 0.012261779978871346, 0.012923871167004108, 0.013025011867284775, 0.08684514462947845, 0.01385527290403843, 0.013694247230887413, 0.013497840613126755, 0.08591169863939285, 0.013508620671927929, 0.012901575304567814, 0.013031918555498123, 0.012757346034049988, 0.011950555257499218, 0.01135649811476469, 0.011005225591361523, 0.010720042511820793, 0.08678240329027176, 0.35611897706985474, 0.009948222897946835, 0.08718682080507278, 0.01089794933795929, 0.010552707128226757, 0.010698691010475159, 0.010902997106313705, 0.010950158350169659, 0.010521777905523777, 0.11649899929761887, 0.010050917975604534, 0.009907473810017109, 0.009695127606391907, 0.009724169969558716, 0.00933199655264616, 0.008804287761449814, 0.008602279238402843, 0.3339099586009979, 0.008477672934532166, 0.12489151209592819, 0.008901814930140972, 0.008907060138881207, 0.009365000762045383, 0.08887595683336258, 0.009471360594034195, 0.009264578111469746, 0.009519105777144432, 0.009064209647476673, 0.009290779009461403, 0.008878035470843315, 0.008698610588908195, 0.11679657548666, 0.008109547197818756, 0.008144359104335308, 0.00777079351246357, 0.1293095052242279, 0.007548265624791384, 0.16100457310676575, 0.007305048406124115, 0.007099029142409563, 0.00714331679046154, 0.11715708673000336, 0.006920402869582176, 0.0067137242294847965, 0.006734189577400684, 0.12028496712446213, 0.006398059893399477, 0.006304321810603142, 0.12212828546762466, 0.09838215261697769, 0.09771572053432465, 0.006509243045002222, 0.006457913666963577, 0.006466329097747803, 0.006469587795436382, 0.006492743268609047, 0.14295630156993866, 0.11402193456888199, 0.006550902500748634, 0.006512685678899288, 0.006408097222447395, 0.09400361776351929, 0.006428759545087814, 0.12290018051862717, 0.006615632213652134, 0.11364497989416122, 0.1332903355360031, 0.006874615326523781, 0.09431494772434235, 0.00705087510868907, 0.007251749746501446, 0.10156331956386566, 0.0923229455947876, 0.007632621098309755, 0.0077856737188994884, 0.007931501604616642, 0.0076796067878603935, 0.007833743467926979, 0.1062694862484932, 0.007496462669223547, 0.007598888128995895, 0.0074333143420517445, 0.007380779832601547, 0.00710982596501708, 0.006987411994487047, 0.006805537734180689, 0.006539673078805208, 0.4193354547023773, 0.4022543430328369, 0.007735721301287413, 0.09143196046352386, 0.10070355981588364, 0.01053552981466055, 0.011278298683464527, 0.012054289691150188, 0.12435832619667053, 0.013348596170544624, 0.013756982982158661, 0.014133349061012268, 0.014134159311652184, 0.01428304798901081, 0.014163056388497353, 0.013919050805270672, 0.013771475292742252, 0.013447429053485394, 0.013099923729896545, 0.01258454006165266, 0.012042242102324963, 0.01159186102449894, 0.011033298447728157, 0.11569203436374664, 0.010185095481574535, 0.009751293808221817, 0.009387088939547539, 0.00890359003096819, 0.09411996603012085, 0.008202861063182354, 0.007886182516813278, 0.007550415582954884, 0.007314017973840237, 0.006978883408010006, 0.0066407788544893265, 0.006371216382831335, 0.006085275672376156, 0.005814630538225174, 0.10570241510868073, 0.0052972882986068726, 0.005170570220798254, 0.1244025006890297, 0.42026227712631226, 0.0052802967838943005, 0.005674514453858137, 0.005986247211694717, 0.006212262436747551, 0.006474826019257307, 0.09783975780010223, 0.09546463191509247, 0.11817018687725067, 0.007391073741018772, 0.007712237536907196, 0.00789677258580923, 0.10530044883489609, 0.008107957430183887, 0.008182089775800705, 0.008358935825526714, 0.008214140310883522, 0.008222942240536213, 0.008096625097095966, 0.09425545483827591, 0.00797300972044468, 0.00784964207559824, 0.007809109520167112, 0.007702161557972431, 0.11497938632965088, 0.10478222370147705, 0.11115290224552155, 0.1050691232085228, 0.007500797044485807, 0.007646339479833841, 0.10790731012821198, 0.0076851570047438145, 0.11538884788751602, 0.0079475874081254, 0.007946714758872986, 0.007700528483837843, 0.11445527523756027, 0.007651421241462231, 0.007544055115431547, 0.007500221487134695, 0.007557556964457035, 0.007245649583637714, 0.10098037868738174, 0.0070492057129740715, 0.007239655125886202, 0.006874171085655689, 0.006794401444494724, 0.006553085520863533, 0.006353647448122501, 0.006240088492631912, 0.006091556046158075, 0.14406535029411316, 0.00569542869925499, 0.12242493778467178, 0.1297294944524765, 0.005697148852050304, 0.005725875496864319, 0.08708999305963516, 0.005607393104583025, 0.005659337155520916, 0.005613395478576422, 0.005612300243228674, 0.14103971421718597, 0.005528522655367851, 0.005532743409276009, 0.11143863201141357, 0.005538785830140114, 0.0055559854954481125, 0.005555214360356331, 0.005514168180525303, 0.005518268793821335, 0.00540023110806942, 0.0052917529828846455, 0.005222635809332132, 0.005129186902195215, 0.13043062388896942, 0.0049333092756569386, 0.004898611456155777, 0.004882605746388435, 0.004779811482876539, 0.004689246416091919, 0.004538593348115683, 0.004478754010051489, 0.004320081323385239, 0.004250182304531336, 0.0041338359005749226, 0.0039936862885952, 0.0038877332117408514, 0.0037704394198954105, 0.0036641531623899937, 0.003521580947563052, 0.0034188618883490562, 0.0033363746479153633, 0.0032060525845736265, 0.0031561676878482103, 0.445940226316452, 0.003320294199511409, 0.0035551204346120358, 0.003799919970333576, 0.125955268740654, 0.004263272043317556, 0.0044646370224654675, 0.004664442967623472, 0.10738570988178253, 0.0050748055800795555, 0.10440720617771149, 0.005502335727214813, 0.005686444230377674, 0.11456625908613205, 0.1008399948477745, 0.006213446147739887, 0.006423978600651026, 0.006583601236343384, 0.006758924573659897, 0.006792450323700905, 0.11902202665805817, 0.006923819426447153, 0.09830372035503387, 0.007029651198536158, 0.007088289130479097, 0.007098084781318903, 0.10004512220621109, 0.10890307277441025, 0.09728624671697617, 0.10691078007221222, 0.007543787360191345, 0.10238193720579147, 0.007803820073604584, 0.007959956303238869, 0.007972948253154755, 0.008040719665586948, 0.008007178083062172, 0.3515246510505676, 0.008367177098989487, 0.09460590779781342, 0.13261766731739044, 0.009782597422599792, 0.010101215913891792, 0.0102974409237504, 0.010605809278786182, 0.1325715184211731, 0.010817456059157848, 0.10418200492858887, 0.010932494886219501, 0.11895117163658142, 0.0109790600836277, 0.011019386351108551, 0.011029875837266445, 0.010880756191909313, 0.010651163756847382, 0.010386854410171509, 0.010167746804654598, 0.1251666098833084, 0.11740636080503464, 0.009627273306250572, 0.11999811977148056, 0.009274830110371113, 0.00910274963825941, 0.008936228230595589, 0.008726958185434341, 0.008567176759243011, 0.10748818516731262, 0.008091412484645844, 0.10157154500484467, 0.007819621823728085, 0.007642380427569151, 0.007491192314773798, 0.007317788433283567, 0.007153811398893595, 0.12689189612865448, 0.0067779067903757095, 0.0066593112424016, 0.12881234288215637, 0.11878389865159988, 0.00640825554728508, 0.006399430800229311, 0.10996729135513306, 0.10230322182178497, 0.006483067292720079, 0.006480866111814976, 0.006465815473347902, 0.10732711851596832, 0.006447168067097664, 0.1104002520442009, 0.006507244426757097, 0.1314200758934021, 0.006627620197832584, 0.006730647757649422, 0.006683225743472576, 0.0066582756116986275, 0.006610133219510317, 0.11979571729898453, 0.1159161627292633, 0.0066117034293711185, 0.1001729741692543, 0.006687799468636513, 0.006714079063385725, 0.006727479863911867, 0.006669677793979645, 0.006637072190642357, 0.1062665581703186, 0.006544627249240875, 0.006485577207058668, 0.006459339056164026, 0.0063222963362932205, 0.006215485744178295, 0.006129661109298468, 0.005953271873295307, 0.005815285257995129, 0.005671136546880007, 0.00550343282520771, 0.12266896665096283, 0.00527803972363472, 0.005181614775210619, 0.00506479199975729, 0.004953539930284023, 0.004865740891546011, 0.004742850083857775, 0.004613467957824469, 0.004483156371861696, 0.004344933200627565, 0.004226210992783308, 0.0041071390733122826, 0.003965620882809162, 0.0038452751468867064, 0.0037269899621605873, 0.13020585477352142, 0.417610228061676, 0.003896174253895879, 0.004198316950351, 0.004475811496376991, 0.0047424910590052605, 0.004947553388774395, 0.1200675368309021, 0.11262939870357513, 0.3689084053039551, 0.11868313699960709, 0.007134655956178904, 0.10659313946962357, 0.09468305110931396, 0.009311612695455551, 0.010006789118051529, 0.010580174624919891, 0.11518926173448563, 0.01161116361618042, 0.31645703315734863, 0.2835877537727356, 0.014374768361449242, 0.015689799562096596, 0.016729526221752167, 0.017637690529227257, 0.018412349745631218, 0.01892964541912079, 0.01933770254254341, 0.019546212628483772, 0.019468797370791435, 0.09872449189424515, 0.019232600927352905, 0.09576798975467682, 0.01865512505173683, 0.01830870658159256, 0.017904020845890045, 0.01741683855652809, 0.016815172508358955, 0.01611420512199402, 0.09881313890218735, 0.014942641369998455, 0.01427896786481142, 0.09244587272405624, 0.11284800618886948, 0.012818283401429653, 0.012406356632709503, 0.011864124797284603, 0.0978870838880539, 0.011013206094503403, 0.01069528330117464, 0.010221428237855434, 0.10661354660987854, 0.11318032443523407, 0.009377009235322475, 0.009096799418330193, 0.008784477598965168, 0.33349278569221497, 0.008699762634932995, 0.008921614848077297, 0.36553955078125, 0.00964929349720478, 0.010194770060479641, 0.010682250373065472, 0.011055054143071175, 0.011238561943173409, 0.11442110687494278, 0.011646569706499577, 0.11046155542135239, 0.11795859783887863, 0.1035965159535408, 0.012182332575321198, 0.01221766322851181, 0.11136496812105179, 0.012358279898762703, 0.012306422926485538, 0.012176458723843098, 0.10182961821556091, 0.011877093464136124, 0.011716087348759174, 0.011535940691828728, 0.01124487817287445, 0.010996614582836628, 0.010660785250365734, 0.01030974742025137, 0.09697519242763519, 0.0096819456666708, 0.11597363650798798, 0.009194507263600826, 0.008992714807391167, 0.008735346607863903, 0.00848668534308672, 0.008227570913732052, 0.00794531311839819, 0.007685985881835222, 0.007405283395200968, 0.007157487794756889, 0.1043115183711052, 0.00668379059061408, 0.11059856414794922, 0.10007254034280777, 0.006322935223579407, 0.10472843050956726, 0.11880797147750854, 0.35467302799224854, 0.006742651108652353, 0.007159825414419174, 0.00748407980427146, 0.007833058014512062, 0.008066991344094276, 0.008242529816925526, 0.008420699276030064, 0.3353010416030884, 0.008945908397436142, 0.31923481822013855, 0.3026074469089508, 0.011701218783855438, 0.012990268878638744, 0.014182687737047672, 0.015160279348492622, 0.016017846763134003, 0.016710905358195305, 0.01721790060400963, 0.017584096640348434, 0.017857568338513374, 0.017866743728518486, 0.01780102029442787, 0.01770009845495224, 0.09429246187210083, 0.017189502716064453, 0.016918526962399483, 0.016507979482412338, 0.016109690070152283, 0.01554748322814703, 0.015059213154017925, 0.014486031606793404, 0.013925411738455296, 0.013303630985319614, 0.012767158448696136, 0.012205271981656551, 0.011556771583855152, 0.011017587967216969, 0.0999329686164856, 0.010068300180137157, 0.009703411720693111, 0.009267797693610191, 0.008848934434354305, 0.10937904566526413, 0.10172142833471298, 0.10436695069074631, 0.10165031254291534, 0.12557123601436615, 0.0077347964979708195, 0.007667840924113989, 0.007568005006760359, 0.007470849901437759, 0.3359571099281311, 0.007648718077689409, 0.00786630343645811, 0.008119919337332249, 0.008243300952017307, 0.008299496956169605, 0.00843353383243084, 0.008401889353990555, 0.008363471366465092, 0.008271396160125732, 0.008211524225771427, 0.00806370284408331, 0.00790447648614645, 0.00781659223139286, 0.0075722201727330685, 0.007465281058102846, 0.007236991077661514, 0.007021977100521326, 0.12078447639942169, 0.006685278378427029, 0.11422211676836014, 0.006497153080999851, 0.006409388966858387, 0.006314689293503761, 0.10500989109277725, 0.006151552312076092, 0.0060816272161901, 0.12349995225667953, 0.13223907351493835, 0.1130303218960762, 0.006034172605723143, 0.006150177679955959, 0.0061522540636360645, 0.0061455233953893185, 0.11849727481603622, 0.006150536239147186, 0.006159503478556871, 0.0061489553190767765, 0.10745234042406082, 0.006091625429689884, 0.006121519021689892, 0.006087033543735743, 0.0060160718858242035, 0.0059518045745790005, 0.11607711017131805, 0.005851351656019688, 0.005836282856762409, 0.005768379662185907, 0.005678897257894278, 0.005606354679912329, 0.005523656960576773, 0.12253034859895706, 0.005385920871049166, 0.10946132987737656, 0.005345970392227173, 0.0053298077546060085, 0.3696203827857971, 0.3958306610584259, 0.11053494364023209, 0.007086599711328745, 0.0078099388629198074, 0.008423203602433205, 0.009045489132404327, 0.00950663723051548, 0.3351532518863678, 0.010817095637321472, 0.11413265764713287, 0.012338276952505112, 0.012985159642994404, 0.09746041148900986, 0.0140231903642416, 0.014445521868765354, 0.014686916954815388, 0.01485805306583643, 0.014917273074388504, 0.014919709414243698, 0.01479626540094614, 0.014639453962445259, 0.014410234987735748, 0.014089006930589676, 0.013754102401435375, 0.013380814343690872, 0.09577958285808563, 0.012661546468734741, 0.11619512736797333, 0.011983398348093033, 0.011720438487827778, 0.11207188665866852, 0.011135936714708805, 0.010831009596586227, 0.09951070696115494, 0.010272476822137833, 0.010035655461251736, 0.009761650115251541, 0.10175316780805588, 0.009270993061363697, 0.11307646334171295, 0.008879706263542175, 0.008696669712662697, 0.00848779920488596, 0.00830809399485588, 0.008079642429947853, 0.11304230988025665, 0.007700655609369278, 0.11096952855587006, 0.10067881643772125, 0.10864214599132538, 0.007361560594290495, 0.007334020920097828, 0.007285197731107473, 0.11531306803226471, 0.0071979002095758915, 0.11878533661365509, 0.1112857311964035, 0.007210714742541313, 0.007224260829389095, 0.007214331533759832, 0.007199042942374945, 0.007109090685844421, 0.10480741411447525, 0.10332952439785004, 0.007035887334495783, 0.007006689440459013, 0.0069826203398406506, 0.006929944269359112, 0.0068481313064694405, 0.11431446671485901, 0.006722717080265284, 0.11498049646615982, 0.006651485338807106, 0.00663589034229517, 0.1135513111948967, 0.0065770577639341354, 0.006580322049558163, 0.10244400054216385, 0.006520196795463562, 0.11984626203775406, 0.006526774261146784, 0.006536608096212149, 0.006520749535411596, 0.10109790414571762, 0.11611749231815338, 0.006572341546416283, 0.006580369081348181, 0.0065663279965519905, 0.00654318742454052, 0.36882832646369934, 0.006847771815955639, 0.007141594309359789, 0.0073910001665353775, 0.007602372672408819, 0.10907836258411407, 0.007932418957352638, 0.008062873966991901, 0.008176151663064957, 0.008208177052438259, 0.10952404886484146, 0.00826527364552021, 0.008296938613057137, 0.008263383992016315, 0.008207553997635841, 0.008154167793691158, 0.008006153628230095, 0.007879026234149933, 0.007741178385913372, 0.007593479473143816, 0.007412512321025133, 0.007246662862598896, 0.11561127752065659, 0.006945643108338118, 0.006805501878261566, 0.006659069564193487, 0.006528179161250591, 0.006367517169564962, 0.00621425686404109, 0.006050470285117626, 0.0059058330953121185, 0.005730682052671909, 0.005567664280533791, 0.005395056214183569, 0.005245281849056482, 0.005103450734168291, 0.004937808029353619, 0.004802499897778034, 0.004649327602237463, 0.11520779132843018, 0.12135592848062515, 0.004421246703714132, 0.0043942928314208984, 0.004351018462330103, 0.11062725633382797, 0.004298241809010506, 0.11776887625455856, 0.004342594649642706, 0.00436270609498024, 0.004379939753562212, 0.004379204474389553, 0.1109737753868103, 0.004400121048092842, 0.0044118426740169525, 0.11234971880912781, 0.0044665527530014515, 0.004509784746915102, 0.004518027417361736, 0.004522716626524925, 0.004517971538007259, 0.004491063300520182, 0.12426497042179108, 0.004478922579437494, 0.10515274107456207, 0.40279629826545715, 0.004908063914626837, 0.0052459449507296085, 0.005554909352213144, 0.005826325621455908, 0.006066545378416777, 0.006289160810410976, 0.006423692684620619, 0.006566556636244059, 0.006639542058110237, 0.006686699111014605, 0.006721511483192444, 0.006718149408698082, 0.006693686358630657, 0.006653182208538055, 0.10411491990089417, 0.0065672071650624275, 0.006574289873242378, 0.006491824984550476, 0.10039254277944565, 0.11198393255472183, 0.006420299410820007, 0.10903153568506241, 0.006462292280048132, 0.006501688156276941, 0.006496905814856291, 0.10470014065504074, 0.11746558547019958, 0.006559568457305431, 0.006586418952792883, 0.006597183179110289, 0.006586762145161629, 0.006529560312628746, 0.006485334597527981, 0.0064193932339549065, 0.006312828976660967, 0.006222289986908436, 0.006099749822169542, 0.00599427567794919, 0.005868323612958193, 0.11113032698631287, 0.005679301917552948, 0.12609164416790009, 0.005574686452746391, 0.10973295569419861, 0.005558586213737726, 0.11574936658143997, 0.005613269284367561, 0.005604857113212347, 0.005620776675641537, 0.005605645012110472, 0.005556907504796982, 0.005506702233105898, 0.005469741765409708, 0.005391499027609825, 0.11893647164106369, 0.005269831512123346, 0.005252629052847624, 0.005188262555748224, 0.005163865629583597, 0.00506975082680583, 0.004992735106498003, 0.00490785064175725, 0.7939157485961914, 0.0054231365211308, 0.0060003069229424, 0.006571692880243063, 0.11075534671545029, 0.11661641299724579, 0.008128037676215172, 0.10279729962348938, 0.10972361266613007, 0.10678974539041519, 0.10067028552293777, 0.010591192170977592, 0.011022979393601418, 0.10580284893512726, 0.011675572954118252, 0.011958188377320766, 0.012155123054981232, 0.012255683541297913, 0.012308851815760136, 0.01227923296391964, 0.012199348770081997, 0.012067907489836216, 0.10192178189754486, 0.011761659756302834, 0.107837975025177, 0.10714338719844818, 0.011462192982435226, 0.011342046782374382, 0.011195269413292408, 0.01097502838820219, 0.010784598998725414, 0.010535946115851402, 0.010263875126838684, 0.01001701969653368, 0.009763097390532494, 0.00948129128664732, 0.009202651679515839, 0.008932340890169144, 0.008610977791249752, 0.008322610519826412, 0.008060956373810768, 0.007792723830789328, 0.007508750539273024, 0.007240576203912497, 0.006995911244302988, 0.006752704735845327, 0.006512077525258064, 0.006289214361459017, 0.11947744339704514, 0.3868582248687744, 0.0061339340172708035, 0.006294503808021545, 0.006452456116676331, 0.12263945490121841, 0.1053580716252327, 0.006815176922827959, 0.0069674113765358925, 0.11120709031820297, 0.11421079933643341, 0.10899025946855545, 0.007509665563702583, 0.0076523348689079285, 0.007759754545986652, 0.00783071294426918, 0.007857689633965492, 0.007855253294110298, 0.10075552016496658, 0.00784884113818407, 0.00782022625207901, 0.10468020290136337, 0.00778338685631752, 0.007751578465104103, 0.007713195402175188, 0.007652993779629469, 0.007562373764812946, 0.0074269892647862434, 0.34487679600715637, 0.007554971147328615, 0.007740628439933062, 0.10377829521894455, 0.008060739375650883, 0.10571333765983582, 0.008334684185683727, 0.008451375178992748, 0.008510767482221127, 0.008569273166358471, 0.008551607839763165, 0.008522795513272285, 0.00845777615904808, 0.008361808024346828, 0.008279665373265743, 0.10957708954811096, 0.6549205183982849, 0.008797052316367626, 0.009483483619987965, 0.10355295985937119, 0.10669856518507004, 0.011233932338654995, 0.011731564067304134, 0.012152479030191898, 0.012558648362755775, 0.0981704592704773, 0.10622093081474304, 0.013249903917312622, 0.013387693092226982, 0.09737379848957062, 0.01360998209565878, 0.013642107136547565, 0.013611968606710434, 0.013536207377910614, 0.01335367001593113, 0.013203214854001999, 0.0967092514038086, 0.012754950672388077, 0.012610593810677528, 0.012315965257585049, 0.012046301737427711, 0.011774356476962566, 0.011459684930741787, 0.11657939851284027, 0.010922640562057495, 0.010674615390598774, 0.010342204943299294, 0.010073911398649216, 0.009794361889362335, 0.00952730793505907, 0.009220220148563385, 0.10456396639347076, 0.008753692731261253, 0.008483519777655602, 0.0082679633051157, 0.00803623627871275, 0.007848024368286133, 0.09946392476558685, 0.0074571543373167515, 0.007240019738674164, 0.007090958766639233, 0.006936711259186268, 0.35483449697494507, 0.006910385098308325, 0.00701166084036231, 0.007086711470037699, 0.007159153465181589, 0.007179525215178728, 0.0072203753516077995, 0.007168331183493137, 0.007127841468900442, 0.0070798881351947784, 0.007025448139756918, 0.11186586320400238, 0.11368165910243988, 0.006883464753627777, 0.006904026959091425, 0.006834825966507196, 0.006791413761675358, 0.006715408060699701, 0.006662485655397177, 0.0065766931511461735, 0.006434374023228884, 0.006350066512823105, 0.006218891125172377, 0.006136260461062193, 0.005974093917757273, 0.005901848431676626, 0.108749158680439, 0.005679768975824118, 0.005561648402363062, 0.005504250060766935, 0.005380900111049414, 0.0994180291891098, 0.005258397199213505, 0.0051908232271671295, 0.005115620791912079, 0.005061892326921225, 0.005006043706089258, 0.004890027455985546, 0.004805274773389101, 0.0047456868924200535, 0.004666341934353113, 0.004549676552414894, 0.11909548193216324, 0.004418870899826288, 0.10707918554544449, 0.11708949506282806, 0.12934380769729614, 0.004487934987992048, 0.00454276567324996, 0.0045945788733661175, 0.11007533967494965, 0.00468301959335804, 0.0047572762705385685, 0.0047628614120185375, 0.1023343950510025, 0.004836343694478273, 0.004872145596891642, 0.11050629615783691, 0.11405061930418015, 0.005068326834589243, 0.12306862324476242, 0.00521441176533699, 0.11543077975511551, 0.005418757442384958, 0.11599716544151306, 0.005648166872560978, 0.1078672781586647, 0.005917558912187815, 0.005948991049081087, 0.006043699104338884, 0.006109059322625399, 0.006144022569060326, 0.006124433595687151, 0.006119223777204752, 0.00608074339106679, 0.006071495357900858, 0.12054954469203949, 0.005991017911583185, 0.005964812356978655, 0.106842540204525]\n",
            "Val loss 0.03402971549005043\n",
            "Val auc roc 0.5112903225806451\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18580be845f94fde903003fbaa6bcc38",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1650.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0378\n",
            "Train Losses : [0.1151173859834671, 0.005995579529553652, 0.005990628153085709, 0.0060065439902246, 0.005999682936817408, 0.1138998419046402, 0.005978303961455822, 0.005963141098618507, 0.11395350098609924, 0.005975788924843073, 0.0060305665247142315, 0.11740146577358246, 0.005987466778606176, 0.005998337175697088, 0.005981797352433205, 0.00594122800976038, 0.12035410851240158, 0.12337452918291092, 0.005984003189951181, 0.10730239003896713, 0.112995944917202, 0.006141435354948044, 0.006244190968573093, 0.006279663648456335, 0.006294106598943472, 0.006292772479355335, 0.006275552790611982, 0.006235173437744379, 0.006173490080982447, 0.006118291057646275, 0.006064099259674549, 0.005971338599920273, 0.1045965850353241, 0.00582185760140419, 0.005774081684648991, 0.005719243548810482, 0.10633853822946548, 0.005620909389108419, 0.0055823009461164474, 0.0055656167678534985, 0.005478337872773409, 0.005397316999733448, 0.005376813933253288, 0.0052507794462144375, 0.005163284484297037, 0.0050713978707790375, 0.004973352886736393, 0.004914339631795883, 0.004788734018802643, 0.004734780173748732, 0.11950033903121948, 0.004564360715448856, 0.12000761926174164, 0.11728055775165558, 0.09901581704616547, 0.12902875244617462, 0.004712924361228943, 0.004789813421666622, 0.11258316040039062, 0.004960707854479551, 0.005079586990177631, 0.11097483336925507, 0.005205739755183458, 0.005279839504510164, 0.005350065883249044, 0.11434270441532135, 0.11487141251564026, 0.005545462481677532, 0.005626557394862175, 0.005656606052070856, 0.005693238694220781, 0.005706686992198229, 0.00574513990432024, 0.10733026266098022, 0.005722170230001211, 0.005762295797467232, 0.005748673342168331, 0.0056849815882742405, 0.11524452269077301, 0.0056763640604913235, 0.005662119016051292, 0.0056198290549218655, 0.10844500362873077, 0.005613493267446756, 0.0055764224380254745, 0.37972354888916016, 0.005834916140884161, 0.006076660938560963, 0.11649575084447861, 0.006521269679069519, 0.10236166417598724, 0.006927222013473511, 0.007096066605299711, 0.007226972840726376, 0.0073685916140675545, 0.007418137975037098, 0.00745724281296134, 0.007471094373613596, 0.0074753789231181145, 0.007420716807246208, 0.007351744920015335, 0.007285925559699535, 0.10126366466283798, 0.007159383501857519, 0.007094919681549072, 0.007038203068077564, 0.006989953573793173, 0.006846894975751638, 0.006737650837749243, 0.1206989660859108, 0.00654209777712822, 0.006473808083683252, 0.006392897106707096, 0.0063024829141795635, 0.09363072365522385, 0.00614545214921236, 0.006056677084416151, 0.005959481466561556, 0.11619848012924194, 0.005835508927702904, 0.005798851139843464, 0.005740416701883078, 0.11327488720417023, 0.005662468262016773, 0.005630400963127613, 0.1075982078909874, 0.005582370329648256, 0.005546622909605503, 0.005549517925828695, 0.09979838132858276, 0.005471726413816214, 0.0054359943605959415, 0.11617676168680191, 0.005417774897068739, 0.005461545195430517, 0.3785724341869354, 0.11003974825143814, 0.00599968247115612, 0.006243598647415638, 0.006451878696680069, 0.006695345509797335, 0.006797559559345245, 0.006908530369400978, 0.006987201515585184, 0.3603629469871521, 0.007419825531542301, 0.0077340020798146725, 0.00804862193763256, 0.10745762288570404, 0.008508621715009212, 0.008689399808645248, 0.09063587337732315, 0.008983299136161804, 0.009088937193155289, 0.0925971269607544, 0.009257826954126358, 0.00931551679968834, 0.009347476065158844, 0.009351006709039211, 0.00930735282599926, 0.34488996863365173, 0.11111324280500412, 0.009752519428730011, 0.00997362844645977, 0.010137254372239113, 0.010352869518101215, 0.10697555541992188, 0.01047400664538145, 0.010502941906452179, 0.010491029359400272, 0.10682296752929688, 0.010444825515151024, 0.010405940003693104, 0.10224828869104385, 0.010250400751829147, 0.010161184705793858, 0.3262048363685608, 0.010326173156499863, 0.010542115196585655, 0.010699109174311161, 0.010779764503240585, 0.01083165779709816, 0.010756457224488258, 0.010687548667192459, 0.10527746379375458, 0.010630782693624496, 0.12337307631969452, 0.010421987622976303, 0.010407001711428165, 0.10486305505037308, 0.010187896899878979, 0.1072516143321991, 0.010026863776147366, 0.0099237821996212, 0.009830256924033165, 0.009673032909631729, 0.009512022137641907, 0.009363126009702682, 0.009140041656792164, 0.008982991799712181, 0.00878909882158041, 0.008547693490982056, 0.008361339569091797, 0.008114085532724857, 0.10890500992536545, 0.11542412638664246, 0.007656094618141651, 0.0075363898649811745, 0.007440519519150257, 0.0072801862843334675, 0.11385378241539001, 0.0070397937670350075, 0.11333093792200089, 0.006909636314958334, 0.006840582937002182, 0.0067754993215203285, 0.12319758534431458, 0.006609100382775068, 0.006567073054611683, 0.006496411748230457, 0.006415932439267635, 0.11524856090545654, 0.006281030829995871, 0.006224219687283039, 0.006168300285935402, 0.006096550263464451, 0.10915612429380417, 0.005968737881630659, 0.005913452710956335, 0.11552361398935318, 0.005862872581928968, 0.005804677959531546, 0.005771963857114315, 0.005710757803171873, 0.11660092324018478, 0.005632409825921059, 0.005602775141596794, 0.005568855907768011, 0.00551025103777647, 0.005454892758280039, 0.005387079901993275, 0.005312316119670868, 0.005233068950474262, 0.005149280186742544, 0.10973586142063141, 0.0050269803032279015, 0.10826724767684937, 0.1137106642127037, 0.0050214859656989574, 0.005022612866014242, 0.005035763140767813, 0.005013844929635525, 0.3982973098754883, 0.10673084855079651, 0.005526858847588301, 0.005785253830254078, 0.005968782119452953, 0.006149518769234419, 0.1203179880976677, 0.006494545843452215, 0.006599366664886475, 0.11015016585588455, 0.006848483812063932, 0.006960567552596331, 0.007044351194053888, 0.00707219447940588, 0.007096888497471809, 0.007092910818755627, 0.007075463887304068, 0.0070276339538395405, 0.006982074584811926, 0.006919516250491142, 0.006827585864812136, 0.00673813559114933, 0.10813305526971817, 0.11739886552095413, 0.006559659726917744, 0.006527957506477833, 0.006489424034953117, 0.006433369126170874, 0.006357621867209673, 0.10333824902772903, 0.0062549603171646595, 0.006188374478369951, 0.1115371584892273, 0.11295749992132187, 0.006128494627773762, 0.00613674521446228, 0.006122452672570944, 0.006086206063628197, 0.006049256771802902, 0.005993442144244909, 0.005934913642704487, 0.005872267298400402, 0.005789261776953936, 0.11910437792539597, 0.005692778620868921, 0.005619275849312544, 0.005550683941692114, 0.12318497151136398, 0.00547005096450448, 0.005451522767543793, 0.005395368207246065, 0.005343309603631496, 0.005289630964398384, 0.0052291229367256165, 0.0051480429247021675, 0.005069263279438019, 0.004992833826690912, 0.004902011714875698, 0.0048241098411381245, 0.11498665064573288, 0.11556318402290344, 0.0047101303935050964, 0.004698071163147688, 0.004681820049881935, 0.00465192086994648, 0.004614409990608692, 0.004577353596687317, 0.004524999298155308, 0.00448095565661788, 0.004428090527653694, 0.11663563549518585, 0.004337451420724392, 0.00431545777246356, 0.0042926473543047905, 0.004250421188771725, 0.0042038862593472, 0.11467432230710983, 0.004157110117375851, 0.0041394722647964954, 0.004130516201257706, 0.004094595089554787, 0.004070236813277006, 0.004028612282127142, 0.003988444805145264, 0.00394489336758852, 0.42354118824005127, 0.00408356636762619, 0.004266034346073866, 0.004406020976603031, 0.004556285683065653, 0.12030442804098129, 0.11700498312711716, 0.004961449187248945, 0.11748386919498444, 0.005269225221127272, 0.10673696547746658, 0.005577756091952324, 0.005723653826862574, 0.005823761224746704, 0.0059216818772256374, 0.11576467007398605, 0.006081423256546259, 0.006143845152109861, 0.0062003545463085175, 0.006227128207683563, 0.006248968653380871, 0.006227276287972927, 0.006193682085722685, 0.006157300900667906, 0.006103003863245249, 0.006049046292901039, 0.005996997468173504, 0.005903301760554314, 0.36734336614608765, 0.006019108928740025, 0.006165964063256979, 0.006308331154286861, 0.006415880285203457, 0.006493383087217808, 0.10160244256258011, 0.006634876597672701, 0.0066945296712219715, 0.006723528262227774, 0.006741134449839592, 0.10500830411911011, 0.0067435200326144695, 0.11628763377666473, 0.00678918045014143, 0.0068137324415147305, 0.11118929833173752, 0.006852346006780863, 0.006870745215564966, 0.006862054578959942, 0.006825173273682594, 0.0067885154858231544, 0.006709717679768801, 0.0066479905508458614, 0.006568919867277145, 0.00647734897211194, 0.10890870541334152, 0.0063375188037753105, 0.006270053796470165, 0.0061887102201581, 0.006109803449362516, 0.0060090357437729836, 0.0059308321215212345, 0.005828895140439272, 0.1045457050204277, 0.005669542122632265, 0.10830539464950562, 0.005583595018833876, 0.11823076009750366, 0.0055739423260092735, 0.005550498142838478, 0.005523572210222483, 0.005491156131029129, 0.10968499630689621, 0.10632064938545227, 0.00548949372023344, 0.0054938290268182755, 0.0055231498554348946, 0.005488013848662376, 0.10388866066932678, 0.005490112118422985, 0.005474991165101528, 0.005468511488288641, 0.005425707437098026, 0.005404711700975895, 0.005338173359632492, 0.00528433732688427, 0.0052378224208951, 0.005170836113393307, 0.005098715890198946, 0.11363241076469421, 0.11847512423992157, 0.005017181392759085, 0.005016869865357876, 0.00498757092282176, 0.0049545313231647015, 0.004929036367684603, 0.004883600398898125, 0.004847109317779541, 0.004800633992999792, 0.1283246874809265, 0.004711252171546221, 0.004681876860558987, 0.004659055266529322, 0.00462776143103838, 0.004588989075273275, 0.004536809399724007, 0.11264944076538086, 0.004472159780561924, 0.004453942179679871, 0.0044258758425712585, 0.004395520314574242, 0.12616349756717682, 0.00433169212192297, 0.004332826007157564, 0.004315607249736786, 0.004300201777368784, 0.004264364019036293, 0.004233109299093485, 0.12125761806964874, 0.004205258563160896, 0.004185955505818129, 0.0041597336530685425, 0.12139716744422913, 0.10969188064336777, 0.004211376886814833, 0.12039749324321747, 0.11276064068078995, 0.004422277212142944, 0.0045153358951210976, 0.1090334877371788, 0.00466697383671999, 0.004746012855321169, 0.12603867053985596, 0.004897542763501406, 0.11289467662572861, 0.005074952729046345, 0.005144238006323576, 0.11302714794874191, 0.115232914686203, 0.4058459401130676, 0.11897789686918259, 0.006220424082130194, 0.00656626233831048, 0.0069206636399030685, 0.007195997983217239, 0.007444755639880896, 0.007634393870830536, 0.007806248497217894, 0.11415234208106995, 0.008072467520833015, 0.008182651363313198, 0.00826425850391388, 0.008287936449050903, 0.008315346203744411, 0.00829972606152296, 0.11375521868467331, 0.6623541116714478, 0.008953801356256008, 0.3180883824825287, 0.010430228896439075, 0.011262108571827412, 0.011991971172392368, 0.012636289931833744, 0.013215371407568455, 0.013656464405357838, 0.014043252915143967, 0.11173072457313538, 0.10273470729589462, 0.09703895449638367, 0.015098177827894688, 0.10859675705432892, 0.015483293682336807, 0.015549197793006897, 0.01554236188530922, 0.015500560402870178, 0.1137213185429573, 0.01533487718552351, 0.015208900906145573, 0.015035661868751049, 0.014831200242042542, 0.01456096488982439, 0.014284116216003895, 0.013991444371640682, 0.013686886988580227, 0.3032151758670807, 0.013354714028537273, 0.013338549993932247, 0.013267600908875465, 0.013183881528675556, 0.01300110574811697, 0.10528209060430527, 0.012696810066699982, 0.012503677979111671, 0.012309330515563488, 0.01210513710975647, 0.011887049302458763, 0.011613866314291954, 0.01135421171784401, 0.011098718270659447, 0.010808544233441353, 0.10661982744932175, 0.10202031582593918, 0.010163512080907822, 0.009967478923499584, 0.00977364182472229, 0.3151581585407257, 0.009677696973085403, 0.009747601114213467, 0.009798160754144192, 0.009787184186279774, 0.00974305160343647, 0.31709200143814087, 0.009900418110191822, 0.01007747184485197, 0.01023640763014555, 0.10282117128372192, 0.010405922308564186, 0.10420703887939453, 0.010576708242297173, 0.11079104244709015, 0.10376564413309097, 0.1022137999534607, 0.010818539187312126, 0.10538206249475479, 0.010933268815279007, 0.01095989253371954, 0.010947948321700096, 0.10408567637205124, 0.010887036100029945, 0.010826541110873222, 0.10585413873195648, 0.010701713152229786, 0.010625832714140415, 0.01050430629402399, 0.010355479083955288, 0.010224724188446999, 0.010076368227601051, 0.009876112453639507, 0.11020127683877945, 0.009553362615406513, 0.11493711918592453, 0.10787441581487656, 0.009252699092030525, 0.11031489819288254, 0.00909640546888113, 0.10217006504535675, 0.10665442049503326, 0.008984080515801907, 0.008939233608543873, 0.008897104300558567, 0.008804361335933208, 0.10268265008926392, 0.11229830235242844, 0.10917066782712936, 0.008649567142128944, 0.008648774586617947, 0.008598962798714638, 0.008550276048481464, 0.008467454463243484, 0.103835828602314, 0.008321303874254227, 0.00824554543942213, 0.008148343302309513, 0.008030270226299763, 0.007928062230348587, 0.0078095849603414536, 0.10405225306749344, 0.007606523111462593, 0.007513766176998615, 0.1131589487195015, 0.007321163080632687, 0.007258768659085035, 0.007177135907113552, 0.3510577976703644, 0.007256783545017242, 0.12017197161912918, 0.007557589560747147, 0.0076885162852704525, 0.007778691593557596, 0.007850763387978077, 0.007886326871812344, 0.11220185458660126, 0.10868801176548004, 0.10994544625282288, 0.008089568465948105, 0.008136450313031673, 0.11508285999298096, 0.008252356201410294, 0.008264449425041676, 0.008284393697977066, 0.008261668495833874, 0.008219719864428043, 0.00815429724752903, 0.008093173615634441, 0.008004575036466122, 0.007906164973974228, 0.007788301911205053, 0.007675656583160162, 0.10461824387311935, 0.007461083587259054, 0.007371238898485899, 0.007265372667461634, 0.007153939455747604, 0.007038302253931761, 0.0069246538914740086, 0.006806171499192715, 0.006666552275419235, 0.00654104258865118, 0.006409845780581236, 0.006292582023888826, 0.006158537697046995, 0.0060375286266207695, 0.005899280775338411, 0.0057808044366538525, 0.005661994684487581, 0.005534293595701456, 0.005408953409641981, 0.00529702752828598, 0.005181997083127499, 0.00506938248872757, 0.11053887754678726, 0.11709697544574738, 0.004861393477767706, 0.00483619375154376, 0.11425799131393433, 0.004802644718438387, 0.0047877137549221516, 0.004756335634738207, 0.10868658870458603, 0.11342255026102066, 0.3828631639480591, 0.005028264131397009, 0.1084805503487587, 0.005492121446877718, 0.0057180533185601234, 0.005899466574192047, 0.12349681556224823, 0.006218334659934044, 0.006388548761606216, 0.006512071471661329, 0.1023617833852768, 0.006708635948598385, 0.006811146624386311, 0.006878472864627838, 0.006904104724526405, 0.006951691582798958, 0.006927756126970053, 0.0069109415635466576, 0.11133734881877899, 0.006901424843817949, 0.11325312405824661, 0.006899499334394932, 0.006909345276653767, 0.006878074258565903, 0.006849490571767092, 0.006817512679845095, 0.006760185118764639, 0.006681236904114485, 0.006619182415306568, 0.0065295761451125145, 0.0064291269518435, 0.006344311870634556, 0.006242590025067329, 0.006147191859781742, 0.006035368889570236, 0.005920257419347763, 0.10576173663139343, 0.005765325389802456, 0.11109345406293869, 0.0056568775326013565, 0.00560405058786273, 0.10463067144155502, 0.005565905477851629, 0.005526186432689428, 0.005501580890268087, 0.00544138066470623, 0.11985665559768677, 0.1107310876250267, 0.005418756511062384, 0.00542458426207304, 0.12156721949577332, 0.005448630545288324, 0.005456846207380295, 0.005451228003948927, 0.005438067950308323, 0.11820671707391739, 0.005447693634778261, 0.1266135573387146, 0.005479775369167328, 0.005498453043401241, 0.005495620891451836, 0.005507355555891991, 0.12178482115268707, 0.005495954770594835, 0.005504865199327469, 0.005482470616698265, 0.10636866092681885, 0.005482061300426722, 0.005478934850543737, 0.10663680732250214, 0.005487728398293257, 0.005508457310497761, 0.11096325516700745, 0.005523923318833113, 0.0055364640429615974, 0.00554341496899724, 0.005523376166820526, 0.005496608559042215, 0.005470482166856527, 0.10818921774625778, 0.11650140583515167, 0.005440136417746544, 0.10971088707447052, 0.005486310459673405, 0.119217649102211, 0.005586571060121059, 0.11170459538698196, 0.10456246137619019, 0.10434373468160629, 0.005886304657906294, 0.005973238032311201, 0.006048977375030518, 0.00610346207395196, 0.006128828506916761, 0.10467072576284409, 0.006192792672663927, 0.11346239596605301, 0.11944340169429779, 0.12221932411193848, 0.006434481590986252, 0.006540731061249971, 0.3796018362045288, 0.006891885306686163, 0.007157136686146259, 0.0073896232061088085, 0.0075745959766209126, 0.007720086257904768, 0.007841343991458416, 0.007957227528095245, 0.008001415058970451, 0.008041012100875378, 0.008061125874519348, 0.00803848635405302, 0.008008486591279507, 0.007964211516082287, 0.10120906680822372, 0.007883813232183456, 0.007834465242922306, 0.007747369818389416, 0.007694309577345848, 0.007590052671730518, 0.0074971928261220455, 0.3352358043193817, 0.007558081299066544, 0.007655255030840635, 0.007761103566735983, 0.007827878929674625, 0.00786023773252964, 0.007869589142501354, 0.007864500395953655, 0.007829507812857628, 0.1195167601108551, 0.007792188785970211, 0.007733303587883711, 0.007704910356551409, 0.10057233273983002, 0.0076230717822909355, 0.007578172720968723, 0.007524359039962292, 0.0074538919143378735, 0.007365753408521414, 0.10530537366867065, 0.007221270352602005, 0.007142041344195604, 0.007088489830493927, 0.006995657924562693, 0.006898644380271435, 0.006811833009123802, 0.006712765898555517, 0.006605858914554119, 0.1068858876824379, 0.006423735525459051, 0.006338205654174089, 0.006249706260859966, 0.0061834692023694515, 0.006085403263568878, 0.0059801992028951645, 0.0058807493187487125, 0.005802256520837545, 0.005691899918019772, 0.005598283372819424, 0.12234069406986237, 0.1062161773443222, 0.0054186806082725525, 0.005376284010708332, 0.005334776360541582, 0.005293191410601139, 0.005246708635240793, 0.005200643558055162, 0.11321572214365005, 0.00511276675388217, 0.005077894777059555, 0.005048776976764202, 0.0049940356984734535, 0.10769633203744888, 0.004925189074128866, 0.10429670661687851, 0.004930120427161455, 0.0049147214740514755, 0.12395083904266357, 0.004936004523187876, 0.10998991876840591, 0.004986578598618507, 0.10822554677724838, 0.005054248031228781, 0.005102427676320076, 0.005130987148731947, 0.11506278067827225, 0.10942161828279495, 0.005255572032183409, 0.11865907907485962, 0.005390089936554432, 0.005459026899188757, 0.005475145298987627, 0.11517360806465149, 0.005579719785600901, 0.10552704334259033, 0.0056601413525640965, 0.11303262412548065, 0.13100910186767578, 0.005887169390916824, 0.005973489955067635, 0.10263102501630783, 0.00612524664029479, 0.006172281224280596, 0.006226657889783382, 0.1021491065621376, 0.006284186616539955, 0.11747599393129349, 0.006399353966116905, 0.10470876842737198, 0.006478626746684313, 0.006505934987217188, 0.09899183362722397, 0.006611347198486328, 0.006629668176174164, 0.006661317776888609, 0.006613860838115215, 0.006617652717977762, 0.006577707827091217, 0.11238706111907959, 0.006548181176185608, 0.00648519629612565, 0.10812894254922867, 0.12179578840732574, 0.0065228817984461784, 0.006532884202897549, 0.006504748947918415, 0.006475673522800207, 0.0064344871789216995, 0.11233343183994293, 0.00639313505962491, 0.0063587576150894165, 0.006336427293717861, 0.11939484626054764, 0.006278349086642265, 0.10012717545032501, 0.10986123979091644, 0.006287047173827887, 0.10246296226978302, 0.006371517665684223, 0.006396070122718811, 0.006412678398191929, 0.006429673172533512, 0.006368621252477169, 0.10470066964626312, 0.3426903188228607, 0.006558023393154144, 0.10782244056463242, 0.10303014516830444, 0.007233991753309965, 0.007407622877508402, 0.1065487265586853, 0.007696916814893484, 0.007833008654415607, 0.10915076732635498, 0.008079687133431435, 0.10121338814496994, 0.008281984366476536, 0.008369783870875835, 0.008363020606338978, 0.008414803072810173, 0.008407212793827057, 0.008384461514651775, 0.008308016695082188, 0.11032547056674957, 0.008277893997728825, 0.00821674894541502, 0.00814693234860897, 0.008088814094662666, 0.008005037903785706, 0.007850688882172108, 0.007774760946631432, 0.007671733386814594, 0.12380535155534744, 0.007436538115143776, 0.007350314874202013, 0.11142002046108246, 0.007230160757899284, 0.0071376776322722435, 0.10388442873954773, 0.10925502330064774, 0.007028506137430668, 0.006991166155785322, 0.1097450703382492, 0.0069197118282318115, 0.006944349966943264, 0.006888143252581358, 0.0068688769824802876, 0.006781651638448238, 0.0067166294902563095, 0.006643127184361219, 0.006563624832779169, 0.006442171521484852, 0.006357649341225624, 0.006268708035349846, 0.006160689052194357, 0.006084251217544079, 0.005985124036669731, 0.11298574507236481, 0.005803390871733427, 0.00572750112041831, 0.005642236210405827, 0.005616533569991589, 0.1107494905591011, 0.00547174783423543, 0.005435293540358543, 0.10103444010019302, 0.0053590103052556515, 0.005315888207405806, 0.005283852573484182, 0.12424789369106293, 0.11840356886386871, 0.005246852058917284, 0.005289214197546244, 0.005267427768558264, 0.005249800626188517, 0.0052469209767878056, 0.005219451617449522, 0.005195652600377798, 0.0051359133794903755, 0.005098633002489805, 0.11622530966997147, 0.005045678932219744, 0.00501857977360487, 0.11127427965402603, 0.004991569090634584, 0.12162937968969345, 0.004985034931451082, 0.00500083202496171, 0.1011226698756218, 0.1073831170797348, 0.00508816447108984, 0.0051394300535321236, 0.005133501254022121, 0.005166393704712391, 0.00519587704911828, 0.005169089417904615, 0.005153894890099764, 0.10762236267328262, 0.005119704641401768, 0.005132782738655806, 0.00512660201638937, 0.005106809549033642, 0.11105122417211533, 0.12660297751426697, 0.005108834709972143, 0.0051228939555585384, 0.005149677395820618, 0.005148145370185375, 0.005098971538245678, 0.0050856322050094604, 0.005047921556979418, 0.005053529981523752, 0.11740267276763916, 0.12786683440208435, 0.12698549032211304, 0.0050830175168812275, 0.005095392931252718, 0.005137567408382893, 0.005181029438972473, 0.005165479611605406, 0.3886260688304901, 0.00536750303581357, 0.005548348184674978, 0.11317463964223862, 0.005868877284228802, 0.006002658978104591, 0.0061370269395411015, 0.0062767937779426575, 0.006348642986267805, 0.006372623611241579, 0.006426599342375994, 0.006452781148254871, 0.006454709451645613, 0.006422361359000206, 0.00642137648537755, 0.006379059981554747, 0.006321588531136513, 0.006282148417085409, 0.0062150852754712105, 0.09692110121250153, 0.12063663452863693, 0.00606674887239933, 0.006112060509622097, 0.0060348897241055965, 0.006032317411154509, 0.005982305854558945, 0.00592787005007267, 0.005882828030735254, 0.09747571498155594, 0.10924284905195236, 0.005759586114436388, 0.12726616859436035, 0.005778764374554157, 0.005772904027253389, 0.005789201240986586, 0.1136721596121788, 0.005779166705906391, 0.00578530877828598, 0.0057812826707959175, 0.10374221950769424, 0.005797842983156443, 0.005763075314462185, 0.103624127805233, 0.00578010780736804, 0.005755983293056488, 0.005745203234255314, 0.005727478303015232, 0.0057210796512663364, 0.005674811080098152, 0.00562621932476759, 0.11733945459127426, 0.005545505788177252, 0.005509145092219114, 0.005485990550369024, 0.005470235366374254, 0.005415817257016897, 0.005333293695002794, 0.005292705725878477, 0.005217068828642368, 0.130955308675766, 0.1256239265203476, 0.005144845228642225, 0.005133103579282761, 0.13114576041698456, 0.1122223511338234, 0.0051800888031721115, 0.005231567192822695, 0.005257542710751295, 0.005259891971945763, 0.005248324014246464, 0.0052443682216107845, 0.005225065164268017, 0.005187339149415493, 0.005149814300239086, 0.0051136077381670475, 0.122806616127491, 0.005057390779256821, 0.10645859688520432, 0.11384052783250809, 0.10862328857183456, 0.005158665124326944, 0.4046472907066345, 0.005432130768895149, 0.005676766391843557, 0.005858196876943111, 0.00602297205477953, 0.006189740262925625, 0.006320956628769636, 0.12442626804113388, 0.10276877880096436, 0.3432236611843109, 0.11748867481946945, 0.09746894240379333, 0.09686823189258575, 0.008058899082243443, 0.008406616747379303, 0.008639558218419552, 0.008845736272633076, 0.00904066301882267, 0.009188942611217499, 0.009260105900466442, 0.009375916793942451, 0.009467615745961666, 0.009399558417499065, 0.009378375485539436, 0.009413246065378189, 0.11787869036197662, 0.00928039476275444, 0.009216556325554848, 0.33756715059280396, 0.10438777506351471, 0.009565263986587524, 0.11179652810096741, 0.009944061748683453, 0.009995021857321262, 0.11027396470308304, 0.10870254039764404, 0.01027897372841835, 0.010356920771300793, 0.01036873273551464, 0.010375475510954857, 0.010383994318544865, 0.010264297015964985, 0.010203803889453411, 0.010130173526704311, 0.010012269020080566, 0.009933491237461567, 0.009821888990700245, 0.009606976993381977, 0.009543604217469692, 0.09650490432977676, 0.009225365705788136, 0.009047398343682289, 0.008960573934018612, 0.11665858328342438, 0.008732753805816174, 0.00858625303953886, 0.008416316471993923, 0.10317208617925644, 0.008251573890447617, 0.09132517129182816, 0.008037146180868149, 0.007953180000185966, 0.007933645509183407, 0.007800872903317213, 0.1177583560347557, 0.007611622102558613, 0.007495405618101358, 0.007498486898839474, 0.007376144640147686, 0.00731508107855916, 0.1047644391655922, 0.007050354965031147, 0.006995974108576775, 0.0951550230383873, 0.1124739795923233, 0.006879993714392185, 0.09707032144069672, 0.10331869125366211, 0.0068114278838038445, 0.006872253492474556, 0.10364816337823868, 0.3214986324310303, 0.0071295686066150665, 0.007291116751730442, 0.0074407500214874744, 0.007596210110932589, 0.00768033042550087, 0.0077429162338376045, 0.007817192003130913, 0.11439041793346405, 0.007875797338783741, 0.00794111005961895, 0.007878529839217663, 0.007965653203427792, 0.1092328280210495, 0.007927303202450275, 0.00782714318484068, 0.007835122756659985, 0.007809533271938562, 0.007718199864029884, 0.007637038826942444, 0.0076333750039339066, 0.11837944388389587, 0.007435844279825687, 0.0073478869162499905, 0.007255855016410351, 0.007239094935357571, 0.007071875501424074, 0.00696320878341794, 0.006895686034113169, 0.13315220177173615, 0.0066980114206671715, 0.0067207845859229565, 0.006641923449933529, 0.006515400018543005, 0.10920478403568268, 0.006355983205139637, 0.0063459426164627075, 0.006323568988591433, 0.006173026282340288, 0.006149190943688154, 0.12144096940755844, 0.1145152896642685, 0.11079483479261398, 0.005989311262965202, 0.006008954718708992, 0.005995024926960468, 0.005968600511550903, 0.005977858789265156, 0.0059301829896867275, 0.005857996642589569, 0.12313561141490936, 0.0058209518902003765, 0.005804556887596846, 0.005779714789241552, 0.005762113723903894, 0.0952492356300354, 0.005615437403321266, 0.005639541894197464, 0.005604694131761789, 0.005582486279308796, 0.005530525464564562, 0.005428305361419916, 0.4025842249393463, 0.3700564205646515, 0.005877584218978882, 0.006235682405531406, 0.0065192123875021935, 0.006664944346994162, 0.006847660522907972, 0.1149505078792572, 0.11645802855491638, 0.0074106864631175995, 0.007587795611470938, 0.0076955147087574005, 0.007830380462110043, 0.007988709025084972, 0.008022952824831009, 0.008036567829549313, 0.008033719845116138, 0.10082404315471649, 0.12106559425592422, 0.008050412870943546, 0.00808011181652546, 0.008149642497301102, 0.008148690685629845, 0.008001398295164108, 0.007955296896398067, 0.007897231727838516, 0.007828998379409313, 0.10245507955551147, 0.007688299287110567, 0.007669201120734215, 0.11056051403284073, 0.12260591238737106, 0.007507259026169777, 0.007552122697234154, 0.007500530686229467, 0.007443415001034737, 0.007359549403190613, 0.007305496837943792, 0.007212016731500626, 0.0071798269636929035, 0.1020938977599144, 0.006958242040127516, 0.006929506082087755, 0.09688408672809601, 0.09509248286485672, 0.10132195055484772, 0.006788605824112892, 0.1018972098827362, 0.006803818512707949, 0.10134685039520264, 0.12311142683029175, 0.00690274266526103, 0.11805681139230728, 0.007028069347143173, 0.11211097985506058, 0.007120837923139334, 0.007139176595956087, 0.0071412245742976665, 0.007183308247476816, 0.007195246871560812, 0.007118900306522846, 0.007071578409522772, 0.00707490649074316, 0.11350616067647934, 0.006921771448105574, 0.10474890470504761, 0.006901316344738007, 0.1182730421423912, 0.0068954103626310825, 0.006888422649353743, 0.006917430087924004, 0.006804128643125296, 0.3357873260974884, 0.006933363154530525, 0.007033875677734613, 0.007172909565269947, 0.007248425390571356, 0.0073302630335092545, 0.007400215603411198, 0.007374481298029423, 0.0073776161298155785, 0.09713789820671082, 0.1019018143415451, 0.10811073333024979, 0.007495868019759655, 0.10039018839597702, 0.12646916508674622, 0.00760289840400219, 0.007670013699680567, 0.007692454848438501, 0.007702385075390339, 0.0076700360514223576, 0.007676346693187952, 0.09753567725419998, 0.007607428822666407, 0.1169220432639122, 0.007612468674778938, 0.10401642322540283, 0.007609806023538113, 0.0075883688405156136, 0.007617352530360222, 0.007517554797232151, 0.007468280848115683, 0.00739804282784462, 0.09713122993707657, 0.007303641177713871, 0.11544052511453629, 0.007265862543135881, 0.007304040249437094, 0.0071623665280640125, 0.007166295778006315, 0.0071317353285849094, 0.007043253164738417, 0.006925557274371386, 0.006848640274256468, 0.10758283734321594, 0.006780401803553104, 0.006696346215903759, 0.006632624659687281, 0.006548149045556784, 0.00646331999450922, 0.006337007042020559, 0.1176767647266388, 0.006235017441213131, 0.006193522829562426, 0.006118678953498602, 0.006079209037125111, 0.005943965632468462, 0.005919812712818384, 0.005792548414319754, 0.00579273235052824, 0.005745710339397192, 0.005624723620712757, 0.1036815494298935, 0.0054587265476584435, 0.0054545155726373196, 0.10806442052125931, 0.0053826915100216866, 0.005329761188477278, 0.005277981981635094, 0.005212162621319294, 0.005206016357988119, 0.11991434544324875, 0.0051293629221618176, 0.005109747871756554, 0.12341803312301636, 0.00503122853115201, 0.11997028440237045, 0.005117087624967098, 0.0050657051615417, 0.005083518102765083, 0.11664918065071106, 0.005109511315822601, 0.11985069513320923, 0.0051712351851165295, 0.00511220982298255, 0.005189865827560425, 0.005143669433891773, 0.1344294399023056, 0.005180431064218283, 0.00521005317568779, 0.005191418342292309, 0.005149369593709707, 0.005171007476747036, 0.1111099049448967, 0.0051354034803807735, 0.005127171985805035, 0.005114759784191847, 0.005116060376167297, 0.10293368995189667, 0.005135623272508383, 0.005077963229268789, 0.005074096843600273, 0.005065817851573229, 0.005026785656809807, 0.0049809967167675495, 0.00492563983425498, 0.004896342288702726, 0.004853474907577038, 0.004815009422600269, 0.004739406518638134, 0.12195812910795212, 0.004680038429796696, 0.004677509889006615, 0.3592079281806946, 0.004770379979163408, 0.0048738219775259495, 0.005023335572332144, 0.005083023104816675, 0.00516796438023448, 0.005230523180216551, 0.10614752769470215, 0.0052961246110498905, 0.005349348299205303, 0.0053994678892195225, 0.005369300022721291, 0.005439333617687225, 0.1110183522105217, 0.005461763124912977, 0.005433680489659309, 0.005436883307993412, 0.005443306639790535, 0.005424429662525654, 0.005374295171350241, 0.0054151201620697975, 0.005402288865298033, 0.00531640462577343, 0.005304454825818539, 0.11254037171602249, 0.1343413144350052, 0.005197805352509022, 0.13981935381889343, 0.005255887284874916, 0.005277364049106836, 0.13359476625919342, 0.005340509582310915, 0.09533973038196564, 0.11801984161138535, 0.005404042080044746, 0.005460497923195362, 0.11692988872528076, 0.13211895525455475, 0.005611493717879057, 0.005723301786929369, 0.0057597048580646515, 0.10278081148862839, 0.005799548700451851, 0.005849974695593119, 0.005918441340327263, 0.11462751775979996, 0.10409324616193771, 0.11121996492147446, 0.006045195274055004, 0.39265820384025574, 0.006363898050040007, 0.006582405883818865, 0.006725956220179796, 0.12770766019821167, 0.00706902239471674, 0.10325557738542557, 0.09946425259113312, 0.00751924142241478, 0.007654843386262655, 0.00782324280589819, 0.007886707782745361, 0.09811076521873474, 0.00797815527766943, 0.00799079891294241, 0.008055845275521278, 0.008022692985832691, 0.008028858341276646, 0.008036758750677109, 0.11804542690515518, 0.1154557392001152, 0.09798778593540192, 0.008066833950579166, 0.008014978840947151, 0.008045398630201817, 0.00797653291374445, 0.008005419746041298, 0.007882381789386272, 0.10518110543489456, 0.007794382981956005, 0.007775689475238323, 0.007733038626611233, 0.007631395012140274, 0.007593879010528326, 0.007524364627897739, 0.007415350992232561, 0.007390351966023445, 0.10787452757358551, 0.007174806669354439, 0.007120417430996895, 0.0070906938053667545, 0.11960628628730774, 0.1074858233332634, 0.10416825860738754, 0.12805375456809998, 0.006936291232705116, 0.006911705248057842, 0.0069459001533687115, 0.006907695438712835, 0.10539393872022629, 0.006951000541448593, 0.12492143362760544, 0.006944485008716583, 0.1263428032398224, 0.35038086771965027, 0.007185571361333132, 0.007309962064027786, 0.007465281058102846, 0.11064315587282181, 0.007717090658843517, 0.00783489178866148, 0.11767608672380447, 0.008010095916688442, 0.008116724900901318, 0.008138036355376244, 0.008140685968101025, 0.10323214530944824, 0.09648917615413666, 0.008235463872551918, 0.00825931690633297, 0.008240747265517712, 0.008244129829108715, 0.008206753060221672, 0.008144104853272438, 0.008137268014252186, 0.1062626913189888, 0.008043257519602776, 0.00797624234110117, 0.007957173511385918, 0.11725010722875595, 0.007840406149625778, 0.10644423216581345, 0.09868381172418594, 0.007725939620286226, 0.007706508040428162, 0.0076667009852826595, 0.007622261065989733, 0.007606850937008858, 0.0075841061770915985, 0.007487310562282801, 0.10698629170656204, 0.007368621416389942, 0.00728718563914299, 0.007268158253282309, 0.007141247391700745, 0.0070670535787940025, 0.00701162638142705, 0.006926574278622866, 0.006841507274657488, 0.006787814665585756, 0.00665231654420495, 0.006581914611160755, 0.10272868722677231, 0.006420664023607969, 0.006356992293149233, 0.006284648552536964, 0.006242301780730486, 0.006103266030550003, 0.006095347926020622, 0.00599164143204689, 0.005869745742529631, 0.0058091371320188046, 0.005720846820622683, 0.005678950343281031, 0.005616896785795689, 0.005506744142621756, 0.0054235574789345264, 0.005329185165464878, 0.10635008662939072, 0.1033044084906578, 0.005168817937374115, 0.005139396525919437, 0.11523698270320892, 0.005130351986736059, 0.005080126225948334, 0.005123497452586889, 0.005080368835479021, 0.8028257489204407, 0.0053664217703044415, 0.005668693222105503, 0.005918677896261215, 0.006141510792076588, 0.09951598197221756, 0.3379408121109009, 0.006942408625036478, 0.0072578247636556625, 0.10388568043708801, 0.00783750880509615, 0.008076509460806847, 0.008257357403635979, 0.008455724455416203, 0.008643550798296928, 0.00873794686049223, 0.008799773640930653, 0.00892468448728323, 0.008957784622907639, 0.008962646126747131, 0.008994001895189285, 0.00896103959530592, 0.00889572687447071, 0.008877678774297237, 0.00887784268707037, 0.10421628504991531, 0.008749326691031456, 0.10401997715234756, 0.008699961937963963, 0.12630441784858704, 0.10600525885820389, 0.09791562706232071, 0.0086012352257967, 0.11757941544055939, 0.008679433725774288, 0.008701921440660954, 0.008627391420304775, 0.008623453788459301]\n",
            "Val loss 0.03284161562872369\n",
            "Val auc roc 0.4930816653264657\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec43ed6830ea47b48fa93a477bed2b6d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1650.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0383\n",
            "Train Losses : [0.008523545227944851, 0.008561949245631695, 0.00856758002191782, 0.008578644134104252, 0.008510472252964973, 0.008524719625711441, 0.008570874109864235, 0.008507397025823593, 0.008489602245390415, 0.008543221279978752, 0.09490220248699188, 0.008427605964243412, 0.10017291456460953, 0.008420014753937721, 0.008504356257617474, 0.008465295657515526, 0.09903670102357864, 0.008471953682601452, 0.10600994527339935, 0.11512435972690582, 0.008376284502446651, 0.09949272125959396, 0.008365998975932598, 0.008374156430363655, 0.11229169368743896, 0.008456090465188026, 0.008421177975833416, 0.00838291086256504, 0.008406935259699821, 0.008336606435477734, 0.008383753709495068, 0.008347030729055405, 0.11009515821933746, 0.008361902087926865, 0.008416739292442799, 0.008355147205293179, 0.008358713239431381, 0.008357714861631393, 0.12149777263402939, 0.10004255920648575, 0.3609965145587921, 0.008312816731631756, 0.008282441645860672, 0.10022379457950592, 0.008305308409035206, 0.008349837735295296, 0.008349389769136906, 0.008398400619626045, 0.09489837288856506, 0.008386233821511269, 0.008349406532943249, 0.008356660604476929, 0.008336234837770462, 0.00833803229033947, 0.09866811335086823, 0.008391767740249634, 0.00834606308490038, 0.008300184272229671, 0.10233321040868759, 0.008314463309943676, 0.00831352174282074, 0.10406184941530228, 0.10721664875745773, 0.008322677575051785, 0.00831108819693327, 0.008337978273630142, 0.008317654021084309, 0.12591703236103058, 0.127613365650177, 0.008281604386866093, 0.008299614302814007, 0.3501809537410736, 0.00832061655819416, 0.008360831066966057, 0.008308921940624714, 0.008314382284879684, 0.008407865650951862, 0.008381372317671776, 0.008367417380213737, 0.11942082643508911, 0.00829180609434843, 0.00838202890008688, 0.008337801322340965, 0.30679550766944885, 0.008328701369464397, 0.008356945589184761, 0.008360213600099087, 0.008339776657521725, 0.008341463282704353, 0.008341358043253422, 0.008422872051596642, 0.008367354981601238, 0.008388024754822254, 0.008378495462238789, 0.008412900380790234, 0.09099411964416504, 0.008344218134880066, 0.00836174562573433, 0.0083315409719944, 0.008375379256904125, 0.008327821269631386, 0.008328613825142384, 0.008312550373375416, 0.00828136783093214, 0.008300373330712318, 0.11822843551635742, 0.008244805969297886, 0.11142867803573608, 0.008300846442580223, 0.008239826187491417, 0.008293257094919682, 0.008267129771411419, 0.008249050937592983, 0.11604847013950348, 0.008206190541386604, 0.00828105490654707, 0.00816559512168169, 0.00820618961006403, 0.008177384734153748, 0.008156214840710163, 0.008151349611580372, 0.1185094565153122, 0.008173715323209763, 0.008176128380000591, 0.008129694499075413, 0.008135553449392319, 0.00814185943454504, 0.11874433606863022, 0.10235175490379333, 0.008089512586593628, 0.008099867030978203, 0.008136673830449581, 0.00806486327201128, 0.008063324727118015, 0.3539228141307831, 0.008093807846307755, 0.11729098856449127, 0.008127887733280659, 0.12393397837877274, 0.008086475543677807, 0.008099347352981567, 0.09911409765481949, 0.10436934232711792, 0.00815726537257433, 0.00814646203070879, 0.008169172331690788, 0.00810749176889658, 0.10145749151706696, 0.008121641352772713, 0.008173205889761448, 0.10841957479715347, 0.008183309808373451, 0.008109602145850658, 0.008112973533570766, 0.11524418741464615, 0.11456356942653656, 0.09831864386796951, 0.008143044076859951, 0.008099060505628586, 0.008141600526869297, 0.008100023493170738, 0.10758154094219208, 0.008075149729847908, 0.35282307863235474, 0.11954361200332642, 0.008166775107383728, 0.008189140819013119, 0.008118473924696445, 0.008182146586477757, 0.008166477084159851, 0.008167766965925694, 0.008167024701833725, 0.11149267852306366, 0.09389834851026535, 0.008223975077271461, 0.008156885392963886, 0.10632027685642242, 0.008207622915506363, 0.008191218599677086, 0.008202845230698586, 0.008172569796442986, 0.008165589533746243, 0.10877863317728043, 0.1030878871679306, 0.00820830650627613, 0.008185959421098232, 0.008179486729204655, 0.008167719468474388, 0.008151756599545479, 0.00815054401755333, 0.008153011091053486, 0.008157790638506413, 0.1120971143245697, 0.008197209797799587, 0.008123520761728287, 0.10341847687959671, 0.09816930443048477, 0.008124127052724361, 0.008113130927085876, 0.008127324283123016, 0.008054463192820549, 0.008148986846208572, 0.008072052150964737, 0.33231642842292786, 0.00809262041002512, 0.008101364597678185, 0.00806567445397377, 0.008126115426421165, 0.11636321246623993, 0.00816420093178749, 0.008150126785039902, 0.008145324885845184, 0.09910838305950165, 0.008151928894221783, 0.008085520938038826, 0.008079849183559418, 0.008144951425492764, 0.11692487448453903, 0.008128964342176914, 0.008068778552114964, 0.008069327101111412, 0.008108679205179214, 0.10384693741798401, 0.10651126503944397, 0.008099371567368507, 0.1257021278142929, 0.008092831820249557, 0.008045436814427376, 0.00810416042804718, 0.00808083638548851, 0.09626457840204239, 0.008026267401874065, 0.00801228266209364, 0.008092280477285385, 0.008020802401006222, 0.00801311619579792, 0.00810005608946085, 0.008045081049203873, 0.008004406467080116, 0.008065914735198021, 0.10457136482000351, 0.008045165799558163, 0.008005838841199875, 0.007994228042662144, 0.12388189136981964, 0.007955493405461311, 0.00794750452041626, 0.10979478806257248, 0.11133597791194916, 0.007920135743916035, 0.007960682734847069, 0.007972419261932373, 0.007972651161253452, 0.007927305065095425, 0.007975142449140549, 0.00792170874774456, 0.007930767722427845, 0.007920114323496819, 0.007855314761400223, 0.10858849436044693, 0.007872280664741993, 0.007838995195925236, 0.09100860357284546, 0.11763521283864975, 0.007826781831681728, 0.007890082895755768, 0.10484720766544342, 0.007863915525376797, 0.007866332307457924, 0.10631398856639862, 0.007825187407433987, 0.007805796340107918, 0.007843630388379097, 0.007813876494765282, 0.007807761896401644, 0.007822375744581223, 0.0077751982025802135, 0.007770997006446123, 0.00779374735429883, 0.007787559647113085, 0.1129298210144043, 0.1044897511601448, 0.00781885627657175, 0.0077618700452148914, 0.007776215672492981, 0.007750456687062979, 0.09471496194601059, 0.007754513528198004, 0.007730824872851372, 0.0077474541030824184, 0.0077212476171553135, 0.00777246942743659, 0.007758197374641895, 0.007695943117141724, 0.10906095057725906, 0.007678796537220478, 0.007650196086615324, 0.007694857195019722, 0.007656953763216734, 0.007682426366955042, 0.00768724549561739, 0.007641814183443785, 0.007642901502549648, 0.00764094153419137, 0.00760466605424881, 0.0076459734700620174, 0.00761146517470479, 0.007632882799953222, 0.32002362608909607, 0.11490700393915176, 0.007645085919648409, 0.007663447875529528, 0.007602199912071228, 0.0075962296687066555, 0.007628042716532946, 0.007627709768712521, 0.007638818118721247, 0.007651329971849918, 0.0075836568139493465, 0.12150242924690247, 0.11190149933099747, 0.007598368916660547, 0.11613472551107407, 0.10309167951345444, 0.007594866678118706, 0.007634161505848169, 0.007618109695613384, 0.007617359980940819, 0.0075651537626981735, 0.00761201698333025, 0.007594227325171232, 0.007571500260382891, 0.11530707031488419, 0.007544704247266054, 0.12154918164014816, 0.10457930713891983, 0.3268607258796692, 0.1070084348320961, 0.007633023429661989, 0.33673563599586487, 0.007679753936827183, 0.33552005887031555, 0.007673503365367651, 0.11724662780761719, 0.0077482652850449085, 0.007746152114123106, 0.007774368394166231, 0.007771697826683521, 0.007803844287991524, 0.007879754528403282, 0.007844112813472748, 0.007896896451711655, 0.00789695791900158, 0.007810296956449747, 0.11461661756038666, 0.13471142947673798, 0.3640943467617035, 0.3137187957763672, 0.00790936965495348, 0.007919970899820328, 0.1137780025601387, 0.007962355390191078, 0.008038613013923168, 0.008040670305490494, 0.008054339326918125, 0.008035355247557163, 0.008047291077673435, 0.008022415451705456, 0.00802446436136961, 0.10289639979600906, 0.008089783601462841, 0.00804989319294691, 0.00805243756622076, 0.008077347651124, 0.008047230541706085, 0.10509003698825836, 0.008096336387097836, 0.11162999272346497, 0.10696834325790405, 0.008056347258388996, 0.008058635517954826, 0.008088868111371994, 0.00805842038244009, 0.008094646967947483, 0.0926179513335228, 0.12845785915851593, 0.10921268910169601, 0.008022391237318516, 0.008039786480367184, 0.00807962566614151, 0.008069142699241638, 0.008024116978049278, 0.008005506359040737, 0.008004436269402504, 0.008041014894843102, 0.00797264277935028, 0.008025838062167168, 0.00803898461163044, 0.09740035980939865, 0.00801418349146843, 0.00800398364663124, 0.09399627894163132, 0.00798005796968937, 0.007945559918880463, 0.10017868131399155, 0.008007640950381756, 0.00792655535042286, 0.007960215210914612, 0.10686426609754562, 0.10514280200004578, 0.0079200379550457, 0.007911141030490398, 0.007947743870317936, 0.007890516892075539, 0.007971222512423992, 0.007963166572153568, 0.09422175586223602, 0.007944815792143345, 0.007881954312324524, 0.007964907214045525, 0.007900887168943882, 0.007872610352933407, 0.007883749902248383, 0.007877988740801811, 0.007882320322096348, 0.007878703996539116, 0.00786733627319336, 0.007828227244317532, 0.007886311039328575, 0.007839753292500973, 0.007806530222296715, 0.007839781232178211, 0.007849974557757378, 0.007855486124753952, 0.007802376989275217, 0.007797786500304937, 0.007780841086059809, 0.007732865866273642, 0.007748431526124477, 0.007758746389299631, 0.11496209353208542, 0.12211751192808151, 0.007717418950051069, 0.007714852225035429, 0.00771741196513176, 0.007720060180872679, 0.007671732921153307, 0.007677975110709667, 0.007744187954813242, 0.0076469602063298225, 0.007676368113607168, 0.00764985429123044, 0.0076575265266001225, 0.007645400706678629, 0.0076176198199391365, 0.007647025864571333, 0.007649979554116726, 0.007597560063004494, 0.00761426892131567, 0.10117954760789871, 0.007612660527229309, 0.10626482963562012, 0.007639018818736076, 0.007618877105414867, 0.007618437055498362, 0.10466350615024567, 0.007567199878394604, 0.0075841136276721954, 0.007533667609095573, 0.0075500160455703735, 0.007551022805273533, 0.12981480360031128, 0.007520208600908518, 0.007505867164582014, 0.007488102652132511, 0.007542992010712624, 0.11573752015829086, 0.1112767681479454, 0.0074598644860088825, 0.007463325746357441, 0.007476970087736845, 0.007476963102817535, 0.007513362914323807, 0.007505262736231089, 0.30292758345603943, 0.007478513289242983, 0.007498111575841904, 0.0075044818222522736, 0.007540923077613115, 0.0074662514962255955, 0.10374066978693008, 0.007498027756810188, 0.007466118782758713, 0.007502560969442129, 0.10848064720630646, 0.0075036524794995785, 0.007506580092012882, 0.007454440928995609, 0.1265200972557068, 0.09579195827245712, 0.0074761477299034595, 0.00745805911719799, 0.007493132259696722, 0.007513908203691244, 0.10024421662092209, 0.11839436739683151, 0.007497286424040794, 0.007469867821782827, 0.0074784886091947556, 0.007435829844325781, 0.007499713450670242, 0.007482551503926516, 0.0075106373988091946, 0.13555753231048584, 0.09748595952987671, 0.0982753112912178, 0.007442667614668608, 0.007427773904055357, 0.007417656481266022, 0.00748542370274663, 0.11148881912231445, 0.1123061254620552, 0.007494285702705383, 0.3712908625602722, 0.007451452314853668, 0.007458619307726622, 0.007522507570683956, 0.007504716981202364, 0.3588070273399353, 0.11585874855518341, 0.09542796015739441, 0.007594360504299402, 0.007572115398943424, 0.007584214210510254, 0.007560961879789829, 0.007631067652255297, 0.007590355351567268, 0.13013648986816406, 0.007603966165333986, 0.007626370992511511, 0.007636234629899263, 0.007606069091707468, 0.007648030761629343, 0.10400968790054321, 0.007581939920783043, 0.10857272148132324, 0.007570168934762478, 0.007619895506650209, 0.0076387375593185425, 0.00762203661724925, 0.007578562013804913, 0.007604681886732578, 0.007612281944602728, 0.007583650294691324, 0.11582367867231369, 0.007559757214039564, 0.007601637858897448, 0.10287471115589142, 0.0075543844141066074, 0.007590245921164751, 0.007541315164417028, 0.11769401282072067, 0.3627714216709137, 0.007560317870229483, 0.007563491351902485, 0.007586081512272358, 0.0076209004037082195, 0.007567650638520718, 0.007608930114656687, 0.007599771488457918, 0.007619101088494062, 0.09770726412534714, 0.007639214396476746, 0.007574127521365881, 0.007589354645460844, 0.007628049701452255, 0.11977004259824753, 0.007607853971421719, 0.007604374550282955, 0.007551087532192469, 0.007550377864390612, 0.10395783185958862, 0.3668583929538727, 0.007566775660961866, 0.007571308873593807, 0.007615924812853336, 0.007630940526723862, 0.007638926152139902, 0.007607209961861372, 0.1134137436747551, 0.09666279703378677, 0.007652141619473696, 0.007632078602910042, 0.00765021750703454, 0.007614622358232737, 0.007676493842154741, 0.007665068842470646, 0.007605307269841433, 0.00758657231926918, 0.101922407746315, 0.007634863723069429, 0.10540757328271866, 0.007600640878081322, 0.007600494660437107, 0.007598896976560354, 0.007576025556772947, 0.007610023953020573, 0.0075566996820271015, 0.0075600347481667995, 0.007555895019322634, 0.11026505380868912, 0.00761228334158659, 0.007530775386840105, 0.13014215230941772, 0.007584191858768463, 0.007570596877485514, 0.007552480325102806, 0.007614022120833397, 0.007529403083026409, 0.007534495089203119, 0.007608789950609207, 0.09974170476198196, 0.007542391773313284, 0.007509930059313774, 0.11608794331550598, 0.0075207543559372425, 0.007541764993220568, 0.007543396670371294, 0.007482675835490227, 0.00753385666757822, 0.007504638284444809, 0.007501297164708376, 0.00750213535502553, 0.11963701993227005, 0.007451416924595833, 0.3583185374736786, 0.007456331048160791, 0.10450463742017746, 0.007484202738851309, 0.00751875014975667, 0.007452463731169701, 0.3529888391494751, 0.007528628222644329, 0.007521334104239941, 0.007536354009062052, 0.0075096506625413895, 0.00754550751298666, 0.10910641402006149, 0.10465463995933533, 0.007602987345308065, 0.00761201698333025, 0.0076079703867435455, 0.11314038932323456, 0.1161733940243721, 0.007583607919514179, 0.0076016574166715145, 0.11959007382392883, 0.007569177076220512, 0.10551052540540695, 0.007549721747636795, 0.007622143253684044, 0.007589111104607582, 0.007548037450760603, 0.3633427619934082, 0.09752373397350311, 0.09353987127542496, 0.0076111978851258755, 0.007615562062710524, 0.007615039590746164, 0.00763745466247201, 0.09968190640211105, 0.007609288673847914, 0.007654575165361166, 0.007652643136680126, 0.12468595802783966, 0.0076563130132853985, 0.007681130897253752, 0.007674349006265402, 0.007620119955390692, 0.10485220700502396, 0.007622432895004749, 0.1035456582903862, 0.11569346487522125, 0.11507668346166611, 0.007639878895133734, 0.0076526254415512085, 0.0076469434425234795, 0.007662098854780197, 0.007655475754290819, 0.007723672315478325, 0.007650665007531643, 0.10696306079626083, 0.0076449839398264885, 0.007689286954700947, 0.007677654270082712, 0.007656925357878208, 0.007682246156036854, 0.007638315670192242, 0.0076375300996005535, 0.007603056728839874, 0.11369886994361877, 0.00760203693062067, 0.09470686316490173, 0.007610381115227938, 0.10129421949386597, 0.0076403249986469746, 0.1190972849726677, 0.007570598740130663, 0.12019474804401398, 0.00758452620357275, 0.007596410345286131, 0.007578455377370119, 0.0076138293370604515, 0.007566080894321203, 0.10594654828310013, 0.007591371890157461, 0.007619522046297789, 0.007554350420832634, 0.007601890712976456, 0.0075837052427232265, 0.13195638358592987, 0.007560627069324255, 0.0075937602669000626, 0.007528798654675484, 0.0992194190621376, 0.0075987703166902065, 0.007545722182840109, 0.1103590652346611, 0.007527219131588936, 0.00752745708450675, 0.0075339968316257, 0.00756260147318244, 0.007501369342207909, 0.007555752526968718, 0.007536262273788452, 0.11969355493783951, 0.12653875350952148, 0.007538669742643833, 0.007528022397309542, 0.007527505047619343, 0.00748542370274663, 0.007547846529632807, 0.09782840311527252, 0.3275083601474762, 0.007524033542722464, 0.007505448069423437, 0.007509179413318634, 0.11706027388572693, 0.0075293793343007565, 0.11899574100971222, 0.007575723342597485, 0.007575358264148235, 0.007547186687588692, 0.0075044212862849236, 0.007524433080106974, 0.007539690472185612, 0.007539762649685144, 0.11542078852653503, 0.007539762649685144, 0.007569360546767712, 0.0075414166785776615, 0.007545930333435535, 0.007521194405853748, 0.007499559782445431, 0.007529822643846273, 0.007501126732677221, 0.007513908203691244, 0.00753159262239933, 0.007550970651209354, 0.10322549194097519, 0.00753331184387207, 0.007529829163104296, 0.007527240552008152, 0.007521652150899172, 0.1283065527677536, 0.007484710309654474, 0.007519492879509926, 0.007451239973306656, 0.007491566240787506, 0.007499210070818663, 0.007432473357766867, 0.007455210201442242, 0.007429227232933044, 0.00740820774808526, 0.007443123031407595, 0.00744770560413599, 0.007424662820994854, 0.007381570525467396, 0.7116346955299377, 0.00747294956818223, 0.007481839973479509, 0.007469868287444115, 0.007503261789679527, 0.007471642456948757, 0.007485357113182545, 0.007508399430662394, 0.007557714357972145, 0.007517781108617783, 0.007522677071392536, 0.007528708316385746, 0.007536759600043297, 0.007467394694685936, 0.11777043342590332, 0.007490442134439945, 0.007525170221924782, 0.007475500460714102, 0.007452894933521748, 0.007488127797842026, 0.007467988412827253, 0.007447888609021902, 0.007472259923815727, 0.007450983393937349, 0.007503063417971134, 0.007429028861224651, 0.007492214906960726, 0.007507499307394028, 0.007436537183821201, 0.0074615213088691235, 0.007456938736140728, 0.0074192555621266365, 0.09885909408330917, 0.00741548091173172, 0.00746065704151988, 0.007454737555235624, 0.00742246164008975, 0.1070324033498764, 0.007441886235028505, 0.007374612148851156, 0.09367620944976807, 0.0074463807977736, 0.007455688435584307, 0.007429521530866623, 0.007369138300418854, 0.007410117890685797, 0.10864884406328201, 0.007359343580901623, 0.007434885483235121, 0.007379455957561731, 0.007404421456158161, 0.007333182729780674, 0.007364741526544094, 0.007380663882941008, 0.007335123140364885, 0.007336709648370743, 0.007378851994872093, 0.007367330137640238, 0.11655594408512115, 0.00733691081404686, 0.007348576094955206, 0.007342019118368626, 0.007300362456589937, 0.11852555721998215, 0.007319255731999874, 0.11717099696397781, 0.007343449164181948, 0.007288975175470114, 0.007289926055818796, 0.007288104388862848, 0.007255762815475464, 0.007278149016201496, 0.00726489769294858, 0.007283307611942291, 0.0072756740264594555, 0.007281007710844278, 0.007284864317625761, 0.007269570603966713, 0.0073100426234304905, 0.1224280595779419, 0.10907081514596939, 0.007220475468784571, 0.007252492941915989, 0.007287520449608564, 0.007278816308826208, 0.12105323374271393, 0.0072635384276509285, 0.007215347606688738, 0.11259424686431885, 0.007226875983178616, 0.007251764182001352, 0.115330770611763, 0.10309506207704544, 0.007227200549095869, 0.007256914861500263, 0.007234889082610607, 0.11402921378612518, 0.007242891937494278, 0.00721556693315506, 0.10880754888057709, 0.007273017428815365, 0.10821159183979034, 0.007225042209029198, 0.007258380297571421, 0.12042225897312164, 0.007269571535289288, 0.007184796035289764, 0.007214500103145838, 0.007223967928439379, 0.007216083351522684, 0.007236302364617586, 0.007179332431405783, 0.007192722521722317, 0.10535448789596558, 0.0071898153983056545, 0.12134219706058502, 0.007240776438266039, 0.0071845222264528275, 0.007241786923259497, 0.007183715235441923, 0.12589102983474731, 0.007182495668530464, 0.007196562830358744, 0.0071939327754080296, 0.00722524244338274, 0.007206325884908438, 0.007150636985898018, 0.0071405149064958096, 0.007171220146119595, 0.0071448092348873615, 0.12560611963272095, 0.00717601552605629, 0.007141857873648405, 0.12368863075971603, 0.09653326869010925, 0.007138208951801062, 0.007159330416470766, 0.007146182004362345, 0.007158556021749973, 0.007137451320886612, 0.007102795876562595, 0.11199589818716049, 0.007152981124818325, 0.007109528873115778, 0.007149248383939266, 0.007141849957406521, 0.10679289698600769, 0.007142576389014721, 0.007115811109542847, 0.007161497604101896, 0.007164175156503916, 0.11661747097969055, 0.007128447759896517, 0.007155576255172491, 0.007092032115906477, 0.007103148847818375, 0.007138187997043133, 0.007103778421878815, 0.007100004702806473, 0.12374211102724075, 0.007060222793370485, 0.0071225836873054504, 0.007067597005516291, 0.007100033573806286, 0.007062133401632309, 0.007069326937198639, 0.007039806805551052, 0.09683694690465927, 0.007092973683029413, 0.09738627821207047, 0.1183185949921608, 0.0070954738184809685, 0.11421860754489899, 0.007067730650305748, 0.00704268179833889, 0.0070626139640808105, 0.11175217479467392, 0.007041518576443195, 0.0070768785662949085, 0.007070073392242193, 0.00708023551851511, 0.007032589055597782, 0.12780827283859253, 0.007064780220389366, 0.007095430977642536, 0.007089075166732073, 0.007040063384920359, 0.007093461230397224, 0.12479288130998611, 0.007049559149891138, 0.09964998066425323, 0.007029134314507246, 0.007014492992311716, 0.007071084808558226, 0.0996018499135971, 0.11042647063732147, 0.0070639741607010365, 0.007022789213806391, 0.0070195929147303104, 0.006990540307015181, 0.0069898623041808605, 0.12736459076404572, 0.007027775980532169, 0.007036028429865837, 0.007043307181447744, 0.00700408685952425, 0.007041391916573048, 0.006996742449700832, 0.007022446021437645, 0.007013741414994001, 0.006987635046243668, 0.0069845886901021, 0.007020183838903904, 0.006992512382566929, 0.0070170629769563675, 0.0070015131495893, 0.006982578430324793, 0.00704229436814785, 0.006951162125915289, 0.006961394101381302, 0.006995630916208029, 0.12756939232349396, 0.10961597412824631, 0.0069212764501571655, 0.11968588829040527, 0.006937381345778704, 0.006931763142347336, 0.006916456390172243, 0.006961575709283352, 0.006977825425565243, 0.006994837895035744, 0.006951631046831608, 0.006940517574548721, 0.006949480623006821, 0.0069094328209757805, 0.006935114040970802, 0.09668657183647156, 0.0069385915994644165, 0.0069178747944533825, 0.00692159915342927, 0.00692453607916832, 0.0068859681487083435, 0.006913269869983196, 0.006920679472386837, 0.006935196928679943, 0.006886697374284267, 0.006905626505613327, 0.006902644410729408, 0.006878434214740992, 0.006889107637107372, 0.10363274812698364, 0.006897505838423967, 0.006843122188001871, 0.0068856896832585335, 0.12704449892044067, 0.006860801484435797, 0.006861706264317036, 0.006876874715089798, 0.10042048990726471, 0.006850389763712883, 0.006829814054071903, 0.10967027395963669, 0.006861077155917883, 0.006854748819023371, 0.006899941712617874, 0.10551135241985321, 0.12841786444187164, 0.12893086671829224, 0.006868951488286257, 0.006842613685876131, 0.006872117053717375, 0.0068510472774505615, 0.006811998784542084, 0.0068783024325966835, 0.10306563973426819, 0.12606695294380188, 0.0068584345281124115, 0.006895998027175665, 0.006822052411735058, 0.1206965371966362, 0.006861811503767967, 0.10869316756725311, 0.006815152242779732, 0.006841286551207304, 0.006803467869758606, 0.006842342205345631, 0.0068089705891907215, 0.006838600616902113, 0.006809520069509745, 0.006822955794632435, 0.006828158162534237, 0.006815634202212095, 0.006839388981461525, 0.006823892239481211, 0.006844745483249426, 0.12135842442512512, 0.0068203359842300415, 0.006838270928710699, 0.00679698446765542, 0.12701351940631866, 0.00683380663394928, 0.006785342004150152, 0.0067917350679636, 0.006813672371208668, 0.10589656978845596, 0.006805140990763903, 0.10629626363515854, 0.006799265742301941, 0.13035017251968384, 0.006768014747649431, 0.1020134910941124, 0.006791399791836739, 0.006789663340896368, 0.006799266207963228, 0.334255188703537, 0.006829821038991213, 0.006793055217713118, 0.006836256477981806, 0.006764532066881657, 0.006786345038563013, 0.10641633719205856, 0.006828338839113712, 0.006820515729486942, 0.00679891649633646, 0.12540189921855927, 0.09931948781013489, 0.006793939974159002, 0.006792510859668255, 0.006825709715485573, 0.0068456856533885, 0.006793178617954254, 0.358091801404953, 0.36982959508895874, 0.006809100974351168, 0.0068349214270710945, 0.006860365625470877, 0.0068491254933178425, 0.11665558815002441, 0.006898851133882999, 0.7354760766029358, 0.006907745264470577, 0.1117875874042511, 0.00688197510316968, 0.006937853526324034, 0.006944275926798582, 0.7061853408813477, 0.006955007091164589, 0.0069814021699130535, 0.12773394584655762, 0.007001212798058987, 0.007050848100334406, 0.11504115164279938, 0.11685239523649216, 0.007032480556517839, 0.007063976023346186, 0.1051127016544342, 0.007093227934092283, 0.1182635948061943, 0.09705957025289536, 0.00710825901478529, 0.0071045164950191975, 0.007113968022167683, 0.007116996217519045, 0.0070917364209890366, 0.0070649548433721066, 0.007109912112355232, 0.007141615264117718, 0.0070886206813156605, 0.007131549995392561, 0.11731388419866562, 0.007104190997779369, 0.09757202863693237, 0.007080939132720232, 0.007112257182598114, 0.007099134381860495, 0.007074191700667143, 0.11541514098644257, 0.10896655172109604, 0.0070998710580170155, 0.007093866355717182, 0.007133136503398418, 0.007094650063663721, 0.00713054183870554, 0.007068555802106857, 0.007078000344336033, 0.10611280798912048, 0.007096568588167429, 0.007090495899319649, 0.1075555607676506, 0.007071308791637421, 0.12002693861722946, 0.099239282310009, 0.007079232018440962, 0.007120765745639801, 0.007102661766111851, 0.10162130743265152, 0.007132510654628277, 0.007127617485821247, 0.10512396693229675, 0.00706345122307539, 0.007079343311488628, 0.3307846486568451, 0.007111124228686094, 0.007124603260308504, 0.007092874962836504, 0.007128801196813583, 0.11200234293937683, 0.007083771284669638, 0.007124145515263081, 0.007129129022359848, 0.10099958628416061, 0.007146462332457304, 0.007091132923960686, 0.007090489845722914, 0.007131992373615503, 0.1087084487080574, 0.00708356499671936, 0.11307432502508163, 0.0071258763782680035, 0.10079090297222137, 0.10019068419933319, 0.007126814220100641, 0.007136099971830845, 0.11886931955814362, 0.007131642661988735, 0.11444434523582458, 0.007122392300516367, 0.11437096446752548, 0.1235581487417221, 0.007095174863934517, 0.11008615791797638, 0.0071242921985685825, 0.007150629535317421, 0.007140100002288818, 0.0071518197655677795, 0.007139681838452816, 0.007119020912796259, 0.1150350496172905, 0.0071389926597476006, 0.007112967316061258, 0.10896986722946167, 0.007124747149646282, 0.12066236883401871, 0.007143018767237663, 0.007104347459971905, 0.007142015267163515, 0.00713699497282505, 0.007114328909665346, 0.007100238464772701, 0.007132474333047867, 0.007115755695849657, 0.32568874955177307, 0.00709746265783906, 0.11829140782356262, 0.007085071876645088, 0.3463278114795685, 0.0071195573545992374, 0.0071832179091870785, 0.007120495196431875, 0.007122631650418043, 0.10251607745885849, 0.11736392229795456, 0.11823955178260803, 0.007169189862906933, 0.09730590134859085, 0.13600298762321472, 0.007163842674344778, 0.10622690618038177, 0.0071459561586380005, 0.007171371951699257, 0.007164448499679565, 0.12217222154140472, 0.0071866498328745365, 0.0071651325561106205, 0.007146694231778383, 0.007207206916064024, 0.007151891477406025, 0.007161668501794338, 0.007182452827692032, 0.0995117798447609, 0.007144332863390446, 0.007195141166448593, 0.12041627615690231, 0.00713410833850503, 0.007144659757614136, 0.007203178014606237, 0.00715804286301136, 0.1093541756272316, 0.0071317581459879875, 0.11051002889871597, 0.007222527172416449, 0.1136494055390358, 0.00719037838280201, 0.1168263778090477, 0.007158454041928053, 0.007139717694371939, 0.11309201270341873, 0.10193264484405518, 0.007193446159362793, 0.0072123464196920395, 0.0071865287609398365, 0.007139545399695635, 0.007149990182369947, 0.10223681479692459, 0.007146809715777636, 0.0071603721007704735, 0.11873158067464828, 0.09825610369443893, 0.007157080341130495, 0.007212411612272263, 0.007183957379311323, 0.10317189991474152, 0.007172754965722561, 0.0071202474646270275, 0.11697692424058914, 0.12941351532936096, 0.007176608312875032, 0.007126414682716131, 0.00715120043605566, 0.007159376982599497, 0.1195126622915268, 0.007183058653026819, 0.007152758538722992, 0.11206547915935516, 0.007123769726604223, 0.007168148644268513, 0.007196969818323851, 0.007123491261154413, 0.007186915259808302, 0.10569192469120026, 0.10987285524606705, 0.0071565741673111916, 0.00716943247243762, 0.10272365808486938, 0.0985971987247467, 0.00715298019349575, 0.007144781295210123, 0.0071504609659314156, 0.007124867755919695, 0.007141921203583479, 0.007117123808711767, 0.007194146979600191, 0.007180030923336744, 0.0071412245742976665, 0.09976564347743988, 0.007160870358347893, 0.007134364917874336, 0.007156196050345898, 0.007155136670917273, 0.0071622459217906, 0.10887353867292404, 0.007131963036954403, 0.007172973360866308, 0.11907562613487244, 0.007110456470400095, 0.007134180050343275, 0.007152907084673643, 0.0071105859242379665, 0.10476840287446976, 0.10661914199590683, 0.007139113266021013, 0.0071343169547617435, 0.007116634864360094, 0.007157609798014164, 0.10653748363256454, 0.007148182950913906, 0.007143470458686352, 0.0071138679049909115, 0.11526285111904144, 0.0071304431185126305, 0.007175493519753218, 0.007100359071046114, 0.10643457621335983, 0.007124882657080889, 0.0071039460599422455, 0.12575305998325348, 0.0070844125002622604, 0.007112080696970224, 0.0071495710872113705, 0.0991806834936142, 0.007157600950449705, 0.0071424259804189205, 0.10813683271408081, 0.007140267174690962, 0.0071531119756400585, 0.00712469220161438, 0.007120111491531134, 0.007143116556107998, 0.12432307004928589, 0.1016441062092781, 0.0071274940855801105, 0.11868824809789658, 0.007129763253033161, 0.007127281278371811, 0.10509469360113144, 0.0071063232608139515, 0.10511032491922379, 0.10568867623806, 0.007133122067898512, 0.007165004499256611, 0.0070970309898257256, 0.10578227043151855, 0.007104547694325447, 0.09666773676872253, 0.007138898596167564, 0.007129251025617123, 0.007095480803400278, 0.1188054084777832, 0.007127806544303894, 0.10220234841108322, 0.007114002481102943, 0.007131438236683607, 0.00715728010982275, 0.007124158088117838, 0.00708454055711627, 0.007141175679862499, 0.11517989635467529, 0.007128660101443529, 0.007078870199620724, 0.10507862269878387, 0.007145406678318977, 0.007095557637512684, 0.09633305668830872, 0.007101060822606087, 0.007136298809200525, 0.1069844588637352, 0.007084971759468317, 0.007146809250116348, 0.007132162805646658, 0.007092973683029413, 0.12654969096183777, 0.1317363828420639, 0.007136624306440353, 0.007103435229510069, 0.10806099325418472, 0.007099365349858999, 0.007074085529893637, 0.007093043532222509, 0.0071197072975337505, 0.007091220933943987, 0.007129476871341467, 0.00712498277425766, 0.007068118546158075, 0.00709671201184392, 0.0071547022089362144, 0.1139933317899704, 0.12373558431863785, 0.007123361807316542, 0.12368732690811157, 0.122202068567276, 0.007107890211045742, 0.007081810384988785, 0.007076863199472427, 0.12233301997184753, 0.0071154492907226086, 0.0071035330183804035, 0.09846138209104538, 0.007111967075616121, 0.007128971628844738, 0.007115477230399847, 0.12392918765544891, 0.007080459035933018, 0.007113109342753887, 0.0070909494534134865, 0.007095793727785349, 0.09830711781978607, 0.007084385957568884, 0.11076956987380981, 0.11664613336324692, 0.007075033150613308, 0.10650277137756348, 0.007089968305081129, 0.007077350281178951, 0.007078559137880802, 0.007104893215000629, 0.11359508335590363, 0.09695304185152054, 0.11916095018386841, 0.10001476854085922, 0.007103641051799059, 0.007092203013598919, 0.09721415489912033, 0.007108522113412619, 0.007063078694045544, 0.37847357988357544, 0.007085185032337904, 0.11593536287546158, 0.00714896572753787, 0.007108599878847599, 0.11986587196588516, 0.007113357074558735, 0.007106042001396418, 0.007109450176358223, 0.0953354462981224, 0.007090729661285877, 0.007104856427758932, 0.0070801349356770515, 0.11252446472644806, 0.1107521578669548, 0.007127056363970041, 0.00714136054739356, 0.007072633132338524, 0.007126712240278721, 0.007079845294356346, 0.1086057648062706, 0.007153527811169624, 0.007097815629094839, 0.007119645364582539, 0.007121879141777754, 0.11325859278440475, 0.007095388602465391, 0.007132695056498051, 0.10825884342193604, 0.007093880791217089, 0.10497047007083893, 0.007116732187569141, 0.007099679205566645, 0.11776867508888245, 0.09461832791566849, 0.007095536682754755, 0.007095671724528074, 0.0070976135320961475, 0.007101386785507202, 0.0071318624541163445, 0.007066874764859676, 0.10964184254407883, 0.007109649479389191, 0.007068614009767771, 0.007136248052120209, 0.0070980931632220745, 0.007135824300348759, 0.007117395289242268, 0.0071235541254282, 0.007112159859389067, 0.00712717417627573, 0.007086328696459532, 0.007078354712575674, 0.007124187424778938, 0.007119480520486832, 0.007070976309478283, 0.007096847519278526, 0.007130571641027927, 0.12462984025478363, 0.0071090031415224075, 0.10520601272583008, 0.11394838988780975, 0.12223043292760849, 0.00711930263787508, 0.11847570538520813, 0.007106148172169924, 0.00712557090446353, 0.10022403299808502, 0.007096726913005114, 0.10438170284032822, 0.007107154931873083, 0.007109060883522034, 0.007127687800675631, 0.10301756113767624, 0.0070882029831409454, 0.10026045143604279, 0.007140828762203455, 0.00707918219268322, 0.007064596749842167, 0.11957952380180359, 0.09118403494358063, 0.007067158352583647, 0.007128971163183451, 0.0071094222366809845, 0.007072737440466881, 0.007102138362824917, 0.007135559804737568, 0.11497451364994049, 0.007071395870298147, 0.007082957308739424, 0.00711358617991209, 0.007073975168168545, 0.37963688373565674, 0.007111316081136465, 0.0071407342329621315, 0.007067561615258455, 0.007112492807209492, 0.007147256284952164, 0.007107493933290243, 0.007100524380803108, 0.007089648861438036, 0.09974809736013412, 0.007072008680552244, 0.007120280992239714, 0.12023665010929108, 0.007084788288921118, 0.007082314230501652, 0.007068137172609568, 0.1170673742890358, 0.007083804812282324, 0.10377102345228195, 0.10654725879430771, 0.007091254461556673, 0.007097518537193537, 0.007115834392607212, 0.007094603031873703, 0.007097471039742231, 0.007130526937544346, 0.10840852558612823, 0.007100309710949659, 0.007090249564498663, 0.007110578939318657, 0.007085423916578293, 0.007087732665240765, 0.365610271692276, 0.007101718802005053, 0.007089003920555115, 0.007101916708052158, 0.10391806811094284, 0.09710381180047989, 0.0070989783853292465, 0.007119302172213793, 0.007078321184962988, 0.0071463678032159805, 0.007131613790988922, 0.0070723071694374084, 0.007068505510687828, 0.007059790194034576, 0.11720585078001022, 0.007117182482033968, 0.007081656716763973, 0.007061378099024296, 0.007114827167242765, 0.359904021024704, 0.007114045321941376, 0.007129569072276354, 0.007124917581677437, 0.007092437241226435, 0.11458620429039001, 0.007089946419000626, 0.007093051914125681, 0.007121552247554064, 0.007081706542521715, 0.1026398092508316, 0.007077761460095644, 0.007083798758685589, 0.007062782999128103, 0.0071023497730493546, 0.10198063403367996, 0.007095770910382271, 0.007096026558429003, 0.007068981882184744, 0.007120068650692701, 0.0071245888248085976, 0.007085531484335661, 0.007057461887598038, 0.007100501097738743, 0.007060284726321697, 0.00707549974322319, 0.0070621115155518055, 0.007086565718054771, 0.007110089994966984, 0.34055405855178833, 0.007109315600246191, 0.007103329058736563, 0.007079374510794878, 0.007070744875818491, 0.11687217652797699]\n",
            "Val loss 0.033102498501084925\n",
            "Val auc roc 0.48616129032258065\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb49931-1ae1-4527-9520-05e132a68e88"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}