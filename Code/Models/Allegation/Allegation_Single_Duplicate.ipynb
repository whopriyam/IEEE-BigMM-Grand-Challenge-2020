{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Allegation_Single_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28932f8d3e0f49958da81bd0c7d619ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b43e9cc2322244adb58760ddf8f1eea4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df05d15de9db4feeb221f96f0c86684c",
              "IPY_MODEL_1370ea03fc1e4618aa4060e887c5a332"
            ]
          }
        },
        "b43e9cc2322244adb58760ddf8f1eea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df05d15de9db4feeb221f96f0c86684c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_91f24584156347c181595566805c7523",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e991079327c0463297aea27a2f63fb82"
          }
        },
        "1370ea03fc1e4618aa4060e887c5a332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c689f946991e4bb395757dae8ea7d27a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1682/1682 [16:35&lt;00:00,  1.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7fb4356f92d4073b1adb836dc8d0016"
          }
        },
        "91f24584156347c181595566805c7523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e991079327c0463297aea27a2f63fb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c689f946991e4bb395757dae8ea7d27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7fb4356f92d4073b1adb836dc8d0016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c37a90cdef66430288f4130bf76723b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e455cf75a0041d8b4ebdc61db6e99a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cafe38b9a9e94aebb75218d085be4584",
              "IPY_MODEL_4c7138ddfec84c4583b936a9601a4aeb"
            ]
          }
        },
        "0e455cf75a0041d8b4ebdc61db6e99a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cafe38b9a9e94aebb75218d085be4584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ea392cb866f4e3facd4841c019a2f6c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd11233b92d343b099e7d4277a66382e"
          }
        },
        "4c7138ddfec84c4583b936a9601a4aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2318c7b5df34495381e06a52e0ac8e0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1682/1682 [16:05&lt;00:00,  1.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1319b5fb3be840c0af9cc5c207d823fc"
          }
        },
        "9ea392cb866f4e3facd4841c019a2f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd11233b92d343b099e7d4277a66382e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2318c7b5df34495381e06a52e0ac8e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1319b5fb3be840c0af9cc5c207d823fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee122779dcdc46eba28aa1e72b5d0546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38fac1aa105842a28bd8634ab903244a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ecdc3e3dae314fd783f7f89964d8b485",
              "IPY_MODEL_f6348dfff33646109c2f653e59f0d0d0"
            ]
          }
        },
        "38fac1aa105842a28bd8634ab903244a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ecdc3e3dae314fd783f7f89964d8b485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1470b9a74adb4eaba6ee30b0d8e6b9d7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d6bb090574e49e79aed1cba7ab00267"
          }
        },
        "f6348dfff33646109c2f653e59f0d0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a71c936c05d4f0ba799187459849d53",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1682/1682 [16:04&lt;00:00,  1.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89aba4a97f284422821a03b5bd158402"
          }
        },
        "1470b9a74adb4eaba6ee30b0d8e6b9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d6bb090574e49e79aed1cba7ab00267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a71c936c05d4f0ba799187459849d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89aba4a97f284422821a03b5bd158402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1fa8c0ad-5747-453b-aeb3-5cff3b57ca2c"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 41.81 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee2d075f-fba2-4e6d-def3-403c3abfeec1"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c927a71-2b78-43ee-ecca-086ae56e261d"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3ffe2735-452b-40d1-ace4-55a1c8c59311"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cd0d1d07-1cc2-4ef0-b8f0-a3c9df3ed0b5"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "6cfe9fe7-3718-4438-d7c7-823831dfe11d"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6b6ca189-e402-4953-b260-2a835086e426"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4e05c04f-202d-41bb-bf66-238bff2bedde"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "314ad313-bfc3-4c22-f05e-86e6c984ca44"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b44cf7e2-984d-406c-c5e9-2bfc39c1edf9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    @Alyssa_Milano @chelseahandler #MeTooMovement....\n",
              "1    #podestaemails Changed everything in my life. ...\n",
              "2    Bollywood director denies #MeToo claims, threa...\n",
              "3    Missouri State student reportedly raped hours ...\n",
              "4                Meet the women worried about #MeToo  \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b060fdf7-a520-41ed-a02c-ff133000f308"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:27<00:00, 14988478.46B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99547c7b-8bc5-4076-ab2c-fdf58544cba8"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1143355.14B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "fa48ed7d-f083-4c0c-973b-ae22c983b271"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea63dfdb-4684-406b-9e03-e296e71417f8"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a59fd01d-f3b3-4ff2-b56c-740d652e061b"
      },
      "source": [
        "col_name = \"Allegation\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728,
          "referenced_widgets": [
            "28932f8d3e0f49958da81bd0c7d619ee",
            "b43e9cc2322244adb58760ddf8f1eea4",
            "df05d15de9db4feeb221f96f0c86684c",
            "1370ea03fc1e4618aa4060e887c5a332",
            "91f24584156347c181595566805c7523",
            "e991079327c0463297aea27a2f63fb82",
            "c689f946991e4bb395757dae8ea7d27a",
            "e7fb4356f92d4073b1adb836dc8d0016",
            "c37a90cdef66430288f4130bf76723b6",
            "0e455cf75a0041d8b4ebdc61db6e99a7",
            "cafe38b9a9e94aebb75218d085be4584",
            "4c7138ddfec84c4583b936a9601a4aeb",
            "9ea392cb866f4e3facd4841c019a2f6c",
            "fd11233b92d343b099e7d4277a66382e",
            "2318c7b5df34495381e06a52e0ac8e0a",
            "1319b5fb3be840c0af9cc5c207d823fc",
            "ee122779dcdc46eba28aa1e72b5d0546",
            "38fac1aa105842a28bd8634ab903244a",
            "ecdc3e3dae314fd783f7f89964d8b485",
            "f6348dfff33646109c2f653e59f0d0d0",
            "1470b9a74adb4eaba6ee30b0d8e6b9d7",
            "5d6bb090574e49e79aed1cba7ab00267",
            "6a71c936c05d4f0ba799187459849d53",
            "89aba4a97f284422821a03b5bd158402"
          ]
        },
        "outputId": "a61fd438-f17f-49e0-8e8c-b64a6f727651"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6731\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28932f8d3e0f49958da81bd0c7d619ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1682.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0573\n",
            "Train Losses : [0.0026740527246147394, 0.13642854988574982, 0.1268995851278305, 0.0043894522823393345, 0.00635528564453125, 0.10837399214506149, 0.010799025185406208, 0.01266624964773655, 0.10484036058187485, 0.01432323083281517, 0.013868787325918674, 0.012207791209220886, 0.009791413322091103, 0.007161314133554697, 0.004782320000231266, 0.0029276704881340265, 0.0016599129885435104, 0.157828688621521, 0.15175358951091766, 0.13961321115493774, 0.4423624873161316, 0.11174945533275604, 0.10443594306707382, 0.014414682053029537, 0.019646981731057167, 0.024465743452310562, 0.09912952035665512, 0.03114132210612297, 0.032509565353393555, 0.10849963128566742, 0.10297036916017532, 0.10615702718496323, 0.027769949287176132, 0.09583136439323425, 0.021722733974456787, 0.0184397641569376, 0.10293030738830566, 0.012324451468884945, 0.009777390398085117, 0.007555428426712751, 0.00564425578340888, 0.004144987091422081, 0.003009867388755083, 0.002174690831452608, 0.0015559748280793428, 0.14572209119796753, 0.15080393850803375, 0.0013082668883726, 0.5299093127250671, 0.0022673585917800665, 0.0032004204113036394, 0.0042706262320280075, 0.005418209824711084, 0.33175069093704224, 0.00886301975697279, 0.09348954260349274, 0.12163779139518738, 0.01668766513466835, 0.08577702939510345, 0.12492454051971436, 0.07449202984571457, 0.02658708766102791, 0.029692763462662697, 0.02837151102721691, 0.08771782368421555, 0.03000817820429802, 0.029911138117313385, 0.0276214387267828, 0.06447348743677139, 0.024987511336803436, 0.17485255002975464, 0.11110631376504898, 0.07602499425411224, 0.12252254039049149, 0.018860744312405586, 0.08899463713169098, 0.016635937616229057, 0.14391207695007324, 0.014753743074834347, 0.013768358156085014, 0.012710905633866787, 0.011739878915250301, 0.01073096226900816, 0.009851483628153801, 0.008824832737445831, 0.09596949815750122, 0.3209906220436096, 0.00789113249629736, 0.008243151009082794, 0.008342014625668526, 0.008419523946940899, 0.008519822731614113, 0.008571736514568329, 0.2929855287075043, 0.008902139961719513, 0.009505285881459713, 0.12366800755262375, 0.010633155703544617, 0.12614251673221588, 0.08172158896923065, 0.091720350086689, 0.012209534645080566, 0.07199281454086304, 0.01297267246991396, 0.013527743518352509, 0.013455553911626339, 0.013220316730439663, 0.01266272272914648, 0.01275834534317255, 0.6457927823066711, 0.2750035524368286, 0.016057178378105164, 0.33549967408180237, 0.020604899153113365, 0.023389816284179688, 0.09273308515548706, 0.02847803197801113, 0.029619647189974785, 0.030974330380558968, 0.03190547227859497, 0.12299303710460663, 0.03223112225532532, 0.03208288550376892, 0.1449957638978958, 0.08877727389335632, 0.2111096829175949, 0.030552612617611885, 0.13147854804992676, 0.03062458522617817, 0.09993015974760056, 0.09508473426103592, 0.12614023685455322, 0.027837516739964485, 0.0892622321844101, 0.1102641373872757, 0.024883750826120377, 0.02440192550420761, 0.022784855216741562, 0.021698856726288795, 0.09624755382537842, 0.019537653774023056, 0.11529268324375153, 0.017048856243491173, 0.0160211194306612, 0.01506385114043951, 0.09230982512235641, 0.013312006369233131, 0.012421300634741783, 0.01159830391407013, 0.1070476621389389, 0.010278931818902493, 0.009676176123321056, 0.009076335467398167, 0.008434019982814789, 0.007940007373690605, 0.09933814406394958, 0.6952164769172668, 0.008165707811713219, 0.3231428861618042, 0.11837127804756165, 0.011939036659896374, 0.013345064595341682, 0.014711836352944374, 0.015789400786161423, 0.016811616718769073, 0.017325257882475853, 0.1278344988822937, 0.018219977617263794, 0.018792251124978065, 0.018894875422120094, 0.1109030470252037, 0.018638182431459427, 0.01829446479678154, 0.018120085820555687, 0.10225531458854675, 0.017459344118833542, 0.10123571753501892, 0.0163775272667408, 0.09581302106380463, 0.08976052701473236, 0.015307758003473282, 0.014974760822951794, 0.11708611994981766, 0.014168750494718552, 0.09388535469770432, 0.013145099394023418, 0.012956016696989536, 0.09740713983774185, 0.01209439430385828, 0.011641247197985649, 0.09178496897220612, 0.010981694795191288, 0.010748750530183315, 0.010432577691972256, 0.009891679510474205, 0.3054947555065155, 0.009854988195002079, 0.10423555970191956, 0.00999725516885519, 0.010130398906767368, 0.01014608796685934, 0.00999506562948227, 0.009921091608703136, 0.10149727761745453, 0.009538614191114902, 0.009536029770970345, 0.009414974600076675, 0.009173393249511719, 0.28841495513916016, 0.00937645137310028, 0.08478772640228271, 0.009732651524245739, 0.009723684750497341, 0.010136272758245468, 0.14877133071422577, 0.299404114484787, 0.0819585770368576, 0.011395777575671673, 0.10109435766935349, 0.01188000850379467, 0.012566402554512024, 0.012859789654612541, 0.012763370759785175, 0.012876720167696476, 0.012898625805974007, 0.013049036264419556, 0.01276439055800438, 0.0870097279548645, 0.012373371049761772, 0.012193935923278332, 0.07787540555000305, 0.2651153802871704, 0.011965309269726276, 0.012173272669315338, 0.15147694945335388, 0.012508203275501728, 0.01249320525676012, 0.012488054111599922, 0.2616252303123474, 0.012530160136520863, 0.012930922210216522, 0.11975403875112534, 0.013516002334654331, 0.2839122414588928, 0.09031833708286285, 0.07422783225774765, 0.01524513028562069, 0.015567823313176632, 0.01598227024078369, 0.016094882041215897, 0.016242876648902893, 0.01618768461048603, 0.09608076512813568, 0.015648651868104935, 0.11099809408187866, 0.09290953725576401, 0.015216859057545662, 0.13462132215499878, 0.10604940354824066, 0.014717316254973412, 0.014506685547530651, 0.10666733980178833, 0.013882765546441078, 0.10088376700878143, 0.013305429369211197, 0.1296335905790329, 0.01275982242077589, 0.10106106102466583, 0.012542370706796646, 0.012121522799134254, 0.011998383328318596, 0.011386686936020851, 0.0770341157913208, 0.12545348703861237, 0.010825913399457932, 0.1291530877351761, 0.11475921422243118, 0.00999096967279911, 0.00993012823164463, 0.009732460603117943, 0.14208610355854034, 0.009404808282852173, 0.009223150089383125, 0.008942368440330029, 0.0087785879150033, 0.008629322052001953, 0.1333068460226059, 0.008239896968007088, 0.007946852594614029, 0.08905909210443497, 0.12436230480670929, 0.0074612656608223915, 0.30416765809059143, 0.007759483531117439, 0.11408896744251251, 0.008450465276837349, 0.00856226310133934, 0.1067684218287468, 0.00907228048890829, 0.11951901018619537, 0.11359155923128128, 0.009492038749158382, 0.009766844101250172, 0.009903942234814167, 0.009813357144594193, 0.009770906530320644, 0.009724839590489864, 0.3433917164802551, 0.009952224791049957, 0.010278373025357723, 0.32753488421440125, 0.10731818526983261, 0.01188831590116024, 0.2878095805644989, 0.013461965136229992, 0.014531586319208145, 0.2750658690929413, 0.1067233756184578, 0.017532775178551674, 0.01856299862265587, 0.01928376778960228, 0.10553136467933655, 0.09632059931755066, 0.02085024304687977, 0.12008664011955261, 0.021257417276501656, 0.02131735533475876, 0.021253105252981186, 0.24380822479724884, 0.10371534526348114, 0.02156507782638073, 0.02163771353662014, 0.2605525255203247, 0.1027742251753807, 0.022337421774864197, 0.022514840587973595, 0.022519130259752274, 0.02236400730907917, 0.10017124563455582, 0.021810125559568405, 0.021421000361442566, 0.02093222551047802, 0.020363952964544296, 0.019730038940906525, 0.01905146613717079, 0.099019855260849, 0.017703069373965263, 0.017038604244589806, 0.01636589877307415, 0.1036943793296814, 0.015084778890013695, 0.09930088371038437, 0.013977698050439358, 0.10069791972637177, 0.10140103846788406, 0.012661906890571117, 0.012280842289328575, 0.011882907710969448, 0.011472724378108978, 0.11483009904623032, 0.010725473053753376, 0.1007833480834961, 0.01011879462748766, 0.10901650786399841, 0.10292694717645645, 0.10325806587934494, 0.009382427670061588, 0.00925569050014019, 0.009105685167014599, 0.008931046351790428, 0.008735491894185543, 0.12039449065923691, 0.00838354043662548, 0.008224683813750744, 0.008047825656831264, 0.007844647392630577, 0.007650683168321848, 0.3386916220188141, 0.007664128206670284, 0.10844123363494873, 0.008031218312680721, 0.008177809417247772, 0.008277223445475101, 0.008329207077622414, 0.008349881507456303, 0.10674882680177689, 0.008332950063049793, 0.008331446908414364, 0.10773243010044098, 0.008289959281682968, 0.10592598468065262, 0.008266571909189224, 0.10550910234451294, 0.008261228911578655, 0.00825087446719408, 0.00820160936564207, 0.008127384819090366, 0.008027602918446064, 0.00790161732584238, 0.0077607035636901855, 0.007605386432260275, 0.00743801798671484, 0.0072594936937093735, 0.007074968423694372, 0.10589443892240524, 0.006762504577636719, 0.006630233954638243, 0.12307515740394592, 0.006411482580006123, 0.006318316329270601, 0.006213761400431395, 0.006103038787841797, 0.0059737772680819035, 0.005849691107869148, 0.005717114545404911, 0.005574687849730253, 0.005440242122858763, 0.005289808381348848, 0.12678571045398712, 0.005085407290607691, 0.005003073252737522, 0.004902093671262264, 0.11212877184152603, 0.004787584766745567, 0.0047455751337111, 0.004692978225648403, 0.12698958814144135, 0.004617903381586075, 0.004607998766005039, 0.1114337220788002, 0.004593555349856615, 0.12390121072530746, 0.385526180267334, 0.1212393045425415, 0.005465640686452389, 0.005856520030647516, 0.10816918313503265, 0.0065571460872888565, 0.006877994630485773, 0.11089424043893814, 0.007462687324732542, 0.007702048867940903, 0.10987107455730438, 0.008126574568450451, 0.008317315950989723, 0.1076539009809494, 0.10729391872882843, 0.10740263015031815, 0.008978288620710373, 0.10527324676513672, 0.00930490531027317, 0.32404497265815735, 0.009971251711249352, 0.010418299585580826, 0.10452067852020264, 0.011140122078359127, 0.01141970045864582, 0.011648199521005154, 0.11474157869815826, 0.011933152563869953, 0.31577762961387634, 0.01252790167927742, 0.11536858230829239, 0.2965143918991089, 0.014124124310910702, 0.014806742779910564, 0.015360730700194836, 0.015789298340678215, 0.10420249402523041, 0.01638478972017765, 0.016557274386286736, 0.10407983511686325, 0.016688602045178413, 0.016655445098876953, 0.275732159614563, 0.016863208264112473, 0.01706831529736519, 0.017159905284643173, 0.10406743735074997, 0.01714157499372959, 0.10410403460264206, 0.10407187044620514, 0.01688501425087452, 0.016728738322854042, 0.016498632729053497, 0.01620500162243843, 0.10418074578046799, 0.015560022555291653, 0.1042623519897461, 0.014921181835234165, 0.10435474663972855, 0.01430150680243969, 0.28941071033477783, 0.0141138955950737, 0.014165417291224003, 0.10446378588676453, 0.1045287624001503, 0.2884482443332672, 0.10429597645998001, 0.014999696053564548, 0.015302295796573162, 0.015499133616685867, 0.015596738085150719, 0.015603589825332165, 0.01552874967455864, 0.015381846576929092, 0.10422642529010773, 0.10429580509662628, 0.014854246750473976, 0.10436923801898956, 0.014486823230981827, 0.014266994781792164, 0.10450322180986404, 0.013782421126961708, 0.013520730659365654, 0.2941097021102905, 0.10464859008789062, 0.10466384142637253, 0.10460271686315536, 0.013851089403033257, 0.10457000881433487, 0.28911060094833374, 0.014523561112582684, 0.014910158701241016, 0.015188238583505154, 0.015364415943622589, 0.015445597469806671, 0.015440227463841438, 0.015357128344476223, 0.10425017774105072, 0.015081536024808884, 0.10429956018924713, 0.014743734151124954, 0.01453466061502695, 0.014277678914368153, 0.10449589788913727, 0.01373332180082798, 0.10466030240058899, 0.10475476831197739, 0.10478092730045319, 0.012886034324765205, 0.012697350233793259, 0.012470915913581848, 0.012213212437927723, 0.011929526925086975, 0.011625450104475021, 0.011306177824735641, 0.010975495912134647, 0.010637903586030006, 0.010296291671693325, 0.009953917004168034, 0.009613296948373318, 0.009276310913264751, 0.10744637250900269, 0.0086903041228652, 0.008433215320110321, 0.008175830356776714, 0.00791972316801548, 0.1090177670121193, 0.007482961751520634, 0.007294769398868084, 0.10996975749731064, 0.006975972093641758, 0.006838905159384012, 0.00669491570442915, 0.006545322947204113, 0.00639204028993845, 0.006236201152205467, 0.006078911013901234, 0.0059213885106146336, 0.005764449015259743, 0.0056088208220899105, 0.11345508694648743, 0.005362956319004297, 0.38422805070877075, 0.005493502598255873, 0.11284995079040527, 0.005913472268730402, 0.006103259976953268, 0.006258795969188213, 0.11128588765859604, 0.006536081898957491, 0.0066565703600645065, 0.006744530983269215, 0.11042404174804688, 0.006896130740642548, 0.11015429347753525, 0.007055690977722406, 0.10989589989185333, 0.10971101373434067, 0.007350812200456858, 0.007444813847541809, 0.0075043486431241035, 0.10920858383178711, 0.34569692611694336, 0.00800744816660881, 0.10825691372156143, 0.008730366826057434, 0.10736682265996933, 0.009358751587569714, 0.009617537260055542, 0.009816921316087246, 0.10652641206979752, 0.010122114792466164, 0.01022903248667717, 0.10625803470611572, 0.01036608126014471, 0.010397304780781269, 0.1062064841389656, 0.106190986931324, 0.010448738932609558, 0.010448722168803215, 0.10618241876363754, 0.01039872970432043, 0.010349593125283718, 0.010263693518936634, 0.10637327283620834, 0.010071908123791218, 0.009965776465833187, 0.009831516072154045, 0.10676316171884537, 0.009565690532326698, 0.009432407096028328, 0.00927766878157854, 0.009104849770665169, 0.008917310275137424, 0.008718251250684261, 0.10792814195156097, 0.00836206041276455, 0.008200442418456078, 0.108522430062294, 0.007912606000900269, 0.007782372646033764, 0.007639678195118904, 0.007486996706575155, 0.1095711886882782, 0.10970529913902283, 0.007170301862061024, 0.1099015474319458, 0.007075688801705837, 0.00703126285225153, 0.3544812798500061, 0.007248078007251024, 0.007482742890715599, 0.0076735420152544975, 0.007821734994649887, 0.007929911836981773, 0.34056177735328674, 0.10800594091415405, 0.008832171559333801, 0.009191499091684818, 0.009489154443144798, 0.009726965799927711, 0.1065407246351242, 0.010103312321007252, 0.10626273602247238, 0.010399440303444862, 0.010500982403755188, 0.10608649998903275, 0.1060023233294487, 0.010722474195063114, 0.1059408187866211, 0.010835638269782066, 0.10588095337152481, 0.10579957813024521, 0.010967934504151344, 0.3104938864707947, 0.011371255852282047, 0.011679336428642273, 0.01191290095448494, 0.3019286096096039, 0.012597702443599701, 0.013026921078562737, 0.013364444486796856, 0.10461907088756561, 0.10453283041715622, 0.01410137489438057, 0.014256912283599377, 0.01433339063078165, 0.014337318018078804, 0.28769341111183167, 0.28575941920280457, 0.015275562182068825, 0.015830878168344498, 0.016267869621515274, 0.016589868813753128, 0.016804002225399017, 0.016917632892727852, 0.10404162108898163, 0.01696210727095604, 0.01690218411386013, 0.016767701134085655, 0.104079969227314, 0.1040901467204094, 0.10413073748350143, 0.016121776774525642, 0.015934690833091736, 0.015693586319684982, 0.015407050028443336, 0.015082387253642082, 0.10433655232191086, 0.014424821361899376, 0.014093566685914993, 0.013738916255533695, 0.013366456143558025, 0.10481182485818863, 0.012660596519708633, 0.012325012125074863, 0.011978567577898502, 0.011624767445027828, 0.10561705380678177, 0.10577433556318283, 0.10590630024671555, 0.010573283769190311, 0.01037684828042984, 0.010163228958845139, 0.009936164133250713, 0.10670073330402374, 0.10691206157207489, 0.10702270269393921, 0.10706562548875809, 0.10713600367307663, 0.009260284714400768, 0.10724674165248871, 0.10716955363750458, 0.009250740520656109, 0.009245086461305618, 0.009206943213939667, 0.009139711037278175, 0.10732423514127731, 0.008997047320008278, 0.008920412510633469, 0.10758036375045776, 0.008764618076384068, 0.008684084750711918, 0.0085821021348238, 0.008461745455861092, 0.008325657807290554, 0.008176774717867374, 0.10852695256471634, 0.10868556797504425, 0.007851598784327507, 0.1088750883936882, 0.0077381432056427, 0.007682178635150194, 0.007607118226587772, 0.10925545543432236, 0.10935264825820923, 0.0074662514962255955, 0.10930945724248886, 0.0074510411359369755, 0.10937979817390442, 0.007463558577001095, 0.007462460547685623, 0.10936082154512405, 0.10936405509710312, 0.10926956683397293, 0.007584326434880495, 0.1090974360704422, 0.007715987972915173, 0.0077649070881307125, 0.1088549941778183, 0.1088017076253891, 0.00792074017226696, 0.007970700971782207, 0.007989286445081234, 0.007979313842952251, 0.007943914271891117, 0.007885481230914593, 0.007807535585016012, 0.007712193299084902, 0.007602133322507143, 0.007479513064026833, 0.3490857481956482, 0.007550480309873819, 0.00771271763369441, 0.007835930213332176, 0.007922382093966007, 0.007974645122885704, 0.10857733339071274, 0.10850084573030472, 0.008134136907756329, 0.008183621801435947, 0.00820078793913126, 0.008188883773982525, 0.10837472230195999, 0.10831040889024734, 0.008186928927898407, 0.008191811852157116, 0.008168610744178295, 0.008120588026940823, 0.008050860837101936, 0.007961818017065525, 0.007856346666812897, 0.007737037725746632, 0.10912326723337173, 0.007525395601987839, 0.007429528050124645, 0.007320658303797245, 0.007201053202152252, 0.007072362583130598, 0.3547650873661041, 0.007128064054995775, 0.007281606085598469, 0.007399073336273432, 0.0074831037782132626, 0.007535393815487623, 0.0075591690838336945, 0.007556566037237644, 0.10921323299407959, 0.007542748469859362, 0.3465929329395294, 0.007835973985493183, 0.008093753829598427, 0.10817303508520126, 0.10783674567937851, 0.008775435388088226, 0.008968117646872997, 0.009113200940191746, 0.009213796816766262, 0.009272917173802853, 0.009293900802731514, 0.009280703961849213, 0.009236312471330166, 0.00916486606001854, 0.009069333784282207, 0.008953060954809189, 0.10758771747350693, 0.00873195007443428, 0.008624985814094543, 0.008500645868480206, 0.008362241089344025, 0.10833264887332916, 0.008111940696835518, 0.10855729877948761, 0.10866096615791321, 0.007901051081717014, 0.007851743139326572, 0.007782599423080683, 0.10898207873106003, 0.10904761403799057, 0.007651271298527718, 0.0076248361729085445, 0.10917015373706818, 0.10911141335964203, 0.00759849650785327, 0.1090993881225586, 0.3451407551765442, 0.1085725724697113, 0.1081400215625763, 0.10770557820796967, 0.009090719744563103, 0.1070447564125061, 0.10665532946586609, 0.106435127556324, 0.1061844676733017, 0.010724482126533985, 0.0109910499304533, 0.011193347163498402, 0.011335066519677639, 0.1055549755692482, 0.10544722527265549, 0.011637981049716473, 0.011697862297296524, 0.011707081459462643, 0.011669840663671494, 0.011591472662985325, 0.10550103336572647, 0.01139868889003992, 0.011285163462162018, 0.01114124245941639, 0.01097130961716175, 0.010779233649373055, 0.010569147765636444, 0.010344342328608036, 0.010108248330652714, 0.00986334029585123, 0.10680787265300751, 0.32473474740982056, 0.009576222859323025, 0.00968482717871666, 0.009749687276780605, 0.00977406557649374, 0.009761965833604336, 0.009716886095702648, 0.00964295119047165, 0.10684222728013992, 0.10696414858102798, 0.009460764937102795, 0.10697555541992188, 0.009389195591211319, 0.009340987540781498, 0.00926606822758913, 0.32739368081092834, 0.10695843398571014, 0.10676443576812744, 0.009901526384055614, 0.010099813342094421, 0.10629813373088837, 0.10619077831506729, 0.010577145032584667, 0.01069552917033434, 0.10600639134645462, 0.010852358303964138, 0.01089221891015768, 0.010888485237956047, 0.010845338925719261, 0.10589902102947235, 0.10595714300870895, 0.10596215724945068, 0.31265032291412354, 0.011067482642829418, 0.011343540623784065, 0.011553793214261532, 0.01170201413333416, 0.011792343109846115, 0.10532630234956741, 0.10528947412967682, 0.10525107383728027, 0.012045412324368954, 0.012078968808054924, 0.012062694877386093, 0.012002167291939259, 0.011902743950486183, 0.011768888682126999, 0.011605445295572281, 0.10551515966653824, 0.10559810698032379, 0.011167358607053757, 0.10574347525835037, 0.010937273502349854, 0.010812516324222088, 0.01066223531961441, 0.10611951351165771, 0.010363913141191006, 0.10629887878894806, 0.10640433430671692, 0.010041446425020695, 0.00994670856744051, 0.009827290661633015, 0.009687541984021664, 0.009530209936201572, 0.10707113891839981, 0.3266338109970093, 0.00944201648235321, 0.10680708289146423, 0.009772083722054958, 0.009895880706608295, 0.10651678591966629, 0.10641328990459442, 0.010190080851316452, 0.010260636918246746, 0.10625547915697098, 0.010340996086597443, 0.010351881384849548, 0.010325278155505657, 0.01026527676731348, 0.1063402071595192, 0.010123430751264095, 0.010040869005024433, 0.3196853697299957, 0.01015600748360157, 0.010324947535991669, 0.010442590340971947, 0.010512436740100384, 0.010539169423282146, 0.010525966063141823, 0.010477551259100437, 0.010397509671747684, 0.10623247921466827, 0.31706321239471436, 0.10611467063426971, 0.010738755576312542, 0.010940419510006905, 0.011085132136940956, 0.011176659725606441, 0.10565269738435745, 0.011281930841505527, 0.011297513730823994, 0.3081800043582916, 0.01157100684940815, 0.10532791912555695, 0.012037538923323154, 0.0122067267075181, 0.012314632534980774, 0.10503760725259781, 0.10501601547002792, 0.012513204477727413, 0.10498166084289551, 0.012580490671098232, 0.10497333109378815, 0.01258214469999075, 0.10500301420688629, 0.012530338950455189, 0.012470644898712635, 0.012370378710329533, 0.012234901078045368, 0.10518808662891388, 0.10526813566684723, 0.011848852969706059, 0.105352483689785, 0.01163211464881897, 0.10548655688762665, 0.011421553790569305, 0.011302321217954159, 0.011155704036355019, 0.010986116714775562, 0.10590676963329315, 0.010652647353708744, 0.10610082000494003, 0.10622347891330719, 0.010283539071679115, 0.10632586479187012, 0.10637527704238892, 0.010067368857562542, 0.010000527836382389, 0.00990721769630909, 0.009790695272386074, 0.10669989883899689, 0.10685107856988907, 0.1069118082523346, 0.10693521797657013, 0.00949278473854065, 0.009469715878367424, 0.10693025588989258, 0.3249359130859375, 0.00969434529542923, 0.009932014159858227, 0.01011646818369627, 0.010250227525830269, 0.010336956940591335, 0.010380453430116177, 0.10617820173501968, 0.010413262993097305, 0.010404434986412525, 0.10619214922189713, 0.1062161922454834, 0.10623379796743393, 0.1061578094959259, 0.010459589771926403, 0.10608821362257004, 0.01052074134349823, 0.010522911325097084, 0.010488904081285, 0.010422748513519764, 0.10623203963041306, 0.010268989950418472, 0.010181710124015808, 0.010069283656775951, 0.10653802752494812, 0.32059288024902344, 0.10640398412942886, 0.10624490678310394, 0.010545032098889351, 0.010728782042860985, 0.0108590517193079, 0.010939894244074821, 0.010974807664752007, 0.010968205519020557, 0.10581627488136292, 0.010909132659435272, 0.010857348330318928, 0.010774187743663788, 0.10602110624313354, 0.010587688535451889, 0.010484595783054829, 0.01035719458013773, 0.010209208354353905, 0.010043303482234478, 0.0098631102591753, 0.009670919738709927, 0.009469302371144295, 0.009260532446205616, 0.009046658873558044, 0.008829467929899693, 0.10779309272766113, 0.008446563966572285, 0.008275861851871014, 0.10844753682613373, 0.007975083775818348, 0.0078408969566226, 0.007698915433138609, 0.1092502698302269, 0.007451730780303478, 0.3490426540374756, 0.007528338115662336, 0.007677100133150816, 0.007790884003043175, 0.10874522477388382, 0.007977237924933434, 0.339904248714447, 0.10814210027456284, 0.008764415048062801, 0.10731729865074158, 0.1070077046751976, 0.009700263850390911, 0.10653797537088394, 0.10630479454994202, 0.10609583556652069, 0.010765179991722107, 0.10579123347997665, 0.011194339953362942, 0.10550731420516968, 0.01152112614363432, 0.011632909066975117, 0.10535331070423126, 0.011769534088671207, 0.011796966195106506, 0.10533405840396881, 0.10533159226179123, 0.011812539771199226, 0.011794662103056908, 0.10537195950746536, 0.011707082390785217, 0.01163899153470993, 0.011537328362464905, 0.011407162994146347, 0.011251719668507576, 0.10573162138462067, 0.10579285025596619, 0.01084369421005249, 0.10595092177391052, 0.10598454624414444, 0.10604777187108994, 0.10602276027202606, 0.010563267394900322, 0.31427332758903503, 0.010807756334543228, 0.011024605482816696, 0.10566242784261703, 0.30754488706588745, 0.10534057766199112, 0.012264003045856953, 0.012634417973458767, 0.01292864978313446, 0.10475564748048782, 0.10470223426818848, 0.013584718108177185, 0.013729923404753208, 0.10455115884542465, 0.013897579163312912, 0.01392433699220419, 0.013896659947931767, 0.10453837364912033, 0.013765741139650345, 0.01366609800606966, 0.01352654304355383, 0.013352591544389725, 0.013149046339094639, 0.10483194142580032, 0.012734427116811275, 0.29882752895355225, 0.012643820606172085, 0.012708354741334915, 0.012720889411866665, 0.29766520857810974, 0.10481104254722595, 0.013235428370535374, 0.013434420339763165, 0.013566373847424984, 0.10460902005434036, 0.013714814558625221, 0.013735306449234486, 0.013703510165214539, 0.10455425083637238, 0.013569698669016361, 0.10464447736740112, 0.10450965911149979, 0.013346428982913494, 0.2939680516719818, 0.013476763851940632, 0.013631626032292843, 0.01372234895825386, 0.013754608109593391, 0.013733948580920696, 0.10457132011651993, 0.013618739321827888, 0.01352760847657919, 0.10470028221607208, 0.10469704121351242, 0.104909248650074, 0.1047731265425682, 0.10477321594953537, 0.10478222370147705, 0.013134312815964222, 0.013093192130327225, 0.10482660681009293, 0.012951549142599106, 0.012853733263909817, 0.012721294537186623, 0.01255843136459589, 0.0123700350522995, 0.10515223443508148, 0.011991825886070728, 0.01180172897875309, 0.1054166629910469, 0.011427769437432289, 0.011242984794080257, 0.0110417939722538, 0.010827578604221344, 0.010602627880871296, 0.010369734838604927, 0.10633377730846405, 0.009944010525941849, 0.009747408330440521, 0.32348713278770447, 0.009648267179727554, 0.106711246073246, 0.10664378106594086, 0.009901588782668114, 0.10651300847530365, 0.10641620308160782, 0.10635887086391449, 0.010257795453071594, 0.010328923352062702, 0.10623855888843536, 0.31534385681152344, 0.010751998052001, 0.011029192246496677, 0.01124731544405222, 0.011409182101488113, 0.011518974788486958, 0.30579590797424316, 0.011930706910789013, 0.10510959476232529, 0.012489023618400097, 0.1049116849899292, 0.012907292693853378, 0.013052085414528847, 0.013138113543391228, 0.013170337304472923, 0.01315333042293787, 0.29490163922309875, 0.013335520401597023, 0.013510526157915592, 0.013621861115098, 0.013675026595592499, 0.013674778863787651, 0.29153576493263245, 0.013880585320293903, 0.014063175767660141, 0.014178810641169548, 0.014233131892979145, 0.014231729321181774, 0.10449336469173431, 0.01414583995938301, 0.014065183699131012, 0.10454607009887695, 0.013847731053829193, 0.013713615946471691, 0.01354608591645956, 0.10468590259552002, 0.10474798828363419, 0.013062885962426662, 0.10485025495290756, 0.012779543176293373, 0.01262457761913538, 0.2993800640106201, 0.01257444079965353, 0.01265027280896902, 0.012676344253122807, 0.012657441198825836, 0.012598578818142414, 0.10500442981719971, 0.012435713782906532, 0.01233415212482214, 0.10514291375875473, 0.01210289541631937, 0.10524474829435349, 0.011878537945449352, 0.1053268238902092, 0.01166233979165554, 0.10545701533555984, 0.011455841362476349, 0.011341705918312073, 0.011203852482140064, 0.1057610884308815, 0.010926441289484501, 0.010785581544041634, 0.01062727440148592, 0.010453807190060616, 0.010267863981425762, 0.010072197765111923, 0.009868726134300232, 0.10675062984228134, 0.10690969228744507, 0.10701310634613037, 0.10708149522542953, 0.009253355674445629, 0.009185322560369968, 0.009098585695028305, 0.10740421712398529, 0.3298341631889343, 0.009140103124082088, 0.009307372383773327, 0.10698801279067993, 0.009572372771799564, 0.009671401232481003, 0.009732827544212341, 0.009759943932294846, 0.00975592527538538, 0.10670584440231323, 0.1067168340086937, 0.009739690460264683, 0.009729750454425812, 0.009692525491118431, 0.009630896151065826, 0.10687100142240524, 0.009498253464698792, 0.009426685981452465, 0.009335721842944622, 0.10716979205608368, 0.009156936779618263, 0.009067783132195473, 0.00896282959729433, 0.008844141848385334, 0.10769815742969513, 0.10780441761016846, 0.10785331577062607, 0.00855102576315403, 0.008509218692779541, 0.1080092191696167, 0.0084216995164752, 0.008374442346394062, 0.008309864439070225, 0.00822978001087904, 0.008136192336678505, 0.10851651430130005, 0.007965737022459507, 0.10873610526323318, 0.007843180559575558, 0.10889390110969543, 0.10887404531240463, 0.007767229340970516, 0.10888657718896866, 0.007769491523504257, 0.0077638220973312855, 0.10891852527856827, 0.10890347510576248, 0.10884208977222443, 0.3425106704235077, 0.008153234608471394, 0.1080184280872345, 0.008698798716068268, 0.008928696624934673, 0.009114683605730534, 0.009258735924959183, 0.10703204572200775, 0.0094836400821805, 0.10682124644517899, 0.3224899172782898, 0.010026924312114716, 0.10620948672294617, 0.01063520647585392, 0.10585024207830429, 0.011128042824566364, 0.3077399730682373, 0.011772655881941319, 0.012154629454016685, 0.10501991212368011, 0.10485196113586426, 0.01306331716477871, 0.013288059271872044, 0.10466686636209488, 0.01360813807696104, 0.013707311823964119, 0.013750807382166386, 0.013743946328759193, 0.013691594824194908, 0.013598885387182236, 0.013470668345689774, 0.10466931760311127, 0.013183792121708393, 0.10478949546813965, 0.01290131639689207, 0.01274761464446783, 0.012569401413202286, 0.012370557524263859, 0.012154566124081612, 0.011924335733056068, 0.011683110147714615, 0.3058793246746063, 0.011489213444292545, 0.011512510478496552, 0.10344342887401581, 0.109251007437706, 0.10074953734874725, 0.3042011559009552, 0.0118478424847126, 0.10547244548797607, 0.012328586541116238, 0.012506922706961632, 0.012629363685846329, 0.012700236402451992, 0.012722617015242577, 0.11369767040014267, 0.012703465297818184, 0.012662879191339016, 0.10585644096136093, 0.09935271739959717, 0.1069478690624237, 0.01248982921242714, 0.1037144735455513, 0.012411379255354404, 0.012346671894192696, 0.012250380590558052, 0.012125336565077305, 0.011976837180554867, 0.011807584203779697, 0.011623104102909565, 0.29193738102912903, 0.011499336920678616, 0.011551788076758385, 0.011561115272343159, 0.011519704945385456, 0.01146638859063387, 0.011380542069673538, 0.011262108571827412, 0.01112810056656599, 0.09433183819055557, 0.0108448825776577, 0.010718269273638725, 0.010570567101240158, 0.010362490080296993, 0.010212458670139313, 0.010031025856733322, 0.0098272655159235, 0.10719823092222214, 0.009456006810069084, 0.009291207417845726, 0.009120800532400608, 0.0089451028034091, 0.3200663924217224, 0.008859813213348389, 0.00891857035458088, 0.008949585258960724, 0.008954099379479885, 0.008932648226618767, 0.008891820907592773, 0.008831767365336418, 0.008753192611038685, 0.008661121129989624, 0.10061391443014145, 0.10410113632678986, 0.10497419536113739, 0.1137678399682045, 0.008466151542961597, 0.008462640456855297, 0.12550431489944458, 0.008441725745797157, 0.33364662528038025, 0.008660852909088135, 0.33340126276016235, 0.1029098853468895, 0.09742168337106705, 0.010129636153578758, 0.10407784581184387, 0.010838914662599564, 0.011133627034723759, 0.09855897724628448, 0.01160843763500452, 0.10703234374523163, 0.011968076229095459, 0.11599192768335342, 0.012231455184519291, 0.012315912172198296, 0.11339285969734192, 0.012407640926539898, 0.09800691902637482, 0.012438426725566387, 0.09825795888900757, 0.012422306463122368, 0.012386273592710495, 0.012314082123339176, 0.11495236307382584, 0.01214026752859354, 0.0120374271646142, 0.09814605861902237, 0.011813757941126823, 0.09837998449802399, 0.10533216595649719, 0.09334389120340347, 0.284503310918808, 0.11388859152793884, 0.2813272774219513, 0.32118162512779236, 0.013098057359457016, 0.10421755164861679, 0.08915313333272934, 0.09679199010133743, 0.015358561649918556, 0.015784641727805138, 0.0934368148446083, 0.1041552945971489, 0.26959341764450073, 0.017257429659366608, 0.1138584166765213, 0.01808423548936844, 0.1019771546125412, 0.0186501145362854, 0.018820758908987045, 0.2646525502204895, 0.01926770992577076, 0.019531985744833946, 0.10898561775684357, 0.019827304407954216, 0.11235196888446808, 0.08734934777021408, 0.10477355122566223, 0.019982166588306427, 0.11275521665811539, 0.019884126260876656, 0.08793541043996811, 0.2524645924568176, 0.11476477235555649, 0.019960792735219002, 0.0200235303491354, 0.020000435411930084, 0.01991024613380432, 0.11352252960205078, 0.11215391010046005, 0.11283233761787415, 0.11298219859600067, 0.01924818567931652, 0.11404750496149063, 0.01891743578016758, 0.10223781317472458, 0.018521077930927277, 0.018285363912582397, 0.018016429618000984, 0.11116424202919006, 0.01744173839688301, 0.01713535748422146, 0.0167950801551342, 0.016454113647341728, 0.01610761135816574, 0.01572537235915661, 0.09531314671039581, 0.01499985158443451, 0.014693647623062134, 0.09064222127199173, 0.014025005511939526, 0.013741632923483849, 0.013426950201392174, 0.013044832274317741, 0.10791383683681488, 0.01251925528049469, 0.10780590772628784, 0.012010361067950726, 0.011696570552885532, 0.11514551937580109, 0.01133550051599741, 0.01112330798059702, 0.11191113293170929, 0.010699720121920109, 0.010520212352275848, 0.010308561846613884, 0.01012074202299118, 0.00989153515547514, 0.00969646219164133, 0.009485743008553982, 0.10786640644073486, 0.009111670777201653, 0.00894438661634922, 0.1085861548781395, 0.10843973606824875, 0.008555582724511623, 0.008454356342554092, 0.33568185567855835, 0.008491743355989456, 0.008614443242549896, 0.11064587533473969, 0.008782852441072464, 0.008845946751534939, 0.008882925845682621, 0.32856130599975586, 0.009142177179455757, 0.009346258826553822, 0.10160720348358154, 0.009689589031040668, 0.009819812141358852, 0.11034460365772247, 0.010033231228590012, 0.09948545694351196, 0.09434767067432404, 0.10432210564613342, 0.010419991798698902, 0.010467191226780415, 0.010542123578488827, 0.10444440692663193, 0.09821572154760361, 0.10006137192249298, 0.01075914315879345, 0.010820050723850727, 0.010791879147291183, 0.1342628300189972, 0.010720551013946533, 0.08887896686792374, 0.010655735619366169, 0.12180067598819733, 0.010572674684226513, 0.09287288784980774, 0.08449786901473999, 0.010467132553458214, 0.010481078177690506, 0.010443148203194141, 0.010274886153638363, 0.11506590992212296, 0.1124105229973793, 0.010079612955451012, 0.010052396915853024, 0.11823759227991104, 0.009891101159155369, 0.09793640673160553, 0.009729955345392227, 0.009661011397838593, 0.09192077815532684, 0.09214772284030914, 0.12148784846067429, 0.009455333463847637, 0.09731394052505493, 0.009503480978310108, 0.12352029979228973, 0.1261778026819229, 0.29966285824775696, 0.009681876748800278, 0.009983314201235771, 0.010172443464398384, 0.11306355148553848, 0.11765212565660477, 0.12274069339036942, 0.0106918690726161, 0.09431903064250946, 0.010897913947701454, 0.010994396172463894, 0.011017797514796257, 0.011023941449820995, 0.1075303703546524, 0.010991240851581097, 0.010961729101836681, 0.10422389209270477, 0.10474299639463425, 0.32006359100341797, 0.011067722924053669, 0.011259316466748714, 0.01140513550490141, 0.09531090408563614, 0.10941797494888306, 0.01171966828405857, 0.011794902384281158, 0.011821743100881577, 0.10743951797485352, 0.3124638497829437, 0.0120968883857131, 0.012297838926315308, 0.012453745119273663, 0.012555564753711224, 0.012612098827958107, 0.012627813033759594, 0.012595773674547672, 0.10446973890066147, 0.012500477954745293, 0.1033487394452095, 0.012382204644382, 0.10725638270378113, 0.012247766368091106, 0.10666288435459137, 0.012102383188903332, 0.01201468612998724, 0.011897866614162922, 0.011764314025640488, 0.1033446416258812, 0.011489183641970158, 0.10304143279790878, 0.011244390159845352, 0.10640250891447067, 0.011024068109691143, 0.010909314267337322, 0.10824253410100937, 0.10911820083856583, 0.11004499346017838, 0.010570179671049118]\n",
            "Val loss 0.055898042395710945\n",
            "Val auc roc 0.5205103233346318\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c37a90cdef66430288f4130bf76723b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1682.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0573\n",
            "Train Losses : [0.0105047058314085, 0.10341686010360718, 0.010360721498727798, 0.01028372161090374, 0.010183977894484997, 0.3195035457611084, 0.10869874805212021, 0.3104580342769623, 0.010736335068941116, 0.011055590584874153, 0.011318890377879143, 0.011528185568749905, 0.01168836560100317, 0.011798582971096039, 0.10869431495666504, 0.1042502373456955, 0.012034830637276173, 0.01207731757313013, 0.012083025649189949, 0.012054354883730412, 0.011996587738394737, 0.1058213859796524, 0.011847348883748055, 0.10579241812229156, 0.011698084883391857, 0.011610418558120728, 0.107024185359478, 0.1088184118270874, 0.3067072927951813, 0.10653954744338989, 0.011750850826501846, 0.011894842609763145, 0.011992774903774261, 0.012047897092998028, 0.2986467182636261, 0.10445691645145416, 0.0125759681686759, 0.012769567780196667, 0.012910757213830948, 0.01299893669784069, 0.01304398849606514, 0.013045866042375565, 0.01301089022308588, 0.10445934534072876, 0.10360020399093628, 0.012864403426647186, 0.10266140848398209, 0.01275660190731287, 0.01268168818205595, 0.10054180771112442, 0.10248104482889175, 0.012444943189620972, 0.012361953034996986, 0.012252110987901688, 0.012119077146053314, 0.011974155902862549, 0.011799667961895466, 0.10376602411270142, 0.011489816009998322, 0.011316545307636261, 0.011148974299430847, 0.3142087757587433, 0.10842054337263107, 0.10667524486780167, 0.011222317814826965, 0.011278119869530201, 0.10845713317394257, 0.011338439770042896, 0.10152219235897064, 0.10428524762392044, 0.01139499805867672, 0.011395520530641079, 0.10646713525056839, 0.011356587521731853, 0.011320079676806927, 0.011255941353738308, 0.011168270371854305, 0.011061402969062328, 0.010935964062809944, 0.010797111317515373, 0.010644287802278996, 0.11050797253847122, 0.10726447403430939, 0.010260224342346191, 0.10956954210996628, 0.010075251571834087, 0.009980049915611744, 0.10709964483976364, 0.009795133024454117, 0.00970040075480938, 0.009593721479177475, 0.009474489837884903, 0.33222487568855286, 0.10372593253850937, 0.10751304030418396, 0.009721657261252403, 0.009820493869483471, 0.009883123449981213, 0.009919469244778156, 0.00992830190807581, 0.10949856787919998, 0.3228602409362793, 0.010147158987820148, 0.010335166938602924, 0.10592357069253922, 0.010632532648742199, 0.10526807606220245, 0.10734397172927856, 0.10593462735414505, 0.10729750245809555, 0.011256752535700798, 0.01135282777249813, 0.011409210041165352, 0.10442082583904266, 0.011465895920991898, 0.10509255528450012, 0.10575973242521286, 0.10529813170433044, 0.01156577654182911, 0.10527332872152328, 0.0116035845130682, 0.011596961878240108, 0.10453847795724869, 0.011543923057615757, 0.011498477309942245, 0.10488539934158325, 0.011380856856703758, 0.10727094858884811, 0.10452847927808762, 0.011236418969929218, 0.011184392496943474, 0.10522094368934631, 0.011058434844017029, 0.10595753788948059, 0.010936299338936806, 0.010864969342947006, 0.010773367248475552, 0.010664205066859722, 0.31681689620018005, 0.010659951716661453, 0.10536639392375946, 0.010834063403308392, 0.01089028362184763, 0.10460834205150604, 0.010951399803161621, 0.010957610793411732, 0.010935915634036064, 0.010888474993407726, 0.010818283073604107, 0.10605618357658386, 0.010665316134691238, 0.010582219809293747, 0.010481729172170162, 0.010365896858274937, 0.010237236507236958, 0.10525432229042053, 0.00999173428863287, 0.009873103350400925, 0.009743656031787395, 0.009605097584426403, 0.00945917796343565, 0.009307091124355793, 0.00915034394711256, 0.10958616435527802, 0.008869072422385216, 0.008740561082959175, 0.008607416413724422, 0.008469161577522755, 0.1100170835852623, 0.008224367164075375, 0.10926036536693573, 0.008039269596338272, 0.11000168323516846, 0.007902570068836212, 0.007839168421924114, 0.3429954946041107, 0.007911017164587975, 0.3419247567653656, 0.008345826528966427, 0.3350296914577484, 0.10673318803310394, 0.009555364958941936, 0.10695015639066696, 0.1060330718755722, 0.10578972846269608, 0.011122292838990688, 0.011439605616033077, 0.10544795542955399, 0.011956608854234219, 0.012158486992120743, 0.012310510501265526, 0.012416244484484196, 0.012479081749916077, 0.10509799420833588, 0.10505221784114838, 0.1050771102309227, 0.012638547457754612, 0.012654230929911137, 0.012635156512260437, 0.1050131544470787, 0.012552535161376, 0.012490750290453434, 0.012403027154505253, 0.012292042374610901, 0.10514995455741882, 0.012058119289577007, 0.011935441754758358, 0.011795486323535442, 0.01164062600582838, 0.10537375509738922, 0.011340088210999966, 0.01119288895279169, 0.10587385296821594, 0.010909391567111015, 0.010771322064101696, 0.010622340254485607, 0.010464120656251907, 0.010298173874616623, 0.010126330889761448, 0.10612978041172028, 0.1066475585103035, 0.10680635273456573, 0.009634332731366158, 0.009546229615807533, 0.00944600347429514, 0.009334941394627094, 0.009214923717081547, 0.009087257087230682, 0.008953274227678776, 0.10773436725139618, 0.008711883798241615, 0.008601291105151176, 0.10800088196992874, 0.10814284533262253, 0.00834859162569046, 0.10827281326055527, 0.10806874930858612, 0.008237087167799473, 0.008210047148168087, 0.10813513398170471, 0.008153359405696392, 0.3396872282028198, 0.6664842963218689, 0.00893190037459135, 0.009509315714240074, 0.010031254962086678, 0.01049604918807745, 0.01090308278799057, 0.01125386729836464, 0.30514925718307495, 0.10513105988502502, 0.012494392693042755, 0.012889786623418331, 0.013220809400081635, 0.01349076721817255, 0.10752714425325394, 0.013905263505876064, 0.2904995083808899, 0.014407831244170666, 0.10465638339519501, 0.014961416833102703, 0.2811548113822937, 0.015569466166198254, 0.015896828845143318, 0.10337037593126297, 0.10301771014928818, 0.01660945452749729, 0.016762766987085342, 0.01685519330203533, 0.016892632469534874, 0.10354524105787277, 0.01687012054026127, 0.01681453548371792, 0.10563630610704422, 0.01663755252957344, 0.016518766060471535, 0.016367092728614807, 0.016186730936169624, 0.015982454642653465, 0.015757709741592407, 0.01551547460258007, 0.1024000346660614, 0.1043718010187149, 0.014845459721982479, 0.014636536128818989, 0.014412504620850086, 0.10524756461381912, 0.013974646106362343, 0.1035614013671875, 0.10572943091392517, 0.10458365827798843, 0.013301054015755653, 0.01315606851130724, 0.012994061224162579, 0.29401877522468567, 0.012872296385467052, 0.012889496982097626, 0.012873698025941849, 0.10596465319395065, 0.012798509560525417, 0.012741000391542912, 0.012658658437430859, 0.012553774751722813, 0.012429422698915005, 0.10689388960599899, 0.0121757285669446, 0.10311058163642883, 0.011942817829549313, 0.01182268001139164, 0.1060173436999321, 0.10525111854076385, 0.10342289507389069, 0.30494821071624756, 0.011599598452448845, 0.011715283617377281, 0.10788843780755997, 0.10741325467824936, 0.011966788209974766, 0.10501936078071594, 0.012080819346010685, 0.012107567861676216, 0.012102717533707619, 0.012069799937307835, 0.01201169565320015, 0.011931244283914566, 0.10584945976734161, 0.30515530705451965, 0.01189576555043459, 0.10708477348089218, 0.10518038272857666, 0.012205355800688267, 0.01227286271750927, 0.012305309996008873, 0.012305277399718761, 0.012275720946490765, 0.012220676988363266, 0.012142486870288849, 0.012043879367411137, 0.011927437037229538, 0.011795809492468834, 0.011651056818664074, 0.01149520743638277, 0.10572723299264908, 0.01119740679860115, 0.30932554602622986, 0.011129608377814293, 0.10475850105285645, 0.011226718313992023, 0.30579322576522827, 0.5963976383209229, 0.012156265787780285, 0.012765169143676758, 0.10392475873231888, 0.10286010056734085, 0.014279601164162159, 0.014681045897305012, 0.015012125484645367, 0.015277283266186714, 0.10329141467809677, 0.10583078116178513, 0.015846140682697296, 0.10320413112640381, 0.016078999266028404, 0.10554777085781097, 0.01619907096028328, 0.2794595956802368, 0.016426322981715202, 0.016578927636146545, 0.10497312247753143, 0.016764327883720398, 0.01680263876914978, 0.016793813556432724, 0.10511685162782669, 0.10605427622795105, 0.01666202023625374, 0.10295406728982925, 0.01652141846716404, 0.016420548781752586, 0.016288109123706818, 0.016127508133649826, 0.2812221646308899, 0.2790716290473938, 0.10246167331933975, 0.016453133895993233, 0.016614845022559166, 0.10261591523885727, 0.10459689050912857, 0.01690487004816532, 0.10520794987678528, 0.016977939754724503, 0.016966821625828743, 0.01691346801817417, 0.01682204380631447, 0.016696995124220848, 0.016542892903089523, 0.016362756490707397, 0.01616046205163002, 0.015939608216285706, 0.10432085394859314, 0.8312703967094421, 0.01614139787852764, 0.01669716089963913, 0.01716659963130951, 0.017554040998220444, 0.10312119871377945, 0.10435836762189865, 0.018401052802801132, 0.018586765974760056, 0.018709661439061165, 0.10461322218179703, 0.018830634653568268, 0.018835561349987984, 0.018793003633618355, 0.018709227442741394, 0.10411183536052704, 0.018478162586688995, 0.018334656953811646, 0.10431283712387085, 0.018006764352321625, 0.01782427728176117, 0.10342012345790863, 0.017435073852539062, 0.10442709177732468, 0.10474191606044769, 0.2747837007045746, 0.10462584346532822, 0.016972780227661133, 0.016970088705420494, 0.016927596181631088, 0.10419309884309769, 0.0167811531573534, 0.016680816188454628, 0.016551269218325615, 0.27629297971725464, 0.016445614397525787, 0.10372475534677505, 0.2755282521247864, 0.016653602942824364, 0.016790524125099182, 0.10408487915992737, 0.10322749614715576, 0.10423251241445541, 0.10423750430345535, 0.10420893132686615, 0.10411728918552399, 0.017269521951675415, 0.01727672480046749, 0.017243485897779465, 0.0171732846647501, 0.017069045454263687, 0.016936205327510834, 0.01677756942808628, 0.10426413267850876, 0.01643799990415573, 0.01625785045325756, 0.10447393357753754, 0.015887586399912834, 0.015695171430706978, 0.015488404780626297, 0.1039414182305336, 0.015078206546604633, 0.10444716364145279, 0.10467355698347092, 0.1039540246129036, 0.014419842511415482, 0.014273300766944885, 0.10479877889156342, 0.01397197786718607, 0.013817361555993557, 0.01364811509847641, 0.013466398231685162, 0.01327395811676979, 0.10524830222129822, 0.012902593240141869, 0.012722196988761425, 0.012532254680991173, 0.012334637343883514, 0.01213012170046568, 0.011920547112822533, 0.01170589029788971, 0.011487836949527264, 0.011266647838056087, 0.011042927391827106, 0.01081755105406046, 0.010590960271656513, 0.010362871922552586, 0.010134369134902954, 0.009905513375997543, 0.009676306508481503, 0.009447368793189526, 0.10709980130195618, 0.10791373252868652, 0.0089005958288908, 0.008754225447773933, 0.008599996566772461, 0.10816898941993713, 0.33599853515625, 0.3345679044723511, 0.008818244561553001, 0.10731916129589081, 0.10734999924898148, 0.009678943082690239, 0.009907957166433334, 0.01009233295917511, 0.10611805319786072, 0.010381626896560192, 0.01049041748046875, 0.01056609209626913, 0.010612123645842075, 0.10635533183813095, 0.010663381777703762, 0.10617854446172714, 0.1056194007396698, 0.3107883036136627, 0.010934057645499706, 0.10551182925701141, 0.011269964277744293, 0.011397769674658775, 0.10473093390464783, 0.011587691493332386, 0.3056965470314026, 0.10560605674982071, 0.10571647435426712, 0.012330866418778896, 0.012503259815275669, 0.012634028680622578, 0.10421573370695114, 0.012819894589483738, 0.012878455221652985, 0.012906832620501518, 0.1053522527217865, 0.10536897927522659, 0.10549349337816238, 0.10566902160644531, 0.012981426902115345, 0.10410156846046448, 0.10409506410360336, 0.013014117255806923, 0.013005238026380539, 0.10472371429204941, 0.012949389405548573, 0.10430076718330383, 0.012872391380369663, 0.01281813345849514, 0.012743467465043068, 0.012650673277676105, 0.012542328797280788, 0.012420102022588253, 0.01228596456348896, 0.10586001724004745, 0.104644276201725, 0.01193047035485506, 0.011822900734841824, 0.011703343130648136, 0.10589999705553055, 0.011470452882349491, 0.011355310678482056, 0.011230670846998692, 0.011097685433924198, 0.010957986116409302, 0.01081243809312582, 0.010662214830517769, 0.010508091188967228, 0.010351305827498436, 0.10730786621570587, 0.10652179270982742, 0.10556040704250336, 0.10573191940784454, 0.009841347113251686, 0.009777418337762356, 0.009702404029667377, 0.10739083588123322, 0.009557037614285946, 0.009485337883234024, 0.009404095821082592, 0.009314612485468388, 0.10587392002344131, 0.10795842111110687, 0.009099988266825676, 0.10841447860002518, 0.009006079286336899, 0.008957885205745697, 0.008898958563804626, 0.008830144070088863, 0.10836922377347946, 0.008700869977474213, 0.1067260354757309, 0.10858624428510666, 0.10844679921865463, 0.10796704143285751, 0.10842178016901016, 0.008639703504741192, 0.10818382352590561, 0.10713940113782883, 0.1069183424115181, 0.008792460896074772, 0.008829750120639801, 0.008847713470458984, 0.10808316618204117, 0.10760991275310516, 0.10723549872636795, 0.1084146797657013, 0.009002589620649815, 0.009039382450282574, 0.10629480332136154, 0.10779055953025818, 0.009133455343544483, 0.009157826192677021, 0.009163147769868374, 0.10771027207374573, 0.327874094247818, 0.009334188885986805, 0.009477726183831692, 0.10742519050836563, 0.009709218516945839, 0.009798702783882618, 0.10592964291572571, 0.009933935478329659, 0.009980910457670689, 0.010005027055740356, 0.10603083670139313, 0.010026292875409126, 0.010023900307714939, 0.1074734777212143, 0.009999940171837807, 0.10652106255292892, 0.009974551387131214, 0.009951568208634853, 0.009913609363138676, 0.1067454144358635, 0.00982959009706974, 0.10707596689462662, 0.1061277687549591, 0.00974756758660078, 0.10675980895757675, 0.00971397664397955, 0.009689351543784142, 0.10686499625444412, 0.10682740062475204, 0.009626521728932858, 0.009606581181287766, 0.10636645555496216, 0.009555206634104252, 0.009523618966341019, 0.1070370301604271, 0.009452912956476212, 0.009413194842636585, 0.009360955096781254, 0.009297738783061504, 0.009224527515470982, 0.009142749011516571, 0.009053682908415794, 0.10770562291145325, 0.10734082013368607, 0.00884344894438982, 0.008787335827946663, 0.00872187688946724, 0.008648203685879707, 0.00856740865856409, 0.008480386808514595, 0.008388317190110683, 0.008291481994092464, 0.10865574330091476, 0.008118993602693081, 0.008040912449359894, 0.007957518100738525, 0.10923746973276138, 0.007809583563357592, 0.00774272158741951, 0.10888266563415527, 0.10900672525167465, 0.007600122131407261, 0.0075667561031877995, 0.007524284068495035, 0.10936158895492554, 0.007447490934282541, 0.10954604297876358, 0.007398220710456371, 0.10960118472576141, 0.10958832502365112, 0.007388508412986994, 0.34784135222435, 0.007558984681963921, 0.007699371315538883, 0.007814154028892517, 0.10872948914766312, 0.10857993364334106, 0.008115233853459358, 0.10837974399328232, 0.10796407610177994, 0.10796625912189484, 0.00850876234471798, 0.1079186424612999, 0.10691258311271667, 0.008791394531726837, 0.008868643082678318, 0.10724702477455139, 0.008989213034510612, 0.009033219888806343, 0.009057204239070415, 0.10734338313341141, 0.009084592573344707, 0.10751339048147202, 0.009107326157391071, 0.009108662605285645, 0.10766324400901794, 0.10754123330116272, 0.009115642867982388, 0.10723470896482468, 0.00913334358483553, 0.10717926174402237, 0.009147477336227894, 0.009145292453467846, 0.009127440862357616, 0.00909582432359457, 0.10737886279821396, 0.009028090164065361, 0.008991303853690624, 0.008943010121583939, 0.1066092774271965, 0.10767056792974472, 0.008831373415887356, 0.10787144303321838, 0.00879370141774416, 0.008766422979533672, 0.008729567751288414, 0.33361998200416565, 0.008802576921880245, 0.10726536810398102, 0.10740610212087631, 0.009107021614909172, 0.009189660660922527, 0.1066785454750061, 0.10692991316318512, 0.00939477514475584, 0.10694155842065811, 0.1065593734383583, 0.10662199556827545, 0.009666135534644127, 0.009722298942506313, 0.009755977429449558, 0.3223388195037842, 0.009946002624928951, 0.10737664252519608, 0.01023402251303196, 0.010347156785428524, 0.010430782102048397, 0.10669166594743729, 0.010552994906902313, 0.010592342354357243, 0.010609012097120285, 0.10620355606079102, 0.010613955557346344, 0.31320300698280334, 0.010759851895272732, 0.010881069116294384, 0.10550526529550552, 0.10555732250213623, 0.011164961382746696, 0.011233251541852951, 0.011274293065071106, 0.10584292560815811, 0.1051766648888588, 0.011353123001754284, 0.011364413425326347, 0.011353702284395695, 0.011322966776788235, 0.011274720542132854, 0.011210017837584019, 0.10482068359851837, 0.011074201203882694, 0.011002374812960625, 0.31204915046691895, 0.31015318632125854, 0.011252938769757748, 0.011456659063696861, 0.011620636098086834, 0.011748296208679676, 0.10458186268806458, 0.3021382987499237, 0.012189963832497597, 0.29944998025894165, 0.10452523082494736, 0.1040571928024292, 0.10564542561769485, 0.01369827426970005, 0.01394299603998661, 0.014136839658021927, 0.10601893812417984, 0.014426779933273792, 0.014525406993925571, 0.014584227465093136, 0.014609255827963352, 0.284336656332016, 0.014762507751584053, 0.014877750538289547, 0.014951823279261589, 0.014988958835601807, 0.01499097514897585, 0.014963571913540363, 0.10350912064313889, 0.014864451251924038, 0.014794436283409595, 0.014703149907290936, 0.014591054059565067, 0.10434528440237045, 0.014351820573210716, 0.014225684106349945, 0.01409485749900341, 0.10407668352127075, 0.10360808670520782, 0.10708075016736984, 0.013603240251541138, 0.29475855827331543, 0.10368612408638, 0.01363736018538475, 0.013674943707883358, 0.2902599275112152, 0.10402495414018631, 0.0140155628323555, 0.2873218059539795, 0.014407829381525517, 0.014627770520746708, 0.014799252152442932, 0.10360002517700195, 0.2849391996860504, 0.01531577855348587, 0.01553367543965578, 0.015699399635195732, 0.10410775244235992, 0.015929069370031357, 0.01599680818617344, 0.016025511547923088, 0.2785778045654297, 0.01617639884352684, 0.10496947914361954, 0.01638910174369812, 0.10310157388448715, 0.016503633931279182, 0.01652045175433159, 0.016501285135746002, 0.016450349241495132, 0.016370370984077454, 0.016265446320176125, 0.10374253243207932, 0.016027461737394333, 0.015896620228886604, 0.1033826544880867, 0.015617172233760357, 0.015469824895262718, 0.01530726533383131, 0.10361514985561371, 0.014980143867433071, 0.10349513590335846, 0.014673471450805664, 0.10305611044168472, 0.014385509304702282, 0.10640908777713776, 0.014114954508841038, 0.1064266487956047, 0.1030115932226181, 0.10384631901979446, 0.013684035278856754, 0.10277192294597626, 0.013507332652807236, 0.013410902582108974, 0.013299276120960712, 0.013174579478800297, 0.01303890347480774, 0.01289357990026474, 0.01273998711258173, 0.10417231917381287, 0.012445989064872265, 0.012304055504500866, 0.01215541921555996, 0.012000869028270245, 0.10689085721969604, 0.01171048078685999, 0.011572923511266708, 0.011429517529904842, 0.10755931586027145, 0.011161663569509983, 0.01103491522371769, 0.010903166607022285, 0.010767025873064995, 0.010627267882227898, 0.01048499345779419, 0.10486314445734024, 0.010225052013993263, 0.010104680433869362, 0.10816560685634613, 0.00988340936601162, 0.009780068881809711, 0.009672347456216812, 0.10750778764486313, 0.10580411553382874, 0.10866729170084, 0.3271283507347107, 0.1058955043554306, 0.009597109630703926, 0.009686900302767754, 0.00975259579718113, 0.0097964433953166, 0.10706337541341782, 0.009854498319327831, 0.3191176652908325, 0.10548554360866547, 0.010197516530752182, 0.10744933038949966, 0.010461146011948586, 0.10582022368907928, 0.10514964908361435, 0.010776229202747345, 0.010855626314878464, 0.10437637567520142, 0.010968664661049843, 0.011004147119820118, 0.011017617769539356, 0.011011095717549324, 0.010987047106027603, 0.010946042835712433, 0.010891267098486423, 0.010824129916727543, 0.010745896026492119, 0.31629353761672974, 0.10460720956325531, 0.10704988986253738, 0.010892239399254322, 0.01094888523221016, 0.010982092469930649, 0.10457678139209747, 0.011014404706656933, 0.01101523358374834, 0.010997477918863297, 0.010963300243020058, 0.1076711118221283, 0.010881725698709488, 0.01083430740982294, 0.10582144558429718, 0.010731655173003674, 0.010675950907170773, 0.010608620010316372, 0.010530571453273296, 0.10496443510055542, 0.010377845726907253, 0.10587096959352493, 0.10634596645832062, 0.010209452360868454, 0.010159634053707123, 0.10558456182479858, 0.010056688450276852, 0.10568305850028992, 0.10577552765607834, 0.009948382154107094, 0.00991536770015955, 0.009869907982647419, 0.10570942610502243, 0.10620919615030289, 0.00975564494729042, 0.009721685200929642, 0.00967550091445446, 0.10827066749334335, 0.10855976492166519, 0.009563123807311058, 0.32271865010261536, 0.009648285806179047, 0.10579318553209305, 0.009836605750024319, 0.009908787906169891, 0.10722500085830688, 0.01001505646854639, 0.1080223023891449, 0.01009605173021555, 0.31999480724334717, 0.010292237624526024, 0.010431135073304176, 0.010541005991399288, 0.010623487643897533, 0.010678915306925774, 0.10568826645612717, 0.010755514726042747, 0.010776583105325699, 0.10536370426416397, 0.10571972280740738, 0.10729756206274033, 0.010844332166016102, 0.010856064967811108, 0.010847993195056915, 0.10663411766290665, 0.010812580585479736, 0.010785377584397793, 0.010743139311671257, 0.10773827135562897, 0.01065028179436922, 0.010599200613796711, 0.010536087676882744, 0.01046260166913271, 0.10603950917720795, 0.32000839710235596, 0.010411298833787441, 0.01047887746244669, 0.010523050092160702, 0.010545928031206131, 0.10696649551391602, 0.010565094649791718, 0.010561725124716759, 0.010541928932070732, 0.10564205050468445, 0.10575643926858902, 0.010482443496584892, 0.10677269101142883, 0.10600096732378006, 0.10576178133487701, 0.01047485414892435, 0.10602119565010071, 0.010483155027031898, 0.01047559641301632, 0.10628437250852585, 0.010443166829645634, 0.10564590990543365, 0.010407719761133194, 0.010381726548075676, 0.010341942310333252, 0.10686028003692627, 0.010255281813442707, 0.010207846760749817, 0.01014926377683878, 0.10608034580945969, 0.010032206773757935, 0.10629092901945114, 0.10627129673957825, 0.10636543482542038, 0.009899195283651352, 0.009875698015093803, 0.00983956828713417, 0.009791743010282516, 0.10745199024677277, 0.009694578126072884, 0.009644607082009315, 0.009584968909621239, 0.10641409456729889, 0.009469049051404, 0.009411840699613094, 0.10722889006137848, 0.00930111762136221, 0.009246385656297207, 0.32801979780197144, 0.009270203299820423, 0.10781021416187286, 0.32428959012031555, 0.009614451788365841, 0.009789589792490005, 0.10617796331644058, 0.10607998818159103, 0.3182543218135834, 0.01049644686281681, 0.10547376424074173, 0.31122300028800964, 0.01131085678935051, 0.0116165392100811, 0.011877607554197311, 0.01209620013833046, 0.012275058776140213, 0.10475935786962509, 0.012554610148072243, 0.012658031657338142, 0.10467322170734406, 0.012803569436073303, 0.10448412597179413, 0.012896380387246609, 0.1048780232667923, 0.012945462018251419, 0.012948228977620602, 0.2960057258605957, 0.10463091731071472, 0.013183935545384884, 0.10445629805326462, 0.013364914804697037, 0.013422900810837746, 0.1053490862250328, 0.01348549872636795, 0.10456546396017075, 0.10416710376739502, 0.10417857021093369, 0.10534914582967758, 0.013578780926764011, 0.10506124794483185, 0.013591093011200428, 0.01357603445649147, 0.013538858853280544, 0.10529321432113647, 0.29149961471557617, 0.013546418398618698, 0.013620434328913689, 0.013663862831890583, 0.013679146766662598, 0.013669081032276154, 0.013636566698551178, 0.013583553954958916, 0.0135126868262887, 0.10433653742074966, 0.10397763550281525, 0.013299605809152126, 0.013226637616753578, 0.013138825073838234, 0.29539355635643005, 0.10460256785154343, 0.1045832633972168, 0.10408180952072144, 0.013272667303681374, 0.2926347851753235, 0.2930525541305542, 0.10382125526666641, 0.10532224178314209, 0.014338127337396145, 0.10490405559539795, 0.10386315733194351, 0.014950036071240902, 0.015095971524715424, 0.10383162647485733, 0.015300612896680832, 0.015363159589469433, 0.015391810797154903, 0.1041439101099968, 0.015392600558698177, 0.10429925471544266, 0.0153487678617239, 0.015304581262171268, 0.015237635932862759, 0.10551243275403976, 0.015076509676873684, 0.014983690343797207, 0.014873991720378399, 0.10462677478790283, 0.10541609674692154, 0.014553808607161045, 0.10391249507665634, 0.014358733780682087, 0.10437913984060287, 0.01416597981005907, 0.014062834903597832, 0.013946141116321087, 0.013818083330988884, 0.01367995422333479, 0.01353357918560505, 0.013379886746406555, 0.10583267360925674, 0.013085300102829933, 0.10589945316314697, 0.0128237409517169, 0.012696058489382267, 0.012561018578708172, 0.10451774299144745, 0.01230288203805685, 0.012178126722574234, 0.012047178111970425, 0.011911038309335709, 0.011770686134696007, 0.307081401348114, 0.01163902971893549, 0.011631716042757034, 0.011607088148593903, 0.01156684197485447, 0.011512490920722485, 0.10608543455600739, 0.30780819058418274, 0.011490515433251858, 0.011558180674910545, 0.10594257712364197, 0.011649598367512226, 0.011675243265926838, 0.3049774765968323, 0.011823482811450958, 0.011934802867472172, 0.012017101049423218, 0.10530288517475128, 0.012131179682910442, 0.012165152467787266, 0.012176346965134144, 0.10517323017120361, 0.012167991138994694, 0.012149329297244549, 0.012113388627767563, 0.10504188388586044, 0.01202493254095316, 0.011972712352871895, 0.011907094158232212, 0.011829647235572338, 0.011742067523300648, 0.10529278963804245, 0.01156845036894083, 0.10537613183259964, 0.01141340658068657, 0.011335194110870361, 0.011247637681663036, 0.10597336292266846, 0.011077173985540867, 0.01099317241460085, 0.10551514476537704, 0.010829865001142025, 0.010749606415629387, 0.010661913082003593, 0.01056751236319542, 0.010467822663486004, 0.010363195091485977, 0.010254932567477226, 0.01014332938939333, 0.010029111057519913, 0.009912744164466858, 0.00979471392929554, 0.009675693698227406, 0.322917640209198, 0.00958123616874218, 0.10630209743976593, 0.009610933251678944, 0.009615825489163399, 0.009606736712157726, 0.009585161693394184, 0.00955232698470354, 0.009509583935141563, 0.10616772621870041, 0.009424087591469288, 0.009380393661558628, 0.009328496642410755, 0.10744740813970566, 0.009228060021996498, 0.009178739041090012, 0.009121936745941639, 0.009058772586286068, 0.10764267295598984, 0.1067197322845459, 0.008909548632800579, 0.008869470097124577, 0.008821680210530758, 0.1071421205997467, 0.008730750530958176, 0.008686834946274757, 0.008635787293314934, 0.10867450386285782, 0.008540629409253597, 0.10708429664373398, 0.008467486128211021, 0.3356333374977112, 0.008526564575731754, 0.008601605892181396, 0.10761425644159317, 0.008722333237528801, 0.10831492394208908, 0.32932063937187195, 0.009000341407954693, 0.00914958119392395, 0.10735971480607986, 0.009397631511092186, 0.009497948922216892, 0.1063971221446991, 0.10690326988697052, 0.10752063989639282, 0.009833745658397675, 0.009901204146444798, 0.10706928372383118, 0.10636474192142487, 0.10684322565793991, 0.10730628669261932, 0.01019824668765068, 0.10517458617687225, 0.3172113299369812, 0.010483806021511555, 0.010633659549057484, 0.010754399001598358, 0.3122508227825165, 0.011065074242651463, 0.10446786135435104, 0.10630185902118683, 0.3044818043708801, 0.10566848516464233, 0.10484623908996582, 0.012399852275848389, 0.012613683938980103, 0.012788999825716019, 0.012928317300975323, 0.10541720688343048, 0.013138012029230595, 0.013211021199822426, 0.013256180100142956, 0.013276432640850544, 0.013273673132061958, 0.013250725343823433, 0.013209312222898006, 0.013151392340660095, 0.013079199939966202, 0.012994537129998207, 0.012898474000394344, 0.10485383868217468, 0.012705469503998756, 0.01260808389633894, 0.012501968070864677, 0.012387942522764206, 0.30109745264053345, 0.10440371930599213, 0.10562524944543839, 0.012356037274003029, 0.012368982657790184, 0.012361920438706875, 0.1051570475101471, 0.10432615131139755, 0.012318155728280544, 0.012295481748878956, 0.012256715446710587, 0.012203465215861797, 0.012137923389673233, 0.302705854177475, 0.012122650630772114, 0.012160401791334152, 0.10618814080953598, 0.012198854237794876, 0.012201237492263317, 0.10444741696119308, 0.012179084122180939, 0.012155307456851006, 0.01211632788181305, 0.012063339352607727, 0.011998158879578114, 0.011922063305974007, 0.01183659490197897, 0.011742928996682167, 0.01164217572659254, 0.10467225313186646, 0.011448387056589127, 0.10613159835338593, 0.10626985877752304, 0.01122176181524992, 0.011153866536915302, 0.011077522300183773, 0.010993571020662785, 0.1051720678806305, 0.010831618681550026, 0.010752527974545956, 0.010666904039680958, 0.010575682856142521, 0.010479507967829704, 0.010379108600318432, 0.010275371372699738, 0.31750306487083435, 0.010198713280260563, 0.010211748071014881, 0.010209978558123112, 0.010194569826126099, 0.010167120024561882, 0.010128605179488659, 0.010080634616315365, 0.010024163872003555, 0.10708040744066238, 0.10554783791303635, 0.00988274160772562, 0.3213590383529663, 0.009928608313202858, 0.00999404862523079, 0.10689198225736618, 0.010092119686305523, 0.010125666856765747, 0.010142423212528229, 0.010144072584807873, 0.010132239200174809, 0.010107804089784622, 0.01007270161062479, 0.010027803480625153, 0.00997423566877842, 0.10706386715173721, 0.009869078174233437, 0.009816777892410755, 0.009757018648087978, 0.009690755046904087, 0.009618922136723995, 0.009542107582092285, 0.009461414068937302, 0.00937688909471035, 0.009289457462728024, 0.009199592284858227, 0.009107688441872597, 0.009014317765831947, 0.008919525891542435, 0.008823900483548641, 0.10732793062925339, 0.10729849338531494, 0.3324365019798279, 0.008668561466038227, 0.10708757489919662, 0.008780047297477722, 0.10862883180379868, 0.10774969309568405, 0.10704735666513443, 0.008993465453386307, 0.10772190243005753, 0.009092941880226135, 0.009129566140472889, 0.009151464328169823, 0.00915946438908577, 0.0091553358361125, 0.009140151552855968, 0.1079142689704895, 0.009104128926992416, 0.10650286823511124, 0.10616133362054825, 0.009079664014279842, 0.009072423912584782, 0.009054462425410748, 0.009027170948684216, 0.008991638198494911, 0.10634104162454605, 0.008921600878238678, 0.10849025845527649, 0.00886609312146902, 0.1076776459813118, 0.008823004551231861, 0.008799426257610321, 0.00876725185662508, 0.008727953769266605, 0.00868198275566101, 0.008630422875285149, 0.008573767729103565, 0.008512822911143303, 0.008447921834886074, 0.10933011025190353, 0.008330884389579296, 0.008277391083538532, 0.10899669677019119, 0.10727863758802414, 0.00815687607973814, 0.10766027867794037, 0.34061700105667114, 0.008213285356760025, 0.008296091109514236, 0.008361204527318478, 0.008410285227000713, 0.008444733917713165, 0.10858122259378433, 0.108054019510746, 0.008536597713828087, 0.008562265895307064, 0.10825131088495255, 0.0085988100618124, 0.008609740994870663, 0.00860931072384119, 0.008598672226071358, 0.1065722331404686, 0.3327890932559967, 0.10867024958133698, 0.008793816901743412, 0.008884486742317677, 0.00895569659769535, 0.10632405430078506, 0.009069341234862804, 0.009112419560551643, 0.009139908477663994, 0.009153852239251137, 0.10796109586954117, 0.009167419746518135, 0.10598013550043106, 0.009178195148706436, 0.009176774881780148, 0.10605315119028091, 0.009164314717054367, 0.009152722544968128, 0.00913158804178238, 0.009101326577365398, 0.10789690166711807, 0.009040068835020065, 0.00900835171341896, 0.10794390738010406, 0.00894529465585947, 0.008912931196391582, 0.3288174867630005, 0.00895247794687748, 0.10592867434024811, 0.00907991360872984, 0.009128882549703121, 0.10619758069515228, 0.009203343652188778, 0.10832564532756805, 0.009263462387025356, 0.00928305834531784, 0.009289448149502277, 0.10742852091789246, 0.009289823472499847, 0.10578733682632446, 0.009289334528148174, 0.10775212943553925, 0.10576459765434265, 0.10664653778076172, 0.1078808456659317, 0.009361420758068562, 0.009379750117659569, 0.3217739164829254, 0.3259899616241455, 0.009728352539241314, 0.00992121733725071, 0.010084841400384903, 0.3188934922218323, 0.01046113483607769, 0.10606489330530167, 0.010864379815757275, 0.10641511529684067, 0.011189743876457214, 0.10531178116798401, 0.011447202414274216, 0.10397578030824661, 0.01164570264518261, 0.10492482036352158, 0.01179426908493042, 0.011846165172755718, 0.011877506971359253, 0.011889543384313583, 0.1048586443066597, 0.10347076505422592, 0.011898308992385864, 0.01189193781465292, 0.011870460584759712, 0.011835433542728424, 0.10607926547527313, 0.011753654107451439, 0.011707093566656113, 0.011650136671960354, 0.01158358808606863, 0.10593944787979126, 0.011449594050645828, 0.10737971216440201, 0.01132973749190569, 0.011268338188529015, 0.10636993497610092, 0.10493581742048264, 0.011104611679911613, 0.011054685339331627, 0.010995430871844292, 0.010928566567599773, 0.607495903968811, 0.011060994118452072, 0.10631948709487915, 0.011401948519051075, 0.01153986994177103, 0.10658758878707886, 0.011757250875234604, 0.011839565820991993, 0.011898989789187908, 0.011937156319618225, 0.10447152704000473, 0.01198110543191433, 0.011987900361418724, 0.011978517286479473, 0.011954808607697487, 0.3036983609199524, 0.10448258370161057, 0.012078525498509407, 0.012135251425206661, 0.012170856818556786, 0.012187416665256023, 0.012186623178422451, 0.10634420812129974, 0.10474537312984467, 0.012163016945123672, 0.012147284112870693, 0.10533177107572556, 0.10597056895494461, 0.012088971212506294, 0.30030563473701477, 0.10372823476791382, 0.012243163771927357, 0.012307335622608662, 0.012349668890237808, 0.012372121214866638, 0.10605096817016602, 0.012386834248900414, 0.302473783493042, 0.012487542815506458, 0.012567894533276558, 0.10559536516666412, 0.10397156327962875, 0.012740807607769966, 0.012777162715792656, 0.01279312465339899, 0.10465816408395767, 0.012795735150575638, 0.10364887863397598, 0.012778465636074543, 0.012757470831274986, 0.012722322717308998, 0.012674345634877682, 0.012614868581295013, 0.10392721742391586, 0.01248996239155531, 0.012424365617334843, 0.01234945934265852, 0.3016808032989502, 0.2976226210594177, 0.10494286566972733, 0.012583790346980095, 0.105536088347435, 0.012795883230865002, 0.10358694195747375, 0.10511115193367004, 0.01302522886544466, 0.10317275673151016, 0.013128693215548992, 0.013158636167645454, 0.01316854264587164, 0.01316056214272976, 0.013136646710336208, 0.013097711838781834, 0.01304619200527668, 0.012983250431716442, 0.012910386547446251, 0.01282798033207655, 0.012738625518977642, 0.29620128870010376, 0.1055232435464859, 0.012692883610725403, 0.012701481580734253, 0.10487817227840424, 0.012692049145698547, 0.012674905359745026, 0.01264357753098011, 0.012599875219166279, 0.012544378638267517, 0.10508154332637787, 0.012427342124283314, 0.012365303002297878, 0.012294112704694271, 0.012215700000524521, 0.012129639275372028, 0.012037946842610836, 0.0119414571672678, 0.10523327440023422, 0.10656888782978058, 0.011689982376992702, 0.10742420703172684, 0.011556941084563732, 0.01148996315896511, 0.011415846645832062, 0.011336381547152996, 0.10700345039367676, 0.31015169620513916, 0.011229278519749641, 0.011257503181695938, 0.011269389651715755, 0.011266685090959072, 0.011251147836446762, 0.01122420746833086, 0.10670460015535355, 0.10619651526212692, 0.1071535050868988, 0.1064414381980896, 0.10680237412452698, 0.011151126585900784, 0.30673620104789734, 0.1040959358215332, 0.10449399799108505, 0.10584043711423874, 0.10637757927179337, 0.10402719676494598, 0.10554276406764984, 0.01182080153375864, 0.011888994835317135, 0.10624202340841293, 0.011986098252236843, 0.01201651431620121, 0.012029701843857765, 0.10476551204919815, 0.01203114166855812, 0.012020383961498737, 0.011996433138847351, 0.011960426345467567, 0.011913953348994255, 0.011857884004712105, 0.10393207520246506, 0.011742730624973774, 0.10596104711294174, 0.10412625968456268, 0.10620418936014175, 0.011579755693674088, 0.011545172892510891, 0.10572890192270279, 0.011467733420431614, 0.011424883268773556]\n",
            "Val loss 0.054557816914859274\n",
            "Val auc roc 0.5\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee122779dcdc46eba28aa1e72b5d0546",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1682.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0573\n",
            "Train Losses : [0.01137327030301094, 0.10607710480690002, 0.011362663470208645, 0.10551953315734863, 0.01135286595672369, 0.304663747549057, 0.01135379821062088, 0.011357597075402737, 0.011359789408743382, 0.01136047299951315, 0.0113598657771945, 0.011358118616044521, 0.011354800313711166, 0.10562466830015182, 0.011347866617143154, 0.10396821051836014, 0.011341025121510029, 0.011337079107761383, 0.01133244950324297, 0.011326873674988747, 0.011320623569190502, 0.10623875260353088, 0.01130798738449812, 0.011301667429506779, 0.10387846827507019, 0.011289075948297977, 0.011282813735306263, 0.011276120319962502, 0.011268464848399162, 0.10400638729333878, 0.011253631673753262, 0.011246699839830399, 0.011239100247621536, 0.011230711825191975, 0.01122196950018406, 0.10568451136350632, 0.011205456219613552, 0.01119746919721365, 0.011189082637429237, 0.3099275529384613, 0.10666381567716599, 0.10535868257284164, 0.011188598349690437, 0.1047200858592987, 0.011192776262760162, 0.011193484999239445, 0.011192936450242996, 0.011191112920641899, 0.01118850614875555, 0.10631182044744492, 0.01118160504847765, 0.10595713555812836, 0.0111755495890975, 0.011171896941959858, 0.011167533695697784, 0.011162171140313148, 0.10674251616001129, 0.011151321232318878, 0.01114589348435402, 0.011139638721942902, 0.011132733896374702, 0.10493073612451553, 0.011119682341814041, 0.011113165877759457, 0.10578777641057968, 0.011100221425294876, 0.011093765497207642, 0.011086910031735897, 0.011079458519816399, 0.10713670402765274, 0.10728438198566437, 0.01106004137545824, 0.011053955182433128, 0.10703565180301666, 0.6041469573974609, 0.011061929166316986, 0.011078246869146824, 0.01109171099960804, 0.011102639138698578, 0.011111309751868248, 0.10574069619178772, 0.011124325916171074, 0.011129203252494335, 0.011132057756185532, 0.011133529245853424, 0.011133859865367413, 0.011132840067148209, 0.10435734689235687, 0.011129407212138176, 0.10736212879419327, 0.011125706136226654, 0.01112336851656437, 0.011119989678263664, 0.011115709319710732, 0.10608400404453278, 0.01110715139657259, 0.10578567534685135, 0.01109900139272213, 0.011094911955296993, 0.011089690029621124, 0.011083967052400112, 0.011077753268182278, 0.011071349494159222, 0.01106331218034029, 0.011055565439164639, 0.011047289706766605, 0.01103861816227436, 0.011029672808945179, 0.011020569130778313, 0.3111296594142914, 0.01101249735802412, 0.011012323200702667, 0.10536894202232361, 0.3095000684261322, 0.011019635945558548, 0.107231505215168, 0.10541900247335434, 0.1038728728890419, 0.011048534885048866, 0.10648883879184723, 0.10397966206073761, 0.011064897291362286, 0.011068842373788357, 0.01107112504541874, 0.1073872372508049, 0.01107376255095005, 0.31177645921707153, 0.10438086092472076, 0.10547912120819092, 0.011102515272796154, 0.1047997698187828, 0.011116682551801205, 0.01112221647053957, 0.011125696823000908, 0.10426412522792816, 0.10696906596422195, 0.10476691275835037, 0.011136578395962715, 0.10589873045682907, 0.01114074420183897, 0.01114197913557291, 0.10661643743515015, 0.10650187730789185, 0.011143074370920658, 0.011142917908728123, 0.011141672730445862, 0.10703923553228378, 0.10627571493387222, 0.011137420311570168, 0.011135757900774479, 0.1046338826417923, 0.01113131083548069, 0.011128476820886135, 0.011124878190457821, 0.10512436926364899, 0.011117187328636646, 0.011113222688436508, 0.01110840030014515, 0.10515185445547104, 0.011098399758338928, 0.011093397624790668, 0.01108788512647152, 0.1054258644580841, 0.01107668224722147, 0.10541696846485138, 0.011066662147641182, 0.011061780154705048, 0.011056188493967056, 0.3084332346916199, 0.011053584516048431, 0.011055787093937397, 0.011056595481932163, 0.011056325398385525, 0.9516440629959106, 0.10612783581018448, 0.011117029935121536, 0.011142918840050697, 0.011165020056068897, 0.011183950118720531, 0.10623268783092499, 0.011214959435164928, 0.1069713905453682, 0.10441010445356369, 0.10647600889205933, 0.10601121187210083, 0.10658301413059235, 0.01128195971250534, 0.011289756745100021, 0.10450607538223267, 0.1044091284275055, 0.011307928711175919, 0.10662443935871124, 0.10603130608797073, 0.10675015300512314, 0.011326394975185394, 0.011329717002809048, 0.3076828718185425, 0.011341812089085579, 0.011350031942129135, 0.011356144212186337, 0.011360540986061096, 0.011363597586750984, 0.011365200392901897, 0.011365647427737713, 0.011364871636033058, 0.011363034136593342, 0.10685241222381592, 0.01135878637433052, 0.10384457558393478, 0.011354522779583931, 0.011351789347827435, 0.011348101310431957, 0.10454607009887695, 0.1061646118760109, 0.011338573880493641, 0.011335472576320171, 0.01133188046514988, 0.011327161453664303, 0.011321987956762314, 0.10616841912269592, 0.011311638168990612, 0.10647212713956833, 0.01130269281566143, 0.011298046447336674, 0.011292923241853714, 0.10601849853992462, 0.011282497085630894, 0.011277386918663979, 0.011271608993411064, 0.011265416629612446, 0.01125888992100954, 0.011251888237893581, 0.011244435794651508, 0.01123672816902399, 0.01122881006449461, 0.011220465414226055, 0.30831634998321533, 0.01121300458908081, 0.10467705875635147, 0.011213014833629131, 0.011212505400180817, 0.10668615251779556, 0.10479975491762161, 0.011209523305296898, 0.01120822038501501, 0.01120613794773817, 0.10594327002763748, 0.011201245710253716, 0.10522136837244034, 0.011196183040738106, 0.011193599551916122, 0.10417172312736511, 0.011187450028955936, 0.011183916591107845, 0.10684365779161453, 0.011176892556250095, 0.10637146234512329, 0.10626954585313797, 0.011168308556079865, 0.01116547454148531, 0.01116220559924841, 0.011158114299178123, 0.011153298430144787, 0.10470014065504074, 0.011143802665174007, 0.30632108449935913, 0.011143053881824017, 0.011145626194775105, 0.011146764270961285, 0.10668598860502243, 0.011147851124405861, 0.01114693470299244, 0.011145662516355515, 0.011144074611365795, 0.10667531192302704, 0.011138648726046085, 0.10684099793434143, 0.0111338309943676, 0.011130903847515583, 0.011127371340990067, 0.10520362854003906, 0.01111991424113512, 0.01111610233783722, 0.011111781932413578, 0.011106680147349834, 0.011101305484771729, 0.01109502837061882, 0.011088685132563114, 0.011081969365477562, 0.10588520020246506, 0.011069178581237793, 0.10481055080890656, 0.10636862367391586, 0.1058385819196701, 0.011051477864384651, 0.011047782376408577, 0.011043641716241837, 0.30682989954948425, 0.011042515747249126, 0.011045219376683235, 0.011046376079320908, 0.10654866695404053, 0.011047310195863247, 0.10635814070701599, 0.10427749902009964, 0.10586259514093399, 0.1042279303073883, 0.011051161214709282, 0.10649266093969345, 0.10465648025274277, 0.01105485949665308, 0.011055359616875648, 0.10647609084844589, 0.011055137030780315, 0.10407056659460068, 0.011054068803787231, 0.011052963323891163, 0.011051035486161709, 0.01104799285531044, 0.011044679209589958, 0.308696985244751, 0.10522609949111938, 0.01104940939694643, 0.011052430607378483, 0.10627532750368118, 0.011056330986320972, 0.10619097948074341, 0.01105897594243288, 0.011059281416237354, 0.10722120851278305, 0.10594883561134338, 0.011059473268687725, 0.011058883741497993, 0.1050870269536972, 0.011056910268962383, 0.01105538010597229, 0.011053089052438736, 0.011049902066588402, 0.011045980267226696, 0.011042052879929543, 0.011036846786737442, 0.1046924963593483, 0.011027327738702297, 0.10620349645614624, 0.1064031645655632, 0.01101627852767706, 0.10420934855937958, 0.011010563001036644, 0.10620789974927902, 0.011004896834492683, 0.10681317746639252, 0.10671821236610413, 0.1044134870171547, 0.10591283440589905, 0.10679072886705399, 0.010998668149113655, 0.010998421348631382, 0.010997199453413486, 0.10440880805253983, 0.010993590578436852, 0.010991488583385944, 0.010988572612404823, 0.010985142551362514, 0.10674178600311279, 0.010978054255247116, 0.1061536893248558, 0.010971328243613243, 0.010967802256345749, 0.01096374075859785, 0.10737939924001694, 0.10491343587636948, 0.010952931828796864, 0.10428666323423386, 0.0109475152567029, 0.010944576933979988, 0.010940562933683395, 0.010936173610389233, 0.10420535504817963, 0.10645130276679993, 0.010925032198429108, 0.10620604455471039, 0.01091920305043459, 0.01091587170958519, 0.01091221533715725, 0.10473422706127167, 0.010904616676270962, 0.010900802910327911, 0.010896014049649239, 0.010891268029808998, 0.010885663330554962, 0.010879973880946636, 0.010873869061470032, 0.010867281816899776, 0.010860858485102654, 0.3145025074481964, 0.31103020906448364, 0.10455486178398132, 0.010873238556087017, 0.010880106128752232, 0.010885251685976982, 0.01088922843337059, 0.010891965590417385, 0.010893288068473339, 0.10677489638328552, 0.10592180490493774, 0.10675200819969177, 0.10491938143968582, 0.010899869725108147, 0.31105098128318787, 0.01090975385159254, 0.1068158969283104, 0.010923223569989204, 0.309058278799057, 0.10651551187038422, 0.01095217652618885, 0.010961681604385376, 0.3075792193412781, 0.010983442887663841, 0.010995818302035332, 0.011005516164004803, 0.011013579554855824, 0.01101978775113821, 0.10738640278577805, 0.011029373854398727, 0.10493388026952744, 0.01103631965816021, 0.10658227652311325, 0.10417703539133072, 0.011044413782656193, 0.10735379159450531, 0.10655225813388824, 0.011050938628613949, 0.10482626408338547, 0.01105411909520626, 0.011054894886910915, 0.011054483242332935, 0.011053289286792278, 0.10527799278497696, 0.011050043627619743, 0.01104798261076212, 0.011045265942811966, 0.10407765209674835, 0.01103932037949562, 0.011036348529160023, 0.011032736860215664, 0.011028353124856949, 0.10578300058841705, 0.011019921861588955, 0.3079696297645569, 0.01101924292743206, 0.011021660640835762, 0.011023037135601044, 0.10522588342428207, 0.01102373655885458, 0.011023401282727718, 0.011022423394024372, 0.011020584031939507, 0.10447228699922562, 0.011016080155968666, 0.011013565585017204, 0.011010506190359592, 0.011006539687514305, 0.10639306902885437, 0.10574202239513397, 0.010996794328093529, 0.30789270997047424, 0.10552416741847992, 0.10669833421707153, 0.011007733643054962, 0.10558554530143738, 0.1063355952501297, 0.011018435470759869, 0.10565031319856644, 0.10404779016971588, 0.011026855558156967, 0.011028826236724854, 0.10642123222351074, 0.10417848080396652, 0.10583009570837021, 0.011034662835299969, 0.10603950917720795, 0.01103702001273632, 0.106070376932621, 0.01103847287595272, 0.011038251221179962, 0.01103716716170311, 0.10629543662071228, 0.3102400600910187, 0.011040600948035717, 0.011045694351196289, 0.011049083434045315, 0.011051285080611706, 0.10716822743415833, 0.011054369620978832, 0.1038348525762558, 0.011055408976972103, 0.10643820464611053, 0.10448027402162552, 0.10597414523363113, 0.0110586266964674, 0.011059105396270752, 0.10425678640604019, 0.011058799922466278, 0.011057655327022076, 0.3071496784687042, 0.011061969213187695, 0.104564368724823, 0.011070643551647663, 0.10620145499706268, 0.011076798662543297, 0.10615109652280807, 0.011081274598836899, 0.10612272471189499, 0.10594218224287033, 0.011086507700383663, 0.10666665434837341, 0.011088819243013859, 0.011089150793850422, 0.011088619939982891, 0.011087360791862011, 0.011085238307714462, 0.01108233630657196, 0.011079412885010242, 0.011075379326939583, 0.011071030981838703, 0.011066374368965626, 0.011061295866966248, 0.011055619455873966, 0.01104988157749176, 0.011043977923691273, 0.10503587126731873, 0.011032365262508392, 0.011027037166059017, 0.011021211743354797, 0.011015178635716438, 0.011008678935468197, 0.30876654386520386, 0.10429809987545013, 0.10489176958799362, 0.10719650238752365, 0.011009027250111103, 0.30885744094848633, 0.10458196699619293, 0.011025767773389816, 0.10674925893545151, 0.0110379159450531, 0.011042475700378418, 0.011045760475099087, 0.1066967099905014, 0.10536982119083405, 0.011052817106246948, 0.011054675094783306, 0.01105481293052435, 0.011054727248847485, 0.31302323937416077, 0.011059560813009739, 0.011063762940466404, 0.31150615215301514, 0.10511071234941483, 0.10578197985887527, 0.31176918745040894, 0.011109067127108574, 0.10614112764596939, 0.011132899671792984, 0.10475756973028183, 0.011151755228638649, 0.011159103363752365, 0.10507887601852417, 0.011170579120516777, 0.10512996464967728, 0.011179205030202866, 0.011182447895407677, 0.011184565722942352, 0.011185604147613049, 0.10692578554153442, 0.10415442287921906, 0.011187005788087845, 0.011186974123120308, 0.1038724035024643, 0.1061524748802185, 0.30997905135154724, 0.10604112595319748, 0.3106963038444519, 0.10634489357471466, 0.30697742104530334, 0.011241554282605648, 0.10725603997707367, 0.10594218224287033, 0.10667978227138519, 0.01129472628235817, 0.10549235343933105, 0.011314007453620434, 0.10604588687419891, 0.011329245753586292, 0.1056843250989914, 0.01134068425744772, 0.01134492363780737, 0.011348078958690166, 0.10565313696861267, 0.011352005414664745, 0.011353357695043087, 0.011353543028235435, 0.011352899484336376, 0.107135109603405, 0.011350798420608044, 0.10454676300287247, 0.10607501864433289, 0.011347994208335876, 0.01134671550244093, 0.01134495995938778, 0.011342491954565048, 0.01133942324668169, 0.011335774324834347, 0.10616802424192429, 0.30632901191711426, 0.10584643483161926, 0.10661570727825165, 0.011339775286614895, 0.011342430487275124, 0.011343862861394882, 0.011344478465616703, 0.011344361118972301, 0.011343401856720448, 0.011341548524796963, 0.011338990181684494, 0.10593148320913315, 0.10735862702131271, 0.01133206207305193, 0.011330099776387215, 0.3104464113712311, 0.01133133377879858, 0.01133410818874836, 0.3046773672103882, 0.011343401856720448, 0.10608776658773422, 0.10650879889726639, 0.011361435055732727, 0.01136613916605711, 0.011369066312909126, 0.10688544064760208, 0.01137378066778183, 0.10598849505186081, 0.0113766985014081, 0.10593286156654358, 0.011378418654203415, 0.011378653347492218, 0.011377714574337006, 0.011376678012311459, 0.10549551993608475, 0.011373153887689114, 0.10669312626123428, 0.10651841014623642, 0.10602246224880219, 0.011368634179234505, 0.01136755384504795, 0.011365549638867378, 0.011363664641976357, 0.10461051762104034, 0.10415326803922653, 0.10403960198163986, 0.011355997994542122, 0.01135458704084158, 0.011352328583598137, 0.011349650099873543, 0.1040094643831253, 0.011343891732394695, 0.30742308497428894, 0.011344167403876781, 0.01134655810892582, 0.011347847059369087, 0.10637842118740082, 0.1058412566781044, 0.10617391765117645, 0.01135180052369833, 0.01135219819843769, 0.011351973749697208, 0.01135101169347763, 0.011349278502166271, 0.011346814222633839, 0.011344135738909245, 0.011340918019413948, 0.011336942203342915, 0.10695813596248627, 0.01132960058748722, 0.011325757950544357, 0.011321622878313065, 0.011317376047372818, 0.011312490329146385, 0.011307448148727417, 0.3074735999107361, 0.10595010966062546, 0.011304914020001888, 0.0113056106492877, 0.10538148134946823, 0.011305896565318108, 0.10604625940322876, 0.011305435560643673, 0.011304808780550957, 0.011303107254207134, 0.30881401896476746, 0.011305171065032482, 0.011307979933917522, 0.011310121044516563, 0.10703038424253464, 0.011312244459986687, 0.01131284236907959, 0.10389867424964905, 0.30882877111434937, 0.10674674808979034, 0.011324552819132805, 0.30671226978302, 0.011338979937136173, 0.01134690921753645, 0.011353584006428719, 0.011359003372490406, 0.011362851597368717, 0.011365598067641258, 0.01136744488030672, 0.011368309147655964, 0.011368360370397568, 0.011367623694241047, 0.01136629655957222, 0.011364271864295006, 0.10521569103002548, 0.011360092088580132, 0.10674794763326645, 0.011355801485478878, 0.011353574693202972, 0.011350789107382298, 0.011347503401339054, 0.3088393807411194, 0.011346576735377312, 0.01134831365197897, 0.011348559521138668, 0.10641349852085114, 0.01134885661303997, 0.011348314583301544, 0.011347130872309208, 0.10572762787342072, 0.10707557201385498, 0.011343571357429028, 0.011342153884470463, 0.10635000467300415, 0.011339065618813038, 0.011337144300341606, 0.10446745902299881, 0.10671894252300262, 0.011331744492053986, 0.011329953558743, 0.10388004034757614, 0.01132613979279995, 0.10405047982931137, 0.011322181671857834, 0.011320156045258045, 0.011317474767565727, 0.1062108501791954, 0.011312139220535755, 0.3093450665473938, 0.011312383227050304, 0.011314417235553265, 0.10630469769239426, 0.011317049153149128, 0.011317639611661434, 0.10497573763132095, 0.011317696422338486, 0.011317247524857521, 0.011316150426864624, 0.011314368806779385, 0.0113123944029212, 0.10633726418018341, 0.011307342909276485, 0.01130477711558342, 0.011301938444375992, 0.011298458091914654, 0.011294802650809288, 0.10651831328868866, 0.011287362314760685, 0.0112838139757514, 0.011279840022325516, 0.011275678873062134, 0.011271089315414429, 0.011266215704381466, 0.011261258274316788, 0.011256218887865543, 0.10691048949956894, 0.011246670968830585, 0.011242019012570381, 0.011236854828894138, 0.10693798214197159, 0.1044321209192276, 0.011224490590393543, 0.10534097254276276, 0.10449514538049698, 0.011215748265385628, 0.011213075369596481, 0.011210016906261444, 0.011206567287445068, 0.1050642654299736, 0.10558059811592102, 0.0111972251906991, 0.011194794438779354, 0.011191394180059433, 0.01118797529488802, 0.011183992959558964, 0.10557719320058823, 0.011176752857863903, 0.011173197068274021, 0.011169171892106533, 0.10636723041534424, 0.10716920346021652, 0.10384958237409592, 0.1041448637843132, 0.011155576445162296, 0.10647492110729218, 0.011152435094118118, 0.011150650680065155, 0.011148297227919102, 0.01114541757851839, 0.011142433620989323, 0.011138563975691795, 0.10557568818330765, 0.011131757870316505, 0.011128591373562813, 0.30893373489379883, 0.10577065497636795, 0.011129098013043404, 0.10640561580657959, 0.011132038198411465, 0.011132736690342426, 0.308698832988739, 0.011138166300952435, 0.31140953302383423, 0.011151343584060669, 0.10399199277162552, 0.10563918948173523, 0.10487186163663864, 0.01117884460836649, 0.31041592359542847, 0.10399507731199265, 0.01120292954146862, 0.011210696771740913, 0.1061486154794693, 0.011222723871469498, 0.011227626353502274, 0.011231102049350739, 0.011233747936785221, 0.011235415935516357, 0.10436030477285385, 0.011237597092986107, 0.011237907223403454, 0.30859437584877014, 0.011242675594985485, 0.011246397159993649, 0.011249134317040443, 0.011250543408095837, 0.1068580150604248, 0.011253108270466328, 0.1066100224852562, 0.10608293861150742, 0.01125556230545044, 0.011255965568125248, 0.10592661052942276, 0.011255807243287563, 0.01125522330403328, 0.1062086969614029, 0.011253365315496922, 0.10618815571069717, 0.011251367628574371, 0.011250052601099014, 0.011248446069657803, 0.011246192269027233, 0.011243659071624279, 0.1057301014661789, 0.011238369159400463, 0.10660677403211594, 0.3080415725708008, 0.011236768215894699, 0.011238953098654747, 0.011240228079259396, 0.011240723542869091, 0.011240668594837189, 0.011240111663937569, 0.3109811246395111, 0.10701113939285278, 0.10389039665460587, 0.10641711950302124, 0.10637155175209045, 0.011258360929787159, 0.3106149733066559, 0.011267743073403835, 0.011273616924881935, 0.011278511956334114, 0.01128231268376112, 0.011284940876066685, 0.011286690831184387, 0.011287784203886986, 0.011288074776530266, 0.011287862434983253, 0.1067010834813118, 0.011286532506346703, 0.10588192194700241, 0.01128493994474411, 0.01128378789871931, 0.011282113380730152, 0.01128009520471096, 0.011277681216597557, 0.011274945922195911, 0.1070331558585167, 0.10463327914476395, 0.011267449706792831, 0.011265239678323269, 0.10529052466154099, 0.011260675266385078, 0.011258256621658802, 0.011255495250225067, 0.011252443306148052, 0.011248990893363953, 0.10413112491369247, 0.011242605745792389, 0.011239409446716309, 0.01123567670583725, 0.30980363488197327, 0.011233562603592873, 0.011234321631491184, 0.011234176345169544, 0.011233526282012463, 0.01123256329447031, 0.011230982840061188, 0.01122900377959013, 0.011226407252252102, 0.01122384611517191, 0.011220867745578289, 0.10669282078742981, 0.01121499016880989, 0.011211945675313473, 0.10655363649129868, 0.011206262744963169, 0.011203544214367867, 0.01120026409626007, 0.10403081774711609, 0.10643153637647629, 0.1040915846824646, 0.10423517227172852, 0.10435672849416733, 0.30730658769607544, 0.011192744597792625, 0.011195927858352661, 0.10620668530464172, 0.01120015699416399, 0.011201693676412106, 0.011202549561858177, 0.01120278425514698, 0.10630620270967484, 0.011202054098248482, 0.011201685294508934, 0.011200487613677979, 0.01119898445904255, 0.011196796782314777, 0.011194288730621338, 0.31072545051574707, 0.10664382576942444, 0.10412205010652542, 0.10549528896808624, 0.011200927197933197, 0.011202629655599594, 0.01120385155081749, 0.309982568025589, 0.011208334937691689, 0.011212006211280823, 0.011214767582714558, 0.011216509155929089, 0.011218132451176643, 0.10624769330024719, 0.10627800226211548, 0.10442131757736206, 0.011221345514059067, 0.01122205425053835, 0.1055607795715332, 0.1058964729309082, 0.011222776025533676, 0.011222978122532368, 0.10708724707365036, 0.01122200395911932, 0.10399698466062546, 0.011221080087125301, 0.011220253072679043, 0.10389241576194763, 0.10493635386228561, 0.10441306233406067, 0.10603590309619904, 0.011217723600566387, 0.3094612956047058, 0.01122143305838108, 0.011224690824747086, 0.10433422774076462, 0.01122906431555748, 0.10399248450994492, 0.011232374235987663, 0.10398738831281662, 0.3052234947681427, 0.011239820159971714, 0.01124420203268528, 0.011247380636632442, 0.10631420463323593, 0.011252228170633316, 0.011254224926233292, 0.011254997923970222, 0.011255474761128426, 0.01125513669103384, 0.011254423297941685, 0.3094678223133087, 0.011256619356572628, 0.011258871294558048, 0.011260420083999634, 0.011261281557381153, 0.011261563748121262, 0.10635343194007874, 0.011261368170380592, 0.01126087922602892, 0.10410436987876892, 0.1056116372346878, 0.1055527925491333, 0.01125944685190916, 0.011258996091783047, 0.01125812903046608, 0.01125682145357132, 0.011255182325839996, 0.011253144592046738, 0.011250754818320274, 0.011248189955949783, 0.011245590634644032, 0.011242321692407131, 0.011239068582654, 0.10694354772567749, 0.01123284362256527, 0.01122955046594143, 0.10607971996068954, 0.011224099434912205, 0.011221312917768955, 0.01121800858527422, 0.011214930564165115, 0.1066233217716217, 0.10623801499605179, 0.01120652724057436, 0.10618950426578522, 0.011202198453247547, 0.01120006199926138, 0.011197387240827084, 0.011194669641554356, 0.1067681759595871, 0.011189468204975128, 0.01118689589202404, 0.1053997352719307, 0.10539478063583374, 0.3059903085231781, 0.011182235553860664, 0.011184128001332283, 0.01118493266403675, 0.10673654079437256, 0.011185874231159687, 0.011186138726770878, 0.011185488663613796, 0.011184797622263432, 0.011183484457433224, 0.011181932874023914, 0.011179895140230656, 0.0111779710277915, 0.3058300018310547, 0.011177039705216885, 0.011178070679306984, 0.011178468354046345, 0.10490871220827103, 0.011178400367498398, 0.011177992448210716, 0.011177390813827515, 0.011176449246704578, 0.011174831539392471, 0.10453896224498749, 0.011171587742865086, 0.011169976554811, 0.011168018914759159, 0.01116590853780508, 0.10454027354717255, 0.011161431670188904, 0.0111592598259449, 0.10400456190109253, 0.011154604144394398, 0.10556809604167938, 0.10562698543071747, 0.011149642989039421, 0.011147686280310154, 0.10502740740776062, 0.011144498363137245, 0.1042397990822792, 0.011141611263155937, 0.011139831505715847, 0.011138185858726501, 0.1043872982263565, 0.011134147644042969, 0.3104732632637024, 0.10711777955293655, 0.011135999113321304, 0.1064370647072792, 0.011138882488012314, 0.31099408864974976, 0.10616524517536163, 0.011148528195917606, 0.01115178782492876, 0.10661033540964127, 0.01115697156637907, 0.011158918030560017, 0.011160152032971382, 0.1061868816614151, 0.10580701380968094, 0.011162634007632732, 0.011163100600242615, 0.011163176968693733, 0.01116300467401743, 0.01116193737834692, 0.011160735040903091, 0.10575152933597565, 0.011158009059727192, 0.1038997620344162, 0.31100502610206604, 0.1065099760890007, 0.10630659759044647, 0.011163583025336266, 0.10661252588033676, 0.0111673753708601, 0.011168930679559708, 0.011169708333909512, 0.10663686692714691, 0.10484493523836136, 0.1063259020447731, 0.10498466342687607, 0.011173090897500515, 0.011173500679433346, 0.10609119385480881, 0.10593778640031815, 0.10647990554571152, 0.1072763055562973, 0.011176023632287979, 0.011176246218383312, 0.011176207102835178, 0.011175710707902908, 0.011174829676747322, 0.011173808015882969, 0.011172092519700527, 0.10527989268302917, 0.10735967755317688, 0.1049945130944252, 0.011167552322149277, 0.01116684265434742, 0.011165677569806576, 0.011163847520947456, 0.01116169523447752, 0.10628309845924377, 0.011158115230500698, 0.3082633316516876, 0.3092310130596161, 0.011163069866597652, 0.10451586544513702, 0.10446547716856003, 0.01117391511797905, 0.011176984757184982, 0.011179151013493538, 0.011180799454450607, 0.011181537061929703, 0.011181836016476154, 0.011182280257344246, 0.011181592009961605, 0.10708308964967728, 0.011180234141647816, 0.011179553344845772, 0.011178674176335335, 0.011177226901054382, 0.011175457388162613, 0.011173564940690994, 0.10394860804080963, 0.011170015670359135, 0.01116802915930748, 0.10666660219430923, 0.01116427406668663, 0.01116249617189169, 0.011160521768033504, 0.011158190667629242, 0.10582882165908813, 0.10514217615127563, 0.011152270250022411, 0.01115062553435564, 0.10448969155550003, 0.011147050186991692, 0.10667441785335541, 0.011143841780722141, 0.10477955639362335, 0.01114124245941639, 0.30695611238479614, 0.10447509586811066, 0.011143279261887074, 0.011144597083330154, 0.01114549022167921, 0.0111455749720335, 0.01114561501890421, 0.011145026423037052, 0.011144268326461315, 0.011142854578793049, 0.01114160381257534, 0.011140198446810246, 0.10408875346183777, 0.011136838234961033, 0.10645634680986404, 0.5998644232749939, 0.01114009227603674, 0.01114493329077959, 0.10683564096689224, 0.10619909316301346, 0.10391879081726074, 0.10400023311376572, 0.011163672432303429, 0.011166404001414776, 0.011168538592755795, 0.3084075450897217, 0.011174275539815426, 0.10687720030546188, 0.011181043460965157, 0.10593582689762115, 0.10399853438138962, 0.10659802705049515, 0.30661600828170776, 0.011195998638868332, 0.10593064874410629, 0.011204038746654987, 0.01120713073760271, 0.1072554960846901, 0.011212187819182873, 0.011213858611881733, 0.10572962462902069, 0.011216655373573303, 0.011217684485018253, 0.011218095198273659, 0.011218075640499592, 0.011217831633985043, 0.10434551537036896, 0.10631146281957626, 0.103949174284935, 0.6029444336891174, 0.01122360210865736, 0.10403843969106674, 0.01123414933681488, 0.01123807393014431, 0.10687743127346039, 0.011245165951550007, 0.10684282332658768, 0.011250492185354233, 0.011252373456954956, 0.011253894306719303, 0.011255006305873394, 0.1067848652601242, 0.011256004683673382, 0.011256558820605278, 0.011256293393671513, 0.011256158351898193, 0.011255541816353798, 0.011254604905843735, 0.10651916265487671, 0.10628502070903778, 0.01125217042863369, 0.10579651594161987, 0.30653828382492065, 0.01125277392566204, 0.10404615849256516, 0.011255825869739056, 0.011256967671215534, 0.10713047534227371, 0.011258179321885109, 0.10672735422849655, 0.011259190738201141, 0.10579057037830353, 0.10619586706161499, 0.011260109953582287, 0.10581251978874207, 0.10640832036733627, 0.011261184699833393, 0.011261045932769775, 0.011260705068707466, 0.31022340059280396, 0.10593388974666595, 0.011264091357588768, 0.011265728622674942, 0.10618018358945847, 0.10673535615205765, 0.011268794536590576, 0.01126945111900568, 0.011269578710198402, 0.10598507523536682, 0.1059088408946991, 0.011270308867096901, 0.011270368471741676, 0.01127003412693739, 0.011269605718553066, 0.011268626898527145, 0.3100253641605377, 0.10386161506175995, 0.011270511895418167, 0.011271566152572632, 0.10600984841585159, 0.011272964999079704, 0.1065690889954567, 0.01127372495830059, 0.011273988522589207, 0.10603733360767365, 0.10552474111318588, 0.011274312622845173, 0.10411188006401062, 0.30888232588768005, 0.011276155710220337, 0.011278053745627403, 0.10488729178905487, 0.011280972510576248, 0.011281627230346203, 0.011282223276793957, 0.10673610866069794, 0.011282741092145443, 0.10548808425664902, 0.011283121071755886, 0.0112829077988863, 0.10604690760374069, 0.3068161904811859, 0.3055378794670105, 0.10554154962301254, 0.10665544867515564, 0.01129547506570816, 0.10611335933208466, 0.3074879050254822, 0.011305353604257107, 0.0113091841340065, 0.011312510818243027, 0.01131498534232378, 0.01131716649979353, 0.011318473145365715, 0.10640780627727509, 0.011320792138576508, 0.10648389905691147, 0.10486233234405518, 0.011323162354528904, 0.011324084363877773, 0.01132414024323225, 0.011324092745780945, 0.011323820799589157, 0.011323553510010242, 0.011322646401822567, 0.10635048896074295, 0.011321253143250942, 0.01132031250745058, 0.01131934579461813, 0.10610596090555191, 0.1042519137263298, 0.10558242350816727, 0.105528824031353, 0.10669135302305222, 0.01131595578044653, 0.011315613053739071, 0.011315016075968742, 0.011314301751554012, 0.011313291266560555, 0.011312290094792843, 0.10396434366703033, 0.01131013035774231, 0.01130931917577982, 0.3076876997947693, 0.011308799497783184, 0.3096127510070801, 0.011311480775475502, 0.10620050877332687, 0.01131520140916109, 0.3089940547943115, 0.011319785378873348, 0.011322233825922012, 0.011324591934680939, 0.011325844563543797, 0.01132713072001934, 0.1038585752248764, 0.011328884400427341, 0.01132932584732771, 0.011329707689583302, 0.011329910717904568, 0.011329655535519123, 0.011329206638038158, 0.011328640393912792, 0.011327758431434631, 0.10656854510307312, 0.011326237581670284, 0.011325354687869549, 0.011324593797326088, 0.011323327198624611, 0.011322231031954288, 0.011320812627673149, 0.011319667100906372, 0.011318120174109936, 0.011316492222249508, 0.3074510395526886, 0.011315194889903069, 0.01131537463515997, 0.3089083731174469, 0.011316698044538498, 0.011317835189402103, 0.011318785138428211, 0.5953589677810669, 0.011323709972202778, 0.10632037371397018, 0.011330803856253624, 0.10624334961175919, 0.10500019788742065, 0.011338851414620876, 0.10592738538980484, 0.011342879384756088, 0.011344569735229015, 0.011345929466187954, 0.3081056773662567, 0.011348914355039597, 0.10604557394981384, 0.011352662928402424, 0.011354032903909683, 0.10712757706642151, 0.10535931587219238, 0.10499519854784012, 0.011358520947396755, 0.011359196156263351, 0.011359715834259987, 0.10545877367258072, 0.10549786686897278, 0.011360837146639824, 0.011360942386090755, 0.30627724528312683, 0.011362477205693722, 0.011363644152879715, 0.011364462785422802, 0.011364862322807312, 0.10399464517831802, 0.10560215264558792, 0.011366268619894981, 0.011366376653313637, 0.10519339144229889, 0.011366494931280613, 0.3081214427947998, 0.011367560364305973, 0.10513816028833389, 0.01136929914355278, 0.01137035246938467, 0.10601981729269028, 0.5989516973495483, 0.10593759268522263, 0.1064407154917717, 0.305502325296402, 0.011385596357285976, 0.31044286489486694, 0.10383370518684387, 0.011397546157240868, 0.10647742450237274, 0.011404206044971943, 0.10620728135108948, 0.011409515514969826, 0.011411524377763271, 0.011413298547267914, 0.011414689011871815, 0.011415788903832436, 0.011416557244956493, 0.011417267844080925, 0.011417452245950699, 0.011417720466852188, 0.011417652480304241, 0.011417425237596035, 0.011417248286306858, 0.011416531167924404, 0.011416194960474968, 0.01141567062586546, 0.011415061540901661, 0.10395568609237671, 0.10611248016357422, 0.10604798793792725, 0.011412687599658966, 0.011412344872951508, 0.011411682702600956, 0.10407347977161407, 0.3074468672275543, 0.011411170475184917, 0.011411771178245544, 0.01141193974763155, 0.3098025619983673, 0.10538553446531296, 0.10554315894842148, 0.011415740475058556, 0.10677004605531693, 0.011417515575885773, 0.01141811441630125, 0.011418478563427925, 0.011418565176427364, 0.10582446306943893, 0.10384828597307205, 0.1047501489520073, 0.011419345624744892, 0.10609246790409088, 0.10504668205976486, 0.011419994756579399, 0.3042377233505249, 0.011420823633670807, 0.10392992943525314, 0.01142245065420866, 0.1053672805428505, 0.01142391562461853, 0.011424165219068527, 0.011424343101680279, 0.1063358411192894, 0.011424629017710686, 0.011425042524933815, 0.10433546453714371, 0.011424537748098373, 0.011424431577324867, 0.011424184776842594, 0.0114238690584898, 0.1056009829044342, 0.10663239657878876, 0.10618072003126144, 0.011422951705753803, 0.10654106736183167, 0.1043168306350708, 0.011422201059758663, 0.011422124691307545, 0.011421944946050644, 0.30931615829467773, 0.011422127485275269, 0.011422608979046345, 0.011422933079302311, 0.011422794312238693, 0.10512848943471909, 0.01142297312617302, 0.011422931216657162, 0.011422617360949516, 0.011422587558627129, 0.30851221084594727, 0.011422557756304741, 0.011422943323850632, 0.10661216825246811, 0.01142344530671835, 0.01142365112900734, 0.01142365112900734, 0.10586317628622055, 0.011423563584685326, 0.011423434130847454, 0.011423083022236824, 0.1051950454711914, 0.3086579442024231, 0.10377271473407745, 0.011423997581005096, 0.10607662796974182, 0.011424706317484379, 0.011425080709159374, 0.011425070464611053, 0.01142528560012579, 0.10617321729660034, 0.011425158008933067, 0.1046900749206543, 0.011425170116126537, 0.3050987422466278, 0.011425438337028027, 0.011426014825701714, 0.10616037249565125, 0.011426636017858982, 0.011426718905568123, 0.011427030898630619, 0.011426925659179688, 0.10586830228567123, 0.011426965706050396, 0.011426776647567749, 0.011426365002989769, 0.011426311917603016, 0.10458534955978394, 0.10481051355600357, 0.011425573378801346, 0.01142539456486702, 0.011425217613577843, 0.011424765922129154, 0.011424600146710873, 0.011423997581005096, 0.011423544026911259, 0.10645687580108643, 0.011423122137784958, 0.1050935685634613, 0.011422263458371162, 0.10605686902999878, 0.01142151653766632, 0.011421196162700653, 0.011420892551541328, 0.1064167395234108, 0.011420054361224174, 0.011420299299061298, 0.01141979731619358, 0.10477717965841293, 0.10449609160423279, 0.011418606154620647, 0.011418291367590427, 0.011417957954108715, 0.10573309659957886, 0.10618139058351517, 0.01141747273504734, 0.011417128145694733, 0.10618061572313309, 0.011416596360504627, 0.1056850478053093, 0.10396236926317215, 0.011415986344218254, 0.011415846645832062, 0.0114156948402524, 0.011415563523769379, 0.011415071785449982, 0.011414764449000359, 0.01141432300209999, 0.011414348147809505, 0.011413916945457458, 0.01141336653381586, 0.011413395404815674, 0.10667052119970322, 0.10653754323720932, 0.011412473395466805, 0.011412076652050018, 0.011411821469664574, 0.01141156442463398, 0.011411244980990887, 0.011411011219024658, 0.011410708539187908, 0.011410247534513474, 0.01141003705561161, 0.011409616097807884, 0.10408749431371689, 0.011408633552491665, 0.30532652139663696, 0.011408578604459763, 0.01140882819890976, 0.10553717613220215, 0.10461001098155975, 0.011408794671297073, 0.01140876766294241, 0.011408893391489983, 0.011408659629523754, 0.10576260089874268, 0.01140835415571928, 0.10541535913944244, 0.011408424004912376, 0.10391286760568619, 0.1063857153058052, 0.01140823494642973, 0.011408097110688686, 0.011408166028559208, 0.011408020742237568, 0.011407991871237755, 0.10713610053062439, 0.011407715268433094, 0.10391917079687119, 0.011407499201595783, 0.011407427489757538, 0.01140733901411295, 0.011407134123146534, 0.011407102458178997, 0.01140687894076109, 0.011406912468373775, 0.011406752280890942, 0.011406653560698032, 0.10388067364692688, 0.011406641453504562, 0.10405298322439194, 0.011406452395021915, 0.011406385339796543, 0.011406433768570423, 0.10599040240049362, 0.01140607986599207, 0.10634897649288177, 0.011406047269701958, 0.10460783541202545, 0.10480508953332901, 0.10437899082899094, 0.011405786499381065, 0.011405960656702518]\n",
            "Val loss 0.055588318074920345\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}