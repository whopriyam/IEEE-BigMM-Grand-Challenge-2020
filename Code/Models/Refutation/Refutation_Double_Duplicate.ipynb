{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refutation_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b560525d144f4af1976263c330704f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6bb4a3ca3866452f80e4a49496cf591c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64fe4cd72c114f2195b8ea009c0707d1",
              "IPY_MODEL_c680b8c5d4ec472bb5c2b303fe33fac1"
            ]
          }
        },
        "6bb4a3ca3866452f80e4a49496cf591c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64fe4cd72c114f2195b8ea009c0707d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4bbd77adbdb46fe894ec140c9a1e288",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1663,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1663,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a670f4a107214648bca06baeb3244a5a"
          }
        },
        "c680b8c5d4ec472bb5c2b303fe33fac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f857f673676f4a66935dcd48efcc01e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1663/1663 [15:59&lt;00:00,  1.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_650dcbb0dcf441f6bc6503fbfb595b8c"
          }
        },
        "c4bbd77adbdb46fe894ec140c9a1e288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a670f4a107214648bca06baeb3244a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f857f673676f4a66935dcd48efcc01e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "650dcbb0dcf441f6bc6503fbfb595b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4fb1adcd7b24575b8486c882c83c061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ef23f3dc17843d6ab311b54eaa2bc3c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a05df238bc248448fe497c7a08bc531",
              "IPY_MODEL_53dd8a64037a4f838e49764ec8e5ca04"
            ]
          }
        },
        "8ef23f3dc17843d6ab311b54eaa2bc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a05df238bc248448fe497c7a08bc531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_217b813ff5384516b99b2d216ef3aafa",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1663,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1663,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc8b7b42353e41d19dbb00dfd3078d0d"
          }
        },
        "53dd8a64037a4f838e49764ec8e5ca04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a01cfb883a084e78a9b355676280391e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1663/1663 [15:40&lt;00:00,  1.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fa62a1a8a854056bd32d00fb14cc11f"
          }
        },
        "217b813ff5384516b99b2d216ef3aafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc8b7b42353e41d19dbb00dfd3078d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a01cfb883a084e78a9b355676280391e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fa62a1a8a854056bd32d00fb14cc11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f3b8ad8b108466b8b126417e8fe7639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d1c8b4b5e2d54db29926b82c30781ce7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8618a428d7534e5bbc995a15e72c8228",
              "IPY_MODEL_93e8d07a3a3845dcadf0777d9b996bb8"
            ]
          }
        },
        "d1c8b4b5e2d54db29926b82c30781ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8618a428d7534e5bbc995a15e72c8228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f45601d58aba43e39ff76039221905be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1663,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1663,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63f9a2717d3e4358bd9c5f94ee7662f4"
          }
        },
        "93e8d07a3a3845dcadf0777d9b996bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8887bff18e17476284d1bf0ad33d3a9f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1663/1663 [15:40&lt;00:00,  1.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4b754f2284f4d059f14fea929889b96"
          }
        },
        "f45601d58aba43e39ff76039221905be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63f9a2717d3e4358bd9c5f94ee7662f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8887bff18e17476284d1bf0ad33d3a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4b754f2284f4d059f14fea929889b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "282c85a6-67f8-42e2-9960-2c189aa587c3"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 44.67 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0217f4d3-7cb8-433d-c7b5-2d003993132b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9249b9d4-8c1c-48f1-a9c7-bc1cb7672638"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "48661442-16b2-4cc0-94cc-2a77c0a6f48a"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fff0eded-d554-408e-9b45-fef43a1c2304"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "4a33fad4-639d-4be1-edbb-914f3ea9f52a"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "619f6a3a-d3bd-43f4-edf8-1bec99216645"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0cd1c791-2d08-49dc-e64d-7b22b86a1a5b"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "1bb00d88-3b11-4271-f761-cf7798ab4874"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e1fc6a8f-28cc-4ed8-d6db-82d2547400c4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      How India's #MeToo campaign different from US  \n",
              "1    What's next for #MeToo after Kavanaugh's confi...\n",
              "2    Fans lose calm as TV personality Mani makes fu...\n",
              "3    Karan johar so painful these days.... Every ac...\n",
              "4    ICYMI - PERRY: The secret lives of sleaze and ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79a78d35-36d0-4ad7-827f-198ec02f7c1d"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 34551873.12B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d230853b-532f-4bee-a02d-ecd2f6e9d6c5"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 913805.24B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c32f59ae-d6c0-4a27-953f-c62eccc97082"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "101f4d97-cc8d-45c4-f10a-e6719e7e46c6"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acecf79e-7c5c-4249-bafa-982772ce7cc6"
      },
      "source": [
        "col_name = \"Refutation\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796,
          "referenced_widgets": [
            "b560525d144f4af1976263c330704f8a",
            "6bb4a3ca3866452f80e4a49496cf591c",
            "64fe4cd72c114f2195b8ea009c0707d1",
            "c680b8c5d4ec472bb5c2b303fe33fac1",
            "c4bbd77adbdb46fe894ec140c9a1e288",
            "a670f4a107214648bca06baeb3244a5a",
            "f857f673676f4a66935dcd48efcc01e5",
            "650dcbb0dcf441f6bc6503fbfb595b8c",
            "f4fb1adcd7b24575b8486c882c83c061",
            "8ef23f3dc17843d6ab311b54eaa2bc3c",
            "7a05df238bc248448fe497c7a08bc531",
            "53dd8a64037a4f838e49764ec8e5ca04",
            "217b813ff5384516b99b2d216ef3aafa",
            "bc8b7b42353e41d19dbb00dfd3078d0d",
            "a01cfb883a084e78a9b355676280391e",
            "7fa62a1a8a854056bd32d00fb14cc11f",
            "4f3b8ad8b108466b8b126417e8fe7639",
            "d1c8b4b5e2d54db29926b82c30781ce7",
            "8618a428d7534e5bbc995a15e72c8228",
            "93e8d07a3a3845dcadf0777d9b996bb8",
            "f45601d58aba43e39ff76039221905be",
            "63f9a2717d3e4358bd9c5f94ee7662f4",
            "8887bff18e17476284d1bf0ad33d3a9f",
            "c4b754f2284f4d059f14fea929889b96"
          ]
        },
        "outputId": "75b91337-7d10-4b3c-9675-285aa23479c3"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6652\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b560525d144f4af1976263c330704f8a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1663.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0374\n",
            "Train Losses : [0.018022222444415092, 0.10701917111873627, 0.011498100124299526, 0.008361482061445713, 0.005474957171827555, 0.0032789018005132675, 0.0018278906354680657, 0.0009618890471756458, 0.0004881558706983924, 0.00024207006208598614, 0.00011980756971752271, 6.023710739100352e-05, 0.2963009476661682, 5.6427968956995755e-05, 9.405624587088823e-05, 0.00014487346925307065, 0.20822866261005402, 0.23234304785728455, 0.0007509778370149434, 0.1608978807926178, 0.0022223275154829025, 0.10527804493904114, 0.11097221821546555, 0.007963174022734165, 0.010451353155076504, 0.11036171019077301, 0.015727443620562553, 0.017958523705601692, 0.01916874758899212, 0.01974979043006897, 0.018941259011626244, 0.017309065908193588, 0.015106354840099812, 0.012415901757776737, 0.10654652863740921, 0.008094266057014465, 0.1171458512544632, 0.005752047058194876, 0.0050068083219230175, 0.0041888440027832985, 0.003587214043363929, 0.13404954969882965, 0.0028615156188607216, 0.1416109949350357, 0.003204199718311429, 0.003531350987032056, 0.003793017938733101, 0.004009154625236988, 0.004003250040113926, 0.0039906492456793785, 0.003812838112935424, 0.003539210883900523, 0.003219825215637684, 0.0029040358494967222, 0.002545728348195553, 0.002224989701062441, 0.0018832801142707467, 0.1443415880203247, 0.0017755974549800158, 0.0019301344873383641, 0.0020387417171150446, 0.00214702938683331, 0.0021674707531929016, 0.177395761013031, 0.002601957181468606, 0.0029223209712654352, 0.003291116328909993, 0.003492335556074977, 0.0036559286527335644, 0.003737544408068061, 0.0037750890478491783, 0.11351938545703888, 0.003953983541578054, 0.004130173474550247, 0.00427228445187211, 0.0042436979711055756, 0.11416883021593094, 0.004428878426551819, 0.004579687025398016, 0.004664586391299963, 0.00459976214915514, 0.004492287524044514, 0.13054825365543365, 0.12662182748317719, 0.004972207359969616, 0.1254381686449051, 0.006001245696097612, 0.006551723461598158, 0.11249973624944687, 0.0075445412658154964, 0.00798247940838337, 0.12288077920675278, 0.008671904914081097, 0.008880236186087132, 0.008834014646708965, 0.11206308007240295, 0.11294232308864594, 0.008831620216369629, 0.11225207895040512, 0.009142118506133556, 0.009152392856776714, 0.008986097760498524, 0.008551003411412239, 0.10475988686084747, 0.09772660583257675, 0.007816909812390804, 0.007624107412993908, 0.007356912828981876, 0.006921095307916403, 0.11039058864116669, 0.006243100389838219, 0.005981205031275749, 0.0056422799825668335, 0.005218351259827614, 0.0048306104727089405, 0.004342637490481138, 0.0039104740135371685, 0.003532577073201537, 0.0030936154071241617, 0.002741381758823991, 0.0024495760444551706, 0.0021356772631406784, 0.1373225450515747, 0.0019141152733936906, 0.1338908076286316, 0.0021552187390625477, 0.00238135177642107, 0.0025651929900050163, 0.13566462695598602, 0.003111841157078743, 0.0034735710360109806, 0.0037934009451419115, 0.00403246097266674, 0.004185248166322708, 0.004271283745765686, 0.004292038735002279, 0.004261056426912546, 0.004157799296081066, 0.004014488309621811, 0.003837721189484, 0.13532188534736633, 0.003702886402606964, 0.11472442746162415, 0.003995263017714024, 0.0041939797811210155, 0.004322534892708063, 0.0043749939650297165, 0.004358534701168537, 0.004282696172595024, 0.004156502895057201, 0.00398891931399703, 0.0037899361923336983, 0.11675865948200226, 0.0036157870199531317, 0.0036127218045294285, 0.00356736546382308, 0.0034750986378639936, 0.003362638410180807, 0.003213572781533003, 0.003053391119465232, 0.002877136692404747, 0.13178591430187225, 0.002776039531454444, 0.002820394467562437, 0.13198421895503998, 0.0030727782286703587, 0.0032812943682074547, 0.003426749724894762, 0.0035126821603626013, 0.003562123281881213, 0.4533366560935974, 0.11629576981067657, 0.00639607198536396, 0.00806344673037529, 0.10965261608362198, 0.10309813171625137, 0.013473947532474995, 0.015142405405640602, 0.10472135245800018, 0.017692547291517258, 0.018494021147489548, 0.10676557570695877, 0.1039508655667305, 0.019350815564393997, 0.019138062372803688, 0.0185505710542202, 0.01765696331858635, 0.0165182426571846, 0.10085895657539368, 0.014167867600917816, 0.10646796226501465, 0.012106744572520256, 0.011127353645861149, 0.010117958299815655, 0.009115626104176044, 0.34186479449272156, 0.008698862977325916, 0.10614733397960663, 0.009605743922293186, 0.009938248433172703, 0.010093276388943195, 0.10085396468639374, 0.010212195105850697, 0.010165502317249775, 0.009999268688261509, 0.09644126147031784, 0.009546874091029167, 0.11353430151939392, 0.009173212572932243, 0.008972624316811562, 0.008668823167681694, 0.008369234390556812, 0.007910721004009247, 0.10469868034124374, 0.0071217031218111515, 0.006784726399928331, 0.006468275096267462, 0.006054349709302187, 0.09312574565410614, 0.005524601321667433, 0.005316074471920729, 0.005014573223888874, 0.004741605371236801, 0.11350636184215546, 0.004451011773198843, 0.00430686678737402, 0.004221765790134668, 0.0040241568349301815, 0.0038500677328556776, 0.003642957890406251, 0.003446607617661357, 0.0032727557700127363, 0.0030925769824534655, 0.002924487227573991, 0.002752937376499176, 0.002594900084659457, 0.00244187586940825, 0.002295807236805558, 0.002154915826395154, 0.0020308915991336107, 0.0019107430707663298, 0.12462460994720459, 0.1397191882133484, 0.001962206559255719, 0.0020913982298225164, 0.0022067399695515633, 0.9839974641799927, 0.12381736934185028, 0.12181348353624344, 0.1056327372789383, 0.0087742879986763, 0.10367409139871597, 0.013409294188022614, 0.10582934319972992, 0.27888983488082886, 0.10810255259275436, 0.024174602702260017, 0.026830818504095078, 0.10728812962770462, 0.03096819669008255, 0.03251924738287926, 0.09428674727678299, 0.034036993980407715, 0.034002918750047684, 0.033635638654232025, 0.0329211950302124, 0.031808119267225266, 0.11412359029054642, 0.02913869544863701, 0.027559613808989525, 0.10312000662088394, 0.024252329021692276, 0.02269444614648819, 0.021001851186156273, 0.019298264756798744, 0.01757623255252838, 0.01595848612487316, 0.11520089954137802, 0.01301276870071888, 0.10451860725879669, 0.010876079089939594, 0.009966169483959675, 0.009100046940147877, 0.00828457809984684, 0.0075243148021399975, 0.006820969749242067, 0.006174728274345398, 0.00558406300842762, 0.3963072597980499, 0.005343187600374222, 0.005584635306149721, 0.00577192148193717, 0.005906961392611265, 0.11458992958068848, 0.006159810349345207, 0.006273121573030949, 0.006334817036986351, 0.006348866969347, 0.11021221429109573, 0.006380177568644285, 0.1104283332824707, 0.006490294821560383, 0.006535759661346674, 0.006533718667924404, 0.006489042192697525, 0.006406520958989859, 0.006291808094829321, 0.11179901659488678, 0.11220822483301163, 0.006161650642752647, 0.006168943829834461, 0.006136259064078331, 0.0060682096518576145, 0.005969455000013113, 0.005844780709594488, 0.005698688328266144, 0.005535457283258438, 0.0053590103052556515, 0.3837759792804718, 0.005629195366054773, 0.0060315970331430435, 0.006374712567776442, 0.006655863486230373, 0.006874165032058954, 0.007031118497252464, 0.00712927058339119, 0.007172603625804186, 0.10884397476911545, 0.10926690697669983, 0.0073752934113144875, 0.007454309146851301, 0.007477219682186842, 0.007449139375239611, 0.0073755704797804356, 0.10869770497083664, 0.007236541248857975, 0.0071671707555651665, 0.10881009697914124, 0.007039752788841724, 0.006977468263357878, 0.00687821302562952, 0.006747260224074125, 0.0065896534360945225, 0.11280316859483719, 0.00633202726021409, 0.11037205159664154, 0.006207596510648727, 0.006155714392662048, 0.006073398049920797, 0.005964748095721006, 0.005834144074469805, 0.0056855808943510056, 0.1118803322315216, 0.005459924694150686, 0.005373804364353418, 0.005268155597150326, 0.11385563015937805, 0.005120519082993269, 0.005070193205028772, 0.38780471682548523, 0.005510909017175436, 0.005974487867206335, 0.0063820998184382915, 0.1126595064997673, 0.007129490375518799, 0.007460837252438068, 0.00772062549367547, 0.007911755703389645, 0.11145691573619843, 0.008217218331992626, 0.008329893462359905, 0.00837780348956585, 0.008365966379642487, 0.008301683701574802, 0.008189983665943146, 0.00803929939866066, 0.007854403927922249, 0.1131177693605423, 0.007525361608713865, 0.34552058577537537, 0.007852951064705849, 0.008252350613474846, 0.10919509083032608, 0.008930731564760208, 0.009202547371387482, 0.009394001215696335, 0.00950339250266552, 0.635136604309082, 0.010991603136062622, 0.012354154139757156, 0.013587549328804016, 0.014666887931525707, 0.015574322082102299, 0.1042056456208229, 0.016988124698400497, 0.017487505450844765, 0.01780836656689644, 0.017961030825972557, 0.017959732562303543, 0.01782025396823883, 0.017559638246893883, 0.01719486154615879, 0.016743339598178864, 0.10411608219146729, 0.10413545370101929, 0.015390987507998943, 0.014943795278668404, 0.014444979839026928, 0.1045340895652771, 0.013461039401590824, 0.012978742830455303, 0.012470688670873642, 0.011946133337914944, 0.011413191445171833, 0.010878710076212883, 0.010348793119192123, 0.009828195907175541, 0.009320873767137527, 0.00882989726960659, 0.008357546292245388, 0.10868909955024719, 0.007569908630102873, 0.10969789326190948, 0.007009843364357948, 0.006774071138352156, 0.006535177584737539, 0.006295462604612112, 0.00605719955638051, 0.0058219581842422485, 0.005591134075075388, 0.005365967284888029, 0.005147386342287064, 0.11498785763978958, 0.004814645275473595, 0.11581595242023468, 0.004641564562916756, 0.399586021900177, 0.0049667805433273315, 0.005314246751368046, 0.11302507668733597, 0.0059711504727602005, 0.0062741562724113464, 0.006527655757963657, 0.11057356745004654, 0.006979701574891806, 0.0071748653426766396, 0.10955605655908585, 0.0075079295784235, 0.007643670774996281, 0.007728987839072943, 0.007767327595502138, 0.007762833498418331, 0.007719882298260927, 0.007643009535968304, 0.007536537945270538, 0.007404823787510395, 0.10965608805418015, 0.007173108868300915, 0.0070683304220438, 0.11020184308290482, 0.006886847782880068, 0.006805427838116884, 0.006701329723000526, 0.11086057126522064, 0.0065270098857581615, 0.006451806519180536, 0.0063555436208844185, 0.006241592578589916, 0.0061128269881010056, 0.0059722247533500195, 0.005822204053401947, 0.005665164906531572, 0.005503214430063963, 0.11379444599151611, 0.005253682844340801, 0.0051576197147369385, 0.005051912274211645, 0.004938489757478237, 0.004819149617105722, 0.004695441573858261, 0.0045687793754041195, 0.0044403704814612865, 0.11725707352161407, 0.11747699230909348, 0.004275347571820021, 0.004274499136954546, 0.1175178661942482, 0.00430657435208559, 0.11716528981924057, 0.004423366393893957, 0.40195900201797485, 0.004971310496330261, 0.005420296918600798, 0.005827980116009712, 0.006189392879605293, 0.3616154193878174, 0.007266466040164232, 0.007975750602781773, 0.008616599254310131, 0.009179745800793171, 0.00965969543904066, 0.010054155252873898, 0.010363188572227955, 0.10602838546037674, 0.31171339750289917, 0.011579141952097416, 0.1051250770688057, 0.012830215506255627, 0.01332901045680046, 0.013706857338547707, 0.1045052632689476, 0.014228646643459797, 0.014376991428434849, 0.10440942645072937, 0.014481434598565102, 0.01444529090076685, 0.01432296633720398, 0.014125359244644642, 0.013862977735698223, 0.013546238653361797, 0.013184926472604275, 0.1048865094780922, 0.10501057654619217, 0.012213335372507572, 0.011918430216610432, 0.10542414337396622, 0.011335420422255993, 0.011047174222767353, 0.010732491500675678, 0.010397740639746189, 0.010048797354102135, 0.009690473787486553, 0.009327474981546402, 0.008963445201516151, 0.008601751178503036, 0.00824502483010292, 0.007895623333752155, 0.10918935388326645, 0.10954644531011581, 0.007143800146877766, 0.006966631859540939, 0.006780346855521202, 0.006587442476302385, 0.006390346679836512, 0.006190912332385778, 0.11211556196212769, 0.005869837943464518, 0.11271548271179199, 0.005679871421307325, 0.005604686215519905, 0.005516251549124718, 0.11356369405984879, 0.11364870518445969, 0.0054123299196362495, 0.005416892468929291, 0.11364759504795074, 0.00544263981282711, 0.005460030864924192, 0.005454932805150747, 0.0054296585731208324, 0.00538654113188386, 0.005327773746103048, 0.11402183026075363, 0.005247632972896099, 0.1141337901353836, 0.005254282150417566, 0.11399777978658676, 0.005331064108759165, 0.005371409468352795, 0.38162845373153687, 0.005816326010972261, 0.0062011610716581345, 0.006538328714668751, 0.006825991906225681, 0.007063601166009903, 0.007251787930727005, 0.007392440922558308, 0.007487921975553036, 0.007541239727288485, 0.007555929943919182, 0.007535532116889954, 0.007483888883143663, 0.007404743693768978, 0.007301829755306244, 0.007178628351539373, 0.007038448471575975, 0.006884370930492878, 0.11058083921670914, 0.006625474896281958, 0.006514774169772863, 0.006389937829226255, 0.11154475808143616, 0.00618611229583621, 0.11186357587575912, 0.006079813465476036, 0.006036452483385801, 0.1121508926153183, 0.005972533952444792, 0.005947994999587536, 0.005902745295315981, 0.0058395517989993095, 0.005760653875768185, 0.005668569356203079, 0.0055653853341937065, 0.0054531521163880825, 0.005333742592483759, 0.005208748858422041, 0.388093501329422, 0.005358164664357901, 0.11307859420776367, 0.005878523923456669, 0.006116323173046112, 0.006312429904937744, 0.11107765883207321, 0.006663162726908922, 0.1104508563876152, 0.007006853353232145, 0.007152971811592579, 0.00725593464449048, 0.007318458985537291, 0.007343731354922056, 0.007334962487220764, 0.10958235710859299, 0.10957880318164825, 0.007374211680144072, 0.0073999944142997265, 0.007391428109258413, 0.10949937254190445, 0.007366594392806292, 0.00734807550907135, 0.007299994584172964, 0.35076043009757996, 0.007591951638460159, 0.007899763062596321, 0.008149316534399986, 0.10814804583787918, 0.0085652070119977, 0.3319873809814453, 0.009332212619483471, 0.10659627616405487, 0.010383734479546547, 0.10588104277849197, 0.011269683949649334, 0.10541223734617233, 0.011978508904576302, 0.012241536751389503, 0.012415716424584389, 0.012506730854511261, 0.012521408498287201, 0.01246730238199234, 0.012352102436125278, 0.01218336634337902, 0.01196898240596056, 0.10537075251340866, 0.30617883801460266, 0.10531135648488998, 0.012110893614590168, 0.012318768538534641, 0.10502153635025024, 0.012582068331539631, 0.10493368655443192, 0.01272211130708456, 0.10489838570356369, 0.012757174670696259, 0.012716226279735565, 0.012611436657607555, 0.012450771406292915, 0.012241891585290432, 0.0119924396276474, 0.1053658127784729, 0.011489367112517357, 0.10563022643327713, 0.011044460348784924, 0.010819174349308014, 0.010566138662397861, 0.010291123762726784, 0.009998928755521774, 0.009694290347397327, 0.009381304495036602, 0.009063560515642166, 0.008744104765355587, 0.1080157458782196, 0.008190037682652473, 0.007947434671223164, 0.007700602523982525, 0.007451887242496014, 0.10971955209970474, 0.11002030968666077, 0.006928866729140282, 0.35682186484336853, 0.00710869999602437, 0.007357473019510508, 0.007556650787591934, 0.0077080805785954, 0.00781406369060278, 0.007877656258642673, 0.007902110926806927, 0.007891019806265831, 0.007848169654607773, 0.007777203805744648, 0.10900532454252243, 0.007642120122909546, 0.6870789527893066, 0.008434521965682507, 0.009232347831130028, 0.009956030175089836, 0.01059694029390812, 0.011149538680911064, 0.011611634865403175, 0.011983580887317657, 0.012267690151929855, 0.10502557456493378, 0.01267546508461237, 0.012802512384951115, 0.012854963541030884, 0.012839696370065212, 0.012763657607138157, 0.01263404730707407, 0.012457662262022495, 0.012241577729582787, 0.10524334013462067, 0.011796957813203335, 0.011569096706807613, 0.011313937604427338, 0.01103673130273819, 0.10592019557952881, 0.010514061897993088, 0.010265648365020752, 0.010001701302826405, 0.009726015850901604, 0.009442104026675224, 0.009153008460998535, 0.008861548267304897, 0.00857000146061182, 0.008280343376100063, 0.007994161918759346, 0.007713099475950003, 0.007438123691827059, 0.351524293422699, 0.007303085643798113, 0.007397445384413004, 0.0074558923952281475, 0.007481273729354143, 0.007476654835045338, 0.007445119321346283, 0.007389507722109556, 0.007312904577702284, 0.007218177430331707, 0.3524281978607178, 0.10946602374315262, 0.10903102904558182, 0.1086038127541542, 0.008303705602884293, 0.00857560895383358, 0.008791926316916943, 0.10743448138237, 0.009141156449913979, 0.009274131618440151, 0.009357123635709286, 0.009394053369760513, 0.10700317472219467, 0.10699237138032913, 0.10691259056329727, 0.10683521628379822, 0.009698741137981415, 0.009767657145857811, 0.009789341129362583, 0.10666448622941971, 0.009783511981368065, 0.009756782092154026, 0.10673355311155319, 0.009669180028140545, 0.009608563967049122, 0.10688086599111557, 0.009466317482292652, 0.009384175762534142, 0.009272688068449497, 0.009135959669947624, 0.008977647870779037, 0.008801662363111973, 0.008611155673861504, 0.008409189060330391, 0.00819847360253334, 0.10857327282428741, 0.00782990176230669, 0.007666368968784809, 0.007493624929338694, 0.0073137893341481686, 0.007129085715860128, 0.11021103709936142, 0.006817272864282131, 0.11066476255655289, 0.006608974654227495, 0.0065196603536605835, 0.006418263074010611, 0.11141646653413773, 0.006251523736864328, 0.11168336123228073, 0.006163648795336485, 0.006127259694039822, 0.006074746139347553, 0.006008140277117491, 0.005929415114223957, 0.005840403959155083, 0.11279483884572983, 0.005700921639800072, 0.005645635072141886, 0.005578809883445501, 0.005501915235072374, 0.11357645690441132, 0.0053862715139985085, 0.005342687945812941, 0.00528753874823451, 0.005222538486123085, 0.11434100568294525, 0.005129353143274784, 0.11449741572141647, 0.11446298658847809, 0.005174268037080765, 0.00521433912217617, 0.11407619714736938, 0.005299752578139305, 0.005342524498701096, 0.005364958196878433, 0.005368887912482023, 0.0053561097010970116, 0.005328419618308544, 0.005287445615977049, 0.11408214271068573, 0.005233941599726677, 0.005217324942350388, 0.005186774302273989, 0.005144129507243633, 0.005090623162686825, 0.00502799591049552, 0.39080533385276794, 0.11412448436021805, 0.37924766540527344, 0.006118430756032467, 0.35862472653388977, 0.007598479278385639, 0.008463806472718716, 0.009269989095628262, 0.010004657320678234, 0.01065912377089262, 0.011227858252823353, 0.10536856204271317, 0.012180784717202187, 0.01256187167018652, 0.10485637187957764, 0.01314480509608984, 0.013349076732993126, 0.013473628088831902, 0.013524395413696766, 0.10463833808898926, 0.013513210229575634, 0.013455856591463089, 0.01334330439567566, 0.10473603010177612, 0.01305918488651514, 0.012890707701444626, 0.012683562003076077, 0.012443708255887032, 0.012176892720162868, 0.011888133361935616, 0.011582334525883198, 0.011263935826718807, 0.10580724477767944, 0.010676085948944092, 0.01040264219045639, 0.010119877755641937, 0.009830726310610771, 0.009538071230053902, 0.009244182147085667, 0.00895113218575716, 0.10775092989206314, 0.008440479636192322, 0.008215940557420254, 0.007988917641341686, 0.10888571292161942, 0.007597813848406076, 0.34791573882102966, 0.10908275097608566, 0.10879174619913101, 0.00806644931435585, 0.00825338251888752, 0.00839478150010109, 0.008493195287883282, 0.10787001997232437, 0.00864065159112215, 0.008689680136740208, 0.008702361024916172, 0.008682005107402802, 0.008632175624370575, 0.008556378073990345, 0.008457791060209274, 0.008339708670973778, 0.00820501334965229, 0.008056527003645897, 0.00789681263267994, 0.007728027179837227, 0.007552470080554485, 0.007371945306658745, 0.007188017945736647, 0.007002187892794609, 0.0068157510831952095, 0.006629834417253733, 0.11113739758729935, 0.006322801113128662, 0.006194881163537502, 0.11195717006921768, 0.11212169378995895, 0.005960420239716768, 0.1122836172580719, 0.11228886991739273, 0.11217114329338074, 0.006052207667380571, 0.006109514739364386, 0.006142933387309313, 0.006154455244541168, 0.1117631271481514, 0.11168336123228073, 0.006252176593989134, 0.0062983958050608635, 0.006320836488157511, 0.1113998293876648, 0.006363274995237589, 0.006381289102137089, 0.0063777221366763115, 0.11135683208703995, 0.006375285796821117, 0.11127398163080215, 0.1112142950296402, 0.0064909802749753, 0.0065406309440732, 0.00656489934772253, 0.006566132418811321, 0.006546607241034508, 0.11100246012210846, 0.11098577082157135, 0.006561738904565573, 0.006583313923329115, 0.006581923924386501, 0.006559961941093206, 0.006519614718854427, 0.006463074591010809, 0.006392394192516804, 0.006309641990810633, 0.006216564681380987, 0.006114859599620104, 0.006006058305501938, 0.005891694687306881, 0.11263660341501236, 0.005709150340408087, 0.00563567690551281, 0.1132008358836174, 0.005523286294192076, 0.005479978397488594, 0.005425740033388138, 0.11371457576751709, 0.0053472756408154964, 0.005319154821336269, 0.005279096309095621, 0.005228494293987751, 0.005168852861970663, 0.005101554095745087, 0.0050276885740458965, 0.0049484409391880035, 0.11529067903757095, 0.1153215542435646, 0.004847382195293903, 0.11528605967760086, 0.004890553653240204, 0.004916424863040447, 0.1150292232632637, 0.004978796001523733, 0.005012678913772106, 0.11469782143831253, 0.005088025238364935, 0.00512702576816082, 0.3866390883922577, 0.005472955759614706, 0.11265891045331955, 0.00607691565528512, 0.006352112162858248, 0.0065880343317985535, 0.0067850686609745026, 0.006944379769265652, 0.007067596074193716, 0.3517131507396698, 0.3460083305835724, 0.008304889313876629, 0.0089779207482934, 0.10682956129312515, 0.010187930427491665, 0.010714619420468807, 0.01116236299276352, 0.011531353928148746, 0.011823449283838272, 0.01204213872551918, 0.012191406451165676, 0.012276437133550644, 0.012302239425480366, 0.0122748501598835, 0.01219961792230606, 0.1051524430513382, 0.012000360526144505, 0.01187820639461279, 0.011721588671207428, 0.01153501681983471, 0.01132271159440279, 0.011094278655946255, 0.010841517709195614, 0.010580443777143955, 0.09918514639139175, 0.3344767093658447, 0.10555992275476456, 0.10228852927684784, 0.010690493509173393, 0.12576811015605927, 0.011032374575734138, 0.11280646920204163, 0.011244487948715687, 0.011303055100142956, 0.011313206516206264, 0.011279157362878323, 0.011201598681509495, 0.011091542430222034, 0.01095049362629652, 0.10410817712545395, 0.11915528774261475, 0.11279192566871643, 0.010534408502280712, 0.010457205586135387, 0.11780691891908646, 0.010282162576913834, 0.010184768587350845, 0.010059351101517677, 0.009911575354635715, 0.09541966021060944, 0.009619646705687046, 0.009475978091359138, 0.009314511902630329, 0.009132828563451767, 0.09989253431558609, 0.008810068480670452, 0.008657310158014297, 0.00849244836717844, 0.008315947838127613, 0.008127003908157349, 0.00794484093785286, 0.007751403376460075, 0.007555280812084675, 0.0073587046936154366, 0.007161553017795086, 0.006965921726077795, 0.00677252234891057, 0.006579771172255278, 0.006394160911440849, 0.006210015621036291, 0.006030553951859474, 0.005855543073266745, 0.005686095915734768, 0.0055201551876962185, 0.005360533948987722, 0.0052056945860385895, 0.11554008722305298, 0.004963759798556566, 0.004869884345680475, 0.004774898756295443, 0.0046783131547272205, 0.13200481235980988, 0.004538957495242357, 0.004489503335207701, 0.004437306895852089, 0.004378325771540403, 0.004315839149057865, 0.004249655641615391, 0.004182512406259775, 0.004111973103135824, 0.13023468852043152, 0.004019287880510092, 0.003992005251348019, 0.1293809562921524, 0.003971377853304148, 0.003972050733864307, 0.003967692144215107, 0.003953198436647654, 0.003931335173547268, 0.0039032609201967716, 0.003868992207571864, 0.003829970257356763, 0.11814378201961517, 0.0037886337377130985, 0.0037820455618202686, 0.003767874790355563, 0.1315051019191742, 0.003770352341234684, 0.003777142846956849, 0.003789709648117423, 0.0037854916881769896, 0.0037733258213847876, 0.0037513133138418198, 0.0037290637847036123, 0.12829546630382538, 0.0037136052269488573, 0.11374851316213608, 0.003763509914278984, 0.11324221640825272, 0.12291426211595535, 0.12720704078674316, 0.12237195670604706, 0.11443787068128586, 0.11755859106779099, 0.11291927099227905, 0.0049693104811012745, 0.1219114139676094, 0.0054502966813743114, 0.005672677420079708, 0.3815667927265167, 0.00634570000693202, 0.00679265009239316, 0.0071890586987137794, 0.007542693521827459, 0.007843613624572754, 0.008096580393612385, 0.008303129114210606, 0.10861620306968689, 0.1075088381767273, 0.10736139118671417, 0.009060843847692013, 0.009226341731846333, 0.009344269521534443, 0.009417906403541565, 0.0094509432092309, 0.009446843527257442, 0.10696561634540558, 0.009407347068190575, 0.009371829219162464, 0.00930684246122837, 0.10716486722230911, 0.009166015312075615, 0.00908941775560379, 0.008989211171865463, 0.00886873621493578, 0.008731037378311157, 0.008578797802329063, 0.10805581510066986, 0.008302616886794567, 0.008175318129360676, 0.008035100996494293, 0.0078843142837286, 0.3440186083316803, 0.007904456928372383, 0.008042695932090282, 0.008142172358930111, 0.008205820806324482, 0.00823622290045023, 0.008236710913479328, 0.00820982363075018, 0.0081589724868536, 0.008086792193353176, 0.00799605343490839, 0.00788931269198656, 0.007768961135298014, 0.007637486793100834, 0.007496537175029516, 0.007348403334617615, 0.10968779772520065, 0.007093930151313543, 0.11011265963315964, 0.006920957006514072, 0.006844759453088045, 0.00675651989877224, 0.11071732640266418, 0.11088510602712631, 0.00659964932128787, 0.006573469843715429, 0.0065306974574923515, 0.006473521236330271, 0.006403776817023754, 0.006323137320578098, 0.00623335363343358, 0.006135913077741861, 0.006032130215317011, 0.11225738376379013, 0.11240426450967789, 0.11239177733659744, 0.005878101103007793, 0.005885893478989601, 0.00587670411914587, 0.005852405447512865, 0.005814601667225361, 0.005764950532466173, 0.005704848561435938, 0.005635983310639858, 0.005559519398957491, 0.005476708058267832, 0.1136477142572403, 0.1137719675898552, 0.0053536659106612206, 0.005343396216630936, 0.11379247903823853, 0.00533906789496541, 0.005341867916285992, 0.005330616142600775, 0.005306601524353027, 0.005271365400403738, 0.005226245149970055, 0.005172431468963623, 0.1144329234957695, 0.11449983716011047, 0.005122223403304815, 0.005132690072059631, 0.005128995515406132, 0.0051127346232533455, 0.005084962584078312, 0.00504731573164463, 0.005000981967896223, 0.1149834394454956, 0.00493844598531723, 0.004918524995446205, 0.004888390656560659, 0.00484923180192709, 0.004802375100553036, 0.00474885618314147, 0.004689475987106562, 0.0046253930777311325, 0.11630109697580338, 0.11638680845499039, 0.4002935290336609, 0.004846952855587006, 0.005107824224978685, 0.005338726565241814, 0.005539411678910255, 0.11281716078519821, 0.1122920960187912, 0.00612699193879962, 0.006313318852335215, 0.006466733757406473, 0.006588463205844164, 0.11063958704471588, 0.006801047828048468, 0.006891108583658934, 0.006952409632503986, 0.3541965186595917, 0.007324876729398966, 0.007616707589477301, 0.00786286685615778, 0.008064436726272106, 0.10822346806526184, 0.008401909843087196, 0.008537433110177517, 0.008632856421172619, 0.008691004477441311, 0.008714796043932438, 0.008707555942237377, 0.008672457188367844, 0.008612515404820442, 0.008530868217349052, 0.10802154242992401, 0.008372832089662552, 0.10825295746326447, 0.008257374167442322, 0.008197760209441185, 0.008118280209600925, 0.00802168995141983, 0.00791042111814022, 0.007786660920828581, 0.007652543485164642, 0.007510210387408733, 0.007361050695180893, 0.007207049056887627, 0.007049140986055136, 0.006888971198350191, 0.006727444939315319, 0.006565445102751255, 0.0064040282741189, 0.11154565215110779, 0.0061387065798044205, 0.00602881470695138, 0.005915073212236166, 0.005798549856990576, 0.005679920315742493, 0.11323672533035278, 0.005491890944540501, 0.005416875705122948, 0.005336304195225239, 0.0052513559348881245, 0.0051627312786877155, 0.0050713978707790375, 0.00497793685644865, 0.004883043467998505, 0.004787320736795664, 0.3969506621360779, 0.004868502728641033, 0.005020967684686184, 0.0051490082405507565, 0.005253735464066267, 0.11378949135541916, 0.005449592135846615, 0.005539213307201862, 0.005606792867183685, 0.00565366493538022, 0.00568151380866766, 0.11283253133296967, 0.005738869775086641, 0.11262816935777664, 0.005828524008393288, 0.005869603715837002, 0.005891215521842241, 0.005895343143492937, 0.005883494392037392, 0.005857426673173904, 0.11255069077014923, 0.005821086931973696, 0.1125299409031868, 0.005833442788571119, 0.0058411662466824055, 0.005832727998495102, 0.005810006055980921, 0.005774420220404863, 0.005727722775191069, 0.11292316764593124, 0.005657654255628586, 0.005631038453429937, 0.0055927992798388, 0.37853339314460754, 0.00577617809176445, 0.005976205226033926, 0.0061448244377970695, 0.006283152382820845, 0.00639256602153182, 0.00647478923201561, 0.006531701888889074, 0.006565256509929895, 0.006577500142157078, 0.006570427678525448, 0.0065458836033940315, 0.00650608167052269, 0.006452517118304968, 0.006387291941791773, 0.006311684846878052, 0.006227470003068447, 0.006135945208370686, 0.36941343545913696, 0.111595518887043, 0.006442251615226269, 0.006620507221668959, 0.00676595838740468, 0.006880047731101513, 0.0069646080955863, 0.007021786645054817, 0.007053684443235397, 0.10997303575277328, 0.007104844786226749, 0.007122809067368507, 0.007118836976587772, 0.00709497882053256, 0.0070536453276872635, 0.006996780633926392, 0.006926261819899082, 0.11038358509540558, 0.006805214565247297, 0.006751623470336199, 0.006685137283056974, 0.006607437040656805, 0.006520268507301807, 0.006425182335078716, 0.0063234129920601845, 0.006216138601303101, 0.006104663945734501, 0.00599008658900857, 0.11238846182823181, 0.11253520101308823, 0.0057798828929662704, 0.11272762715816498, 0.005745997652411461, 0.0057342383079230785, 0.0057092695496976376, 0.005672538187354803, 0.005625479388982058, 0.0055694542825222015, 0.005505726207047701, 0.005435305647552013, 0.00535941356793046, 0.1139368787407875, 0.005243696738034487, 0.0051996177062392235, 0.005147902760654688, 0.005089492071419954, 0.005025552120059729, 0.00495690293610096, 0.11516861617565155, 0.11524806916713715, 0.0048682126216590405, 0.004867788404226303, 0.004856182262301445, 0.1152985468506813, 0.004852258134633303, 0.004857092630118132, 0.004850118886679411, 0.11534681916236877, 0.0048543731682002544, 0.11522334814071655, 0.11502454429864883, 0.004984919913113117, 0.005043848417699337, 0.11448410898447037, 0.005160378757864237, 0.00521615706384182, 0.005254161078482866, 0.005275835283100605, 0.005282693542540073, 0.0052760299295187, 0.005257468670606613, 0.005228016991168261, 0.005189256276935339, 0.005142308305948973, 0.005088063422590494, 0.005027751438319683, 0.004962249659001827, 0.0048923855647444725, 0.0048189410008490086, 0.0047428058460354805, 0.004664348904043436, 0.004584282636642456, 0.004503060597926378, 0.004421140067279339, 0.11714459210634232, 0.004302874207496643, 0.004261393565684557, 0.004215361550450325, 0.004165396559983492, 0.004112130496650934, 0.11832765489816666, 0.004043246153742075, 0.11847511678934097, 0.004042531829327345, 0.004051534924656153, 0.004051276482641697, 0.004042642191052437, 0.004026629496365786, 0.004003932233899832, 0.003975307568907738, 0.003941763192415237, 0.003903631353750825, 0.003861837787553668, 0.0038168365135788918, 0.0037690612953156233, 0.0037190548609942198, 0.0036673112772405148, 0.12045459449291229, 0.0036035245284438133, 0.003586994484066963, 0.0035653822124004364, 0.0035390742123126984, 0.003508937545120716, 0.003475310280919075, 0.003438798012211919, 0.0033997634891420603, 0.12189599126577377, 0.00335849542170763, 0.0033521472942084074, 0.003340240800753236, 0.0033234187867492437, 0.0033022628631442785, 0.0032773299608379602, 0.0032490745652467012, 0.0032179562840610743, 0.12292438000440598, 0.0031907279044389725, 0.0031906755175441504, 0.0031849455554038286, 0.0031740691047161818, 0.0031586638651788235, 0.0031392304226756096, 0.0031163538806140423, 0.00309033808298409, 0.12369013577699661, 0.003072165185585618, 0.003076236927881837, 0.003074588952586055, 0.0030676545575261116, 0.0030560707673430443, 0.0030403966084122658, 0.003021012991666794, 0.0029984761495143175, 0.002973186783492565, 0.0029454934410750866, 0.0029157556127756834, 0.002884283661842346, 0.002851377474144101, 0.1255044937133789, 0.0028225257992744446, 0.002822477836161852, 0.002817756962031126, 0.1255432367324829, 0.0028366378974169493, 0.0028570268768817186, 0.0028706637676805258, 0.002878225641325116, 0.12496619671583176, 0.0029178636614233255, 0.002947175642475486, 0.002968696877360344, 0.002983096055686474, 0.002990902867168188, 0.002992816036567092, 0.002989375963807106, 0.0029811381828039885, 0.12445534020662308, 0.00299350474961102, 0.0030109079089015722, 0.003021394368261099, 0.0030256060417741537, 0.003024129197001457, 0.003017551964148879, 0.0030063691083341837, 0.0029912064783275127, 0.0029724587220698595, 0.002950565656647086, 0.0029259875882416964, 0.002899019280448556, 0.0028700788971036673, 0.002839377848431468, 0.002807280281558633, 0.002774032298475504, 0.0027399063110351562, 0.12638962268829346, 0.46210575103759766, 0.0029160366393625736, 0.0031079957261681557, 0.003283838974311948, 0.0034429430961608887, 0.0035848740953952074, 0.0037097798194736242, 0.11950808763504028, 0.003954178653657436, 0.004072278272360563, 0.004172504413872957, 0.004255636129528284, 0.004322616383433342, 0.11704826354980469, 0.0044582621194422245, 0.004525161813944578, 0.11623707413673401, 0.004659140016883612, 0.004724517464637756, 0.004773433785885572, 0.004807072225958109, 0.39381328225135803, 0.005095471628010273, 0.005335340742021799, 0.0055458941496908665, 0.005727379582822323, 0.0058804284781217575, 0.00600624643266201, 0.11186251044273376, 0.006231721490621567, 0.00633059348911047, 0.11122632771730423, 0.00650534313172102, 0.0065803369507193565, 0.006630937568843365, 0.006659290753304958, 0.11066928505897522, 0.006708212662488222, 0.11058748513460159, 0.006778234150260687, 0.0068061379715800285, 0.006813113112002611, 0.006801388692110777, 0.0067723714746534824, 0.11065885424613953, 0.0067228954285383224, 0.1106303483247757, 0.1106240302324295, 0.006758005823940039, 0.006780821364372969, 0.006783443037420511, 0.006767610553652048, 0.006735527887940407, 0.11063386499881744, 0.11085329949855804, 0.006706892512738705, 0.006712819915264845, 0.00670024286955595, 0.006671291310340166, 0.006627741735428572, 0.11088437587022781, 0.006554592400789261, 0.006522512063384056, 0.11099988967180252, 0.00646956404671073, 0.006446219980716705, 0.006408624816685915, 0.006358448415994644, 0.006297248415648937, 0.006226541008800268, 0.006147684529423714, 0.0060620117001235485, 0.11213325709104538, 0.005923663266003132, 0.005866925232112408, 0.11261812597513199, 0.005778782069683075, 0.11273038387298584, 0.005747361574321985, 0.005736615974456072, 0.0057129450142383575, 0.005677901208400726, 0.005632856395095587, 0.005578989628702402, 0.005517679266631603, 0.005449836608022451, 0.005376463755965233, 0.0052986410446465015, 0.11423519253730774, 0.005179950967431068, 0.005134734325110912, 0.0050826664082705975, 0.0050246380269527435, 0.11495190858840942, 0.0049408781342208385, 0.004911203868687153, 0.11520740389823914, 0.0048758541233837605, 0.004866915754973888, 0.004848069045692682, 0.004820487927645445, 0.1154698058962822, 0.004789224360138178, 0.11547945439815521, 0.004811855964362621, 0.00482792966067791, 0.004831806756556034, 0.004824629984796047, 0.004807427525520325, 0.004781543277204037, 0.1156306341290474, 0.11563491076231003, 0.004794166423380375, 0.004820492584258318, 0.004833728540688753, 0.004834863822907209, 0.004825264681130648, 0.004805999342352152, 0.004778073634952307, 0.39572617411613464, 0.39093104004859924, 0.005401696544140577, 0.005816261749714613, 0.11165576428174973, 0.006588498130440712, 0.006940524559468031, 0.10962221026420593, 0.007571041118353605, 0.00784721877425909, 0.00807998888194561, 0.10822463780641556, 0.008474813774228096, 0.10777675360441208]\n",
            "Val loss 0.03508476590042014\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4fb1adcd7b24575b8486c882c83c061",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1663.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0352\n",
            "Train Losses : [0.008813777007162571, 0.10747842490673065, 0.009100032038986683, 0.009210105054080486, 0.10709565132856369, 0.009374217130243778, 0.009429353289306164, 0.009450376965105534, 0.10694339126348495, 0.009458070620894432, 0.10693586617708206, 0.009460212662816048, 0.00944486539810896, 0.10704096406698227, 0.009390648454427719, 0.009352009743452072, 0.009288993664085865, 0.009204638190567493, 0.009101729840040207, 0.008982984349131584, 0.008850469253957272, 0.008706831373274326, 0.008553934283554554, 0.008393429219722748, 0.008227319456636906, 0.008056850172579288, 0.10869845002889633, 0.007759540807455778, 0.10908539593219757, 0.007541697006672621, 0.007444293703883886, 0.34920185804367065, 0.007512102834880352, 0.1090090200304985, 0.00780942477285862, 0.007931891828775406, 0.10858054459095001, 0.008132347837090492, 0.008210277184844017, 0.008257933892309666, 0.008277610875666142, 0.00827217660844326, 0.008244034834206104, 0.008195662871003151, 0.008129454217851162, 0.008047777228057384, 0.00795273669064045, 0.00784611701965332, 0.007729822304099798, 0.0076055508106946945, 0.10933330655097961, 0.007388880476355553, 0.007292754482477903, 0.0071878875605762005, 0.0070760296657681465, 0.006958493962883949, 0.006836109794676304, 0.3583550751209259, 0.11032134294509888, 0.35364529490470886, 0.007439383305609226, 0.007808736525475979, 0.008131661452353, 0.3357079327106476, 0.00893528014421463, 0.009404618293046951, 0.32086625695228577, 0.010473895817995071, 0.011061999946832657, 0.011576146818697453, 0.012015921995043755, 0.012382459826767445, 0.012678094208240509, 0.10483591258525848, 0.013130269013345242, 0.013290051370859146, 0.10478217899799347, 0.013495376333594322, 0.013544920831918716, 0.013544032350182533, 0.10463827103376389, 0.01347019337117672, 0.013400448486208916, 0.10459895431995392, 0.013211795128881931, 0.013095254078507423, 0.012957661412656307, 0.012815495021641254, 0.012596025131642818, 0.012415806762874126, 0.012169249355793, 0.011939373798668385, 0.011681558564305305, 0.011380238458514214, 0.011109568178653717, 0.01083879079669714, 0.010568555444478989, 0.10517065972089767, 0.010083233006298542, 0.009877442382276058, 0.009664013050496578, 0.009456481784582138, 0.10789116472005844, 0.11652175337076187, 0.008927367627620697, 0.008788716979324818, 0.008641181513667107, 0.008487134240567684, 0.12238006293773651, 0.008216910995543003, 0.008096103556454182, 0.11306087672710419, 0.10388697683811188, 0.10180250555276871, 0.3199051022529602, 0.008045468479394913, 0.00823731254786253, 0.008391287177801132, 0.008509372361004353, 0.12079216539859772, 0.11531202495098114, 0.008822576142847538, 0.008909963071346283, 0.008965868502855301, 0.11449165642261505, 0.009042052552103996, 0.00906200148165226, 0.00905565358698368, 0.10993484407663345, 0.009022628888487816, 0.008996405638754368, 0.00894828513264656, 0.00888082291930914, 0.008796686306595802, 0.008696953766047955, 0.1057324931025505, 0.008509863168001175, 0.008423141203820705, 0.008319164626300335, 0.008209454827010632, 0.11990270763635635, 0.008010094054043293, 0.10678993910551071, 0.10289158672094345, 0.007842378690838814, 0.007804760709404945, 0.11473436653614044, 0.007719296030700207, 0.007687768433243036, 0.10614553838968277, 0.007615112699568272, 0.007579553872346878, 0.10685338824987411, 0.007507531903684139, 0.10369256883859634, 0.11232853680849075, 0.0074917953461408615, 0.1192944273352623, 0.0075294384732842445, 0.007542161736637354, 0.007519397418946028, 0.0074987211264669895, 0.00746399350464344, 0.007399732246994972, 0.007342331577092409, 0.007251651491969824, 0.007172019686549902, 0.007062986027449369, 0.006975718308240175, 0.006851542741060257, 0.006747676990926266, 0.006639569066464901, 0.006506579928100109, 0.006386131979525089, 0.11024919152259827, 0.006199084222316742, 0.12163151800632477, 0.006066698115319014, 0.006014638114720583, 0.005966539960354567, 0.005889758002012968, 0.00581895187497139, 0.005742559675127268, 0.0056626261211931705, 0.11353231221437454, 0.005540628917515278, 0.005485943518579006, 0.00543056009337306, 0.11400014907121658, 0.0053436304442584515, 0.005311215296387672, 0.005271624308079481, 0.005224740132689476, 0.005172635428607464, 0.00511521752923727, 0.005053861066699028, 0.004989076405763626, 0.004925432149320841, 0.004851361736655235, 0.11527636647224426, 0.0047472198493778706, 0.004709154833108187, 0.004665885120630264, 0.1159181147813797, 0.11598268896341324, 0.004631154704838991, 0.11514785140752792, 0.004682221915572882, 0.0047108689323067665, 0.004728074185550213, 0.004733736626803875, 0.004729967098683119, 0.0047172196209430695, 0.004697389900684357, 0.004669503774493933, 0.004636035300791264, 0.39622560143470764, 0.004777416121214628, 0.004937082529067993, 0.11261443793773651, 0.005232605151832104, 0.005364845972508192, 0.005471643526107073, 0.005563344340771437, 0.005632434505969286, 0.005683478433638811, 0.005717512685805559, 0.00573578430339694, 0.005740503780543804, 0.005732263438403606, 0.3775164783000946, 0.005923811811953783, 0.00610565347597003, 0.006259795278310776, 0.006387100089341402, 0.00648901704698801, 0.006566954310983419, 0.006623042281717062, 0.10996418446302414, 0.006719963159412146, 0.00675948103889823, 0.006780188996344805, 0.006783236749470234, 0.0067706662230193615, 0.006743294186890125, 0.006703721825033426, 0.006652939133346081, 0.006592487450689077, 0.1097574308514595, 0.12068150192499161, 0.10934307426214218, 0.0065233465284109116, 0.11335484683513641, 0.10729609429836273, 0.006642989814281464, 0.006687758024781942, 0.006712887901812792, 0.006720003671944141, 0.00671113608404994, 0.0066877249628305435, 0.006650243420153856, 0.006604310125112534, 0.11056414246559143, 0.006523445248603821, 0.11770562827587128, 0.0064897420816123486, 0.006474545691162348, 0.006446625106036663, 0.006407193373888731, 0.1114828884601593, 0.11105287075042725, 0.10524576157331467, 0.0064010415226221085, 0.006425098050385714, 0.00643208809196949, 0.006425288040190935, 0.006404166575521231, 0.006370521150529385, 0.11149820685386658, 0.10802774876356125, 0.0063375444151461124, 0.006341143045574427, 0.12036662548780441, 0.006352400407195091, 0.006357996724545956, 0.11391615122556686, 0.370813250541687, 0.11440252512693405, 0.0068919239565730095, 0.007121856790035963, 0.007317503448575735, 0.007478340994566679, 0.007606567814946175, 0.00769937876611948, 0.007781533524394035, 0.007827665656805038, 0.007847491651773453, 0.007848622277379036, 0.007825099863111973, 0.007789039984345436, 0.10975245386362076, 0.007718666456639767, 0.007683869451284409, 0.0076338606886565685, 0.0075790416449308395, 0.007504364475607872, 0.10998865962028503, 0.11106440424919128, 0.10653693974018097, 0.007369466125965118, 0.007355968467891216, 0.007333947811275721, 0.007296224590390921, 0.007245594169944525, 0.007183195557445288, 0.11054861545562744, 0.11137811094522476, 0.007069525308907032, 0.007048986852169037, 0.10984670370817184, 0.007011129520833492, 0.0069921258836984634, 0.11021869629621506, 0.006957534234970808, 0.006940022110939026, 0.11022865772247314, 0.00690841069445014, 0.11027859151363373, 0.11027950048446655, 0.1101846843957901, 0.0070129139348864555, 0.10999533534049988, 0.10988449305295944, 0.007211839314550161, 0.109632208943367, 0.007361058611422777, 0.007421140559017658, 0.007457776460796595, 0.34740617871284485, 0.10906428098678589, 0.007989379577338696, 0.008210770785808563, 0.008393238298594952, 0.008538827300071716, 0.008649690076708794, 0.008728234097361565, 0.008776970207691193, 0.00879855826497078, 0.00879538431763649, 0.008769934065639973, 0.10768169164657593, 0.00870892871171236, 0.008672433905303478, 0.10789026319980621, 0.10778923332691193, 0.008596946485340595, 0.008578040637075901, 0.00853942334651947, 0.008483375422656536, 0.008411808870732784, 0.008326922543346882, 0.008230541832745075, 0.008124133571982384, 0.008009511046111584, 0.007888047955930233, 0.007760960143059492, 0.10912760347127914, 0.10919863730669022, 0.10931211709976196, 0.007463793735951185, 0.007427145726978779, 0.007376657333225012, 0.0073144580237567425, 0.00724180880934, 0.007160221692174673, 0.007071047089993954, 0.006975437048822641, 0.35588374733924866, 0.007015636656433344, 0.007128530647605658, 0.1097182109951973, 0.007320752367377281, 0.6925365924835205, 0.007988226599991322, 0.00852828100323677, 0.009016616269946098, 0.009451179765164852, 0.009831338189542294, 0.010157826356589794, 0.01043213251978159, 0.010656364262104034, 0.010833214037120342, 0.010965563356876373, 0.011056659743189812, 0.10568945854902267, 0.011176013387739658, 0.01120575051754713, 0.10564865171909332, 0.011218084022402763, 0.01120208390057087, 0.10563947260379791, 0.10572222620248795, 0.011133788153529167, 0.011102590709924698, 0.01104531530290842, 0.01096506230533123, 0.010864769108593464, 0.010746859945356846, 0.010614038445055485, 0.010468359105288982, 0.10627786070108414, 0.010192093439400196, 0.010059373453259468, 0.009916063398122787, 0.009763832204043865, 0.009604335762560368, 0.009439218789339066, 0.009269673377275467, 0.009096893481910229, 0.008922070264816284, 0.008745932020246983, 0.00856948271393776, 0.008393239229917526, 0.008218024857342243, 0.008044308982789516, 0.007872524671256542, 0.007702984381467104, 0.007536169141530991, 0.007372262887656689, 0.007211459334939718, 0.007053892593830824, 0.1102442592382431, 0.006788685452193022, 0.00667601590976119, 0.006562424823641777, 0.0064483145251870155, 0.006334102712571621, 0.006220410577952862, 0.11183526366949081, 0.0060333069413900375, 0.0059556253254413605, 0.005875094328075647, 0.005792406853288412, 0.005707759875804186, 0.005621938966214657, 0.00553529430180788, 0.11348593980073929, 0.005397334694862366, 0.11376604437828064, 0.005320425145328045, 0.005291192792356014, 0.005255512427538633, 0.0052142636850476265, 0.005168031435459852, 0.005117662716656923, 0.005063698161393404, 0.005006751976907253, 0.004947364330291748, 0.004885811358690262, 0.004822679329663515, 0.004758254624903202, 0.004692976363003254, 0.004627026151865721, 0.004560642875730991, 0.00449409568682313, 0.004427656065672636, 0.004361400846391916, 0.004295493476092815, 0.004230172839015722, 0.004165512043982744, 0.004101403057575226, 0.0040382095612585545, 0.003975844942033291, 0.003914338536560535, 0.003853891510516405, 0.0037944167852401733, 0.003735952777788043, 0.003678583074361086, 0.0036222697235643864, 0.00356703856959939, 0.0035128833260387182, 0.003459832863882184, 0.003407827578485012, 0.0033568975050002337, 0.0033070510253310204, 0.12248428165912628, 0.12254451215267181, 0.0032538557425141335, 0.003260501427575946, 0.003262079320847988, 0.12247800827026367, 0.0032835970632731915, 0.0033012330532073975, 0.003312722546979785, 0.12209846824407578, 0.0033510569483041763, 0.12176577001810074, 0.12147923558950424, 0.0034986427053809166, 0.0035603567957878113, 0.0036115662660449743, 0.12028823792934418, 0.11998536437749863, 0.0038054108154028654, 0.00387955061160028, 0.003941535949707031, 0.11859898269176483, 0.004065566696226597, 0.0041264696046710014, 0.1178094744682312, 0.11751449853181839, 0.004341439809650183, 0.11678700149059296, 0.004520128946751356, 0.11613889038562775, 0.004708509426563978, 0.004796103574335575, 0.004867853131145239, 0.004924776032567024, 0.004967861343175173, 0.11476678401231766, 0.1146300882101059, 0.005130964331328869, 0.005192530807107687, 0.0052391416393220425, 0.005271942354738712, 0.005292214918881655, 0.005300973076373339, 0.005299353506416082, 0.005288428161293268, 0.005269145127385855, 0.005242419894784689, 0.11417240649461746, 0.005206643603742123, 0.005195138044655323, 0.005175601691007614, 0.005148922558873892, 0.005116013810038567, 0.11456776410341263, 0.0050704325549304485, 0.005055076442658901, 0.00503260362893343, 0.11483913660049438, 0.0050052134320139885, 0.0049978140741586685, 0.0049825385212898254, 0.004960254766047001, 0.004931674338877201, 0.0048976256512105465, 0.004858853295445442, 0.00481578940525651, 0.004769293591380119, 0.004719716962426901, 0.004667478147894144, 0.004613168071955442, 0.11630003899335861, 0.11638806015253067, 0.004542038310319185, 0.0045412201434373856, 0.11639707535505295, 0.004553221631795168, 0.004563931841403246, 0.004566140938550234, 0.0045606414787471294, 0.00454814312979579, 0.004529429599642754, 0.4016753137111664, 0.004668069537729025, 0.004810692742466927, 0.11500416696071625, 0.005074030719697475, 0.005194202531129122, 0.11380402743816376, 0.005414034239947796, 0.005513306707143784, 0.00559374550357461, 0.005656704306602478, 0.005703477188944817, 0.005735296290367842, 0.005753527395427227, 0.005759225692600012, 0.11269582808017731, 0.11262401193380356, 0.0058213272131979465, 0.005852010566741228, 0.1124066561460495, 0.005910517647862434, 0.005937016103416681, 0.1122169941663742, 0.005987629760056734, 0.006010596640408039, 0.0060198381543159485, 0.006016785744577646, 0.11208678781986237, 0.006016355007886887, 0.006017088424414396, 0.006006570067256689, 0.11213000118732452, 0.005993191618472338, 0.005988570395857096, 0.00597315514460206, 0.005948164034634829, 0.11234379559755325, 0.005910630337893963, 0.11231765896081924, 0.11229164153337479, 0.005947535391896963, 0.005970869213342667, 0.3703822195529938, 0.006189436186105013, 0.11130448430776596, 0.00656204205006361, 0.0067254845052957535, 0.11032469570636749, 0.00701148621737957, 0.007133862003684044, 0.1097225695848465, 0.0073433490470051765, 0.007430338300764561, 0.007493853103369474, 0.007535583805292845, 0.007557380478829145, 0.00756128691136837, 0.007548817899078131, 0.10923517495393753, 0.007521392311900854, 0.10929906368255615, 0.0075146290473639965, 0.0075073097832500935, 0.10932407528162003, 0.007488951552659273, 0.346782386302948, 0.007675740867853165, 0.007841785438358784, 0.10857844352722168, 0.10843675583600998, 0.008278918452560902, 0.008403177373111248, 0.008497605100274086, 0.10780689865350723, 0.00864680577069521, 0.00870213657617569, 0.008732830174267292, 0.008740922436118126, 0.10767368972301483, 0.008739705197513103, 0.008729279972612858, 0.008700941689312458, 0.008656018413603306, 0.10782917588949203, 0.008565166965126991, 0.008518056012690067, 0.10798181593418121, 0.00842456053942442, 0.008376825600862503, 0.008316040970385075, 0.10798702389001846, 0.008200638927519321, 0.008144686929881573, 0.008077224716544151, 0.10855763405561447, 0.007952718064188957, 0.007893960922956467, 0.007824725471436977, 0.007746398914605379, 0.10903063416481018, 0.00760650122538209, 0.10922233760356903, 0.007508964277803898, 0.00746327405795455, 0.007407046388834715, 0.0073417010717093945, 0.007268135901540518, 0.10977362096309662, 0.10982159525156021, 0.007121332921087742, 0.007090200204402208, 0.11000251770019531, 0.007034431677311659, 0.3539043664932251, 0.10976259410381317, 0.007377216126769781, 0.0075354245491325855, 0.10901964455842972, 0.10883376747369766, 0.007956774905323982, 0.008077988401055336, 0.008171183988451958, 0.008238116279244423, 0.00828101672232151, 0.008302174508571625, 0.008303075097501278, 0.008286249823868275, 0.008253366686403751, 0.10906717926263809, 0.008186412043869495, 0.10816054791212082, 0.00814158096909523, 0.008115666918456554, 0.008075342513620853, 0.008022104389965534, 0.007957510650157928, 0.007883152924478054, 0.10879528522491455, 0.007748986594378948, 0.007687080185860395, 0.007615888491272926, 0.007536676246672869, 0.007450711913406849, 0.007358917035162449, 0.007262444589287043, 0.1098017618060112, 0.007095920387655497, 0.00702296057716012, 0.006944136694073677, 0.006860247813165188, 0.006772241555154324, 0.006680809427052736, 0.0065867225639522076, 0.006490327417850494, 0.006392650306224823, 0.006293663755059242, 0.11169180274009705, 0.3678157925605774, 0.006265594158321619, 0.11127591133117676, 0.0065047163516283035, 0.11079300194978714, 0.006727077532559633, 0.35660824179649353, 0.0071058995090425014, 0.1095227599143982, 0.007607294712215662, 0.007824820466339588, 0.10851505398750305, 0.008198745548725128, 0.008355526253581047, 0.008480948396027088, 0.008577016182243824, 0.008645784109830856, 0.008689727634191513, 0.008710701018571854, 0.008711066097021103, 0.10767368972301483, 0.00869721733033657, 0.008682573214173317, 0.008650979958474636, 0.008604316040873528, 0.008544206619262695, 0.008472681045532227, 0.008390771225094795, 0.008300341665744781, 0.33807119727134705, 0.008317267522215843, 0.00840437225997448, 0.00846607144922018, 0.008504056371748447, 0.008520691655576229, 0.33441489934921265, 0.008717948570847511, 0.008882428519427776, 0.10735197365283966, 0.009152654558420181, 0.10720179229974747, 0.009375859051942825, 0.00946116354316473, 0.009517708793282509, 0.10686102509498596, 0.00959433987736702, 0.009614952839910984, 0.10679520666599274, 0.009629161097109318, 0.009622820653021336, 0.00959615595638752, 0.009551344439387321, 0.009490394033491611, 0.009415296837687492, 0.009327802807092667, 0.009229732677340508, 0.009122357703745365, 0.009007272310554981, 0.008885900489985943, 0.10765020549297333, 0.10774236917495728, 0.1078309416770935, 0.10784974694252014, 0.008554814383387566, 0.008526339195668697, 0.008483294397592545, 0.10799940675497055, 0.008397645317018032, 0.008354028686881065, 0.00829793605953455, 0.008230803534388542, 0.008154342882335186, 0.008069614879786968, 0.007977787405252457, 0.007880092598497868, 0.007777499500662088, 0.007670961320400238, 0.007561121601611376, 0.007448766380548477, 0.10953641682863235, 0.00725509412586689, 0.007170406635850668, 0.10993676632642746, 0.007024492137134075, 0.006960547063499689, 0.006890543270856142, 0.00681538600474596, 0.006735855247825384, 0.006652701646089554, 0.006566657684743404, 0.006478200666606426, 0.006387962494045496, 0.006296361330896616, 0.006203815340995789, 0.006110689602792263, 0.006017443258315325, 0.00592424999922514, 0.005831276066601276, 0.005738884676247835, 0.1129525899887085, 0.005589676555246115, 0.005528992507606745, 0.0054659065790474415, 0.11384034156799316, 0.005366853903979063, 0.005327851511538029, 0.005284441635012627, 0.11405589431524277, 0.005219245795160532, 0.1142173632979393, 0.00519723491743207, 0.005191246513277292, 0.005177605897188187, 0.005157284904271364, 0.11439651995897293, 0.0051315659657120705, 0.0051241363398730755, 0.11441308259963989, 0.11444460600614548, 0.005155101418495178, 0.005178152117878199, 0.11421890556812286, 0.00522661255672574, 0.005250715184956789, 0.11400733143091202, 0.005300499498844147, 0.005324929486960173, 0.005338504444807768, 0.005342111457139254, 0.11366403102874756, 0.11376435309648514, 0.005398266948759556, 0.00542739313095808, 0.11346084624528885, 0.11335708200931549, 0.005545491818338633, 0.005591236986219883, 0.005623493809252977, 0.11295740306377411, 0.1128280758857727, 0.0057475510984659195, 0.11259753257036209, 0.11242570728063583, 0.005944517441093922, 0.112046018242836, 0.006094951182603836, 0.006160896271467209, 0.006209878250956535, 0.006243354175239801, 0.11147274821996689, 0.006303705275058746, 0.0063296775333583355, 0.0063418797217309475, 0.11136355251073837, 0.006365074776113033, 0.006374947726726532, 0.006372531410306692, 0.0063590677455067635, 0.00633583078160882, 0.006303780246526003, 0.006264153402298689, 0.006217728368937969, 0.006165364757180214, 0.006107961293309927, 0.006046127527952194, 0.005980726797133684, 0.005912133492529392, 0.005841018166393042, 0.005767853930592537, 0.005693031940609217, 0.005617076065391302, 0.005540137179195881, 0.005462685599923134, 0.11362603306770325, 0.005339650437235832, 0.005290682427585125, 0.0052385455928742886, 0.005183769389986992, 0.005126739852130413, 0.0050678374245762825, 0.005007518455386162, 0.1149614229798317, 0.004915484692901373, 0.004880663938820362, 0.11525160819292068, 0.004831799305975437, 0.004815516993403435, 0.004793339408934116, 0.11559192091226578, 0.39476677775382996, 0.004933541174978018, 0.11479777842760086, 0.005239337682723999, 0.005377569701522589, 0.005495835095643997, 0.005595128983259201, 0.005676636938005686, 0.005741544999182224, 0.005790628492832184, 0.005825904197990894, 0.11224117875099182, 0.005891366396099329, 0.005920825991779566, 0.005937597714364529, 0.005942781455814838, 0.11223771423101425, 0.005956245586276054, 0.005963174160569906, 0.0059593962505459785, 0.005946131888777018, 0.11241717636585236, 0.005928030237555504, 0.11211548000574112, 0.0059389229863882065, 0.005944705568253994, 0.005940089467912912, 0.005926043260842562, 0.005903600249439478, 0.11235084384679794, 0.005870327819138765, 0.005857341922819614, 0.005836153868585825, 0.005807625129818916, 0.005772519391030073, 0.005731647834181786, 0.0056857881136238575, 0.005635573994368315, 0.005581701640039682, 0.00552466930821538, 0.0054649775847792625, 0.11357007920742035, 0.0053713503293693066, 0.005334577523171902, 0.005293289665132761, 0.005248094908893108, 0.005199604667723179, 0.005148375406861305, 0.005094750784337521, 0.005039166659116745, 0.004981978330761194, 0.004923560656607151, 0.00486410316079855, 0.004803991876542568, 0.11568224430084229, 0.004712965805083513, 0.11589621752500534, 0.004672447685152292, 0.39766252040863037, 0.00481010228395462, 0.11497625708580017, 0.005085757002234459, 0.005210723727941513, 0.005317612085491419, 0.005406395066529512, 0.11330569535493851, 0.005568574648350477, 0.00564124621450901, 0.005698238965123892, 0.11263394355773926, 0.005802263505756855, 0.005848769098520279, 0.005881358869373798, 0.11224847286939621, 0.005942043382674456, 0.11210620403289795, 0.006016544532030821, 0.006049435120075941, 0.006069215480238199, 0.006077154539525509, 0.11177705228328705, 0.006094294600188732, 0.006102282553911209, 0.006099495571106672, 0.006086858455091715, 0.11365658044815063, 0.006069340743124485, 0.006062799599021673, 0.00604707607999444, 0.11203502118587494, 0.006023919675499201, 0.006014901679009199, 0.005997026804834604, 0.0059712128713727, 0.11220362037420273, 0.005931647960096598, 0.005915948189795017, 0.0058922553434967995, 0.005861559417098761, 0.005824664141982794, 0.0057822600938379765, 0.005735252518206835, 0.005684001371264458, 0.0056293136440217495, 0.005571594461798668, 0.0055114757269620895, 0.005449226126074791, 0.0053854105062782764, 0.005320238880813122, 0.00525408610701561, 0.0051871854811906815, 0.005119852256029844, 0.005052282940596342, 0.0049846358597278595, 0.004917163867503405, 0.11526966094970703, 0.1154172271490097, 0.004803566262125969, 0.004787984769791365, 0.11566872149705887, 0.11555380374193192, 0.3943150043487549, 0.004983693826943636, 0.005147011484950781, 0.005289886146783829, 0.0054130107164382935, 0.11336290091276169, 0.0056357006542384624, 0.005734813865274191, 0.005816241260617971, 0.005881098099052906, 0.005930645857006311, 0.00596596859395504, 0.1121002733707428, 0.006030920427292585, 0.006059669423848391, 0.006075891200453043, 0.0060807266272604465, 0.00607534172013402, 0.006060793064534664, 0.11212313175201416, 0.006039705127477646, 0.006031528115272522, 0.006014629732817411, 0.005989790894091129, 0.005958058871328831, 0.0059201386757195, 0.0058769164606928825, 0.11249415576457977, 0.005808297544717789, 0.11264519393444061, 0.005778087303042412, 0.00576685881242156, 0.005747735965996981, 0.005721688736230135, 0.005689439829438925, 0.0056517762131989, 0.005609484389424324, 0.005563042126595974, 0.005513068288564682, 0.005460009444504976, 0.005404493305832148, 0.005346809048205614, 0.005287406034767628, 0.005226648412644863, 0.005164712201803923, 0.005102093331515789, 0.005038815550506115, 0.0049752662889659405, 0.004911479540169239, 0.004847757052630186, 0.004784168675541878, 0.004720854572951794, 0.1159425675868988, 0.004624535329639912, 0.004588369745761156, 0.0045497240498661995, 0.004509054124355316, 0.11662812530994415, 0.004451346583664417, 0.004431798122823238, 0.11683716624975204, 0.11688564717769623, 0.11674787849187851, 0.0044792937114834785, 0.004513978958129883, 0.11635881662368774, 0.1161947250366211, 0.004649328999221325, 0.004701454192399979, 0.1156194731593132, 0.004801955074071884, 0.004849432036280632, 0.004885702393949032, 0.1150805875658989, 0.11491420865058899, 0.005022175144404173, 0.005073552951216698, 0.005112858023494482, 0.005140973255038261, 0.00515887513756752, 0.005167434457689524, 0.005167674273252487, 0.005160425323992968, 0.11446375399827957, 0.11429061740636826, 0.005187663249671459, 0.005208594724535942, 0.005219806917011738, 0.005222303327172995, 0.005216867662966251, 0.005204369314014912, 0.005185513757169247, 0.005161073058843613, 0.005131683312356472, 0.005097950343042612, 0.005060401279479265, 0.11471730470657349, 0.005005405284464359, 0.004985682666301727, 0.11495030671358109, 0.11501077562570572, 0.004984013270586729, 0.11479932069778442, 0.005032456014305353, 0.005056723952293396, 0.0050713904201984406, 0.005077284295111895, 0.005075328517705202, 0.005066243465989828, 0.005050831940025091, 0.005029694642871618, 0.11507865786552429, 0.11477530747652054, 0.11469008773565292, 0.0050670076161623, 0.11448882520198822, 0.005148617550730705, 0.005187071394175291, 0.005214136093854904, 0.005231079645454884, 0.005238688085228205, 0.005237833596765995, 0.005229508504271507, 0.005214275326579809, 0.005193071905523539, 0.005166516173630953, 0.0051352400332689285, 0.0050998288206756115, 0.005060759373009205, 0.11475376039743423, 0.005003413651138544, 0.00498259486630559, 0.0049568586982786655, 0.004926763940602541, 0.11514338105916977, 0.0048850965686142445, 0.004871162120252848, 0.004851899575442076, 0.00482787424698472, 0.004799541551619768, 0.11556324362754822, 0.004761377349495888, 0.11561664193868637, 0.004760994575917721, 0.004764925222843885, 0.004761888179928064, 0.004752561450004578, 0.11565493792295456, 0.11561884731054306, 0.004777625668793917, 0.11559902131557465, 0.11529155820608139, 0.0049011725932359695, 0.004949248395860195, 0.1148262470960617, 0.005041354335844517, 0.005084569100290537, 0.005116421729326248, 0.11437074095010757, 0.11428984999656677, 0.0052397847175598145, 0.005286880768835545, 0.0053216940723359585, 0.005345393903553486, 0.005358853843063116, 0.0053630974143743515, 0.005358977243304253, 0.005347444675862789, 0.005329156294465065, 0.0053049903362989426, 0.005275622941553593, 0.005241513717919588, 0.005203460343182087, 0.005161789711564779, 0.005117184482514858, 0.005070049781352282, 0.005020642653107643, 0.004969425965100527, 0.004916741978377104, 0.39295026659965515, 0.004968735389411449, 0.005058629438281059, 0.005133364349603653, 0.005194105673581362, 0.0052419379353523254, 0.005277599208056927, 0.005302402190864086, 0.005317130591720343, 0.005322890821844339, 0.11382845789194107, 0.005339726340025663, 0.0053496453911066055, 0.005350845400243998, 0.0053442129865288734, 0.005330602638423443, 0.11385513097047806, 0.11383599042892456, 0.005339859984815121, 0.0053550247102975845, 0.11374363303184509, 0.005387928336858749, 0.005404600873589516, 0.11357569694519043, 0.005439999047666788, 0.005457587074488401, 0.005465432535856962, 0.005464576184749603, 0.11346101015806198, 0.00546970684081316, 0.005474180914461613, 0.11343743652105331, 0.005488384515047073, 0.005496684927493334, 0.11332888901233673, 0.0055173649452626705, 0.0055283368565142155, 0.005530196707695723, 0.0055237035267055035, 0.005509923677891493, 0.1133643314242363, 0.005492642987519503, 0.11335812509059906, 0.005504515953361988, 0.005511783529073, 0.11327745765447617, 0.11325813829898834, 0.11316869407892227, 0.0056279925629496574, 0.005672002676874399, 0.005703290458768606, 0.005723119713366032, 0.11273238807916641, 0.005762371234595776, 0.11261353641748428, 0.005818636156618595, 0.005844248924404383, 0.005858486518263817, 0.005862525664269924, 0.005857441108673811, 0.005843921098858118, 0.005823032930493355, 0.005795650649815798, 0.1126682236790657, 0.1126774325966835, 0.11261625587940216, 0.0058013456873595715, 0.005823106039315462, 0.005833958275616169, 0.1125093549489975, 0.005857222713530064, 0.11240793764591217, 0.005899879150092602, 0.00591930840164423, 0.005927893798798323, 0.005926675163209438, 0.11228401958942413, 0.0059287515468895435, 0.37137389183044434, 0.0060923462733626366, 0.006231242325156927, 0.11133307963609695, 0.006476572714745998, 0.006583078298717737, 0.11070563644170761, 0.00676858751103282, 0.006847717333585024, 0.0069083864800632, 0.006952277850359678, 0.006980687379837036, 0.0069949994795024395, 0.006996637675911188, 0.006986708380281925, 0.006966480985283852, 0.006937211845070124, 0.00689982483163476, 0.006855192594230175, 0.006804450880736113, 0.006748094689100981, 0.11064937710762024, 0.006652547512203455, 0.006611100863665342, 0.006563717499375343, 0.006511049345135689, 0.006453829351812601, 0.006392873357981443, 0.006328508723527193, 0.006261443719267845, 0.006192103493958712, 0.006120886653661728, 0.00604828679934144, 0.11215788871049881, 0.005929281935095787, 0.005880022421479225, 0.11249509453773499, 0.005801073275506496, 0.005769037175923586, 0.0057319109328091145, 0.005690379999577999, 0.005645113531500101, 0.00559648871421814, 0.00554515840485692, 0.0054913852363824844, 0.11351568251848221, 0.005406725220382214, 0.005373319610953331, 0.005335868801921606, 0.3835645020008087, 0.11358720809221268, 0.005532874725759029, 0.005639002192765474, 0.005727849435061216, 0.005800571292638779, 0.0058582378551363945, 0.11233837902545929, 0.005962093826383352, 0.00600774958729744, 0.11206398904323578, 0.006090114824473858, 0.11177985370159149, 0.00617876835167408, 0.1116027683019638, 0.0062722559086978436, 0.006312526762485504, 0.006339556537568569, 0.0063544996082782745, 0.006358469370752573, 0.006352460477501154, 0.006337625440210104, 0.0063148862682282925, 0.006285053677856922, 0.11150481551885605, 0.11157701909542084, 0.006245283875614405, 0.006243878975510597, 0.006233328487724066, 0.006214800290763378, 0.006188941188156605, 0.0061566769145429134, 0.006118669640272856, 0.006075819954276085, 0.0060286251828074455, 0.0059777372516691685, 0.11229544878005981, 0.005895250476896763, 0.005861446261405945, 0.00582282105460763, 0.005780047737061977, 0.005733531434088945, 0.005683958064764738, 0.005631687119603157, 0.005577143281698227, 0.11327863484621048, 0.37959450483322144, 0.005611445754766464, 0.005714176222681999, 0.005799916572868824, 0.11241874098777771, 0.0059530409052968025, 0.006020352244377136, 0.006072835996747017, 0.0061116404831409454, 0.11176307499408722, 0.11169713735580444, 0.0062401676550507545, 0.11144985258579254, 0.1113315150141716, 0.0064170826226472855, 0.1110871210694313, 0.006545215845108032, 0.006599965505301952, 0.006639507133513689, 0.11069746315479279, 0.006707888562232256, 0.006736189126968384, 0.0067515503615140915, 0.006755301728844643, 0.006748505402356386, 0.006732210982590914, 0.006707656662911177, 0.006675527431070805, 0.0066367569379508495, 0.006592232268303633, 0.006542585324496031, 0.006488689687103033, 0.0064308494329452515, 0.1112845242023468, 0.006334467325359583, 0.006293618120253086, 0.006247963290661573, 0.006198266055434942, 0.11178534477949142, 0.006116489414125681, 0.006082328502088785, 0.006043153814971447, 0.005999613553285599, 0.005952219944447279, 0.0059015825390815735, 0.11245537549257278, 0.0058198352344334126, 0.005786438938230276, 0.005748646799474955, 0.0057068211026489735, 0.0056617287918925285, 0.0056136976927518845, 0.005563234910368919, 0.005510601215064526, 0.005456269718706608, 0.005400531459599733, 0.005343651864677668, 0.38374829292297363, 0.005376169923692942, 0.11348584294319153, 0.11321863532066345, 0.1129743680357933, 0.005749523174017668, 0.11248510330915451, 0.005946552846580744, 0.0060331933200359344, 0.0061035663820803165, 0.006159006152302027, 0.006200472358614206, 0.11162135750055313, 0.006274427752941847, 0.006306358613073826, 0.006326211150735617, 0.0063352943398058414, 0.006334423087537289, 0.006324706599116325, 0.006307101808488369, 0.11145356297492981, 0.006279051303863525, 0.006267160642892122, 0.006247799843549728, 0.006221611052751541, 0.006189503241330385, 0.006152025423943996, 0.006110089365392923, 0.006064075045287609, 0.11205315589904785, 0.005989341530948877, 0.11219960451126099, 0.005949724465608597, 0.005933747626841068, 0.005911417305469513, 0.005883367732167244, 0.005850293207913637, 0.11254332214593887, 0.005798284895718098, 0.005777424201369286, 0.005751052405685186, 0.00571975763887167, 0.005684106145054102, 0.1129596009850502, 0.0056284829042851925, 0.00560655677691102, 0.005579566583037376, 0.005548007320612669, 0.3791389465332031, 0.005620576906949282, 0.0057118358090519905, 0.005787375383079052, 0.005848478991538286, 0.1123552918434143, 0.11220428347587585, 0.006032527424395084, 0.006092248950153589, 0.11176545917987823, 0.006198287010192871, 0.00624416908249259, 0.0062771993689239025, 0.00629841536283493, 0.11141099780797958, 0.11136401444673538, 0.006380048114806414, 0.006410379894077778, 0.006428976077586412, 0.006436848547309637, 0.0064349910244345665, 0.1111811101436615, 0.006433143280446529, 0.11113987118005753, 0.36248746514320374, 0.006608129478991032, 0.0067444550804793835, 0.006859347689896822, 0.006954345386475325, 0.007030721288174391, 0.007089922204613686, 0.00713327806442976, 0.10979834944009781, 0.007205582689493895, 0.00723440945148468, 0.00724994158372283, 0.007253333460539579, 0.007246039807796478, 0.00722889881581068, 0.007203306537121534, 0.10983525961637497, 0.007156901992857456, 0.0071350266225636005, 0.007105330936610699, 0.10997524112462997, 0.007052658591419458, 0.007028332445770502, 0.006996716372668743, 0.006958387792110443, 0.006914252415299416, 0.1103392168879509, 0.0068383957259356976, 0.006804828532040119, 0.006765397265553474, 0.0067205349914729595, 0.11068444699048996, 0.11075012385845184, 0.006638258695602417, 0.006623352877795696, 0.0066009629517793655, 0.006571759004145861, 0.006536649074405432, 0.006496345158666372, 0.0064515359699726105, 0.006402637343853712, 0.006350367330014706, 0.006295050494372845, 0.1115715429186821, 0.11162588000297546, 0.006191001273691654, 0.0061716195195913315, 0.1117689236998558, 0.11179038137197495, 0.006154528819024563, 0.0061584156937897205, 0.3674173951148987, 0.006287580356001854, 0.00640145456418395, 0.006496581248939037, 0.006574444938451052, 0.006636324338614941, 0.0066833291202783585, 0.0067169079557061195, 0.006738020572811365, 0.006748009938746691, 0.11054068803787231, 0.11050797253847122, 0.0067970543168485165, 0.006816953420639038, 0.006825643591582775, 0.006824174430221319, 0.00681354058906436, 0.006794735323637724, 0.006768657360225916, 0.006736102048307657, 0.00669792341068387, 0.0066546183079481125, 0.006607017945498228, 0.006555565632879734, 0.006500903982669115, 0.006443425081670284, 0.006383690983057022, 0.006321908440440893, 0.0062585617415606976, 0.006193837616592646, 0.006128258537501097, 0.00606170017272234, 0.11210685968399048, 0.11220179498195648, 0.0059322272427380085, 0.005906509235501289, 0.005876112729310989, 0.005841511767357588, 0.005803236737847328, 0.11265530437231064, 0.11272957175970078, 0.0057426635175943375, 0.005735766142606735, 0.005722573958337307, 0.0057036313228309155, 0.00567964231595397, 0.005651172716170549, 0.0056188274174928665, 0.11308929324150085, 0.11314184963703156, 0.11312998831272125, 0.1130838468670845, 0.0056333341635763645, 0.1129189059138298, 0.005702973809093237, 0.005734367296099663, 0.005755634047091007, 0.005767724011093378, 0.005771450232714415, 0.00576771842315793, 0.005757302977144718, 0.005740864668041468, 0.005719048902392387, 0.1128450334072113, 0.005686212796717882, 0.005673737730830908, 0.005655600223690271, 0.11298451572656631, 0.005629350896924734, 0.005619697738438845, 0.3774004280567169, 0.11278167366981506, 0.11246202141046524, 0.005972400773316622, 0.006083689164370298, 0.0061774710193276405, 0.006255049724131823, 0.006317564751952887, 0.006366177462041378, 0.006402106955647469, 0.006426346488296986, 0.006440052762627602, 0.006444133352488279, 0.006439565680921078, 0.006427405402064323, 0.11121196299791336, 0.006407576613128185, 0.006398969795554876, 0.006383100990206003, 0.0063607594929635525, 0.00633269501850009, 0.006299587432295084, 0.0062619708478450775, 0.00622057169675827, 0.006175598595291376, 0.0061277663335204124, 0.11192555725574493, 0.006049270275980234]\n",
            "Val loss 0.03387147133990404\n",
            "Val auc roc 0.5\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f3b8ad8b108466b8b126417e8fe7639",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1663.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0345\n",
            "Train Losses : [0.006016626488417387, 0.006012951489537954, 0.006008898373693228, 0.11209238320589066, 0.00600230460986495, 0.00599956838414073, 0.005996392574161291, 0.0059928311966359615, 0.11210241913795471, 0.005986867938190699, 0.005984533112496138, 0.005981524474918842, 0.005978254601359367, 0.005974495783448219, 0.005970402620732784, 0.005966024473309517, 0.005961297079920769, 0.0059563471004366875, 0.005951202008873224, 0.005945857148617506, 0.1122509017586708, 0.11220820248126984, 0.005935750436037779, 0.005933855194598436, 0.0059314901009202, 0.005928606726229191, 0.005925337318331003, 0.005921582225710154, 0.11228800565004349, 0.0059157367795705795, 0.00591325806453824, 0.11230035871267319, 0.005909412167966366, 0.005907847546041012, 0.005905759986490011, 0.0059031290002167225, 0.005900098010897636, 0.005896653048694134, 0.0058928849175572395, 0.11232897639274597, 0.005886618047952652, 0.11235294491052628, 0.005883492529392242, 0.005882279481738806, 0.005880414973944426, 0.005878095515072346, 0.005875312257558107, 0.11239660531282425, 0.11239609122276306, 0.005871427711099386, 0.11239628493785858, 0.005872815381735563, 0.0058734966441988945, 0.0058733681216835976, 0.005872573237866163, 0.11238638311624527, 0.005871600471436977, 0.005871267523616552, 0.00587024912238121, 0.11239572614431381, 0.11238924413919449, 0.0058708516880869865, 0.0058718337677419186, 0.005872074980288744, 0.005871591158211231, 0.005870413966476917, 0.11240354180335999, 0.005868821870535612, 0.005868248175829649, 0.0058669461868703365, 0.005865150596946478, 0.11241615563631058, 0.005862381774932146, 0.005861384328454733, 0.005859677214175463, 0.11243641376495361, 0.11242581903934479, 0.0058586630038917065, 0.005859214346855879, 0.005859064869582653, 0.005858270917087793, 0.11244306713342667, 0.005857092794030905, 0.005856670439243317, 0.005855684168636799, 0.005854092072695494, 0.005851974245160818, 0.1124531477689743, 0.0058486140333116055, 0.0058474307879805565, 0.005845481064170599, 0.005843183491379023, 0.005840225610882044, 0.11248649656772614, 0.11248541623353958, 0.005836382508277893, 0.11246181279420853, 0.005837731529027224, 0.005838343873620033, 0.005838233046233654, 0.005837569013237953, 0.005836091935634613, 0.005834156647324562, 0.005831805989146233, 0.0058289216831326485, 0.11248935014009476, 0.005824446678161621, 0.005822564475238323, 0.005820252932608128, 0.005817525554448366, 0.112543024122715, 0.11255329102277756, 0.005813742987811565, 0.0058135692961514, 0.005812786519527435, 0.005811418406665325, 0.00580943189561367, 0.005806997884064913, 0.005804111249744892, 0.11259573698043823, 0.005799620412290096, 0.005797808989882469, 0.11257023364305496, 0.005795064382255077, 0.005794050637632608, 0.11257662624120712, 0.005792609415948391, 0.005792080424726009, 0.005791041534394026, 0.005789288319647312, 0.005787153262645006, 0.1126282587647438, 0.1125941127538681, 0.1126275584101677, 0.005787416826933622, 0.00578901544213295, 0.0057898228988051414, 0.005789895076304674, 0.00578923849388957, 0.005787994246929884, 0.1126394122838974, 0.005786281079053879, 0.005785652436316013, 0.005784471519291401, 0.11259562522172928, 0.0057826959528028965, 0.11260207742452621, 0.005783200263977051, 0.005783489439636469, 0.11261638253927231, 0.11262315511703491, 0.005787220783531666, 0.005789028014987707, 0.005790092051029205, 0.005790296476334333, 0.11261395364999771, 0.005791076924651861, 0.00579155795276165, 0.005791329313069582, 0.005790387280285358, 0.11261231452226639, 0.0057892752811312675, 0.005788856651633978, 0.005787810776382685, 0.005786231253296137, 0.11261063069105148, 0.0057840002700686455, 0.005783021450042725, 0.11259869486093521, 0.005781982559710741, 0.005781601648777723, 0.005780606530606747, 0.005778983701020479, 0.005776956677436829, 0.005774438381195068, 0.00577148562297225, 0.005768191069364548, 0.005764585454016924, 0.00576068414375186, 0.005756504368036985, 0.11267635226249695, 0.005749760195612907, 0.005746975541114807, 0.11271749436855316, 0.0057427235879004, 0.005740963388234377, 0.005738798063248396, 0.005736144259572029, 0.005733119323849678, 0.005729809403419495, 0.005726090632379055, 0.005722160451114178, 0.00571789126843214, 0.005713535938411951, 0.0057088453322649, 0.11279495060443878, 0.005701384041458368, 0.11281730979681015, 0.005697149783372879, 0.005695567466318607, 0.005693377926945686, 0.1128365695476532, 0.005690156947821379, 0.005688838195055723, 0.005687098950147629, 0.005684937350451946, 0.0056822458282113075, 0.00567923067137599, 0.005675872787833214, 0.11285803467035294, 0.0056705656461417675, 0.0056684729643166065, 0.11290625482797623, 0.005665219388902187, 0.005663990043103695, 0.005662285257130861, 0.11291301250457764, 0.005659652873873711, 0.005658797454088926, 0.0056572421453893185, 0.005655329208821058, 0.005652830004692078, 0.005650065373629332, 0.005646938923746347, 0.005643438082188368, 0.005639675073325634, 0.11297985911369324, 0.00563368434086442, 0.005631281528621912, 0.005628571379929781, 0.005625459831207991, 0.37706685066223145, 0.11299514770507812, 0.005640103947371244, 0.005647972691804171, 0.0056544216349720955, 0.005659609101712704, 0.11291363090276718, 0.005668934900313616, 0.00567311467602849, 0.005676147527992725, 0.005678271409124136, 0.005679660476744175, 0.005680164787918329, 0.00568005396053195, 0.11287088692188263, 0.005680323112756014, 0.11288733780384064, 0.0056822458282113075, 0.005683218594640493, 0.11286188662052155, 0.005685373675078154, 0.0056862980127334595, 0.005686553660780191, 0.11287570744752884, 0.11283217370510101, 0.005690164864063263, 0.1128380224108696, 0.00569514324888587, 0.005697380751371384, 0.005698829423636198, 0.005699423607438803, 0.005699394270777702, 0.005698774009943008, 0.005697588436305523, 0.00569585245102644, 0.005693699698895216, 0.00569112366065383, 0.005688266828656197, 0.005685113370418549, 0.0056816041469573975, 0.005677834153175354, 0.0056738113053143024, 0.11289991438388824, 0.005667436867952347, 0.11290635168552399, 0.005664031486958265, 0.005662704352289438, 0.0056609115563333035, 0.005658681970089674, 0.005656063556671143, 0.005653223022818565, 0.005649902392178774, 0.005646335426717997, 0.3766208291053772, 0.005650513339787722, 0.005657050292938948, 0.005662418901920319, 0.005666650366038084, 0.0056697833351790905, 0.005672045983374119, 0.005673512816429138, 0.005674234125763178, 0.11287970095872879, 0.005675836466252804, 0.005676621105521917, 0.0056767771020531654, 0.005676261615008116, 0.11289184540510178, 0.005675871390849352, 0.005675806198269129, 0.005675181280821562, 0.0056740352883934975, 0.0056723798625171185, 0.005670327227562666, 0.005667885299772024, 0.11290348321199417, 0.005664031021296978, 0.005662599578499794, 0.005660710856318474, 0.005658314097672701, 0.0056556882336735725, 0.11293578147888184, 0.005651498679071665, 0.005649871658533812, 0.0056477440521121025, 0.0056452639400959015, 0.005642563104629517, 0.005639445502310991, 0.11297518014907837, 0.113004669547081, 0.1129845678806305, 0.11298704892396927, 0.005639245267957449, 0.005641317460685968, 0.0056426334194839, 0.11300824582576752, 0.0056451973505318165, 0.1130073294043541, 0.005649060010910034, 0.005650803446769714, 0.005651789251714945, 0.005652152001857758, 0.11292552947998047, 0.005653077736496925, 0.11292418837547302, 0.112923264503479, 0.005658767186105251, 0.005661124363541603, 0.005662745330482721, 0.00566347548738122, 0.005663571413606405, 0.005663086660206318, 0.005662201903760433, 0.005660700611770153, 0.005658798851072788, 0.0056565143167972565, 0.005653864238411188, 0.005650942213833332, 0.005647706333547831, 0.005644212942570448, 0.005640507210046053, 0.005636613350361586, 0.005632517859339714, 0.11301496624946594, 0.005625986028462648, 0.11302974820137024, 0.005622361321002245, 0.00562097504734993, 0.1130201518535614, 0.005618991330265999, 0.0056182448752224445, 0.005617009475827217, 0.0056154076009988785, 0.11303550004959106, 0.005612910259515047, 0.005612017121165991, 0.0056106629781425, 0.005608821753412485, 0.11306851357221603, 0.005606101825833321, 0.11305692791938782, 0.005605674814432859, 0.005605578422546387, 0.00560504337772727, 0.005603863392025232, 0.005602257326245308, 0.005600308999419212, 0.11301173269748688, 0.005597335286438465, 0.005596144124865532, 0.005594626534730196, 0.113096222281456, 0.0055923303589224815, 0.005591435823589563, 0.005590072367340326, 0.005588390398770571, 0.37759435176849365, 0.005594887770712376, 0.0056021553464233875, 0.005608201492577791, 0.005613021086901426, 0.005616781767457724, 0.005619678180664778, 0.005621734075248241, 0.00562305236235261, 0.005623644683510065, 0.005623646080493927, 0.005623107776045799, 0.00562206469476223, 0.005620601121336222, 0.00561872124671936, 0.005616504233330488, 0.11303703486919403, 0.1130695641040802, 0.005613746587187052, 0.11301717907190323, 0.1130349338054657, 0.005617846734821796, 0.00561978854238987, 0.005620934534817934, 0.11301188915967941, 0.005623163189738989, 0.11302150040864944, 0.005626781843602657, 0.0056283739395439625, 0.005629381630569696, 0.005629578605294228, 0.005629326682537794, 0.005628559272736311, 0.11300303041934967, 0.1130845695734024, 0.005629098974168301, 0.11297944188117981, 0.005632319487631321, 0.005633828695863485, 0.005634595640003681, 0.11300769448280334, 0.005636395886540413, 0.0056371367536485195, 0.11296221613883972, 0.11297230422496796, 0.005641825497150421, 0.11296934634447098, 0.00564703531563282, 0.005649339407682419, 0.11292868107557297, 0.00565372034907341, 0.00565568171441555, 0.005656946450471878, 0.005657502915710211, 0.11293961852788925, 0.11293136328458786, 0.005661457311362028, 0.005663243122398853, 0.005664309952408075, 0.005664787255227566, 0.005664588883519173, 0.005663927178829908, 0.005662794690579176, 0.005661231931298971, 0.005659325048327446, 0.0056570000015199184, 0.005654362961649895, 0.005651493091136217, 0.005648376420140266, 0.11296837031841278, 0.005643432028591633, 0.005641407333314419, 0.005639033857733011, 0.005636424757540226, 0.005633538123220205, 0.0056304014287889, 0.005626986268907785, 0.005623474717140198, 0.005619709845632315, 0.005615845788270235, 0.005611818749457598, 0.005607737228274345, 0.005603519733995199, 0.005599094554781914, 0.005594732705503702, 0.005590172950178385, 0.11312034726142883, 0.00558286951854825, 0.005579875782132149, 0.005576641298830509, 0.11313469707965851, 0.005571587011218071, 0.1131245568394661, 0.005569048225879669, 0.00556806568056345, 0.11315236240625381, 0.005566843319684267, 0.1131623238325119, 0.113153837621212, 0.11315339803695679, 0.005573132541030645, 0.11313573271036148, 0.005579313728958368, 0.1131054162979126, 0.005585900507867336, 0.005588824395090342, 0.005591020453721285, 0.005592427682131529, 0.005593116860836744, 0.11309745907783508, 0.005594749469310045, 0.005595598369836807, 0.005595785565674305, 0.005595454480499029, 0.005594732705503702, 0.005593435373157263, 0.005591807421296835, 0.005589796230196953, 0.11309652775526047, 0.00558680621907115, 0.005585659295320511, 0.005584145896136761, 0.11316816508769989, 0.005581778474152088, 0.11312659084796906, 0.1131148487329483, 0.005583427380770445, 0.0055845496244728565, 0.005585151258856058, 0.005585089325904846, 0.11311361938714981, 0.37773215770721436, 0.005595759022980928, 0.005604681093245745, 0.005612138658761978, 0.005618406925350428, 0.1130456030368805, 0.005629417952150106, 0.005634228233247995, 0.005638134200125933, 0.005641039460897446, 0.005643130745738745, 0.005644616205245256, 0.005645385477691889, 0.005645567085593939, 0.005645270459353924, 0.005644428543746471, 0.005643228534609079, 0.11295740306377411, 0.0056414916180074215, 0.005640918388962746, 0.0056398059241473675, 0.005638351663947105, 0.005636502988636494, 0.005634421948343515, 0.0056319646537303925, 0.005629236344248056, 0.1130141094326973, 0.005625121295452118, 0.005623348988592625, 0.005621306598186493, 0.005619053263217211, 0.0056165107525885105, 0.005613692104816437, 0.00561067508533597, 0.3772791922092438, 0.00561398034915328, 0.005619274917989969, 0.0056236195378005505, 0.11297949403524399, 0.005631354171782732, 0.005634720902889967, 0.005637283902615309, 0.005639142356812954, 0.005640256218612194, 0.37668514251708984, 0.11294639110565186, 0.005660178605467081, 0.005668608006089926, 0.005675598978996277, 0.005681499373167753, 0.0056863282807171345, 0.005690157413482666, 0.11282781511545181, 0.005697046872228384, 0.005700108129531145, 0.005702316295355558, 0.005703894421458244, 0.005704787559807301, 0.005705249961465597, 0.11281449347734451, 0.11292858421802521, 0.005708223674446344, 0.005709642544388771, 0.1128137931227684, 0.005712511949241161, 0.11277513206005096, 0.005716071929782629, 0.0057177492417395115, 0.005718798376619816, 0.005719213280826807, 0.11276453733444214, 0.005720188841223717, 0.11276125907897949, 0.005722408182919025, 0.11275787651538849, 0.005725504830479622, 0.005726974457502365, 0.005727764684706926, 0.005728045012801886, 0.005727819632738829, 0.005727093666791916, 0.005725981201976538, 0.005724547430872917, 0.0057227942161262035, 0.11277088522911072, 0.0057199811562895775, 0.11276376247406006, 0.005719080101698637, 0.0057188463397324085, 0.005718064960092306, 0.005716957151889801, 0.11276862025260925, 0.0057153343223035336, 0.005714762024581432, 0.005713689606636763, 0.005712300539016724, 0.005710639525204897, 0.005708626005798578, 0.11281448602676392, 0.005705465562641621, 0.005704239010810852, 0.0057027000002563, 0.005700794514268637, 0.005698658525943756, 0.0056962138041853905, 0.0056935930624604225, 0.005690757650882006, 0.37578290700912476, 0.00569367129355669, 0.005698530003428459, 0.005702479276806116, 0.005705506075173616, 0.005707874894142151, 0.0057094949297606945, 0.005710519850254059, 0.005710952449589968, 0.11280706524848938, 0.11277716606855392, 0.005714237689971924, 0.005715651903301477, 0.0057165417820215225, 0.005716907791793346, 0.005716710817068815, 0.005716096144169569, 0.11280522495508194, 0.11277574300765991, 0.11277277022600174, 0.005719146691262722, 0.005720853805541992, 0.005721812602132559, 0.005722435191273689, 0.005722343921661377, 0.11276523768901825, 0.005722568370401859, 0.0057227760553359985, 0.005722491070628166, 0.005721861496567726, 0.005720804445445538, 0.005719468928873539, 0.11279545724391937, 0.005717254243791103, 0.00571643840521574, 0.11274021118879318, 0.005715292878448963, 0.11275583505630493, 0.005715681239962578, 0.005715836305171251, 0.005715695675462484, 0.005715035367757082, 0.0057139927521348, 0.11276868730783463, 0.11276974529027939, 0.0057135652750730515, 0.00571401184424758, 0.005713963881134987, 0.11276115477085114, 0.1127861961722374, 0.005715974606573582, 0.005717108026146889, 0.005717687774449587, 0.0057177371345460415, 0.1127907857298851, 0.005718140862882137, 0.11277011036872864, 0.0057196724228560925, 0.005720518529415131, 0.005720757879316807, 0.005720422137528658, 0.11275710165500641, 0.005720371846109629, 0.11276523768901825, 0.00572159793227911, 0.0057221222668886185, 0.005722238682210445, 0.005721862427890301, 0.005721141118556261, 0.005720049608498812, 0.005718576721847057, 0.005716787185519934, 0.005714836996048689, 0.005712646525353193, 0.00571016687899828, 0.11278925836086273, 0.005706303287297487, 0.0057047512382268906, 0.005702911410480738, 0.005700837355107069, 0.005698459688574076, 0.11282463371753693, 0.005694825202226639, 0.005693377461284399, 0.005691616330295801, 0.005689579993486404, 0.005687264259904623, 0.005684869829565287, 0.11287376284599304, 0.11287765949964523, 0.005680972244590521, 0.005680478177964687, 0.11287643015384674, 0.005679969675838947, 0.005679788999259472, 0.11287616193294525, 0.005679850000888109, 0.005679927300661802, 0.00567959900945425, 0.005678829271346331, 0.005677797365933657, 0.005676449276506901, 0.005674727726727724, 0.11287550628185272, 0.11290721595287323, 0.005672635044902563, 0.005672645755112171, 0.0056722816079854965, 0.11288883537054062, 0.005671828985214233, 0.11288518458604813, 0.005672725848853588, 0.005673241801559925, 0.005673220846801996, 0.11287675052881241, 0.11284168809652328, 0.005675125401467085, 0.0056762550957500935, 0.00567681435495615, 0.0056769405491650105, 0.11287478357553482, 0.11287745833396912, 0.1128782257437706, 0.0056817918084561825, 0.11286742240190506, 0.005686468444764614, 0.005688569974154234, 0.005690089426934719, 0.0056910282000899315, 0.005691403057426214, 0.1128055527806282, 0.11283227801322937, 0.11282842606306076, 0.005697167478501797, 0.005699248053133488, 0.005700756795704365, 0.0057016643695533276, 0.005702073685824871, 0.005702023394405842, 0.11281523108482361, 0.005702255759388208, 0.005702382884919643, 0.005702138878405094, 0.005701524671167135, 0.11282515525817871, 0.005700644105672836, 0.005700421519577503, 0.11281097680330276, 0.005700142588466406, 0.0057001616805791855, 0.005699735600501299, 0.005698950495570898, 0.00569786224514246, 0.0056964196264743805, 0.005694729276001453, 0.005692820530384779, 0.005690721794962883, 0.11285783350467682, 0.005687307100743055, 0.005685914307832718, 0.11284755170345306, 0.005683965981006622, 0.0056831385008990765, 0.11285733431577682, 0.00568208284676075, 0.0056816525757312775, 0.005680934526026249, 0.005679830443114042, 0.005678459536284208, 0.005676845088601112, 0.005675000604242086, 0.11292869597673416, 0.005672064144164324, 0.005670917220413685, 0.005669505335390568, 0.0056677889078855515, 0.005665878765285015, 0.005663793999701738, 0.005661467555910349, 0.005659021437168121, 0.005656363442540169, 0.005653641652315855, 0.005650762002915144, 0.005647870246320963, 0.005644791293889284, 0.005641578231006861, 0.0056383805349469185, 0.005635116249322891, 0.005631832871586084, 0.11298657208681107, 0.1130303218960762, 0.005625494755804539, 0.11302519589662552, 0.005624380428344011, 0.0056239464320242405, 0.11302405595779419, 0.005623542703688145, 0.11303815245628357, 0.005624351091682911, 0.11301425844430923, 0.0056261722929775715, 0.005627029575407505, 0.11297345161437988, 0.005628831218928099, 0.11299887299537659, 0.005631372332572937, 0.1129903644323349, 0.11298279464244843, 0.005637395195662975, 0.11297036707401276, 0.0056424736976623535, 0.005644733551889658, 0.11296944320201874, 0.005648868624120951, 0.11295042932033539, 0.11295770108699799, 0.005656640976667404, 0.005659301299601793, 0.0056613110937178135, 0.005662667565047741, 0.005663536489009857, 0.005663988646119833, 0.11290809512138367, 0.11289102584123611, 0.005666760727763176, 0.11288821697235107, 0.005670133978128433, 0.11289017647504807, 0.11287873238325119, 0.1128644347190857, 0.005680748261511326, 0.005683728959411383, 0.005686081014573574, 0.005687749478965998, 0.005688855424523354, 0.0056894696317613125, 0.005689698737114668, 0.005689518991857767, 0.11286276578903198, 0.0056893909350037575, 0.3757653534412384, 0.005696418229490519, 0.005702353082597256, 0.11282023042440414, 0.005712713114917278, 0.005717158783227205, 0.005720804911106825, 0.005723733454942703, 0.11276918649673462, 0.005729033146053553, 0.11274129152297974, 0.005734252277761698, 0.11272177845239639, 0.1127099096775055, 0.005743191111832857, 0.005746051669120789, 0.005748315714299679, 0.005749969277530909, 0.005751071497797966, 0.0057517592795193195, 0.3746175765991211, 0.005758753977715969, 0.00576452910900116, 0.0057694194838404655, 0.005773456767201424, 0.0057767052203416824, 0.1126493364572525, 0.11255272477865219, 0.005786324385553598, 0.00578933022916317, 0.11258377134799957, 0.00579467648640275, 0.005797042045742273, 0.005798808764666319, 0.0058000278659164906, 0.00580078549683094, 0.005801067687571049, 0.11256107687950134, 0.005801741033792496, 0.11255382746458054, 0.005803379695862532, 0.005804051179438829, 0.0058044022880494595, 0.005804234184324741, 0.005803894717246294, 0.005803063977509737, 0.11255800724029541, 0.00580201018601656, 0.005801623687148094, 0.005800895858556032, 0.005799910984933376, 0.005798623897135258, 0.005797181278467178, 0.005795532371848822, 0.11258658021688461, 0.0057927826419472694, 0.005791839677840471, 0.005790449678897858, 0.11257007718086243, 0.005788382142782211, 0.005787625443190336, 0.005786488298326731, 0.005785197950899601, 0.005783656146377325, 0.005781932268291712, 0.005780072882771492, 0.005777915008366108, 0.005775784142315388, 0.005773480981588364, 0.005771045107394457, 0.11264825612306595, 0.005767154041677713, 0.005765510722994804, 0.005763757508248091, 0.005761811509728432, 0.0057596685364842415, 0.0057574850507080555, 0.005755120888352394, 0.005752719473093748, 0.005750158801674843, 0.005747552029788494, 0.005744840484112501, 0.005742075853049755, 0.005739281419664621, 0.11271415650844574, 0.005734701175242662, 0.11273172497749329, 0.005731964949518442, 0.11275695264339447, 0.005730695091187954, 0.005730188451707363, 0.0057294247671961784, 0.005728423595428467, 0.005727157928049564, 0.005725680850446224, 0.1127576157450676, 0.005723399110138416, 0.005722519010305405, 0.005721408873796463, 0.00572003610432148, 0.37517333030700684, 0.005723160691559315, 0.005727019160985947, 0.005730193108320236, 0.11269062012434006, 0.11273915320634842, 0.005739336367696524, 0.11271031945943832, 0.005745588336139917, 0.005748384166508913, 0.1126805916428566, 0.005753190256655216, 0.00575532391667366, 0.005756939761340618, 0.005757975857704878, 0.005758706014603376, 0.005759004037827253, 0.11267442256212234, 0.0057596792466938496, 0.11268008500337601, 0.005761156789958477, 0.11266597360372543, 0.005763208959251642, 0.1126469299197197, 0.005765725392848253, 0.0057669105008244514, 0.11262479424476624, 0.005768963601440191, 0.0057700080797076225, 0.11263436079025269, 0.11265312880277634, 0.0057737501338124275, 0.005775176454335451, 0.005776092875748873, 0.005776660982519388, 0.005776857957243919, 0.005776638630777597, 0.005776251666247845, 0.11262310296297073, 0.005775625351816416, 0.11265736818313599, 0.11262472718954086, 0.005777332466095686, 0.11262612789869308, 0.005779799539595842, 0.1126248762011528, 0.005782530643045902, 0.00578376604244113, 0.005784656386822462, 0.005784959066659212, 0.005785044748336077, 0.0057848296128213406, 0.005784251261502504, 0.005783530883491039, 0.005782516673207283, 0.005781299434602261, 0.005779874511063099, 0.005778314545750618, 0.00577667448669672, 0.005774752702564001, 0.005772886332124472, 0.11264237016439438, 0.005769719835370779, 0.005768447648733854, 0.112650565803051, 0.11264330893754959, 0.005766620393842459, 0.005766517482697964, 0.005766130052506924, 0.005765505135059357, 0.005764606408774853, 0.005763553082942963, 0.005762278567999601, 0.005760801024734974, 0.005759241990745068, 0.005757540464401245, 0.1126699447631836, 0.11268549412488937, 0.11269370466470718, 0.11270919442176819, 0.005756608210504055, 0.005757509730756283, 0.11266381293535233, 0.00575915677472949, 0.005759929306805134, 0.005760303698480129, 0.005760371685028076, 0.0057602240704, 0.005759617779403925, 0.005758919287472963, 0.0057579209096729755, 0.005756805185228586, 0.005755457561463118, 0.005753999575972557, 0.005752369295805693, 0.1126803308725357, 0.005749852396547794, 0.0057487161830067635, 0.005747489631175995, 0.11273683607578278, 0.005745574366301298, 0.005744860041886568, 0.005743850953876972, 0.0057426514104008675, 0.005741335917264223, 0.005739867687225342, 0.005738247651606798, 0.0057365782558918, 0.005734688602387905, 0.005732771474868059, 0.005730803590267897, 0.11275147646665573, 0.005727450828999281, 0.005726119969040155, 0.005724625196307898, 0.11275064200162888, 0.005722359754145145, 0.005721359979361296, 0.005720213055610657, 0.11278711259365082, 0.3752559423446655, 0.00572311133146286, 0.005727036856114864, 0.005730262491852045, 0.3749256730079651, 0.005740186665207148, 0.005746445152908564, 0.00575178861618042, 0.005756368860602379, 0.005760169122368097, 0.11265776306390762, 0.005766982678323984, 0.005769835319370031, 0.00577222416177392, 0.005774038843810558, 0.0057754297740757465, 0.005776400677859783, 0.1126481145620346, 0.005778314080089331, 0.005779175087809563, 0.005779617931693792, 0.3740691542625427, 0.005784650798887014, 0.11254901438951492, 0.005793125834316015, 0.005796794313937426, 0.005799761973321438, 0.005802263971418142, 0.005804184824228287, 0.11256182938814163, 0.005807704757899046, 0.005809229798614979, 0.0058103506453335285, 0.005811115726828575, 0.005811480339616537, 0.11254746466875076, 0.005812398623675108, 0.005812822375446558, 0.0058128912933170795, 0.005812756717205048, 0.005812392104417086, 0.11254139244556427, 0.37347209453582764, 0.005816415883600712, 0.005820266902446747, 0.005823476240038872, 0.005826101638376713, 0.11248910427093506, 0.11250448226928711, 0.0058335550129413605, 0.005835912190377712, 0.005837755743414164, 0.005839111749082804, 0.005840174853801727, 0.005840843077749014, 0.005841228179633617, 0.005841352045536041, 0.11250361800193787, 0.00584159092977643, 0.005841790232807398, 0.11244460940361023, 0.005842216778546572, 0.005842355079948902, 0.005842292215675116, 0.005842088256031275, 0.005841528996825218, 0.11250044405460358, 0.00584076251834631, 0.11251195520162582, 0.005840874742716551, 0.11246084421873093, 0.1124739795923233, 0.0058426703326404095, 0.11248287558555603, 0.005844680592417717, 0.005845542065799236, 0.005846167914569378, 0.0058465017937123775, 0.005846538580954075, 0.005846379790455103, 0.005845883395522833, 0.005845276638865471, 0.005844526924192905, 0.005843630526214838, 0.005842504091560841, 0.005841364152729511, 0.11245282739400864, 0.005839457735419273, 0.005838703364133835, 0.00583770452067256, 0.005836759693920612, 0.005835566204041243, 0.0058341859839856625, 0.005832882132381201, 0.11247914284467697, 0.11251398921012878, 0.005830464418977499, 0.11255218833684921, 0.0058303214609622955, 0.11248758435249329, 0.005830861162394285, 0.0058311219327151775, 0.005831120070070028, 0.005830924026668072, 0.005830471403896809, 0.7490086555480957, 0.005837626755237579, 0.005844383966177702, 0.005850224755704403, 0.005855342838913202, 0.00585964135825634, 0.005863368511199951, 0.005866554100066423, 0.0058692279271781445, 0.005871371831744909, 0.005873050540685654, 0.11238422989845276, 0.005876175593584776, 0.005877519492059946, 0.005878479685634375, 0.005879206582903862, 0.005879662930965424, 0.005879868287593126, 0.00587982963770628, 0.11237714439630508, 0.005879953037947416, 0.005879954434931278, 0.005879879463464022, 0.005879538133740425, 0.0058790226466953754, 0.005878368858247995, 0.005877599120140076, 0.0058768088929355145, 0.005875858478248119, 0.005874721333384514, 0.005873604211956263, 0.005872415844351053, 0.005871024448424578, 0.005869738291949034, 0.0058683245442807674, 0.005866981111466885, 0.11241166293621063, 0.11240026354789734, 0.00586420763283968, 0.005863772239536047, 0.005863054655492306, 0.005862347315996885, 0.0058614653535187244, 0.005860484205186367, 0.0058594620786607265, 0.3726418912410736, 0.1124262660741806, 0.1124161034822464, 0.00586585933342576, 0.005868068430572748, 0.0058699739165604115, 0.005871390923857689, 0.0058725145645439625, 0.005873328540474176, 0.005873957648873329, 0.00587437255308032, 0.0058743916451931, 0.11238718777894974, 0.1124056875705719, 0.0058754743076860905, 0.005875969771295786, 0.00587622681632638, 0.005876343231648207, 0.005876200273633003, 0.11239249259233475, 0.005876131355762482, 0.005876082926988602, 0.005875895731151104, 0.005875601898878813, 0.005875054746866226, 0.00587443495169282, 0.005873795598745346, 0.005872936453670263, 0.005872061010450125, 0.0058710309676826, 0.005870006047189236, 0.005868843290954828, 0.00586770148947835, 0.11242325603961945, 0.005865775980055332, 0.11241095513105392, 0.005864711478352547, 0.11240441352128983, 0.005864220671355724, 0.005864066071808338, 0.005863780155777931, 0.0058632902801036835, 0.0058626881800591946, 0.00586211821064353, 0.005861274432390928, 0.00586047163233161, 0.005859463009983301, 0.005858492571860552, 0.005857414565980434, 0.005856298841536045, 0.005855115596204996, 0.005853938404470682, 0.11241405457258224, 0.005851924419403076, 0.11245717853307724, 0.005850803107023239, 0.005850300658494234, 0.005849662236869335, 0.005849029868841171, 0.005848259665071964, 0.005847380496561527, 0.005846451967954636, 0.005845462437719107, 0.005844403989613056, 0.005843332968652248, 0.005842242855578661, 0.005841024219989777, 0.005839797668159008, 0.11247026175260544, 0.005837805103510618, 0.005837058648467064, 0.11248093098402023, 0.0058356779627501965, 0.005835127085447311, 0.005834519397467375, 0.00583376782014966, 0.005832955706864595, 0.11247644573450089, 0.005831663962453604, 0.0058311643078923225, 0.005830514710396528, 0.005829871632158756, 0.00582902692258358, 0.11248486489057541, 0.005827763583511114, 0.005827301647514105, 0.0058266776613891125, 0.0058259605430066586, 0.00582519406452775, 0.0058243609964847565, 0.005823440849781036, 0.0058224936947226524, 0.005821505095809698, 0.11248599737882614, 0.3734043836593628, 0.0058219702914357185, 0.0058237528428435326, 0.005825139116495848, 0.005826275330036879, 0.11251706629991531, 0.005828300956636667, 0.005829210858792067, 0.005829809699207544, 0.11249077320098877, 0.11250514537096024, 0.0058319782838225365, 0.00583272147923708, 0.005833284463733435, 0.005833631381392479, 0.0058337668888270855, 0.11248758435249329, 0.005834200419485569, 0.005834398325532675, 0.0058344341814517975, 0.005834256298840046, 0.005834044888615608, 0.005833703558892012, 0.005833253730088472, 0.005832757335156202, 0.11248838901519775, 0.0058318632654845715, 0.005831539630889893, 0.005831071175634861, 0.11247188597917557, 0.11251147836446762, 0.005830600392073393, 0.005830674432218075, 0.0058305407874286175, 0.005830348934978247, 0.005830025300383568, 0.005829613655805588, 0.1124994158744812, 0.005828957539051771, 0.11248648166656494, 0.005828809458762407, 0.005828716326504946, 0.11250321567058563, 0.11249655485153198, 0.005829112604260445, 0.0058293770998716354, 0.005829515401273966, 0.005829515401273966, 0.005829327739775181, 0.00582902692258358, 0.005828795954585075, 0.0058283149264752865, 0.11251415312290192, 0.11249452084302902, 0.005827753338962793, 0.005827788729220629, 0.0058276718482375145, 0.005827479995787144, 0.005827159155160189, 0.005826775915920734, 0.005826281383633614, 0.1125003919005394, 0.005825479049235582, 0.11253537237644196, 0.11250044405460358, 0.11250556260347366, 0.00582608999684453, 0.005826460663229227, 0.005826677195727825, 0.00582671957090497, 0.00582670234143734, 0.005826539825648069, 0.005826354492455721, 0.005826007574796677, 0.005825575906783342, 0.005825059954077005, 0.005824521649628878, 0.0058239228092134, 0.005823228973895311, 0.005822564475238323, 0.005821833852678537, 0.11250888556241989, 0.0058206492103636265, 0.005820098798722029, 0.0058195567689836025, 0.005818926729261875, 0.005818258970975876, 0.0058175609447062016, 0.005816897843033075, 0.0058160629123449326, 0.005815293174237013, 0.005814476870000362, 0.0058136326260864735, 0.1125478744506836, 0.005812299903482199, 0.11252062767744064, 0.005811474286019802, 0.005811147857457399, 0.005810812581330538, 0.005810264963656664, 0.11254026740789413, 0.00580959627404809, 0.11254695802927017, 0.005809263791888952, 0.005809130612760782, 0.005808962509036064, 0.005808655638247728, 0.11254649609327316, 0.0058082290925085545, 0.11253663897514343, 0.005808155983686447, 0.1125594973564148, 0.11254746466875076, 0.005808833986520767, 0.005809109192341566, 0.005809328053146601, 0.00580947520211339, 0.005809321533888578, 0.11253145337104797, 0.11253321170806885, 0.005809617228806019, 0.005809900350868702, 0.112543985247612, 0.11254950612783432, 0.7497628331184387, 0.0058146510273218155, 0.11251185089349747, 0.00582139752805233, 0.005824280437082052, 0.00582678709179163, 0.005828908644616604, 0.005830822978168726, 0.005832294467836618, 0.005833625793457031, 0.11249962449073792, 0.0058359247632324696, 0.11248362064361572, 0.005837952718138695, 0.005838838405907154, 0.11248446255922318, 0.00584038021042943, 0.005841079633682966, 0.005841568578034639, 0.005841987207531929, 0.005842277780175209, 0.3728983402252197, 0.11240679770708084, 0.005845499224960804, 0.005846849177032709, 0.005847918801009655, 0.005848832428455353, 0.005849597975611687, 0.005850206129252911, 0.0058506326749920845, 0.005850988440215588, 0.3727685809135437, 0.005852752830833197, 0.005854000803083181, 0.005855095572769642, 0.005855982657521963, 0.005856703966856003, 0.11242242157459259, 0.005858065560460091, 0.005858525633811951, 0.005858954042196274, 0.005859214812517166, 0.3725624084472656, 0.005860940087586641, 0.005862061399966478, 0.005863077938556671, 0.0058639016933739185, 0.005864511243999004, 0.3724869191646576, 0.11241942644119263, 0.372470498085022, 0.11250157654285431, 0.11240573227405548, 0.005875586066395044, 0.005877677351236343, 0.1124170646071434, 0.005881124641746283, 0.005882528610527515, 0.005883773788809776, 0.005884875543415546, 0.005885787773877382, 0.005886455066502094, 0.005887083243578672, 0.11233926564455032, 0.005888114217668772, 0.005888588260859251, 0.005888986401259899, 0.005889168009161949, 0.005889337044209242, 0.11237801611423492, 0.005889664404094219, 0.00588977849110961, 0.005889906547963619, 0.005889900028705597, 0.005889845080673695, 0.00588980782777071, 0.005889602936804295, 0.005889466032385826, 0.005889323074370623, 0.005888954270631075, 0.005888746120035648, 0.005888432264328003, 0.11235401034355164, 0.005887977778911591, 0.11232899874448776, 0.005887750070542097, 0.11235298961400986, 0.005887667648494244, 0.005887657403945923, 0.005887600127607584, 0.3721051812171936, 0.11235262453556061, 0.005889024119824171, 0.005889709107577801, 0.00589025067165494, 0.11234226077795029, 0.005891209933906794, 0.11235768347978592, 0.0058920131996273994, 0.005892391316592693, 0.005892653483897448, 0.00589285371825099, 0.11233190447092056, 0.11240864545106888, 0.11235442012548447, 0.005893924739211798, 0.005894253961741924, 0.00589441554620862, 0.11233854293823242, 0.005894800182431936, 0.005895056761801243, 0.005895144306123257, 0.005895168986171484, 0.0058952271938323975, 0.11232160031795502, 0.005895243026316166, 0.005895194131880999, 0.11232735216617584, 0.005895255133509636, 0.005895344074815512, 0.11233027279376984, 0.005895331967622042, 0.1123388409614563, 0.005895574111491442, 0.005895598325878382, 0.005895648617297411, 0.005895568057894707, 0.005895582027733326, 0.005895513109862804, 0.0058954316191375256, 0.005895292852073908, 0.11235248297452927, 0.005895142909139395, 0.0058950395323336124, 0.005894900299608707, 0.005894801113754511, 0.0058946446515619755, 0.005894534755498171, 0.005894421599805355, 0.005894203204661608, 0.005894052796065807, 0.005893881432712078, 0.005893644876778126, 0.0058934628032147884, 0.005893262103199959, 0.005893069785088301, 0.005892840679734945, 0.3720220923423767, 0.005892883520573378, 0.11235236376523972, 0.005893320310860872, 0.005893494468182325, 0.005893688648939133, 0.005893894005566835, 0.005893849302083254, 0.1123460903763771, 0.005893999710679054, 0.005894122179597616, 0.11234252154827118, 0.11234303563833237, 0.005894273053854704, 0.0058943526819348335, 0.11233451217412949, 0.005894479341804981, 0.0058945221826434135, 0.005894615314900875, 0.0058946008794009686, 0.0058945417404174805, 0.11233608424663544, 0.0058946008794009686, 0.005894579458981752, 0.0058945585042238235, 0.11234599351882935, 0.11235319077968597, 0.1123252809047699, 0.005894714035093784, 0.005894734524190426, 0.005894801579415798, 0.11233492195606232, 0.005894833244383335, 0.005894835107028484, 0.11233944445848465, 0.005894883070141077, 0.005894969217479229, 0.005894950591027737, 0.005894950591027737, 0.005894940812140703, 0.005894869565963745, 0.005894883535802364, 0.00589479086920619, 0.11232440173625946, 0.005894776899367571, 0.005894758738577366, 0.0058947280049324036, 0.005894683767110109, 0.005894671659916639, 0.005894633941352367, 0.00589459203183651, 0.0058945841155946255, 0.11233409494161606, 0.005894571077078581, 0.11234349757432938, 0.005894509609788656, 0.005894509144127369, 0.005894509144127369, 0.371948778629303, 0.005894521716982126, 0.0058946022763848305, 0.005894540809094906, 0.005894572474062443]\n",
            "Val loss 0.034424348123910195\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64684e0f-3efe-428d-d688-f2172d736e1a"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}