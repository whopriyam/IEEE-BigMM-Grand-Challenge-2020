{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Justification_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99701d74f5b24866a82801c7f39884c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8eacc7d751a2459aa2e8e498733ae36e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_acbc9a375cc84d28a0ccaaddeec60d2b",
              "IPY_MODEL_573cdeefee644de686ec4e21622c7c33"
            ]
          }
        },
        "8eacc7d751a2459aa2e8e498733ae36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acbc9a375cc84d28a0ccaaddeec60d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a75a9ce7c84a42e0bb9b6595273a22b4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1701,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1701,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfb91675c3da44b088c0ea4275b87b7b"
          }
        },
        "573cdeefee644de686ec4e21622c7c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0729574dd58c41b6a9e99926f4c53f68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1701/1701 [32:13&lt;00:00,  1.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a32f27c4ae74edd997e8bbc1da05aba"
          }
        },
        "a75a9ce7c84a42e0bb9b6595273a22b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfb91675c3da44b088c0ea4275b87b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0729574dd58c41b6a9e99926f4c53f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a32f27c4ae74edd997e8bbc1da05aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1353da8779654302bce43ec95670aa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8f0c45d195640c4814ff89bfa3bca12",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77ac73aed7a1443e8a6d7128fb69e8c0",
              "IPY_MODEL_64f02ee8cf33407696aaeecf1dac6ec5"
            ]
          }
        },
        "d8f0c45d195640c4814ff89bfa3bca12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77ac73aed7a1443e8a6d7128fb69e8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ed51ce576474cdfa07556e44874f976",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1701,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1701,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5aac5cd27d284c63a05fbe7fa306f803"
          }
        },
        "64f02ee8cf33407696aaeecf1dac6ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5d2da6cf1d4492591b006d9ffacb042",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1701/1701 [31:38&lt;00:00,  1.12s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a941d4265b0d44c7bf399ac4027dace0"
          }
        },
        "3ed51ce576474cdfa07556e44874f976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5aac5cd27d284c63a05fbe7fa306f803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5d2da6cf1d4492591b006d9ffacb042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a941d4265b0d44c7bf399ac4027dace0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9053c184e6f548f3954a6e3d5c3e32b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9ab4196e82c4fd2860354e34cae48e5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_934dfe685be54f09bd340ff976009fdc",
              "IPY_MODEL_0527481a5ec0435290ad08dd09933dd8"
            ]
          }
        },
        "e9ab4196e82c4fd2860354e34cae48e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "934dfe685be54f09bd340ff976009fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5442d6ea3a9e4b4baefc09988f634725",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1701,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1701,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9dc1a81808c44f1aa0b42c2a5f72ed99"
          }
        },
        "0527481a5ec0435290ad08dd09933dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5dac5c04b64043c182d5daab9087bf48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1701/1701 [31:32&lt;00:00,  1.11s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e7b1163e1a44e02acfbea522e3fe8f8"
          }
        },
        "5442d6ea3a9e4b4baefc09988f634725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9dc1a81808c44f1aa0b42c2a5f72ed99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5dac5c04b64043c182d5daab9087bf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e7b1163e1a44e02acfbea522e3fe8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f16dc8cf-09e0-4aaa-e7df-43dd339e0dd4"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 33.53 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba131d0b-3be3-40c4-d0d3-452aa00b6c2e"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e198c39-ffa9-42df-ac9d-84a283dc378a"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4f2c0638-b69b-494e-b18a-0e39a69e419b"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "03d56872-ad7c-4e4d-861f-054661e9b5df"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "2f26619e-2b19-4878-e9c1-1f5aeec2a5e2"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "452d0636-dfbc-4cbf-8ee9-b9493874e794"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6e94d7a9-bb9e-4c7b-a950-4d44a2d5b7b2"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "65758838-e2be-4758-8cf1-4491b7e98d58"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7a9d5ffc-73f4-4413-f117-ce8057618b9a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                        ption - no:2 \n",
              "1    Bryan Singer is now denying accusations about ...\n",
              "2    #BIGNEWS: Today I got know that #ArjunSarja ha...\n",
              "3                                        ption - no:2 \n",
              "4    One Year of #MeToo: How the Movement Eludes Go...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d369527d-d9a8-4866-a878-b33f84f668c2"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 50253356.66B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71923219-78ba-48db-9d81-4753d2db7447"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1913971.05B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "388b1c73-62ad-4ad5-ca99-6c706cb749b3"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae8816ee-781a-4cb0-daad-2c97a46be8ae"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd7b2953-5546-45e6-9841-f75d66de42fc"
      },
      "source": [
        "col_name = \"Justification\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796,
          "referenced_widgets": [
            "99701d74f5b24866a82801c7f39884c9",
            "8eacc7d751a2459aa2e8e498733ae36e",
            "acbc9a375cc84d28a0ccaaddeec60d2b",
            "573cdeefee644de686ec4e21622c7c33",
            "a75a9ce7c84a42e0bb9b6595273a22b4",
            "bfb91675c3da44b088c0ea4275b87b7b",
            "0729574dd58c41b6a9e99926f4c53f68",
            "5a32f27c4ae74edd997e8bbc1da05aba",
            "1353da8779654302bce43ec95670aa54",
            "d8f0c45d195640c4814ff89bfa3bca12",
            "77ac73aed7a1443e8a6d7128fb69e8c0",
            "64f02ee8cf33407696aaeecf1dac6ec5",
            "3ed51ce576474cdfa07556e44874f976",
            "5aac5cd27d284c63a05fbe7fa306f803",
            "d5d2da6cf1d4492591b006d9ffacb042",
            "a941d4265b0d44c7bf399ac4027dace0",
            "9053c184e6f548f3954a6e3d5c3e32b6",
            "e9ab4196e82c4fd2860354e34cae48e5",
            "934dfe685be54f09bd340ff976009fdc",
            "0527481a5ec0435290ad08dd09933dd8",
            "5442d6ea3a9e4b4baefc09988f634725",
            "9dc1a81808c44f1aa0b42c2a5f72ed99",
            "5dac5c04b64043c182d5daab9087bf48",
            "2e7b1163e1a44e02acfbea522e3fe8f8"
          ]
        },
        "outputId": "ccc52a6f-8905-4dda-e63f-2780652e3fc2"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6806\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99701d74f5b24866a82801c7f39884c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1701.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0543\n",
            "Train Losses : [0.007411154452711344, 0.0042834957130253315, 0.0019517461769282818, 0.662787914276123, 0.001543339341878891, 0.0026787682436406612, 0.00408428069204092, 0.006187549792230129, 0.16442932188510895, 0.00916366372257471, 0.010191075503826141, 0.015019414946436882, 0.03070858120918274, 0.02521258220076561, 0.02211599610745907, 0.01948525756597519, 0.01347440481185913, 0.012239125557243824, 0.15690983831882477, 0.4894624948501587, 0.10834725946187973, 0.06752495467662811, 0.014726342633366585, 0.014890817925333977, 0.01947859674692154, 0.08614395558834076, 0.022792983800172806, 0.3030731678009033, 0.020709555596113205, 0.2238401621580124, 0.024351462721824646, 0.028492245823144913, 0.022845478728413582, 0.135244220495224, 0.020492594689130783, 0.02140527404844761, 0.015724817290902138, 0.013911133632063866, 0.011384527198970318, 0.09704623371362686, 0.008744243532419205, 0.007604176644235849, 0.005505813285708427, 0.13733477890491486, 0.08081501722335815, 0.04588593915104866, 0.06066463515162468, 0.058105506002902985, 0.00457210186868906, 0.017314983531832695, 0.011131851933896542, 0.46999064087867737, 0.010620391927659512, 0.4003262519836426, 0.15678569674491882, 0.005427764728665352, 0.006290249526500702, 0.006993436720222235, 0.007725715171545744, 0.007730144076049328, 0.008174034766852856, 0.008324547670781612, 0.008686925284564495, 0.008403440937399864, 0.5475596189498901, 0.009105507284402847, 0.011261075735092163, 0.10459619760513306, 0.012247133068740368, 0.09007568657398224, 0.11347848176956177, 0.01639922894537449, 0.4085347354412079, 0.01791011169552803, 0.03287277743220329, 0.025298474356532097, 0.023736316710710526, 0.02856670692563057, 0.24832786619663239, 0.022940704599022865, 0.018602874130010605, 0.018883300945162773, 0.09655281901359558, 0.014993440359830856, 0.013616824522614479, 0.0733165368437767, 0.009544065222144127, 0.01015964150428772, 0.008445738814771175, 0.12803728878498077, 0.006696262862533331, 0.005775791127234697, 0.00535250548273325, 0.05650854855775833, 0.005055317189544439, 0.005055402871221304, 0.0041252607479691505, 0.00423944229260087, 0.059154026210308075, 0.16174909472465515, 0.004412752576172352, 0.06568879634141922, 0.004063415806740522, 0.00398882944136858, 0.003694811137393117, 0.004173584748059511, 0.09344139695167542, 0.0058420454151928425, 0.005901083815842867, 0.07409670948982239, 0.5662924647331238, 0.00475317845121026, 0.007040569558739662, 0.00719160120934248, 0.007704393472522497, 0.011397182010114193, 0.009071272797882557, 0.01255060825496912, 0.011703797616064548, 0.010989746078848839, 0.010370471514761448, 0.010003984905779362, 0.06422596424818039, 0.16224628686904907, 0.010633126832544804, 0.012016115710139275, 0.014112603850662708, 0.012518737465143204, 0.00824225414544344, 0.009368878789246082, 0.007536132354289293, 0.007909316569566727, 0.12671391665935516, 0.006187557242810726, 0.07973045855760574, 0.005633087363094091, 0.006165789440274239, 0.0050757648423314095, 0.1207103431224823, 0.005625363904982805, 0.004640807397663593, 0.06966320425271988, 0.006196783855557442, 0.004114700946956873, 0.3167082667350769, 0.004883980378508568, 0.1021428257226944, 0.0051057529635727406, 0.005293642170727253, 0.004815502092242241, 0.004404006991535425, 0.00505422055721283, 0.08882229775190353, 0.004533332772552967, 0.1624760627746582, 0.40647682547569275, 0.006405609659850597, 0.06471067667007446, 0.35192641615867615, 0.009908261708915234, 0.08841359615325928, 0.018173974007368088, 0.021214092150330544, 0.023495839908719063, 0.023202311247587204, 0.024455711245536804, 0.024888142943382263, 0.06950830668210983, 0.04742690920829773, 0.02546308934688568, 0.02452295646071434, 0.02273683249950409, 0.018468687310814857, 0.02064281515777111, 0.1430823802947998, 0.01817934960126877, 0.014522631652653217, 0.014535329304635525, 0.012361537665128708, 0.01197419036179781, 0.010271596722304821, 0.18605998158454895, 0.008136311545968056, 0.006926022935658693, 0.007310085464268923, 0.0064139715395867825, 0.0053014433942735195, 0.05563242733478546, 0.004728088621050119, 0.00428979005664587, 0.004260084591805935, 0.003510956186801195, 0.003821789752691984, 0.20659126341342926, 0.18804793059825897, 0.003293719608336687, 0.18338662385940552, 0.12914542853832245, 0.13760444521903992, 0.11478852480649948, 0.004614966455847025, 0.0048456620424985886, 0.0049873837269842625, 0.0056183720007538795, 0.0867610052227974, 0.006023255176842213, 0.006551389116793871, 0.11644557863473892, 0.006694828160107136, 0.006990098860114813, 0.17156384885311127, 0.0073996479623019695, 0.007532656192779541, 0.007537407334893942, 0.007724459748715162, 0.1097588762640953, 0.10395481437444687, 0.007620037999004126, 0.10226963460445404, 0.09667736291885376, 0.3375927209854126, 0.131236270070076, 0.11767945438623428, 0.011789992451667786, 0.013032383285462856, 0.01406835112720728, 0.014631496742367744, 0.01525456178933382, 0.01509107742458582, 0.015180759131908417, 0.015189573168754578, 0.015117621049284935, 0.014457895420491695, 0.013659510761499405, 0.012988034635782242, 0.01222181599587202, 0.011516782455146313, 0.010845469310879707, 0.01007909793406725, 0.2804815173149109, 0.11086350679397583, 0.009557045064866543, 0.12603142857551575, 0.11572770774364471, 0.01010921224951744, 0.01065052580088377, 0.010485349223017693, 0.010580211877822876, 0.010215732268989086, 0.11342500895261765, 0.009922191500663757, 0.10520640760660172, 0.08659365773200989, 0.009431838057935238, 0.009286485612392426, 0.0929882749915123, 0.00910712219774723, 0.00892126839607954, 0.00878538191318512, 0.08937118202447891, 0.10139136761426926, 0.13479392230510712, 0.11338407546281815, 0.1274781972169876, 0.09292352199554443, 0.11927835643291473, 0.00922451913356781, 0.009354555048048496, 0.009465687908232212, 0.009485329501330853, 0.00943216122686863, 0.009292827919125557, 0.10209266096353531, 0.008965743705630302, 0.008808407001197338, 0.008615521714091301, 0.008310012519359589, 0.10721567273139954, 0.007893955335021019, 0.0076284026727080345, 0.007368586026132107, 0.007123276125639677, 0.006804890930652618, 0.0064804209396243095, 0.006188213359564543, 0.005966274533420801, 0.005583428777754307, 0.005323302932083607, 0.005047265440225601, 0.10005859285593033, 0.004563628695905209, 0.1297874003648758, 0.004416976124048233, 0.004315151832997799, 0.43545985221862793, 0.004790974780917168, 0.0054321493953466415, 0.006029497366398573, 0.00620606541633606, 0.006794448010623455, 0.0069513157941401005, 0.006989044137299061, 0.007445572409778833, 0.007333912421017885, 0.00733293266966939, 0.007281923200935125, 0.32683706283569336, 0.12710002064704895, 0.1311551183462143, 0.08622116595506668, 0.010239474475383759, 0.11211449652910233, 0.011377593502402306, 0.13019974529743195, 0.11265318095684052, 0.013445881195366383, 0.11318988353013992, 0.10612326860427856, 0.318637490272522, 0.015872731804847717, 0.07766643166542053, 0.017991013824939728, 0.08480749279260635, 0.2868463397026062, 0.021426517516374588, 0.10611823946237564, 0.02377663180232048, 0.024971021339297295, 0.02523452416062355, 0.0929330512881279, 0.1032099723815918, 0.4398086667060852, 0.02682693675160408, 0.028861945495009422, 0.030457792803645134, 0.03133448213338852, 0.030997026711702347, 0.03138134628534317, 0.14448368549346924, 0.029654812067747116, 0.028303394094109535, 0.026768838986754417, 0.025554439052939415, 0.02372092939913273, 0.2863529622554779, 0.021512843668460846, 0.02080954611301422, 0.019891511648893356, 0.11978206038475037, 0.018108980730175972, 0.10390829294919968, 0.016467830166220665, 0.015576123259961605, 0.2906423509120941, 0.014634039252996445, 0.014523833990097046, 0.0143700260668993, 0.013962212949991226, 0.3008793294429779, 0.10640780627727509, 0.014400798827409744, 0.01461970154196024, 0.08963745832443237, 0.12480929493904114, 0.015334914438426495, 0.015180363319814205, 0.014677931554615498, 0.12734170258045197, 0.014171076007187366, 0.0139368437230587, 0.013452417217195034, 0.08942883461713791, 0.012669374234974384, 0.012223406694829464, 0.011736526153981686, 0.07668747752904892, 0.011341921053826809, 0.010599786415696144, 0.0989132970571518, 0.009632189758121967, 0.28573790192604065, 0.09941598773002625, 0.009921334683895111, 0.010207710787653923, 0.20907865464687347, 0.010563953779637814, 0.01078395638614893, 0.011492053978145123, 0.010805915109813213, 0.01067333947867155, 0.010504746809601784, 0.12285293638706207, 0.010065315291285515, 0.009819271042943, 0.009676598012447357, 0.0091715632006526, 0.00884854793548584, 0.00862954743206501, 0.008081652224063873, 0.007899221032857895, 0.00735874380916357, 0.31519633531570435, 0.11208388209342957, 0.12294001877307892, 0.11593406647443771, 0.008265282027423382, 0.10026025772094727, 0.008846203796565533, 0.009263536892831326, 0.10573244839906693, 0.1142062321305275, 0.12231824547052383, 0.01033402606844902, 0.010369217954576015, 0.010546506382524967, 0.08842993527650833, 0.10146649181842804, 0.0971580445766449, 0.09143979847431183, 0.011238902807235718, 0.011175073683261871, 0.011210396885871887, 0.011090596206486225, 0.32873958349227905, 0.08480346947908401, 0.011965216137468815, 0.012499356642365456, 0.01293858140707016, 0.012888695113360882, 0.012892788276076317, 0.09770384430885315, 0.012943568639457226, 0.012867886573076248, 0.012437768280506134, 0.012180620804429054, 0.011968701146543026, 0.10581061244010925, 0.011261808685958385, 0.010989293456077576, 0.010609659366309643, 0.010077658109366894, 0.10030722618103027, 0.07968113571405411, 0.009199658408761024, 0.13341353833675385, 0.008693945594131947, 0.00871145911514759, 0.008348851464688778, 0.3256072402000427, 0.00838931743055582, 0.10823576152324677, 0.009146193973720074, 0.10746227949857712, 0.00954087171703577, 0.009820044040679932, 0.00972719956189394, 0.009770828299224377, 0.009951626881957054, 0.009653385728597641, 0.3163925111293793, 0.009930932894349098, 0.01037990115582943, 0.08688737452030182, 0.10469035804271698, 0.011428946629166603, 0.11788441985845566, 0.011714443564414978, 0.011862961575388908, 0.011859219521284103, 0.01184056606143713, 0.09971663355827332, 0.11203744262456894, 0.011632491834461689, 0.12390399724245071, 0.011599447578191757, 0.011413492262363434, 0.10185831040143967, 0.011031392961740494, 0.3187382221221924, 0.011453088372945786, 0.01171143539249897, 0.012014003470540047, 0.08938370645046234, 0.11236285418272018, 0.01248115859925747, 0.012522256933152676, 0.012486401945352554, 0.012387985363602638, 0.012278040871024132, 0.08568869531154633, 0.011730358935892582, 0.011471915058791637, 0.011241317726671696, 0.010857445187866688, 0.0105080371722579, 0.010162347927689552, 0.09693274646997452, 0.10168465971946716, 0.00925989169627428, 0.008873999118804932, 0.008656513877213001, 0.0083787702023983, 0.11469165980815887, 0.3184788227081299, 0.008250747807323933, 0.08900436758995056, 0.008786202408373356, 0.37527501583099365, 0.010003122501075268, 0.12290898710489273, 0.011342598125338554, 0.29657381772994995, 0.01328953541815281, 0.01448580902069807, 0.015160247683525085, 0.016002656891942024, 0.0165020190179348, 0.016940075904130936, 0.09576168656349182, 0.017318427562713623, 0.11468977481126785, 0.017269715666770935, 0.01721159927546978, 0.017033616080880165, 0.016498491168022156, 0.016036832705140114, 0.10052002221345901, 0.10484347492456436, 0.12204943597316742, 0.014585967175662518, 0.12103433907032013, 0.10559376329183578, 0.10768609493970871, 0.28401708602905273, 0.1101997047662735, 0.014148293063044548, 0.01451188512146473, 0.014732725918293, 0.014668234623968601, 0.014620369300246239, 0.014426399022340775, 0.10765019804239273, 0.013924016617238522, 0.10767704248428345, 0.013494378887116909, 0.09019307792186737, 0.10567568987607956, 0.012777164578437805, 0.012518202885985374, 0.012215079739689827, 0.011905226856470108, 0.01152429636567831, 0.099251888692379, 0.12254904955625534, 0.010512353852391243, 0.08952277898788452, 0.010080686770379543, 0.009833697229623795, 0.009609260596334934, 0.009253703989088535, 0.00898563303053379, 0.10050518065690994, 0.008443904109299183, 0.008183455094695091, 0.007947375066578388, 0.00770121905952692, 0.1161382794380188, 0.0072401161305606365, 0.007055358961224556, 0.11923585087060928, 0.006696642842143774, 0.006575970444828272, 0.09735886007547379, 0.00632837601006031, 0.1154511570930481, 0.006207427009940147, 0.006184253841638565, 0.006089489907026291, 0.09320218116044998, 0.005973168183118105, 0.005960771348327398, 0.0058954558335244656, 0.0057632033713161945, 0.0056896633468568325, 0.005568657070398331, 0.12384243309497833, 0.0054141078144311905, 0.10037068277597427, 0.10703686624765396, 0.10256845504045486, 0.10027285665273666, 0.005699066445231438, 0.005788788665086031, 0.10102147608995438, 0.10970385372638702, 0.006201440934091806, 0.3454858064651489, 0.10854104161262512, 0.00758138345554471, 0.008209923282265663, 0.12280317395925522, 0.00924771185964346, 0.086454376578331, 0.0853218287229538, 0.0940558910369873, 0.011031433008611202, 0.011401571333408356, 0.11093877255916595, 0.012152559123933315, 0.012270198203623295, 0.012291127815842628, 0.012436355464160442, 0.012363211251795292, 0.012143383733928204, 0.1115412637591362, 0.12644805014133453, 0.011734592728316784, 0.12183427065610886, 0.011388897895812988, 0.011168296448886395, 0.011009912937879562, 0.10164594650268555, 0.010664171539247036, 0.01024527195841074, 0.12084951251745224, 0.009853227064013481, 0.08586814254522324, 0.009553752839565277, 0.12255682796239853, 0.10547488182783127, 0.009182833135128021, 0.00908738374710083, 0.11294233053922653, 0.1102382242679596, 0.009009523317217827, 0.008770746178925037, 0.3175709545612335, 0.00915013812482357, 0.009556219913065434, 0.10871906578540802, 0.00998744461685419, 0.010270154103636742, 0.01038383785635233, 0.010476076044142246, 0.010474407114088535, 0.010452409274876118, 0.10193296521902084, 0.10354908555746078, 0.010109649039804935, 0.010123585350811481, 0.010037959553301334, 0.12498294562101364, 0.009705882519483566, 0.08962837606668472, 0.009447061456739902, 0.009398180991411209, 0.009133150801062584, 0.008912747725844383, 0.11942125111818314, 0.008697559125721455, 0.11903120577335358, 0.008329063653945923, 0.00817911233752966, 0.008029899559915066, 0.10644661635160446, 0.11746221780776978, 0.0077054062858223915, 0.007626093924045563, 0.00755922868847847, 0.007416949607431889, 0.007269763387739658, 0.10811757296323776, 0.10480566322803497, 0.007000131066888571, 0.006932772696018219, 0.006850354839116335, 0.10617326945066452, 0.006724857725203037, 0.006662091240286827, 0.11289980262517929, 0.00656177569180727, 0.006505982484668493, 0.10787037014961243, 0.006455809809267521, 0.11867111921310425, 0.006432258524000645, 0.7222339510917664, 0.007319947704672813, 0.008159435354173183, 0.1001836508512497, 0.009763027541339397, 0.010484158992767334, 0.01113482378423214, 0.10715098679065704, 0.012161805294454098, 0.012567571364343166, 0.012843569740653038, 0.3146440386772156, 0.01380476076155901, 0.014396767131984234, 0.014829984866082668, 0.015158516354858875, 0.015354999341070652, 0.11322717368602753, 0.015493465587496758, 0.015466183423995972, 0.10192914307117462, 0.015247628092765808, 0.10785585641860962, 0.014914647676050663, 0.014638376422226429, 0.01433287188410759, 0.013966615311801434, 0.013571933843195438, 0.013132801279425621, 0.01269480586051941, 0.10228268057107925, 0.011796512641012669, 0.10260789096355438, 0.10386516898870468, 0.010817124508321285, 0.010549006052315235, 0.11304134875535965, 0.10546139627695084, 0.10848820209503174, 0.10178378224372864, 0.009732672944664955, 0.009651553817093372, 0.009513289667665958, 0.009365571662783623, 0.009172485210001469, 0.008951365947723389, 0.11471856385469437, 0.008566289208829403, 0.008400559425354004, 0.11155102401971817, 0.008071748539805412, 0.007927433587610722, 0.0077635194174945354, 0.115312859416008, 0.007487523835152388, 0.007346934638917446, 0.11021225154399872, 0.10819561779499054, 0.007113249506801367, 0.0070571028627455235, 0.006984810344874859, 0.006891205441206694, 0.11279454827308655, 0.11445315927267075, 0.006739762146025896, 0.006719449069350958, 0.006681433878839016, 0.006608440075069666, 0.1088871955871582, 0.006500077899545431, 0.006447537802159786, 0.006381191313266754, 0.006297671236097813, 0.006197260692715645, 0.11493320763111115, 0.11211465299129486, 0.006043950095772743, 0.006029312498867512, 0.0059912195429205894, 0.005935086403042078, 0.005866006948053837, 0.3796835243701935, 0.11669229716062546, 0.006449599750339985, 0.006756317336112261, 0.007011661771684885, 0.3489631116390228, 0.007842914201319218, 0.008399426937103271, 0.10774050652980804, 0.009395299479365349, 0.009818943217396736, 0.11001062393188477, 0.01053297333419323, 0.10417255014181137, 0.01111716777086258, 0.011326084844768047, 0.10751337558031082, 0.011611703783273697, 0.011679936200380325, 0.5945097804069519, 0.01286773569881916, 0.01393110305070877, 0.014861978590488434, 0.10355766862630844, 0.016393164172768593, 0.10296305269002914, 0.0175333209335804, 0.10799646377563477, 0.018277939409017563, 0.10488645732402802, 0.018659895285964012, 0.018692800775170326, 0.01859455555677414, 0.018380645662546158, 0.018066948279738426, 0.017669128254055977, 0.017202865332365036, 0.016682539135217667, 0.01612265594303608, 0.015533524565398693, 0.10386354476213455, 0.014405772089958191, 0.013872232288122177, 0.013328815810382366, 0.10505454242229462, 0.012334108352661133, 0.011875270865857601, 0.011417118832468987, 0.010959403589367867, 0.010508354753255844, 0.01006682775914669, 0.10533348470926285, 0.10699774324893951, 0.1076497957110405, 0.008856130763888359, 0.00865422934293747, 0.10949640721082687, 0.008291814476251602, 0.008126761764287949, 0.007948080077767372, 0.007758390624076128, 0.007560466416180134, 0.00735824927687645, 0.3545820713043213, 0.007351882290095091, 0.10939670354127884, 0.10963243246078491, 0.007912689819931984, 0.10784412175416946, 0.10781674087047577, 0.10765580087900162, 0.008743958547711372, 0.008932564407587051, 0.009066916070878506, 0.009149378165602684, 0.009184502065181732, 0.009177036583423615, 0.009129420854151249, 0.10741221904754639, 0.009015715681016445, 0.008945244364440441, 0.008846321143209934, 0.008720938116312027, 0.1071263924241066, 0.11140540987253189, 0.008444790728390217, 0.008374670520424843, 0.008278390392661095, 0.008158679120242596, 0.10660240054130554, 0.007938646711409092, 0.007832352072000504, 0.1073058471083641, 0.10594112426042557, 0.007626338396221399, 0.10727177560329437, 0.007584779057651758, 0.007558536250144243, 0.1087893694639206, 0.00750007014721632, 0.0074698165990412235, 0.3391623795032501, 0.34029415249824524, 0.11004594713449478, 0.009160580113530159, 0.00980812031775713, 0.010378860868513584, 0.10255887359380722, 0.011341371573507786, 0.011744224466383457, 0.012046722695231438, 0.012268106453120708, 0.012404240667819977, 0.11134954541921616, 0.012543117627501488, 0.012553774751722813, 0.10048846900463104, 0.012468434870243073, 0.1112247183918953, 0.01232960820198059, 0.1084100529551506, 0.3063303828239441, 0.012550534680485725, 0.10178790241479874, 0.013133255764842033, 0.013340527191758156, 0.013445043936371803, 0.10462566465139389, 0.013513307087123394, 0.10551396757364273, 0.013489161618053913, 0.01339756976813078, 0.013263868167996407, 0.01307475846260786, 0.012835528701543808, 0.012560577131807804, 0.012249958701431751, 0.011916978284716606, 0.09929119050502777, 0.11024229973554611, 0.011071715503931046, 0.010828524827957153, 0.1067313700914383, 0.01036619022488594, 0.010137445293366909, 0.009895534254610538, 0.009645060636103153, 0.10658777505159378, 0.32233119010925293, 0.009394438937306404, 0.009548140689730644, 0.10423354059457779, 0.009794948622584343, 0.009872829541563988, 0.10540976375341415, 0.11139552295207977, 0.010054659098386765, 0.010081121698021889, 0.01007165014743805, 0.010017064400017262, 0.009938033297657967, 0.00980372168123722, 0.009651156142354012, 0.10625520348548889, 0.009348392486572266, 0.10776607692241669, 0.009116025641560555, 0.008995886892080307, 0.008850435726344585, 0.10067332535982132, 0.10441569983959198, 0.00851923506706953, 0.008441485464572906, 0.11435817927122116, 0.00827239640057087, 0.1061648428440094, 0.008156110532581806, 0.008089527487754822, 0.11694831401109695, 0.10526730120182037, 0.10544906556606293, 0.11462859809398651, 0.008113081566989422, 0.00816546380519867, 0.10280723869800568, 0.1067095696926117, 0.008304999209940434, 0.10824262350797653, 0.10929876565933228, 0.10112954676151276, 0.10309544950723648, 0.008849932812154293, 0.008970883674919605, 0.009036894887685776, 0.009053916670382023, 0.1040174812078476, 0.10543909668922424, 0.009128047153353691, 0.009136660024523735, 0.009116576053202152, 0.009058095514774323, 0.00897173210978508, 0.008844948373734951, 0.008706456981599331, 0.008539140224456787, 0.008376833982765675, 0.11685886979103088, 0.00805640034377575, 0.007917250506579876, 0.10660970956087112, 0.10272271931171417, 0.007611376233398914, 0.007554924581199884, 0.007458728272467852, 0.007356869522482157, 0.0072325351648032665, 0.007102881092578173, 0.346318781375885, 0.0071866498328745365, 0.007373901084065437, 0.10593102127313614, 0.10662630945444107, 0.007892034016549587, 0.008038340136408806, 0.008149877190589905, 0.008210009895265102, 0.008237269707024097, 0.008240203373134136, 0.008204584009945393, 0.008141237311065197, 0.008038613013923168, 0.007941916584968567, 0.1038290336728096, 0.007738933898508549, 0.007636079099029303, 0.007530590053647757, 0.0074102431535720825, 0.007266717497259378, 0.11947081983089447, 0.007020562421530485, 0.006921695079654455, 0.006800978444516659, 0.006686261855065823, 0.006543991155922413, 0.006401021033525467, 0.006267668213695288, 0.0061129434034228325, 0.005974412430077791, 0.005822557024657726, 0.11706259846687317, 0.005577515810728073, 0.005488647148013115, 0.005385295022279024, 0.005279322154819965, 0.005174694582819939, 0.00506393751129508, 0.004947943612933159, 0.41139888763427734, 0.005049010273069143, 0.00523005984723568, 0.10653245449066162, 0.00557368341833353, 0.005718175787478685, 0.005848937667906284, 0.38507726788520813, 0.00636169221252203, 0.10797075182199478, 0.11657541245222092, 0.007573544513434172, 0.007949572056531906, 0.11872325837612152, 0.008591038174927235, 0.00886099599301815, 0.009090344421565533, 0.10121886432170868, 0.009438581764698029, 0.11227423697710037, 0.009716267697513103, 0.009825914166867733, 0.10962230712175369, 0.00995632354170084, 0.009985188953578472, 0.10529632866382599, 0.10856868326663971, 0.010033567436039448, 0.10776138305664062, 0.10398004949092865, 0.10486363619565964, 0.010239683091640472, 0.010278914123773575, 0.10939116030931473, 0.10394209623336792, 0.31911882758140564, 0.010815172456204891, 0.011190780438482761, 0.011490223929286003, 0.011709459125995636, 0.10371438413858414, 0.012010655365884304, 0.10556886345148087, 0.012207350693643093, 0.012244156561791897, 0.012222887948155403, 0.10657850652933121, 0.012097708880901337, 0.012002889066934586, 0.01186130940914154, 0.10672372579574585, 0.011549483053386211, 0.011373751796782017, 0.011171849444508553, 0.10248259454965591, 0.010765488259494305, 0.10724449902772903, 0.010417875833809376, 0.0102427052333951, 0.010047835297882557, 0.10956894606351852, 0.6319807171821594, 0.31380581855773926, 0.011511112563312054, 0.012532118707895279, 0.013443753123283386, 0.014244625344872475, 0.10717752575874329, 0.015565265901386738, 0.01608443446457386, 0.016475606709718704, 0.016744745895266533, 0.016892049461603165, 0.016946058720350266, 0.10479289293289185, 0.016859881579875946, 0.016735829412937164, 0.2821510434150696, 0.016783038154244423, 0.016919367015361786, 0.016951514407992363, 0.10705400258302689, 0.10531473904848099, 0.10216089338064194, 0.016775429248809814, 0.016658592969179153, 0.10675983130931854, 0.016303345561027527, 0.016073131933808327, 0.10245291143655777, 0.015540463849902153, 0.015244610607624054, 0.10498044639825821, 0.014618903398513794, 0.28700628876686096, 0.10212600231170654, 0.10362864285707474, 0.01465305220335722, 0.10679884254932404, 0.014744843356311321, 0.014718705788254738, 0.014622743241488934, 0.01446670014411211, 0.014258130453526974, 0.10609070956707001, 0.10607535392045975, 0.013629386201500893, 0.10495280474424362, 0.013248802162706852, 0.013038904406130314, 0.012791845947504044, 0.012517184019088745, 0.012218882329761982, 0.011902001686394215, 0.01157261710613966, 0.011233951896429062, 0.010890273377299309, 0.010544874705374241, 0.010199676267802715, 0.1080850213766098, 0.009589278139173985, 0.1076200008392334, 0.009110675193369389, 0.008894633501768112, 0.008671646006405354, 0.00844532810151577, 0.10868372768163681, 0.008049755357205868, 0.34005892276763916, 0.008052785880863667, 0.10656231641769409, 0.008346602320671082, 0.10868081450462341, 0.008602254092693329, 0.10879863053560257, 0.008823088370263577, 0.008903772570192814, 0.008944254368543625, 0.10827137529850006, 0.00898758601397276, 0.008988871239125729, 0.10955303907394409, 0.0089660519734025, 0.1067267656326294, 0.008950740098953247, 0.008928190916776657, 0.008875363506376743, 0.008796945214271545, 0.1057257205247879, 0.008638721890747547, 0.10775954276323318, 0.008518102578818798, 0.008453366346657276, 0.008366599678993225, 0.10960786044597626, 0.008201436139643192, 0.008120459504425526, 0.008021365851163864, 0.007906125858426094, 0.0077774557285010815, 0.007638326846063137, 0.007489633746445179, 0.007334324065595865, 0.007174124009907246, 0.11169742792844772, 0.0069024646654725075, 0.006786257028579712, 0.00666293129324913, 0.006533478852361441, 0.0064000594429671764, 0.11263445019721985, 0.006180712953209877, 0.00608986709266901, 0.005992108955979347, 0.005888674408197403, 0.11186158657073975, 0.005722861271351576, 0.005656158551573753, 0.005581213627010584, 0.005499972030520439, 0.005412822123616934, 0.00532164890319109, 0.11516080796718597, 0.11483415961265564, 0.0051825945265591145, 0.11487957090139389, 0.11422087252140045, 0.1151818260550499, 0.11419317126274109, 0.11403757333755493, 0.005650851875543594, 0.005782566498965025, 0.005887167528271675, 0.11282049119472504, 0.11147811263799667, 0.006217567250132561, 0.0063283625058829784, 0.006410988979041576, 0.11025972664356232, 0.0065561942756175995, 0.35761433839797974, 0.006985183339565992, 0.007307788357138634, 0.10950807482004166, 0.00787923764437437, 0.008124819956719875, 0.008324096910655499, 0.10854451358318329, 0.10822366178035736, 0.008848738856613636, 0.10558206588029861, 0.009163495153188705, 0.009282510727643967, 0.10712048411369324, 0.009456845000386238, 0.009511832147836685, 0.009527584537863731, 0.009504788555204868, 0.009449571371078491, 0.00936630368232727, 0.009258574806153774, 0.00912779662758112, 0.008979746140539646, 0.008816033601760864, 0.0086388373747468, 0.00845441035926342, 0.008261103183031082, 0.10973954945802689, 0.10896072536706924, 0.339914470911026, 0.008080119267106056, 0.008280390873551369, 0.10902298986911774, 0.008611004799604416, 0.00874263234436512, 0.008830483071506023, 0.008879563771188259, 0.008893118239939213, 0.10748087614774704, 0.10733764618635178, 0.00893790740519762, 0.008949590846896172, 0.10812944173812866, 0.008944554254412651, 0.10624672472476959, 0.008941314183175564, 0.008925198577344418, 0.008878814987838268, 0.008805940859019756, 0.008709956891834736, 0.008594700135290623, 0.10642707347869873, 0.008379948325455189, 0.10536231845617294, 0.008218158036470413, 0.00813926849514246, 0.008040419779717922, 0.007927305065095425, 0.007799814455211163, 0.007663240656256676, 0.007515776436775923, 0.10837522894144058, 0.007262278348207474, 0.10857938975095749, 0.34931012988090515, 0.1083066388964653, 0.11119620501995087, 0.00790649838745594, 0.00814990047365427, 0.008343970403075218, 0.008494315668940544, 0.10929585993289948, 0.00873769260942936, 0.00882740132510662, 0.00887882150709629, 0.10672882944345474, 0.00894398894160986, 0.10868595540523529, 0.008997155353426933, 0.0090061379596591, 0.10569708794355392, 0.3339776396751404, 0.00933960173279047, 0.009627070277929306, 0.10307100415229797, 0.01010112464427948, 0.01028432510793209, 0.010412514209747314, 0.010486564598977566, 0.01051601767539978, 0.010503177531063557, 0.01044715940952301, 0.01036461628973484, 0.010246890597045422, 0.010102247819304466, 0.009937952272593975, 0.10847731679677963, 0.009625102393329144, 0.009474099613726139, 0.10803590714931488, 0.009186314418911934, 0.009049683809280396, 0.11022503674030304, 0.008790802210569382, 0.11056897044181824, 0.008593875914812088, 0.11012569069862366, 0.00844654906541109, 0.008371280506253242, 0.3319772183895111, 0.10483687371015549, 0.008765729144215584, 0.008968600071966648, 0.009123851545155048, 0.009230964817106724, 0.009297119453549385, 0.10767675936222076, 0.10929272323846817, 0.10629666596651077, 0.009566795080900192, 0.009627328254282475, 0.009648268111050129, 0.009631572291254997, 0.009582439437508583, 0.00950318481773138, 0.10518644005060196, 0.009340078569948673, 0.009250842966139317, 0.009140287525951862, 0.3336088955402374, 0.32734987139701843, 0.10773034393787384, 0.010303763672709465, 0.01076626218855381, 0.10865721851587296, 0.01154257357120514, 0.01185334101319313, 0.012090539559721947, 0.1046244353055954, 0.012431524693965912, 0.012538486160337925, 0.01258294377475977, 0.012570362538099289, 0.012507720850408077, 0.012398657388985157, 0.012251487001776695, 0.01207034196704626, 0.011860379949212074, 0.01162745337933302, 0.10267118364572525, 0.10335969179868698, 0.10701277107000351, 0.010916419327259064, 0.010778076015412807, 0.010613873600959778, 0.1049172580242157, 0.010291927494108677, 0.010132555849850178, 0.009953842498362064, 0.10854528844356537, 0.009618931449949741, 0.009458829648792744, 0.009284029714763165, 0.00909749697893858, 0.008901326917111874, 0.10552041232585907, 0.008550811558961868, 0.008391995914280415, 0.008223914541304111, 0.11015430092811584, 0.007927925325930119, 0.10614840686321259, 0.007711081299930811, 0.00761179905384779, 0.10647628456354141, 0.007435917854309082, 0.00735554238781333, 0.0072621991857886314, 0.007157087791711092, 0.3588901460170746, 0.007241428829729557, 0.1107868105173111, 0.007583524100482464, 0.34641367197036743, 0.008168078027665615, 0.00855556596070528, 0.008888212963938713, 0.00916701927781105, 0.009392520412802696, 0.1083773747086525, 0.009755246341228485, 0.009893498383462429, 0.32241320610046387, 0.010399855673313141, 0.010747076012194157, 0.011027947068214417, 0.011244126595556736, 0.01139835361391306, 0.011494091711938381, 0.011536814272403717, 0.10593281686306, 0.01155028771609068, 0.011523611843585968, 0.011455566622316837, 0.011350726708769798, 0.011214524507522583, 0.011051121167838573, 0.010864968411624432, 0.010660027153789997, 0.10714059323072433, 0.10758179426193237, 0.01014908216893673, 0.10800664126873016, 0.009903458878397942, 0.00977901741862297, 0.31965744495391846, 0.3227122128009796, 0.010332759469747543, 0.10669548809528351, 0.10651939362287521, 0.011621696874499321, 0.1061529591679573, 0.012311541475355625, 0.10603273659944534, 0.012840285897254944, 0.10313490033149719, 0.013218538835644722, 0.013337078504264355, 0.10311804711818695, 0.013454177416861057, 0.2947382926940918, 0.013814526610076427, 0.10503242164850235, 0.014345556497573853, 0.014522395096719265, 0.01461967546492815, 0.5448991060256958, 0.015554087236523628, 0.01633995585143566, 0.10426513105630875, 0.01761273294687271, 0.26717641949653625, 0.2639082372188568, 0.02004096657037735, 0.2566916346549988, 0.022272828966379166, 0.02334750071167946, 0.02421850524842739, 0.10539419203996658, 0.10366704314947128, 0.025971947237849236, 0.0262796301394701, 0.02640806883573532, 0.02637302502989769, 0.026191143319010735, 0.10559259355068207, 0.02556920051574707, 0.02515006624162197, 0.24484984576702118, 0.024560222402215004, 0.10431891679763794, 0.024144381284713745, 0.02382650226354599, 0.02341749519109726, 0.10631301254034042, 0.022479193285107613, 0.10541242361068726, 0.10515020787715912, 0.021057700738310814, 0.10561827570199966, 0.020128408446907997, 0.01964065246284008, 0.10455458611249924, 0.10376155376434326, 0.018234392628073692, 0.017783788964152336, 0.017305923625826836, 0.016808412969112396, 0.016296658664941788, 0.10438957065343857, 0.01532871276140213, 0.10479236394166946, 0.10445473343133926, 0.014144731685519218, 0.013793198391795158, 0.01342817209661007, 0.013053199276328087, 0.012671810574829578, 0.3018697202205658, 0.10522448271512985, 0.10511698573827744, 0.10517394542694092, 0.104300856590271, 0.10441312938928604, 0.012528879567980766, 0.012553113512694836, 0.0125277666375041, 0.012458087876439095, 0.012349220924079418, 0.012206383980810642, 0.1056821346282959, 0.0119013087823987, 0.0117393359541893, 0.30684155225753784, 0.01170678436756134, 0.10519824922084808, 0.011913605965673923, 0.0119695533066988, 0.011976978741586208, 0.011939934454858303, 0.1054886132478714, 0.10543642938137054, 0.5876621007919312, 0.012537731789052486, 0.013188859447836876, 0.013746440410614014, 0.10419227927923203, 0.28527194261550903, 0.015383544377982616, 0.10422397404909134, 0.016583766788244247, 0.10423511266708374, 0.10434107482433319, 0.10374698042869568, 0.018239999189972878, 0.018491240218281746, 0.10396569967269897, 0.018767019733786583, 0.018801236525177956, 0.018748119473457336, 0.018618343397974968, 0.018419485539197922, 0.10439279675483704, 0.01793179102241993, 0.10423974692821503, 0.0173984132707119, 0.27245771884918213, 0.10426334291696548, 0.01723623089492321, 0.017222221940755844, 0.017136886715888977, 0.01698787696659565, 0.016783660277724266, 0.01653137244284153, 0.016238998621702194, 0.015912223607301712, 0.015558088198304176, 0.10442116111516953, 0.014858979731798172, 0.014513753354549408, 0.01415166724473238, 0.01377594843506813, 0.013390710577368736, 0.012999692000448704, 0.10475805401802063, 0.012275764718651772, 0.011938154697418213, 0.10582173615694046, 0.011314867995679379, 0.011024709790945053, 0.010726443491876125, 0.010422691702842712, 0.010115387849509716, 0.009806131012737751, 0.009496067650616169, 0.009186703711748123, 0.00887886993587017, 0.00857351254671812, 0.008271466940641403, 0.00797310471534729, 0.10909614711999893, 0.10943184047937393, 0.007319102995097637, 0.007162146270275116, 0.006995567120611668, 0.006821007002145052, 0.00664000166580081, 0.006454220041632652, 0.11197011917829514, 0.11194777488708496, 0.0061012329533696175, 0.006034331861883402, 0.11223550140857697, 0.11235115677118301, 0.11213229596614838, 0.006040030159056187, 0.00608807010576129, 0.36821624636650085, 0.006423300132155418, 0.006694781593978405, 0.006925439927726984, 0.1093723401427269, 0.007326498162001371, 0.007496538572013378, 0.0076291379518806934, 0.007726623676717281, 0.007791606709361076, 0.007826954126358032, 0.007835384458303452, 0.007819509133696556, 0.007782162167131901, 0.007725400850176811, 0.007652148604393005, 0.007564195431768894, 0.007463985588401556, 0.007352970540523529, 0.007233302108943462, 0.10939135402441025, 0.11079195886850357, 0.006990588735789061, 0.006938320118933916, 0.0068721650168299675, 0.006794073153287172, 0.006705635227262974, 0.006608325522392988, 0.006503795273602009, 0.11077860742807388, 0.11082252115011215, 0.3658614754676819, 0.11164101958274841, 0.006836365442723036, 0.10970866680145264, 0.11068964749574661, 0.007577782496809959, 0.007796911988407373, 0.007975638844072819, 0.008115638978779316, 0.00821925699710846, 0.0082893380895257, 0.008328503929078579, 0.10815517604351044, 0.33585819602012634, 0.008700814098119736, 0.008971955627202988, 0.009194149635732174, 0.10676045715808868, 0.10664384812116623, 0.009753760881721973, 0.009903078898787498, 0.3197113275527954, 0.010398860089480877, 0.010726668871939182, 0.010992596857249737, 0.011199594475328922, 0.011350767686963081, 0.1057773306965828, 0.011561136692762375, 0.011622694320976734, 0.1051568016409874, 0.10500454902648926, 0.10500286519527435, 0.1050034612417221, 0.011877276934683323, 0.011910776607692242, 0.011900045908987522, 0.011849744245409966, 0.01176409237086773, 0.011647728271782398, 0.01150472555309534, 0.10525389760732651, 0.10484037548303604, 0.10629665106534958, 0.10487765818834305, 0.011030777357518673, 0.010966640897095203, 0.3122374713420868, 0.011087191291153431, 0.011245150119066238, 0.01135209295898676, 0.011411181651055813, 0.1056828573346138, 0.011460571549832821, 0.011453808285295963, 0.10577134042978287, 0.011390993371605873, 0.30691900849342346, 0.011585896834731102]\n",
            "Val loss 0.051472653488025946\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1353da8779654302bce43ec95670aa54",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1701.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0532\n",
            "Train Losses : [0.10563017427921295, 0.011966757476329803, 0.012101431377232075, 0.30312734842300415, 0.01255607046186924, 0.012856396846473217, 0.013085581362247467, 0.01324709877371788, 0.1061139851808548, 0.10429809987545013, 0.013563832268118858, 0.013614874333143234, 0.013612314127385616, 0.10475572198629379, 0.013528164476156235, 0.10425890237092972, 0.013396555557847023, 0.2918670177459717, 0.1055355966091156, 0.10468516498804092, 0.013941596262156963, 0.014080778695642948, 0.014155570417642593, 0.28772255778312683, 0.014492906630039215, 0.014733411371707916, 0.01489709410816431, 0.014990140683948994, 0.015016344375908375, 0.014984133653342724, 0.014898979105055332, 0.014766986481845379, 0.014594470150768757, 0.014387696050107479, 0.2886839210987091, 0.10429797321557999, 0.014342186041176319, 0.10380469262599945, 0.10374270379543304, 0.014474420808255672, 0.014468110166490078, 0.014410136267542839, 0.014304785057902336, 0.10462800413370132, 0.014042390510439873, 0.29223349690437317, 0.014050782658159733, 0.01414941344410181, 0.014186996035277843, 0.014169241301715374, 0.01410288643091917, 0.013991964049637318, 0.10438402742147446, 0.10419980436563492, 0.013632573187351227, 0.013502496294677258, 0.10391774028539658, 0.013207945041358471, 0.10433885455131531, 0.012916775420308113, 0.10649825632572174, 0.01263204962015152, 0.012477099895477295, 0.012297391891479492, 0.012096807360649109, 0.01187914703041315, 0.10475757718086243, 0.011461775749921799, 0.011259952560067177, 0.011045790277421474, 0.10525447130203247, 0.3158966600894928, 0.010767214000225067, 0.10518433898687363, 0.010938218794763088, 0.010988332331180573, 0.01099917571991682, 0.010974777862429619, 0.30956318974494934, 0.011150689795613289, 0.011327161453664303, 0.10578461736440659, 0.011585447005927563, 0.10660482197999954, 0.011764864437282085, 0.1044209748506546, 0.011876223608851433, 0.10620301216840744, 0.011930705048143864, 0.01192349474877119, 0.1048826351761818, 0.011857599020004272, 0.1058792769908905, 0.10466669499874115, 0.011758120730519295, 0.10692445188760757, 0.01168923731893301, 0.011631444096565247, 0.01154279988259077, 0.10495512932538986, 0.10514642298221588, 0.011288895271718502, 0.011204788461327553, 0.1051294282078743, 0.011018027551472187, 0.010914725251495838, 0.010789231397211552, 0.010644754394888878, 0.01048360113054514, 0.010309281758964062, 0.010123822838068008, 0.009929726831614971, 0.009729108773171902, 0.009523581713438034, 0.009314730763435364, 0.009103992953896523, 0.008892814628779888, 0.33325302600860596, 0.008756699040532112, 0.10680890828371048, 0.008866139687597752, 0.10790175199508667, 0.6491369009017944, 0.009611274115741253, 0.1052703782916069, 0.010782524012029171, 0.011293270625174046, 0.011734231375157833, 0.01210528053343296, 0.012408832088112831, 0.012647554278373718, 0.012824850156903267, 0.012944185175001621, 0.10607902705669403, 0.01308563631027937, 0.013110540807247162, 0.10396097600460052, 0.2961587905883789, 0.01336382795125246, 0.013572601601481438, 0.013716665096580982, 0.013801979832351208, 0.10631173849105835, 0.013874691911041737, 0.10479579120874405, 0.10387768596410751, 0.013886209577322006, 0.10327321290969849, 0.013840166851878166, 0.013780754990875721, 0.013682612217962742, 0.013549304567277431, 0.013387074694037437, 0.013198626227676868, 0.10424220561981201, 0.10525815933942795, 0.012680161744356155, 0.012517674826085567, 0.10593637079000473, 0.012187055312097073, 0.012018273584544659, 0.011831087991595268, 0.10720434039831161, 0.011468591168522835, 0.011291790753602982, 0.011101155541837215, 0.01090021152049303, 0.010689872317016125, 0.010473879054188728, 0.010253398679196835, 0.3201637864112854, 0.010086624883115292, 0.010109965689480305, 0.010103104636073112, 0.010069907642900944, 0.010012052953243256, 0.009933140128850937, 0.009835777804255486, 0.009722442366182804, 0.009595395065844059, 0.009456896223127842, 0.009308685548603535, 0.009153035469353199, 0.10769183188676834, 0.10741116851568222, 0.008788117207586765, 0.10779348760843277, 0.00863056443631649, 0.008554048836231232, 0.008464647457003593, 0.33763331174850464, 0.008513485081493855, 0.00862891785800457, 0.0087123466655612, 0.10800950229167938, 0.10734956711530685, 0.008928569033741951, 0.008987436071038246, 0.009017743170261383, 0.10719789564609528, 0.0090497937053442, 0.1071200966835022, 0.009077208116650581, 0.1071891188621521, 0.009100348688662052, 0.009098018519580364, 0.009072784334421158, 0.009026717394590378, 0.008962653577327728, 0.008882559835910797, 0.008788586594164371, 0.008683070540428162, 0.008567104116082191, 0.107673279941082, 0.008356984704732895, 0.008260252885520458, 0.008154022507369518, 0.008039969950914383, 0.10974915325641632, 0.10824253410100937, 0.0077890632674098015, 0.007728178054094315, 0.10891473293304443, 0.10966461151838303, 0.10864541679620743, 0.007626018021255732, 0.0076247393153607845, 0.007605808787047863, 0.007571707479655743, 0.007523332256823778, 0.007462897803634405, 0.007391893304884434, 0.0073120249435305595, 0.007224153261631727, 0.007130029611289501, 0.007030060980468988, 0.00692611513659358, 0.006818482652306557, 0.11028432101011276, 0.006637257058173418, 0.1116471141576767, 0.11056307703256607, 0.11035652458667755, 0.11136437952518463, 0.006568456068634987, 0.006593617144972086, 0.006602463778108358, 0.006596431601792574, 0.0065768007189035416, 0.006545618176460266, 0.006503769662231207, 0.006452674511820078, 0.0063935560174286366, 0.006327506620436907, 0.006255720742046833, 0.006178770214319229, 0.006097755394876003, 0.00601324625313282, 0.11341553926467896, 0.00587630458176136, 0.00582019193097949, 0.005758538842201233, 0.005692478269338608, 0.005622735247015953, 0.11323808878660202, 0.005512426141649485, 0.005468647927045822, 0.11346451193094254, 0.11343513429164886, 0.005416153464466333, 0.005417782347649336, 0.11314765363931656, 0.005429558455944061, 0.005437920801341534, 0.11349746584892273, 0.11286884546279907, 0.005512290168553591, 0.11250127106904984, 0.005609145853668451, 0.11336979269981384, 0.005722014233469963, 0.3752337098121643, 0.11214488744735718, 0.00629789475351572, 0.006533887702971697, 0.11158782988786697, 0.3541406989097595, 0.007379419635981321, 0.007762293331325054, 0.008101783692836761, 0.008397845551371574, 0.008650852367281914, 0.00886210985481739, 0.009033102542161942, 0.009166422300040722, 0.009264099411666393, 0.009329319931566715, 0.009364113211631775, 0.10789862275123596, 0.009400968439877033, 0.009402971714735031, 0.009381004609167576, 0.009337151423096657, 0.009274164214730263, 0.10751832276582718, 0.009144987910985947, 0.009078267961740494, 0.008995844051241875, 0.10727255791425705, 0.00883763562887907, 0.10762089490890503, 0.10740343481302261, 0.008698097430169582, 0.008661575615406036, 0.1082645133137703, 0.008583199232816696, 0.10768614709377289, 0.008525996468961239, 0.10855341702699661, 0.008486596867442131, 0.008460775949060917, 0.008417048491537571, 0.008357786573469639, 0.00828480627387762, 0.10794060677289963, 0.008148263208568096, 0.33936965465545654, 0.008256127126514912, 0.00839494913816452, 0.10807891190052032, 0.008622059598565102, 0.008710935711860657, 0.10828417539596558, 0.008847121149301529, 0.008895182982087135, 0.328275203704834, 0.1067405641078949, 0.009432750754058361, 0.00964698288589716, 0.10754311084747314, 0.009995754808187485, 0.010131168179214, 0.010227459482848644, 0.010287688113749027, 0.010314131155610085, 0.010310700163245201, 0.010280286893248558, 0.010225187987089157, 0.010149029083549976, 0.01005405280739069, 0.10614821314811707, 0.009864461608231068, 0.009769381023943424, 0.00965893268585205, 0.009536047466099262, 0.10817096382379532, 0.0093057407066226, 0.009195785969495773, 0.009075472131371498, 0.008945660665631294, 0.008808184415102005, 0.10737080872058868, 0.008561215363442898, 0.008448561653494835, 0.008327995426952839, 0.008201243355870247, 0.008069691248238087, 0.007934286259114742, 0.007795996963977814, 0.007655864581465721, 0.007514586206525564, 0.34924909472465515, 0.007466976996511221, 0.10813191533088684, 0.007623586803674698, 0.3420959413051605, 0.00796503759920597, 0.10771190375089645, 0.008449878543615341, 0.008655043318867683, 0.008821964263916016, 0.008953460492193699, 0.00905134342610836, 0.10593533515930176, 0.009199470281600952, 0.009252308867871761, 0.10914319008588791, 0.009320419281721115, 0.009337380528450012, 0.10811112821102142, 0.10790347307920456, 0.009379768744111061, 0.10663039237260818, 0.1064566820859909, 0.009467110969126225, 0.10644233226776123, 0.009527354501187801, 0.00953994132578373, 0.009527099318802357, 0.00949120707809925, 0.10666485130786896, 0.009407071396708488, 0.6393638849258423, 0.009862530045211315, 0.10740116238594055, 0.10571431368589401, 0.011170519515872002, 0.10487803816795349, 0.011885016225278378, 0.01217429619282484, 0.10604684799909592, 0.10456722229719162, 0.012852231040596962, 0.10467645525932312, 0.01317758671939373, 0.103624626994133, 0.10526964068412781, 0.10454568266868591, 0.10321494936943054, 0.013740507885813713, 0.013802576810121536, 0.10432972759008408, 0.013845169916749, 0.01382640190422535, 0.01376791950315237, 0.013674823567271233, 0.01355217956006527, 0.10312075912952423, 0.5609869360923767, 0.013780297711491585, 0.10531990975141525, 0.01460246928036213, 0.014926905743777752, 0.1038428395986557, 0.10388143360614777, 0.015629243105649948, 0.015779538080096245, 0.01586805284023285, 0.015896551311016083, 0.015873562544584274, 0.015802403911948204, 0.10385622084140778, 0.2819974720478058, 0.015763547271490097, 0.01586642861366272, 0.015908844769001007, 0.10253066569566727, 0.01589646190404892, 0.01584153063595295, 0.01574471779167652, 0.01560906507074833, 0.1034252867102623, 0.015294783748686314, 0.10256883502006531, 0.014971865341067314, 0.014793113805353642, 0.10346669703722, 0.014418244361877441, 0.014223091304302216, 0.01400638185441494, 0.013772445730865002, 0.10205023735761642, 0.013320035301148891, 0.013097895309329033, 0.10803240537643433, 0.10551842302083969, 0.012509357184171677, 0.10326185077428818, 0.012192357331514359, 0.1029885783791542, 0.10620305687189102, 0.10335388034582138, 0.01173083670437336, 0.2988910377025604, 0.10160916298627853, 0.01193130575120449, 0.012036099098622799, 0.012095974758267403, 0.012115278281271458, 0.012104191817343235, 0.012061260640621185, 0.10543914139270782, 0.011939202435314655, 0.01186839584261179, 0.011766416020691395, 0.104710653424263, 0.011557645164430141, 0.10840638726949692, 0.01136747281998396, 0.011262541636824608, 0.011143065057694912, 0.10525156557559967, 0.3107714354991913, 0.011039330624043941, 0.011132346466183662, 0.01119371596723795, 0.10872255265712738, 0.011253965087234974, 0.011260204948484898, 0.10471278429031372, 0.011234789155423641, 0.011206354945898056, 0.011147492565214634, 0.011064969003200531, 0.10567929595708847, 0.01089667621999979, 0.10170825570821762, 0.010739973746240139, 0.10797189176082611, 0.010595680214464664, 0.10552152991294861, 0.010468544438481331, 0.3169490694999695, 0.010562174022197723, 0.30826544761657715, 0.011014570482075214, 0.10907167941331863, 0.10085967183113098, 0.011840981431305408, 0.01205483265221119, 0.012218312360346317, 0.012335128150880337, 0.012404137291014194, 0.01242863666266203, 0.01242357399314642, 0.012394934892654419, 0.012326229363679886, 0.10358008742332458, 0.10646355897188187, 0.10357212275266647, 0.10700064897537231, 0.10697547346353531, 0.012088130228221416, 0.012060309760272503, 0.012004596181213856, 0.011928665451705456, 0.10189473628997803, 0.011745273135602474, 0.011642057448625565, 0.01153113879263401, 0.10188483446836472, 0.011292063631117344, 0.011164535768330097, 0.10559290647506714, 0.10830085724592209, 0.010848959907889366, 0.010752959176898003, 0.010640863329172134, 0.010515131056308746, 0.010373394936323166, 0.10487458109855652, 0.11094121634960175, 0.010039304383099079, 0.311996728181839, 0.010068338364362717, 0.10536849498748779, 0.010274435393512249, 0.01034192182123661, 0.01037361565977335, 0.010390101000666618, 0.10737971216440201, 0.10349532216787338, 0.10276363790035248, 0.010435889475047588, 0.10511685162782669, 0.010467791929841042, 0.01046399399638176, 0.11080241203308105, 0.010431800037622452, 0.010398204438388348, 0.10456378012895584, 0.010316935367882252, 0.11153502762317657, 0.1042870506644249, 0.10689298808574677, 0.010250710882246494, 0.01024329848587513, 0.010207446292042732, 0.010151534341275692, 0.010080388747155666, 0.10330522060394287, 0.009926490485668182, 0.009844023734331131, 0.009758698754012585, 0.009645876474678516, 0.10695446282625198, 0.009449978359043598, 0.6389933228492737, 0.009764593094587326, 0.010122760199010372, 0.10709384828805923, 0.010730152949690819, 0.10708099603652954, 0.011233845725655556, 0.1066911593079567, 0.01162462867796421, 0.011772723868489265, 0.10549917072057724, 0.01199914701282978, 0.012067412957549095, 0.1009085476398468, 0.012138396501541138, 0.01215004175901413, 0.1042693480849266, 0.012121862731873989, 0.10367918759584427, 0.10530120879411697, 0.012069030664861202, 0.012029414996504784, 0.011974618770182133, 0.011897193267941475, 0.011790789663791656, 0.011667652986943722, 0.011525170877575874, 0.011377287097275257, 0.011216283775866032, 0.011050824075937271, 0.01087065041065216, 0.010679281316697598, 0.11191457509994507, 0.010348583571612835, 0.01019352301955223, 0.10134169459342957, 0.009903432801365852, 0.009768672287464142, 0.009625150822103024, 0.009468844160437584, 0.009320891462266445, 0.11060243099927902, 0.11207250505685806, 0.008959951810538769, 0.008862465620040894, 0.008767981082201004, 0.008646978065371513, 0.008530240505933762, 0.00841115415096283, 0.008291207253932953, 0.10546904057264328, 0.10947617888450623, 0.007997560314834118, 0.3395933508872986, 0.3347001075744629, 0.1063028872013092, 0.008695251308381557, 0.00897662341594696, 0.00920175015926361, 0.009391712956130505, 0.33047884702682495, 0.00989762507379055, 0.10480530560016632, 0.10634010285139084, 0.01078104693442583, 0.011015263386070728, 0.30064427852630615, 0.011596300639212132, 0.011928931809961796, 0.10531184822320938, 0.10439468175172806, 0.012715962715446949, 0.10514600574970245, 0.013102340511977673, 0.10976952314376831, 0.10702884942293167, 0.01352213229984045, 0.013606277294456959, 0.01365166250616312, 0.10244599729776382, 0.2995692789554596, 0.2887074947357178, 0.014365305192768574, 0.10454520583152771, 0.10463155061006546, 0.015399900265038013, 0.0976046547293663, 0.01588887721300125, 0.09988513588905334, 0.016223421320319176, 0.01632741279900074, 0.1049063503742218, 0.10445629805326462, 0.1040656566619873, 0.016488898545503616, 0.016470544040203094, 0.016412977129220963, 0.016315778717398643, 0.016177427023649216, 0.016022130846977234, 0.015821406617760658, 0.10136989504098892, 0.015412374399602413, 0.2819311320781708, 0.015246818773448467, 0.015230194665491581, 0.01518682949244976, 0.015111425891518593, 0.014993797056376934, 0.10507984459400177, 0.01472547184675932, 0.10242857038974762, 0.014446582645177841, 0.014296991750597954, 0.014128993265330791, 0.013937861658632755, 0.10380494594573975, 0.013547948561608791, 0.013366728089749813, 0.013159053400158882, 0.012953348457813263, 0.012733577750623226, 0.012490415014326572, 0.012261005118489265, 0.012036732397973537, 0.011795920319855213, 0.011554328724741936, 0.011312245391309261, 0.011079528369009495, 0.010834509506821632, 0.31488844752311707, 0.11256668716669083, 0.010627866722643375, 0.010608742013573647, 0.010574308224022388, 0.01052483543753624, 0.010453242808580399, 0.010372903198003769, 0.01027193758636713, 0.010166889056563377, 0.10312272608280182, 0.10475540161132812, 0.009879978373646736, 0.10868750512599945, 0.10905998945236206, 0.10966194421052933, 0.11114072799682617, 0.009730872698128223, 0.009710175916552544, 0.009687742218375206, 0.009632637724280357, 0.00956985354423523, 0.1068088486790657, 0.009452885948121548, 0.009386642836034298, 0.009308543056249619, 0.10457740724086761, 0.009149369783699512, 0.00907780323177576, 0.008991938084363937, 0.008897226303815842, 0.008796419948339462, 0.008686785586178303, 0.008577447384595871, 0.008448776789009571, 0.10550254583358765, 0.10976038128137589, 0.008184121921658516, 0.11092797666788101, 0.008063321933150291, 0.00801861472427845, 0.007954765111207962, 0.007879958488047123, 0.10893072932958603, 0.007750056684017181, 0.1044018417596817, 0.0076635838486254215, 0.007615640759468079, 0.007557698991149664, 0.35190051794052124, 0.34052595496177673, 0.007944241166114807, 0.008218827657401562, 0.111431784927845, 0.008686835877597332, 0.008892860263586044, 0.009048327803611755, 0.009186840616166592, 0.009284740313887596, 0.10541042685508728, 0.009428703226149082, 0.009486018680036068, 0.009521358646452427, 0.10615624487400055, 0.009547709487378597, 0.1073128953576088, 0.32213249802589417, 0.009779899381101131, 0.009958429262042046, 0.010093222372233868, 0.10866707563400269, 0.010305551812052727, 0.010386391542851925, 0.010433162562549114, 0.10268358886241913, 0.010492982342839241, 0.010504958219826221, 0.010483204387128353, 0.010442313738167286, 0.010394413024187088, 0.010327060706913471, 0.010232041589915752, 0.010132728144526482, 0.010023403912782669, 0.10344938188791275, 0.1100856140255928, 0.32186511158943176, 0.1039629653096199, 0.010036475956439972, 0.10363533347845078, 0.010258417576551437, 0.01034319307655096, 0.010394972749054432, 0.010431180708110332, 0.010420278646051884, 0.010410088114440441, 0.010367745533585548, 0.010308398865163326, 0.10664176195859909, 0.010175188072025776, 0.010106909088790417, 0.010034214705228806, 0.009932843036949635, 0.009836588054895401, 0.009716486558318138, 0.11455921083688736, 0.11032701283693314, 0.009446058422327042, 0.009375945664942265, 0.1065097227692604, 0.009231604635715485, 0.10721147805452347, 0.009105164557695389, 0.009048578329384327, 0.10559888929128647, 0.10571524500846863, 0.11107291281223297, 0.008912110701203346, 0.008888158947229385, 0.008859029039740562, 0.008812041021883488, 0.10505576431751251, 0.008713750168681145, 0.10583335906267166, 0.10760314017534256, 0.008636419661343098, 0.00861884281039238, 0.3251115679740906, 0.11350982636213303, 0.008897269144654274, 0.10944485664367676, 0.10614420473575592, 0.00929634552448988, 0.009412546642124653, 0.009487556293606758, 0.009547901339828968, 0.009573758579790592, 0.10977447777986526, 0.1126786544919014, 0.1041855737566948, 0.009691504761576653, 0.009723694063723087, 0.10981437563896179, 0.00975016225129366, 0.00975106842815876, 0.009728597477078438, 0.0096876360476017, 0.009636078029870987, 0.00957118533551693, 0.00949510745704174, 0.009405994787812233, 0.009299811907112598, 0.10248404741287231, 0.009115448221564293, 0.009029478766024113, 0.008931856602430344, 0.008828746154904366, 0.10504517704248428, 0.008643235079944134, 0.008557136170566082, 0.6481049656867981, 0.10479017347097397, 0.10943363606929779, 0.10884921997785568, 0.009753049351274967, 0.010026302188634872, 0.01025818008929491, 0.01044034119695425, 0.10887264460325241, 0.010745860636234283, 0.010858087800443172, 0.010931163094937801, 0.010988025926053524, 0.011002402752637863, 0.011008422821760178, 0.010977767407894135, 0.010930431075394154, 0.10677068680524826, 0.10192418843507767, 0.010803453624248505, 0.10794167965650558, 0.11278054863214493, 0.010730914771556854, 0.010694646276533604, 0.0106440968811512, 0.010580324567854404, 0.010500181466341019, 0.010406827554106712, 0.01030823215842247, 0.010191602632403374, 0.010074392892420292, 0.10430557280778885, 0.009842179715633392, 0.009737449698150158, 0.009618405252695084, 0.11043689399957657, 0.009406779892742634, 0.10752521455287933, 0.009237196296453476, 0.009154834784567356, 0.009063720703125, 0.008968517184257507, 0.008863670751452446, 0.008753913454711437, 0.008644729852676392, 0.10613091289997101, 0.00843866728246212, 0.008347902446985245, 0.008250338025391102, 0.00814884901046753, 0.008045529946684837, 0.3369920551776886, 0.00801357626914978, 0.008068406954407692, 0.008103848434984684, 0.10669448226690292, 0.00815459992736578, 0.10633016377687454, 0.008204066194593906, 0.008219774812459946, 0.008219695650041103, 0.008205405436456203, 0.008176197297871113, 0.008135945536196232, 0.00808588694781065, 0.008028573356568813, 0.10977327823638916, 0.007921434938907623, 0.11123257130384445, 0.007849191315472126, 0.0078093199990689754, 0.007762384135276079, 0.007709618657827377, 0.007649964652955532, 0.007577032782137394, 0.0075040413066744804, 0.007424595765769482, 0.007345377933233976, 0.007257257122546434, 0.007170494180172682, 0.007079895120114088, 0.0069877225905656815, 0.1107492670416832, 0.10826408118009567, 0.0067987507209181786, 0.10749416798353195, 0.10779997706413269, 0.006732712499797344, 0.006723774131387472, 0.006704864092171192, 0.36561518907546997, 0.006816404405981302, 0.006932858377695084, 0.007027437444776297, 0.1078508198261261, 0.0071899364702403545, 0.007257481571286917, 0.0073064579628407955, 0.11212142556905746, 0.007387862540781498, 0.0074197654612362385, 0.11427421122789383, 0.1073349341750145, 0.007521981373429298, 0.007554118987172842, 0.10728499293327332, 0.6785824298858643, 0.008003954775631428, 0.008363689295947552, 0.008683324791491032, 0.3385362923145294, 0.009395409375429153, 0.009780751541256905, 0.10528821498155594, 0.010446888394653797, 0.010728266090154648, 0.010965107940137386, 0.011159908957779408, 0.01131461188197136, 0.01143261045217514, 0.01151659619063139, 0.10338333249092102, 0.011628439649939537, 0.01165839098393917, 0.011661862954497337, 0.011641155928373337, 0.10178516060113907, 0.10251078009605408, 0.10397450625896454, 0.011564088985323906, 0.011542897671461105, 0.011499554850161076, 0.011438438668847084, 0.011360730975866318, 0.011268550530076027, 0.011162781156599522, 0.3047906756401062, 0.10611536353826523, 0.011199905537068844, 0.011249595321714878, 0.01127246581017971, 0.10379599034786224, 0.01128191128373146, 0.10319028049707413, 0.10734786838293076, 0.011284686625003815, 0.011275154538452625, 0.10955259203910828, 0.011230318807065487, 0.011194893158972263, 0.10357590764760971, 0.011106031015515327, 0.011052093468606472, 0.010982821695506573, 0.010897073894739151, 0.10503185540437698, 0.01073042768985033, 0.10847657918930054, 0.010582296177744865, 0.010505934245884418, 0.010417734272778034, 0.010319551452994347, 0.11021113395690918, 0.010130418464541435, 0.107240229845047, 0.10642596334218979, 0.009927581064403057, 0.10488414019346237, 0.009830274619162083, 0.10597647726535797, 0.009748405776917934, 0.009702020324766636, 0.6312885284423828, 0.009991195052862167, 0.01029436755925417, 0.10321467369794846, 0.10302063077688217, 0.011048204265534878, 0.011249185539782047, 0.10078643262386322, 0.011569871567189693, 0.10595918446779251, 0.011812790296971798, 0.01189989410340786, 0.011956088244915009, 0.011986942030489445, 0.10331753641366959, 0.011992378160357475, 0.011977904476225376, 0.011943114921450615, 0.01188608817756176, 0.011811919510364532, 0.011722980998456478, 0.011620189994573593, 0.11109476536512375, 0.011416980996727943, 0.1037793979048729, 0.011236622929573059, 0.011144384741783142, 0.011041035875678062, 0.1113305613398552, 0.10992821305990219, 0.010775579139590263, 0.11116590350866318, 0.10741868615150452, 0.11109031736850739, 0.01058732345700264, 0.01055126078426838, 0.10224685072898865, 0.010466068051755428, 0.010416897013783455, 0.1073136031627655, 0.010311608202755451, 0.10400890558958054, 0.01021668966859579, 0.010164919309318066, 0.010100223124027252, 0.10288184136152267, 0.009969713166356087, 0.10665085911750793, 0.00985810998827219, 0.00980041828006506, 0.009731621481478214, 0.009652510285377502, 0.10558802634477615, 0.11186013370752335, 0.009460560977458954, 0.00940726138651371, 0.1042577400803566, 0.00930088758468628, 0.00924697332084179, 0.009182766079902649, 0.009109369479119778, 0.009027588181197643, 0.00894139800220728, 0.10547030717134476, 0.1057950034737587, 0.008735493756830692, 0.10442382842302322, 0.008647755719721317, 0.10554666072130203, 0.008579467423260212, 0.008544445969164371, 0.008498544804751873, 0.008443577215075493, 0.10536280274391174, 0.008340051397681236, 0.008290340192615986, 0.1056516170501709, 0.10665743052959442, 0.11338964849710464, 0.1136729046702385, 0.008206714875996113, 0.008211866021156311, 0.008203934878110886, 0.00818365253508091, 0.008151425048708916, 0.1083131730556488, 0.33747342228889465, 0.00822810921818018, 0.10822611302137375, 0.10548272728919983, 0.008586117066442966, 0.008685109205543995, 0.10588934272527695, 0.33422157168388367, 0.009082266129553318, 0.10575645416975021, 0.10333801060914993, 0.00968189537525177, 0.009845895692706108, 0.10363893955945969, 0.010111099109053612, 0.010213625617325306, 0.010289215482771397, 0.01033856626600027, 0.10441552102565765, 0.30755549669265747, 0.10349385440349579, 0.10840296745300293, 0.10340014845132828, 0.10731688141822815, 0.011349658481776714, 0.10369665175676346, 0.011634697206318378, 0.011741673573851585, 0.01181462500244379, 0.10897018760442734, 0.011912724003195763, 0.10138195753097534, 0.011968851089477539, 0.011974511668086052, 0.011956989765167236, 0.011917363852262497, 0.10317730158567429, 0.011818156577646732, 0.011755896732211113, 0.01168280839920044, 0.011592686176300049, 0.011486243456602097, 0.011376293376088142, 0.11005263775587082, 0.011157632805407047, 0.011050381697714329, 0.10466407239437103, 0.010843372903764248, 0.10294703394174576, 0.010664112865924835, 0.010574989952147007, 0.11242564022541046, 0.010403351858258247, 0.3108580708503723, 0.010405081324279308, 0.010465094819664955, 0.01050130557268858, 0.010514218360185623, 0.01050723809748888, 0.31954461336135864, 0.010625842958688736, 0.010736416094005108, 0.010817201808094978, 0.10501108318567276, 0.3023168742656708, 0.011150448583066463, 0.011329283006489277, 0.011471142992377281, 0.011577759869396687, 0.011653194203972816, 0.011699363589286804, 0.011720441281795502, 0.011715242639183998, 0.011689508333802223, 0.11146542429924011, 0.011616168543696404, 0.11089630424976349, 0.011539008468389511, 0.011491349898278713, 0.011425801552832127, 0.011346814222633839, 0.11005724221467972, 0.3158179819583893, 0.01128997653722763, 0.011363725177943707, 0.10383736342191696, 0.108411505818367, 0.01152084767818451, 0.10229462385177612, 0.011594580486416817, 0.011610018089413643, 0.011601869016885757, 0.011574404314160347, 0.01152734737843275, 0.10249297320842743, 0.011418980546295643, 0.10863596946001053, 0.10782674700021744, 0.11046332120895386, 0.011273696087300777, 0.011243493296205997, 0.011195006780326366, 0.102166548371315, 0.011085395701229572, 0.011025399900972843, 0.010950962081551552, 0.010865571908652782, 0.010769393295049667, 0.10456585139036179, 0.30581897497177124, 0.010674204677343369, 0.01072861347347498, 0.1046084612607956, 0.010807993821799755, 0.010832645930349827, 0.010825536213815212, 0.010807746089994907, 0.010768542066216469, 0.3232346773147583, 0.010836178436875343, 0.10414424538612366, 0.011010785587131977, 0.011072291992604733, 0.31476736068725586, 0.10797635465860367, 0.10301034152507782, 0.011678586713969707, 0.011830532923340797, 0.01193822268396616, 0.012014188803732395, 0.012069267220795155, 0.012094785459339619, 0.012085063382983208, 0.012059982866048813, 0.012018191628158092, 0.10299336910247803, 0.10097496956586838, 0.011874471791088581, 0.01182599738240242, 0.011756650172173977, 0.011681588366627693, 0.011596808210015297, 0.10249454528093338, 0.10452387481927872, 0.10346335172653198, 0.011305650696158409, 0.01123439148068428, 0.01116145309060812, 0.10117010772228241, 0.011003716848790646, 0.010942161083221436, 0.01084110140800476, 0.10546864569187164, 0.010676763020455837, 0.01058568712323904, 0.010485563427209854, 0.10358921438455582, 0.10414900630712509, 0.010251506231725216, 0.01018889807164669, 0.3189113736152649, 0.010193620808422565, 0.010240682400763035, 0.10208399593830109, 0.10380440205335617, 0.01038519199937582, 0.1105399951338768, 0.10807473212480545, 0.01050431840121746, 0.010540932416915894, 0.010536358691751957, 0.010531335137784481, 0.01049188431352377, 0.10842074453830719, 0.010422702878713608, 0.010389097034931183, 0.01033025048673153, 0.010261488147079945, 0.010181811638176441, 0.010111277922987938, 0.010006285272538662, 0.11167141795158386, 0.1008705422282219, 0.10717325657606125, 0.009742114692926407, 0.009695460088551044, 0.009635837748646736, 0.009565838612616062, 0.009504121728241444, 0.009414528496563435, 0.009321215562522411, 0.009241524152457714, 0.009131859056651592, 0.10188335180282593, 0.008950664661824703, 0.008874925784766674, 0.00878913328051567, 0.008695278316736221, 0.11377851665019989, 0.008523565717041492, 0.008449799381196499, 0.008380511775612831, 0.008284306153655052, 0.008206252008676529, 0.008109243586659431, 0.008021796122193336, 0.007930494844913483, 0.3413943350315094, 0.007887205109000206, 0.34078043699264526, 0.1098337396979332, 0.10888532549142838, 0.008468382991850376, 0.10722670704126358, 0.008765947073698044, 0.008894401602447033, 0.10378921031951904, 0.0090944804251194, 0.10828358680009842, 0.10159601271152496, 0.10430828481912613, 0.11088281869888306, 0.10998016595840454, 0.11053082346916199, 0.009782334789633751, 0.009869970381259918, 0.009929457679390907, 0.009967961348593235, 0.009989446960389614, 0.009986205957829952, 0.009978363290429115, 0.009940608404576778, 0.009902597405016422, 0.10411246865987778, 0.009810228832066059, 0.10284146666526794, 0.009738799184560776, 0.009692042134702206, 0.0096393758431077, 0.00958927907049656, 0.10819949954748154, 0.009467193856835365, 0.009396434761583805, 0.009339651092886925, 0.009251806885004044, 0.11323995888233185, 0.009112436324357986, 0.009043863974511623, 0.10584984719753265, 0.008933336474001408, 0.008862021379172802, 0.008791789412498474, 0.1013321578502655, 0.008678572252392769, 0.008615381084382534, 0.008546547964215279, 0.008476926945149899, 0.008394701406359673, 0.10890784859657288, 0.10463552922010422, 0.00823642872273922, 0.008190704509615898, 0.008136575110256672, 0.008077302947640419, 0.10326496511697769, 0.10577505081892014, 0.10672414302825928, 0.007944051176309586, 0.11243310570716858, 0.33668291568756104, 0.00806319247931242, 0.008179527707397938, 0.008274980820715427, 0.00836117286235094, 0.008403416723012924, 0.008435848169028759, 0.008458257652819157, 0.008470953442156315, 0.008460134267807007, 0.008450872264802456, 0.008423722349107265, 0.10287978500127792, 0.008366537280380726, 0.008326832205057144, 0.008283442817628384, 0.0082481000572443, 0.008186246268451214, 0.113108329474926, 0.10837239772081375, 0.008074482902884483, 0.10309057682752609, 0.1035773977637291, 0.008036574348807335, 0.008029384538531303, 0.3341580629348755, 0.008145022206008434, 0.008229035884141922, 0.008309018798172474, 0.008370067924261093, 0.10787700116634369, 0.008457675576210022, 0.008501825854182243, 0.008514215238392353, 0.11090188473463058, 0.00855732336640358, 0.008559086360037327, 0.008539898321032524, 0.00851557869464159, 0.11092543601989746, 0.10243247449398041, 0.008488872088491917, 0.008478345349431038, 0.008451461791992188, 0.0084163723513484, 0.1122308075428009, 0.10017024725675583, 0.008348680101335049, 0.11273089051246643, 0.10709916055202484, 0.008365290239453316, 0.10405520349740982, 0.0083837378770113, 0.008374298922717571, 0.00835409201681614, 0.008353471755981445, 0.008309311233460903, 0.10421736538410187, 0.00825093686580658, 0.008216286078095436, 0.10529531538486481, 0.10849665105342865, 0.008170326240360737, 0.10218537598848343, 0.10983169823884964, 0.10745707899332047, 0.008195355534553528, 0.008213718421757221, 0.008196281269192696, 0.008200624957680702, 0.008174470625817776, 0.3285110890865326, 0.008262788876891136, 0.008339370600879192, 0.008398567326366901, 0.008451325818896294, 0.11275526136159897, 0.008541329763829708, 0.008559409528970718, 0.10128708183765411, 0.008589938282966614, 0.008604253642261028, 0.008591498248279095, 0.008597401902079582, 0.00856349803507328, 0.11059004068374634, 0.008497855626046658, 0.008470874279737473, 0.008443735539913177, 0.10531564056873322, 0.3357729911804199, 0.008469415828585625, 0.008562498725950718, 0.10242275148630142, 0.008731561712920666, 0.00877643097192049, 0.008831482380628586, 0.008832271210849285, 0.00884665921330452, 0.11697717756032944, 0.00884701032191515, 0.11822693794965744, 0.008860036730766296, 0.008846725337207317, 0.00885828584432602, 0.008831876330077648, 0.008788098581135273, 0.00874426867812872, 0.1103125661611557, 0.008640534244477749, 0.008593681268393993, 0.00856023095548153, 0.008494909852743149, 0.11319199949502945, 0.008401498198509216, 0.008354621939361095, 0.10265787690877914, 0.008240807801485062, 0.008196656592190266, 0.00816783681511879, 0.10787995159626007, 0.00807284191250801, 0.3404603600502014, 0.008111829869449139, 0.008186763152480125, 0.00825793668627739, 0.008295899257063866, 0.008314209058880806, 0.1066395565867424, 0.008347957395017147, 0.10571826249361038, 0.11195658147335052, 0.3389894664287567, 0.008580583147704601, 0.11717511713504791, 0.3427678346633911, 0.009150238707661629, 0.009388076141476631, 0.10641172528266907, 0.31794869899749756, 0.01010065246373415, 0.10266068577766418, 0.010642513632774353, 0.010880447924137115, 0.011060049757361412, 0.01121582742780447, 0.1062888354063034, 0.011463199742138386, 0.011579500511288643, 0.011628839187324047, 0.10625153034925461, 0.011711272411048412, 0.011738734319806099, 0.01173359900712967, 0.011729724705219269, 0.1037762239575386, 0.3180605173110962, 0.011785805225372314, 0.10086775571107864, 0.011978983879089355, 0.10325001180171967, 0.012121204286813736, 0.012165943160653114, 0.012185342609882355, 0.012184469029307365, 0.012160094454884529, 0.012120363302528858, 0.0120748495683074, 0.011990380473434925, 0.011908827349543571, 0.10732774436473846, 0.10328902304172516, 0.11009898036718369, 0.10047531127929688, 0.10327894985675812, 0.011609403416514397, 0.011568049900233746, 0.011523236520588398, 0.10390324145555496, 0.011419679969549179, 0.011358586139976978, 0.11064355820417404, 0.011235255748033524, 0.5941503643989563, 0.10411160439252853, 0.10300445556640625, 0.011976215988397598, 0.012182086706161499, 0.012356170453131199, 0.1060619130730629, 0.012637905776500702, 0.01272590458393097, 0.012791665270924568, 0.012852930463850498, 0.10640589892864227, 0.01287920493632555, 0.012872929684817791, 0.012849697843194008, 0.012811105698347092, 0.01275886781513691, 0.012679100036621094, 0.012593536637723446, 0.012494949623942375, 0.3066554069519043, 0.012437235563993454, 0.012456906959414482, 0.1017407551407814, 0.012464549392461777, 0.012451871298253536, 0.3073906898498535, 0.012535583227872849, 0.012614152394235134, 0.10545647889375687, 0.012728646397590637, 0.01277053914964199, 0.10882538557052612, 0.012784167192876339, 0.012771272100508213, 0.012745318002998829, 0.012697141617536545, 0.5842657685279846, 0.5601449608802795, 0.01351569127291441, 0.2903138995170593, 0.1047496348619461, 0.015247023664414883, 0.01575968600809574, 0.01622033677995205, 0.016597047448158264, 0.27818384766578674, 0.2671959400177002, 0.01788775809109211, 0.018346643075346947, 0.018740657716989517, 0.019050706177949905, 0.019306832924485207, 0.019502604380249977, 0.019660543650388718, 0.01973431557416916, 0.01977510005235672, 0.2653658092021942, 0.10027844458818436, 0.020044006407260895, 0.020126570016145706, 0.020151106640696526, 0.10854481160640717, 0.10014337301254272, 0.10418225824832916, 0.020086340606212616, 0.10917479544878006, 0.01998671144247055, 0.019903134554624557, 0.019784847274422646, 0.10048869997262955, 0.019516170024871826, 0.019397404044866562, 0.10265824943780899, 0.01904476061463356, 0.018897149711847305, 0.01868267171084881, 0.01848072186112404, 0.018257634714245796, 0.10246705263853073, 0.017855482175946236, 0.01763094589114189, 0.017428487539291382, 0.017173195257782936, 0.016953838989138603, 0.016717800870537758, 0.8102085590362549, 0.016741354018449783, 0.016955142840743065, 0.11144225299358368, 0.01727879047393799, 0.017387056723237038, 0.10694701969623566, 0.10147049278020859, 0.09976261109113693, 0.017624681815505028, 0.017651434987783432, 0.01764697954058647, 0.017603209242224693, 0.017534945160150528, 0.01742541417479515, 0.11375103145837784, 0.10164639353752136, 0.10262586176395416, 0.017056673765182495, 0.09930264949798584, 0.0168886911123991, 0.016773341223597527, 0.1074225977063179, 0.09993716329336166, 0.016456112265586853, 0.016338512301445007, 0.10207992792129517, 0.016114016994833946]\n",
            "Val loss 0.05348999533583136\n",
            "Val auc roc 0.4894681636934279\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9053c184e6f548f3954a6e3d5c3e32b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1701.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0531\n",
            "Train Losses : [0.016001537442207336, 0.015964437276124954, 0.01597292348742485, 0.10529159754514694, 0.015925290063023567, 0.015902560204267502, 0.01589101180434227, 0.015869081020355225, 0.015858592465519905, 0.09896590560674667, 0.01583254709839821, 0.015803802758455276, 0.015804199501872063, 0.10910087823867798, 0.01575014367699623, 0.01575155183672905, 0.10526832938194275, 0.01569894142448902, 0.015698743984103203, 0.10729658603668213, 0.01565973088145256, 0.015650678426027298, 0.015620593912899494, 0.015603206120431423, 0.10104150325059891, 0.015578520484268665, 0.015553534962236881, 0.015531682409346104, 0.015529252588748932, 0.01550198718905449, 0.015469386242330074, 0.015457401983439922, 0.10041910409927368, 0.29708847403526306, 0.015411344356834888, 0.015412897802889347, 0.11136051267385483, 0.10032331943511963, 0.015385100618004799, 0.015380707569420338, 0.015376631170511246, 0.015379481017589569, 0.015368478372693062, 0.015348066575825214, 0.015326038934290409, 0.015301109291613102, 0.015306221321225166, 0.10866931825876236, 0.015258845873177052, 0.015248003415763378, 0.015224363654851913, 0.10762085020542145, 0.015200329013168812, 0.01518511213362217, 0.015169810503721237, 0.015141122043132782, 0.015134929679334164, 0.015122215263545513, 0.01511466410011053, 0.015067524276673794, 0.11192607879638672, 0.10564269870519638, 0.10383062809705734, 0.015016179531812668, 0.014989160001277924, 0.014978127554059029, 0.014961891807615757, 0.014950362965464592, 0.014927664771676064, 0.014933238737285137, 0.014895999804139137, 0.28233322501182556, 0.10233244299888611, 0.10524984449148178, 0.014875270426273346, 0.014859722927212715, 0.014874166809022427, 0.01486895326524973, 0.014837967231869698, 0.10151316970586777, 0.014815699309110641, 0.014816928654909134, 0.1069302037358284, 0.014806992374360561, 0.014784589409828186, 0.01477121189236641, 0.10002782195806503, 0.014758508652448654, 0.1073594018816948, 0.10958611965179443, 0.014707781374454498, 0.014700470492243767, 0.014688470400869846, 0.014686662703752518, 0.27288657426834106, 0.014655588194727898, 0.10162654519081116, 0.28445374965667725, 0.014675912447273731, 0.014690716750919819, 0.014690455049276352, 0.014695344492793083, 0.10258420556783676, 0.10087572783231735, 0.014697879552841187, 0.014669117517769337, 0.2938389182090759, 0.10051987320184708, 0.014688528142869473, 0.014693144708871841, 0.014693534933030605, 0.2931192219257355, 0.014710014685988426, 0.014722570776939392, 0.014733274467289448, 0.10009385645389557, 0.014750250615179539, 0.014752003364264965, 0.014750158414244652, 0.014731804840266705, 0.01472476776689291, 0.014727238565683365, 0.014730418100953102, 0.014696164056658745, 0.1052166074514389, 0.014681067317724228, 0.014681549742817879, 0.014657337218523026, 0.014662392437458038, 0.10017038881778717, 0.10660785436630249, 0.10414991527795792, 0.014599102549254894, 0.014592981897294521, 0.014586050994694233, 0.014573093503713608, 0.014574001543223858, 0.10051897168159485, 0.1080016940832138, 0.014527061954140663, 0.014509359374642372, 0.01450166292488575, 0.10898109525442123, 0.014480574987828732, 0.01447051577270031, 0.014443562366068363, 0.014452769421041012, 0.01441320963203907, 0.10092181712388992, 0.10514005273580551, 0.10758713632822037, 0.014374827966094017, 0.014356457628309727, 0.014352940954267979, 0.01434915792196989, 0.014339027926325798, 0.014324696734547615, 0.014293714426457882, 0.014274785295128822, 0.014262281358242035, 0.01425805315375328, 0.10662367194890976, 0.0142117440700531, 0.10357557982206345, 0.014179099351167679, 0.014179476536810398, 0.014163116924464703, 0.014141307212412357, 0.10628756135702133, 0.10156010091304779, 0.28929534554481506, 0.014102019369602203, 0.29599109292030334, 0.014116622507572174, 0.014126692898571491, 0.01412484422326088, 0.28183773159980774, 0.014138431288301945, 0.014174413867294788, 0.0141642726957798, 0.01417465042322874, 0.014181092381477356, 0.014176391996443272, 0.014167500659823418, 0.014164799824357033, 0.10024305433034897, 0.014174099080264568, 0.10858126729726791, 0.014152627438306808, 0.014142662286758423, 0.10155992209911346, 0.014128739945590496, 0.014134548604488373, 0.01410591322928667, 0.01411085668951273, 0.014098161831498146, 0.014080156572163105, 0.014083038084208965, 0.014066389761865139, 0.014048323035240173, 0.29164111614227295, 0.10617052763700485, 0.014020363800227642, 0.014012837782502174, 0.09963614493608475, 0.014021116308867931, 0.1004197895526886, 0.014002177864313126, 0.01401486061513424, 0.014007176272571087, 0.013977093622088432, 0.013974270783364773, 0.013961431570351124, 0.2920822501182556, 0.10147050023078918, 0.013950266875326633, 0.013957183808088303, 0.1015486866235733, 0.1088889017701149, 0.10039735585451126, 0.013945329003036022, 0.10277160257101059, 0.2794766426086426, 0.10025323182344437, 0.013986540958285332, 0.013975336216390133, 0.013995284214615822, 0.013994739390909672, 0.013979400508105755, 0.10092329233884811, 0.10621673613786697, 0.013984416611492634, 0.10085523873567581, 0.013973862864077091, 0.11406727135181427, 0.013965999707579613, 0.013962664641439915, 0.0139537388458848, 0.013967886567115784, 0.2893931269645691, 0.013962826691567898, 0.013942730613052845, 0.10297667235136032, 0.013961500488221645, 0.11022438853979111, 0.013942372053861618, 0.013936073519289494, 0.013944783248007298, 0.28785955905914307, 0.10958600789308548, 0.01395319402217865, 0.1049555093050003, 0.01396137848496437, 0.1085204929113388, 0.013950461521744728, 0.28394627571105957, 0.013964254409074783, 0.013977821916341782, 0.013993542641401291, 0.013980669900774956, 0.013978727161884308, 0.013979512266814709, 0.013995443470776081, 0.013984800316393375, 0.01397792436182499, 0.10386792570352554, 0.013950356282293797, 0.01394711621105671, 0.2985113859176636, 0.10168064385652542, 0.01393752172589302, 0.013960969634354115, 0.013953848741948605, 0.013956082053482533, 0.2771117687225342, 0.10264284908771515, 0.013947369530797005, 0.0139522859826684, 0.29225558042526245, 0.013977753929793835, 0.013989944942295551, 0.013994679786264896, 0.10478095710277557, 0.10257523506879807, 0.09937567263841629, 0.014029075391590595, 0.014010336250066757, 0.01401523407548666, 0.10899995267391205, 0.11175080388784409, 0.014011360704898834, 0.014022422954440117, 0.014016475528478622, 0.014017078094184399, 0.014005906879901886, 0.10094626992940903, 0.10221827030181885, 0.013992193154990673, 0.10320226848125458, 0.013957099989056587, 0.013971615582704544, 0.01394660770893097, 0.013937864452600479, 0.0139257051050663, 0.013915996998548508, 0.10208059102296829, 0.01390817854553461, 0.10820485651493073, 0.10767284780740738, 0.28897082805633545, 0.013866635970771313, 0.013890378177165985, 0.013878013007342815, 0.013871949166059494, 0.013888965360820293, 0.0138674546033144, 0.013858726248145103, 0.10901376605033875, 0.013849500566720963, 0.013831944204866886, 0.10078510642051697, 0.013828082010149956, 0.01381809450685978, 0.013812352903187275, 0.01379396952688694, 0.10133887827396393, 0.013777419924736023, 0.10766547173261642, 0.01377209834754467, 0.10617303103208542, 0.10283751785755157, 0.013733601197600365, 0.013729434460401535, 0.10291118174791336, 0.013706340454518795, 0.28624650835990906, 0.01372329518198967, 0.10879439860582352, 0.013701550662517548, 0.0137113556265831, 0.013710316270589828, 0.013695262372493744, 0.11419352889060974, 0.10275886207818985, 0.013708590529859066, 0.013693214394152164, 0.10821694880723953, 0.10929732769727707, 0.10989800095558167, 0.013667264021933079, 0.013681195676326752, 0.2762606739997864, 0.11067903786897659, 0.29738014936447144, 0.013703788630664349, 0.013692802749574184, 0.01370858121663332, 0.013711544685065746, 0.013716328889131546, 0.11161093413829803, 0.10661989450454712, 0.013736987486481667, 0.013724278658628464, 0.10882489383220673, 0.013736606575548649, 0.11217649281024933, 0.10262707620859146, 0.013726397417485714, 0.013722050003707409, 0.28268903493881226, 0.013715026900172234, 0.013744781725108624, 0.10126367211341858, 0.013748234137892723, 0.01375057827681303, 0.01373768225312233, 0.10711122304201126, 0.013736584223806858, 0.013720449060201645, 0.013715633191168308, 0.013711447827517986, 0.013722912408411503, 0.10026709735393524, 0.013705545105040073, 0.10748142749071121, 0.09999281167984009, 0.2848302125930786, 0.013679621741175652, 0.013681428506970406, 0.10988874733448029, 0.01367839053273201, 0.10329434275627136, 0.013700746931135654, 0.013694163411855698, 0.10043349862098694, 0.01367231272161007, 0.013679060153663158, 0.013671024702489376, 0.10318904370069504, 0.013672100380063057, 0.013642913661897182, 0.013644439168274403, 0.1077195331454277, 0.10061421245336533, 0.013633965514600277, 0.01361988764256239, 0.013616872951388359, 0.013602470979094505, 0.09760643541812897, 0.1105010136961937, 0.013570388779044151, 0.013580082915723324, 0.013568764552474022, 0.01356787420809269, 0.11254757642745972, 0.013527522794902325, 0.013523628003895283, 0.013525169342756271, 0.013507346622645855, 0.10498210042715073, 0.01348185259848833, 0.013470610603690147, 0.01346269529312849, 0.013443906791508198, 0.01343717984855175, 0.30289292335510254, 0.013435746543109417, 0.013426047749817371, 0.013420451432466507, 0.013430369086563587, 0.013419043272733688, 0.013420089147984982, 0.3018482029438019, 0.1007600873708725, 0.1083405464887619, 0.01341033261269331, 0.013407135382294655, 0.013411949388682842, 0.013421348296105862, 0.013407456688582897, 0.013399705290794373, 0.013393650762736797, 0.1029929369688034, 0.013395697809755802, 0.013381949625909328, 0.1051533967256546, 0.10680080205202103, 0.013357603922486305, 0.01334360521286726, 0.013357553631067276, 0.013339679688215256, 0.013342197053134441, 0.013320288620889187, 0.013321528211236, 0.09973134100437164, 0.013300219550728798, 0.013291016221046448, 0.013267388567328453, 0.013259483501315117, 0.013246282003819942, 0.10110219568014145, 0.013226846233010292, 0.013223287649452686, 0.013221233151853085, 0.013207128271460533, 0.013197893276810646, 0.10588518530130386, 0.013162052258849144, 0.013153338804841042, 0.013136178255081177, 0.013126322068274021, 0.013110682368278503, 0.013119104318320751, 0.013106442987918854, 0.1055070236325264, 0.10360726714134216, 0.100605309009552, 0.013044042512774467, 0.013041260652244091, 0.013024485670030117, 0.01302562840282917, 0.10264703631401062, 0.09980679303407669, 0.01300257258117199, 0.10352780669927597, 0.0986923947930336, 0.012970934621989727, 0.09914601594209671, 0.012976255267858505, 0.012973232194781303, 0.012955198995769024, 0.29680851101875305, 0.11406983435153961, 0.01294600311666727, 0.012943624518811703, 0.012962322682142258, 0.10027145594358444, 0.012962237000465393, 0.012948671355843544, 0.012947420589625835, 0.29942435026168823, 0.012945013120770454, 0.012960977852344513, 0.012951824814081192, 0.012960482388734818, 0.012953681871294975, 0.012955318205058575, 0.012932190671563148, 0.10413680970668793, 0.012945764698088169, 0.012928007170557976, 0.10309234261512756, 0.0129097830504179, 0.11325165629386902, 0.012901092879474163, 0.012912447564303875, 0.10147315263748169, 0.30191975831985474, 0.10495780408382416, 0.10081825405359268, 0.1038631945848465, 0.012911897152662277, 0.012910017743706703, 0.012917248532176018, 0.012925890274345875, 0.012921077199280262, 0.012905959971249104, 0.012918446213006973, 0.012896565720438957, 0.012893452309072018, 0.10903571546077728, 0.012873915955424309, 0.1053406298160553, 0.28469911217689514, 0.012867286801338196, 0.012875199317932129, 0.012877856381237507, 0.10346722602844238, 0.11379314959049225, 0.012889375910162926, 0.012868613936007023, 0.012878384441137314, 0.012883996590971947, 0.10801941156387329, 0.012856232933700085, 0.10091659426689148, 0.012850972823798656, 0.012849517166614532, 0.012847570702433586, 0.012833617627620697, 0.012827383354306221, 0.01283263973891735, 0.1003677248954773, 0.10870014131069183, 0.01281814370304346, 0.3062242269515991, 0.012806517072021961, 0.012816795147955418, 0.012805436737835407, 0.012806963175535202, 0.012807070277631283, 0.012798876501619816, 0.3045973479747772, 0.012809675186872482, 0.012813176028430462, 0.10326013714075089, 0.012817713432013988, 0.012807046994566917, 0.012800761498510838, 0.10383015871047974, 0.012796067632734776, 0.012795011512935162, 0.012787570245563984, 0.012791629880666733, 0.012778665870428085, 0.012775375507771969, 0.2998092472553253, 0.012765669263899326, 0.29654034972190857, 0.012788163498044014, 0.30206093192100525, 0.012807893566787243, 0.1115368977189064, 0.012836513109505177, 0.012835943140089512, 0.012856886722147465, 0.012852715328335762, 0.10358979552984238, 0.012871443293988705, 0.01286126859486103, 0.012872926890850067, 0.012869451195001602, 0.012868463061749935, 0.10124299675226212, 0.012860547751188278, 0.012843471020460129, 0.29305070638656616, 0.012856833636760712, 0.012867564335465431, 0.012871534563601017, 0.11152385175228119, 0.012865022756159306, 0.10721994936466217, 0.012857004068791866, 0.0128664281219244, 0.012846467085182667, 0.012858946807682514, 0.1062537282705307, 0.10291925817728043, 0.012833676300942898, 0.012826688587665558, 0.1017363965511322, 0.09902791678905487, 0.012820074334740639, 0.012821592390537262, 0.012805554084479809, 0.11267542839050293, 0.012815974652767181, 0.01280396431684494, 0.01279312465339899, 0.10433633625507355, 0.30426332354545593, 0.09949858486652374, 0.012796753086149693, 0.10496978461742401, 0.10326153039932251, 0.101799376308918, 0.012801285833120346, 0.012799642980098724, 0.10220905393362045, 0.01280602440237999, 0.01279554981738329, 0.09904839843511581, 0.10989527404308319, 0.012788059189915657, 0.11229739338159561, 0.10217929631471634, 0.09940657764673233, 0.012779179960489273, 0.012800185941159725, 0.01277946773916483, 0.012791814282536507, 0.10070981830358505, 0.012765232473611832, 0.10022947192192078, 0.28643715381622314, 0.11243101954460144, 0.10871344059705734, 0.012780586257576942, 0.107996866106987, 0.01280081644654274, 0.10820555686950684, 0.10651218146085739, 0.01279143150895834, 0.10113143175840378, 0.012795674614608288, 0.012796035967767239, 0.01279910746961832, 0.1096300482749939, 0.012791331857442856, 0.10776691883802414, 0.012783026322722435, 0.012776036746799946, 0.012793413363397121, 0.10708076506853104, 0.10095497965812683, 0.012769355438649654, 0.012754683382809162, 0.012768639251589775, 0.012755237519741058, 0.012743359431624413, 0.012751065194606781, 0.012726584449410439, 0.10015906393527985, 0.012723162770271301, 0.10301664471626282, 0.012699930928647518, 0.012700176797807217, 0.012697280384600163, 0.012695548124611378, 0.10152041167020798, 0.10474193096160889, 0.012656939215958118, 0.012656008824706078, 0.012662925757467747, 0.10911796241998672, 0.012629257515072823, 0.10151000320911407, 0.10655078291893005, 0.012613599188625813, 0.1059926450252533, 0.01261099148541689, 0.012616494670510292, 0.012609376572072506, 0.012593378312885761, 0.012599853798747063, 0.10288484394550323, 0.5551453232765198, 0.012598578818142414, 0.012612258084118366, 0.01260976493358612, 0.012617416679859161, 0.012618143111467361, 0.01263523194938898, 0.10946036875247955, 0.01262658555060625, 0.012638707645237446, 0.10574819892644882, 0.012638171203434467, 0.3065797984600067, 0.10015370696783066, 0.012637674808502197, 0.012659882195293903, 0.10307291150093079, 0.012642391957342625, 0.012661523185670376, 0.10085202753543854, 0.012663456611335278, 0.012668713927268982, 0.3103965222835541, 0.012662078253924847, 0.10846593976020813, 0.012666704133152962, 0.012674152851104736, 0.012673771940171719, 0.012689436785876751, 0.10876499116420746, 0.11086265742778778, 0.012673317454755306, 0.012671897187829018, 0.012665189802646637, 0.012680714018642902, 0.012673051096498966, 0.012670604512095451, 0.012662670575082302, 0.2969612181186676, 0.012643883004784584, 0.012657015584409237, 0.012661543674767017, 0.012653636746108532, 0.012645457871258259, 0.012650230899453163, 0.10428144782781601, 0.012646941468119621, 0.012642462737858295, 0.012647469528019428, 0.2914630174636841, 0.012622852809727192, 0.012634598650038242, 0.012646706774830818, 0.10026443004608154, 0.012625106610357761, 0.01264414843171835, 0.10072016716003418, 0.012620748952031136, 0.11354143172502518, 0.012614448554813862, 0.012624799273908138, 0.012624070048332214, 0.012623867951333523, 0.01261679083108902, 0.30821990966796875, 0.012593776918947697, 0.012617824599146843, 0.012593301013112068, 0.5793976783752441, 0.012632313184440136, 0.0126399677246809, 0.012659173458814621, 0.10528753697872162, 0.012666110880672932, 0.1062539666891098, 0.012685897760093212, 0.012695063836872578, 0.012685971334576607, 0.09977442771196365, 0.012701001949608326, 0.10305625945329666, 0.012693752534687519, 0.012701629661023617, 0.012681985273957253, 0.012703754007816315, 0.2977212071418762, 0.1056956797838211, 0.01268920861184597, 0.10350316762924194, 0.012716182507574558, 0.012716309167444706, 0.1112302914261818, 0.012701818719506264, 0.01270276214927435, 0.01269857119768858, 0.012699040584266186, 0.01271019596606493, 0.10094555467367172, 0.012704539112746716, 0.012689194642007351, 0.10955645143985748, 0.10965563356876373, 0.012678306549787521, 0.01267270091921091, 0.012683712877333164, 0.1040860116481781, 0.012656252831220627, 0.10135559737682343, 0.10735072940587997, 0.012641756795346737, 0.012646802701056004, 0.01264374703168869, 0.10295988619327545, 0.01264157798141241, 0.012628463096916676, 0.10512946546077728, 0.012619146145880222, 0.012612957507371902, 0.012608394026756287, 0.012613157741725445, 0.012608425691723824, 0.09950265288352966, 0.012591095641255379, 0.10844703763723373, 0.01256934367120266, 0.1112586259841919, 0.28858816623687744, 0.012562916614115238, 0.01257574837654829, 0.10495080798864365, 0.10516904294490814, 0.012582619674503803, 0.012579182162880898, 0.01257315743714571, 0.012573343701660633, 0.10122057050466537, 0.012574789114296436, 0.012560641393065453, 0.012549660168588161, 0.012546945363283157, 0.012563031166791916, 0.01253938302397728, 0.012551515363156796, 0.012539762072265148, 0.012539257295429707, 0.012532399035990238, 0.012519598938524723, 0.012511404231190681, 0.012503700330853462, 0.012485634535551071, 0.012480349279940128, 0.2961777150630951, 0.10178402811288834, 0.012469766661524773, 0.3105689585208893, 0.10451182723045349, 0.012483319267630577, 0.29469743371009827, 0.012508018873631954, 0.012499704025685787, 0.012514591217041016, 0.012511092238128185, 0.012511503882706165, 0.012527815997600555, 0.012516162358224392, 0.012517906725406647, 0.012523841112852097, 0.01253238134086132, 0.012525546364486217, 0.10071729123592377, 0.29263952374458313, 0.012512804940342903, 0.012530247680842876, 0.012513293884694576, 0.012520366348326206, 0.10181870311498642, 0.01253354549407959, 0.012515462003648281, 0.10220681875944138, 0.10900378972291946, 0.012509915977716446, 0.012511900626122952, 0.012502000667154789, 0.28920653462409973, 0.012510505504906178, 0.012508752755820751, 0.012521091848611832, 0.012514178641140461, 0.11008037626743317, 0.012521826662123203, 0.012511447072029114, 0.012509830296039581, 0.01251289714127779, 0.012519755400717258, 0.012491918168962002, 0.012488988228142262, 0.01249103806912899, 0.012480602599680424, 0.012492732144892216, 0.012485330924391747, 0.11055184155702591, 0.11076867580413818, 0.012457146309316158, 0.012466027401387691, 0.012445207685232162, 0.10078182071447372, 0.01244636531919241, 0.10885708779096603, 0.012424647808074951, 0.10972457379102707, 0.01243418175727129, 0.012413215823471546, 0.012423633597791195, 0.012410251423716545, 0.01239853911101818, 0.01240179780870676, 0.012394252233207226, 0.012379299849271774, 0.012379487045109272, 0.012366315349936485, 0.012371933087706566, 0.012356405146420002, 0.012347682379186153, 0.012348480522632599, 0.0998360812664032, 0.113823302090168, 0.1001945286989212, 0.012306476943194866, 0.01231072936207056, 0.10050047188997269, 0.012293687090277672, 0.012305320240557194, 0.01229273434728384, 0.012280955910682678, 0.012274397537112236, 0.012272541411221027, 0.01226300373673439, 0.10902249813079834, 0.012254544533789158, 0.012249107472598553, 0.012244348414242268, 0.012229078449308872, 0.1056424081325531, 0.1012684777379036, 0.10076159983873367, 0.01220826618373394, 0.012208886444568634, 0.5666374564170837, 0.012214344926178455, 0.012223253026604652, 0.1035892590880394, 0.3014674186706543, 0.2975192666053772, 0.10877172648906708, 0.012288875877857208, 0.012289738282561302, 0.012303351424634457, 0.012308682315051556, 0.10328834503889084, 0.10232725739479065, 0.012333681806921959, 0.0123375179246068, 0.012340681627392769, 0.012353372760117054, 0.0123435715213418, 0.10651685297489166, 0.012353600934147835, 0.012353453785181046, 0.012358056381344795, 0.012337446212768555, 0.012339201755821705, 0.10137656331062317, 0.10227759182453156, 0.012347773648798466, 0.012331009842455387, 0.10587381571531296, 0.09981033951044083, 0.10435955226421356, 0.012339141219854355, 0.012318436056375504, 0.012333233840763569, 0.10226269066333771, 0.012306703254580498, 0.01231254544109106, 0.10212681442499161, 0.012299155816435814, 0.11172939836978912, 0.30007481575012207, 0.012300059199333191, 0.11332065612077713, 0.10224290937185287, 0.012307263910770416, 0.012321669608354568, 0.012303650379180908, 0.01232538465410471, 0.012312067672610283, 0.10323406755924225, 0.10465259104967117, 0.012308727018535137, 0.012300672009587288, 0.012307845056056976, 0.012304795905947685, 0.012301436625421047, 0.012300912290811539, 0.01229409221559763, 0.012282207608222961, 0.10078743100166321, 0.10405689477920532, 0.10181212425231934, 0.012269878759980202, 0.10944440960884094, 0.012278685346245766, 0.012274553067982197, 0.012256789021193981, 0.012254033237695694, 0.012243688106536865, 0.29107820987701416, 0.10497598350048065, 0.10645747929811478, 0.10098621249198914, 0.012256179004907608, 0.10532934218645096, 0.10514838248491287, 0.01225726306438446, 0.012250691652297974, 0.012249328196048737, 0.012265807017683983, 0.10395383834838867, 0.10095573216676712, 0.30021095275878906, 0.10020562261343002, 0.012272768653929234, 0.012274780310690403, 0.10509241372346878, 0.012276124209165573, 0.10691859573125839, 0.012276437133550644, 0.012266346253454685, 0.012265081517398357, 0.01226956956088543, 0.10833673924207687, 0.10339868068695068, 0.012272912077605724, 0.012269380502402782, 0.012257896363735199, 0.012276840396225452, 0.01227320171892643, 0.012254130095243454, 0.01224787998944521, 0.012246002443134785, 0.01225712988525629, 0.012245427817106247, 0.012232138775289059, 0.012243319302797318, 0.012222177349030972, 0.10073864459991455, 0.10595005005598068, 0.10365190356969833, 0.012211496941745281, 0.10162542015314102, 0.11062004417181015, 0.012194490991532803, 0.012208535335958004, 0.3110494911670685, 0.012189893051981926, 0.012200900353491306, 0.012203484773635864, 0.01218947023153305, 0.10157164186239243, 0.012198120355606079, 0.1070457324385643, 0.09868387132883072, 0.012189293280243874, 0.012205460108816624, 0.012183579616248608, 0.30734097957611084, 0.10193848609924316, 0.012193923816084862, 0.012196231633424759, 0.01219925843179226, 0.10108053684234619, 0.012211909517645836, 0.10404980927705765, 0.012199227698147297, 0.012212012894451618, 0.0122099369764328, 0.1066133975982666, 0.101385697722435, 0.012191451154649258, 0.012186541222035885, 0.012180393561720848, 0.11185991764068604, 0.01217989344149828, 0.0121910460293293, 0.012190046720206738, 0.012183395214378834, 0.1002681776881218, 0.10814507305622101, 0.012165704742074013, 0.012173770926892757, 0.012172027491033077, 0.012173348106443882, 0.012165028601884842, 0.012158302590250969, 0.10490812361240387, 0.11379027366638184, 0.10665515065193176, 0.012136567384004593, 0.10162164270877838, 0.01213215570896864, 0.10294139385223389, 0.01212548092007637, 0.10085763782262802, 0.012116419151425362, 0.01211915910243988, 0.012123774737119675, 0.10719355940818787, 0.012123115360736847, 0.11090182512998581, 0.01211613044142723, 0.10008854418992996, 0.012098168954253197, 0.0121110575273633, 0.3101770281791687, 0.012094701640307903, 0.01209702156484127, 0.10292714089155197, 0.012112597934901714, 0.012103968299925327, 0.10668148100376129, 0.012105248868465424, 0.012099326588213444, 0.012103728950023651, 0.012087525799870491, 0.10542786121368408, 0.012105382978916168, 0.1030026376247406, 0.10832291841506958, 0.012097779661417007, 0.012092928402125835, 0.012097625061869621, 0.10523215681314468, 0.10404088348150253, 0.012073732912540436, 0.01208204124122858, 0.012086449190974236, 0.012066182680428028, 0.3104044497013092, 0.012072901241481304, 0.012065366841852665, 0.012075040489435196, 0.012062715366482735, 0.012059101834893227, 0.10144975036382675, 0.01206265203654766, 0.10799044370651245, 0.10562445223331451, 0.012075988575816154, 0.012068365700542927, 0.1010819524526596, 0.10009439289569855, 0.1142052635550499, 0.012062960304319859, 0.10609085857868195, 0.1147657185792923, 0.012058290652930737, 0.012053185142576694, 0.012066114693880081, 0.01204387005418539, 0.2974254786968231, 0.012043479830026627, 0.10177947580814362, 0.10252200067043304, 0.11302215605974197, 0.012054650112986565, 0.10497569292783737, 0.012059574015438557, 0.11104395240545273, 0.012072935700416565, 0.1019645482301712, 0.10375592857599258, 0.012065683491528034, 0.012069746851921082, 0.01205266360193491, 0.012062563560903072, 0.0120585598051548, 0.012058363296091557, 0.09710957109928131, 0.10353908687829971, 0.012063182890415192, 0.012057075276970863, 0.012057504616677761, 0.012043572962284088, 0.012038561515510082, 0.012035834603011608, 0.2982514202594757, 0.01203348208218813, 0.012034116312861443, 0.012035569176077843, 0.10250058770179749, 0.012038776651024818, 0.012030488811433315, 0.012027422897517681, 0.012026322074234486, 0.10153576731681824, 0.2958119511604309, 0.012041483074426651, 0.10791240632534027, 0.012033001519739628, 0.012036120519042015, 0.2994137406349182, 0.012049728073179722, 0.012040321715176105, 0.29293158650398254, 0.10930350422859192, 0.10155646502971649, 0.1081472858786583, 0.11250019073486328, 0.012079666368663311, 0.5821048021316528, 0.01209673285484314, 0.01210082322359085, 0.012102869339287281, 0.012128077447414398, 0.10117370635271072, 0.012130746617913246, 0.01212856825441122, 0.0121393334120512, 0.01214285846799612, 0.10445817559957504, 0.012141471728682518, 0.10256850719451904, 0.012143981643021107, 0.012147725559771061, 0.012160556390881538, 0.012154154479503632, 0.0121460584923625, 0.012149319052696228, 0.10132890194654465, 0.10973034799098969, 0.012145576067268848, 0.10454530268907547, 0.012136710807681084, 0.01214880496263504, 0.012139218859374523, 0.012142728082835674, 0.09879638254642487, 0.012138099409639835, 0.10500291734933853, 0.012121051549911499, 0.012124213390052319, 0.012120957486331463, 0.012111777439713478, 0.012113532982766628, 0.012128003872931004, 0.09839259088039398, 0.2931244373321533, 0.10289110243320465, 0.012112454511225224, 0.012118822894990444, 0.012113368138670921, 0.10632972419261932, 0.01210746355354786, 0.012105656787753105, 0.10175096988677979, 0.012100833468139172, 0.10536673665046692, 0.10338719934225082, 0.012104621157050133, 0.012102585285902023, 0.012112299911677837, 0.1002928614616394, 0.10127472877502441, 0.012105978094041348, 0.11294073611497879, 0.012110990472137928, 0.11124859005212784, 0.012109006755053997, 0.01209696289151907, 0.012094501405954361, 0.012084073387086391, 0.10061786323785782, 0.09872972965240479, 0.012079203501343727, 0.10306831449270248, 0.01207344513386488, 0.1033395454287529, 0.11461538821458817, 0.012092199176549911, 0.012089227326214314, 0.01208884734660387, 0.012065211310982704, 0.30133381485939026, 0.012084277346730232, 0.012084807269275188, 0.2985292077064514, 0.012074185535311699, 0.012094237841665745, 0.012082268483936787, 0.012087802402675152, 0.11025215685367584, 0.012084849178791046, 0.10474191606044769, 0.012079591862857342, 0.012099791318178177, 0.012084276415407658, 0.012080177664756775, 0.012082535773515701, 0.012078926898539066, 0.012082134373486042, 0.012090160511434078, 0.012085363268852234, 0.012079932726919651, 0.012072072364389896, 0.294517457485199, 0.012071183882653713, 0.012064497917890549, 0.1050986722111702, 0.012081341817975044, 0.10276597738265991, 0.10919108986854553, 0.012059143744409084, 0.012076159939169884, 0.012058444321155548, 0.012054793536663055, 0.1128925234079361, 0.012060903012752533, 0.012058782391250134, 0.012061379849910736, 0.012050526216626167, 0.10180255770683289, 0.3076736032962799, 0.012061279267072678, 0.10999895632266998, 0.10675246268510818, 0.012050752528011799, 0.10742930322885513, 0.11290393769741058, 0.012056676670908928, 0.012057505548000336, 0.012072372250258923, 0.012061093002557755, 0.10634510219097137, 0.012053843587636948, 0.012051312252879143, 0.10199203342199326, 0.012052354402840137, 0.01205572672188282, 0.012046147137880325, 0.012044289149343967, 0.10993360728025436, 0.10221528261899948, 0.012039225548505783, 0.5886036157608032, 0.10146571695804596, 0.012054640799760818, 0.10844191908836365, 0.012077340856194496, 0.1030874103307724, 0.012077787891030312, 0.012078437954187393, 0.10238471627235413, 0.012065940536558628, 0.012073269113898277, 0.012068415991961956, 0.012076468206942081, 0.012074846774339676, 0.012072944082319736, 0.012067565694451332, 0.012079968117177486, 0.012068592011928558, 0.012066172435879707, 0.3032758831977844, 0.0120674017816782, 0.012071212753653526, 0.012086479924619198, 0.10053132474422455, 0.012077921070158482, 0.01208522729575634, 0.01208136510103941, 0.30622273683547974, 0.012081978842616081, 0.01208403054624796, 0.01207639928907156, 0.012081028893589973, 0.10289976000785828, 0.012070178054273129, 0.012085595168173313, 0.012068754993379116, 0.012069172225892544, 0.012070488184690475, 0.01207757368683815, 0.10679249465465546, 0.012080411426723003, 0.10588310658931732, 0.012062509544193745, 0.01205648947507143, 0.012069430202245712, 0.10732127726078033, 0.012057134881615639, 0.012065796181559563, 0.012064996175467968, 0.01204744540154934, 0.012045360170304775, 0.012061478570103645, 0.012042121030390263, 0.10996529459953308, 0.012054874561727047, 0.10167528688907623, 0.10216031968593597, 0.012038033455610275, 0.012033217586576939, 0.01203655730932951, 0.10267457365989685, 0.1082191914319992, 0.012026454322040081, 0.3011524975299835, 0.012033694423735142, 0.012042928487062454, 0.01204333920031786, 0.012025980278849602, 0.012026608921587467, 0.012025165371596813, 0.012041088193655014, 0.01202076394110918, 0.012018403969705105, 0.10699185729026794, 0.01201342511922121, 0.10651899874210358, 0.012031272053718567, 0.012017535045742989, 0.10594091564416885, 0.01202832069247961, 0.012020139023661613, 0.012006728909909725, 0.01200709119439125, 0.012011594139039516, 0.012021465227007866, 0.012015813030302525, 0.012006892822682858, 0.01199901569634676, 0.012007634155452251, 0.10504455119371414, 0.0119926193729043, 0.01199139840900898, 0.10180861502885818, 0.011983104050159454, 0.011998596601188183, 0.011993027292191982, 0.01198130939155817, 0.01199332345277071, 0.011985139921307564, 0.102350614964962, 0.011969972401857376, 0.10147207230329514, 0.011983460746705532, 0.10162721574306488, 0.011982346884906292, 0.10715501010417938, 0.011961176060140133, 0.29807400703430176, 0.1067906841635704, 0.10141954571008682, 0.011963173747062683, 0.011980341747403145, 0.011970707215368748, 0.30150148272514343, 0.011968834325671196, 0.3070797920227051, 0.011964425444602966, 0.011970787309110165, 0.011976848356425762, 0.011994173750281334, 0.01197699923068285, 0.01197834312915802, 0.11153091490268707, 0.1027441993355751, 0.10263939946889877, 0.011982979252934456, 0.10619018971920013, 0.011996232904493809, 0.011986170895397663, 0.011996004730463028, 0.011980952695012093, 0.1119508147239685, 0.011980227194726467, 0.011988013982772827, 0.011992793530225754, 0.10116873681545258, 0.30436304211616516, 0.011993663385510445, 0.011996261775493622, 0.011982257477939129, 0.011986300349235535, 0.01198934018611908, 0.011979334056377411, 0.01197897456586361, 0.01198399718850851, 0.011988695710897446, 0.01198688242584467, 0.011974269524216652, 0.011976469308137894, 0.01197220478206873, 0.011965898796916008, 0.011984091252088547, 0.011968947947025299, 0.011970859952270985, 0.011982593685388565, 0.011963695287704468, 0.01196883525699377, 0.01195587869733572, 0.011959539726376534, 0.011952713131904602, 0.011957067996263504, 0.1114601418375969, 0.011969027109444141, 0.011959632858633995, 0.011957076378166676, 0.011941964738070965, 0.10344577580690384, 0.10474773496389389, 0.011936758644878864, 0.10689415037631989, 0.011938609182834625, 0.011942237615585327, 0.01193904597312212, 0.30342307686805725, 0.011951909400522709, 0.10609167814254761, 0.011938427574932575, 0.011947942897677422, 0.10226231068372726, 0.011938642710447311, 0.3018650710582733, 0.011935767717659473, 0.10660112649202347, 0.011952407658100128, 0.011940928176045418, 0.01195045281201601, 0.10712043195962906, 0.011945880018174648, 0.01195358857512474, 0.10702776908874512, 0.01193880382925272, 0.01193811185657978, 0.3030987083911896, 0.31507861614227295, 0.10753437876701355, 0.10268759727478027, 0.011947966180741787, 0.10779847204685211, 0.011947933584451675, 0.011955955997109413, 0.01195964403450489, 0.011955623514950275, 0.011959783732891083, 0.011956079863011837, 0.10946878045797348, 0.10089897364377975, 0.011965840123593807, 0.10106343775987625, 0.011957882903516293, 0.011955437250435352, 0.011963906697928905, 0.10187259316444397, 0.1059892475605011, 0.011964517645537853, 0.10390198975801468, 0.01194949820637703, 0.011944008059799671, 0.011957432143390179, 0.011945473030209541, 0.10189379751682281, 0.10726150870323181, 0.011942476965487003, 0.10662728548049927, 0.011946755461394787, 0.011962305754423141, 0.011962227523326874, 0.011960514821112156, 0.011953700333833694, 0.10291069746017456, 0.011943083256483078, 0.10249391943216324, 0.011957772076129913, 0.10403504967689514, 0.10500305891036987, 0.011938782408833504, 0.01194075495004654, 0.1049136221408844, 0.011954644694924355, 0.011951378546655178, 0.011939432471990585, 0.011952225118875504, 0.011944029480218887, 0.011932460591197014, 0.011930533684790134, 0.011929193511605263, 0.011949934996664524, 0.10785773396492004, 0.01194660272449255, 0.10253880172967911, 0.10930231213569641, 0.011928643099963665, 0.011933316476643085, 0.30173662304878235, 0.011933795176446438, 0.011939148418605328, 0.01193174347281456, 0.011931052431464195, 0.10203493386507034, 0.0119254682213068, 0.011925701051950455, 0.011930439621210098, 0.011933316476643085, 0.11149831861257553, 0.01192593201994896, 0.011924328282475471, 0.01193324476480484, 0.11316309869289398, 0.011929393745958805, 0.0119340093806386, 0.11486104130744934, 0.01192769967019558, 0.01192285306751728, 0.10896655172109604, 0.3094954490661621, 0.011937704868614674, 0.011923981830477715, 0.01192447915673256, 0.011925840750336647, 0.011939829215407372, 0.10713483393192291, 0.011934810318052769, 0.011932097375392914, 0.10674390941858292, 0.10379732400178909, 0.30485299229621887, 0.011921999044716358, 0.011932047083973885, 0.011930533684790134, 0.011919533833861351, 0.10200746357440948, 0.10147176682949066, 0.011926207691431046, 0.10026498883962631, 0.011940144933760166, 0.011935879476368427, 0.01193250436335802, 0.011940320022404194, 0.011937004514038563, 0.11027400940656662, 0.10466726124286652, 0.011926066130399704, 0.011921018362045288, 0.011939586140215397, 0.011923843063414097, 0.011937023140490055, 0.10823576152324677, 0.011937790550291538, 0.011929355561733246, 0.011920305900275707, 0.011930747888982296, 0.10500996559858322, 0.1141490489244461, 0.011923098005354404, 0.11279422789812088, 0.011929349042475224, 0.10544230788946152, 0.011914805509150028, 0.10383781045675278, 0.3072032034397125, 0.011919040232896805, 0.011920631863176823, 0.01191651914268732, 0.011921851895749569, 0.011915475130081177, 0.01192015502601862, 0.011934486217796803, 0.29787150025367737, 0.011913547292351723, 0.011928348802030087, 0.011914368718862534, 0.01191340759396553, 0.011918491683900356, 0.1051671952009201, 0.011921393685042858, 0.10071597248315811, 0.011923341080546379, 0.011934090405702591, 0.01193193532526493, 0.011933661065995693, 0.011916318908333778, 0.011935096234083176, 0.011916907504200935, 0.011923689395189285, 0.011922447010874748, 0.10340360552072525, 0.0119263781234622, 0.30535492300987244, 0.011922631412744522, 0.011921889148652554, 0.011928307823836803, 0.011915699578821659]\n",
            "Val loss 0.05250902128351086\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2e23e7-5f48-45bc-a91f-2a46d6961883"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}