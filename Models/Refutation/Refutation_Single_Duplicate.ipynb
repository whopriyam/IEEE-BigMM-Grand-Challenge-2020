{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Refutation_Single_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fba776b5cc1e465a8968b95488f262c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31f3c88caa704c638cda7af799d5b2ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2cbd1b2e5b244c398e157658e655df8",
              "IPY_MODEL_e1702e2dcbc14c78a87716b7b47e26f5"
            ]
          }
        },
        "31f3c88caa704c638cda7af799d5b2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2cbd1b2e5b244c398e157658e655df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_019d5fe9f9434e2aa711a76ce3b264e6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1629,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1629,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_195e1e6a4a2342b4b3ae900f38dc4766"
          }
        },
        "e1702e2dcbc14c78a87716b7b47e26f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4fa32894d454f5b9998ef88678c1335",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1629/1629 [28:50&lt;00:00,  1.06s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d306963b20a747a7a8ae0015587cf6ab"
          }
        },
        "019d5fe9f9434e2aa711a76ce3b264e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "195e1e6a4a2342b4b3ae900f38dc4766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4fa32894d454f5b9998ef88678c1335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d306963b20a747a7a8ae0015587cf6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93e7d0f7525d4964bb6bbcfafaae166f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c52da0c0bf84cc1a89ea131ca1d67cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2ea0edde820b42da89aa2166ff7a872e",
              "IPY_MODEL_c8140ff53a8e4e13a0ab4bead843a266"
            ]
          }
        },
        "6c52da0c0bf84cc1a89ea131ca1d67cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ea0edde820b42da89aa2166ff7a872e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28016620003848deaf3e724090747463",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1629,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1629,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7820fb3441ae408b9ba06ef853e18080"
          }
        },
        "c8140ff53a8e4e13a0ab4bead843a266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fab15870326248f29db89dd90c5437ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1629/1629 [28:15&lt;00:00,  1.04s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d72daea0b497455e905c5160cc7006f7"
          }
        },
        "28016620003848deaf3e724090747463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7820fb3441ae408b9ba06ef853e18080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fab15870326248f29db89dd90c5437ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d72daea0b497455e905c5160cc7006f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f07a997eb8c43fdb79758f2de8a7a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63f5d66b5cf64ac5b533a6a50bf233f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0a11f95fea5244cd95ed60aff6ca1e8f",
              "IPY_MODEL_4593c6b2aa0748b3b6b1ad9fc1fb26b1"
            ]
          }
        },
        "63f5d66b5cf64ac5b533a6a50bf233f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a11f95fea5244cd95ed60aff6ca1e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ceed018d2a8941a8a651b91d60200382",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1629,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1629,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd6bccbaeb5d4a0aadf0b1633120e570"
          }
        },
        "4593c6b2aa0748b3b6b1ad9fc1fb26b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eeedcee05fe84c5581dd320c32b06f85",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1629/1629 [28:17&lt;00:00,  1.04s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3d0c059d62c48c3b4973b47f5bc40bd"
          }
        },
        "ceed018d2a8941a8a651b91d60200382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd6bccbaeb5d4a0aadf0b1633120e570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eeedcee05fe84c5581dd320c32b06f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3d0c059d62c48c3b4973b47f5bc40bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "14e3f3ba-9f5f-4858-bd68-2923de45b864"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 35.14 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29fd0f36-84b9-4255-f55a-a672f0ec1c3b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08b1adb7-7734-4255-abdd-2b87abac11bc"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "94418427-0fef-4ba5-f29f-bbf04877d571"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "86374a4a-8f30-4a4d-f3ab-503a3ddf11ef"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "44e27c61-4c8b-40e2-c079-8fd1b8252e8d"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "720d91b2-4c19-4bac-ed1a-a4a87e24db9d"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c6fc8820-e68b-4bf4-9465-e6220deeff70"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "68b4331d-682e-4f32-a8f9-f522c1762cb7"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "68e1ce32-019b-4ea4-ac3d-31aa55bb59b2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Times are tough for many in my close sphere. #...\n",
              "1    I am in real confusion,,, should I wait furthe...\n",
              "2    @LoriKMorrison My #life in in general why ask ...\n",
              "3    Trump Tells Supporters At Rally That #MeToo Ke...\n",
              "4    Midterms, #MeToo and badass women: Inside Vari...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "794eb006-d364-4c9c-8591-4ec25480e16b"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:07<00:00, 51897709.88B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc220ec7-841c-402c-e779-b3151b1effef"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 4408214.00B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "767621ad-2131-4290-cf0d-15773d8cb05c"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa74cda2-a9b2-4992-ce13-8b10ff169cc9"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "263695c0-85d7-4e82-b502-8223a0f791b8"
      },
      "source": [
        "col_name = \"Refutation\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "fba776b5cc1e465a8968b95488f262c1",
            "31f3c88caa704c638cda7af799d5b2ed",
            "e2cbd1b2e5b244c398e157658e655df8",
            "e1702e2dcbc14c78a87716b7b47e26f5",
            "019d5fe9f9434e2aa711a76ce3b264e6",
            "195e1e6a4a2342b4b3ae900f38dc4766",
            "b4fa32894d454f5b9998ef88678c1335",
            "d306963b20a747a7a8ae0015587cf6ab",
            "93e7d0f7525d4964bb6bbcfafaae166f",
            "6c52da0c0bf84cc1a89ea131ca1d67cb",
            "2ea0edde820b42da89aa2166ff7a872e",
            "c8140ff53a8e4e13a0ab4bead843a266",
            "28016620003848deaf3e724090747463",
            "7820fb3441ae408b9ba06ef853e18080",
            "fab15870326248f29db89dd90c5437ac",
            "d72daea0b497455e905c5160cc7006f7",
            "9f07a997eb8c43fdb79758f2de8a7a9a",
            "63f5d66b5cf64ac5b533a6a50bf233f0",
            "0a11f95fea5244cd95ed60aff6ca1e8f",
            "4593c6b2aa0748b3b6b1ad9fc1fb26b1",
            "ceed018d2a8941a8a651b91d60200382",
            "dd6bccbaeb5d4a0aadf0b1633120e570",
            "eeedcee05fe84c5581dd320c32b06f85",
            "f3d0c059d62c48c3b4973b47f5bc40bd"
          ]
        },
        "outputId": "066974de-e917-4071-8bf0-52780eb87379"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6519\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fba776b5cc1e465a8968b95488f262c1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1629.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0264\n",
            "Train Losses : [0.007863480597734451, 0.004513217601925135, 0.002239254070445895, 0.11167991161346436, 0.1237010508775711, 0.0028254215139895678, 0.004539284855127335, 0.005691208876669407, 0.006759046111255884, 0.009050984866917133, 0.00771847041323781, 0.006965160369873047, 0.18452346324920654, 0.006268440280109644, 0.006353503093123436, 0.005866514518857002, 0.005280445329844952, 0.11625715345144272, 0.0048828586004674435, 0.08613202720880508, 0.005602773744612932, 0.006939388345927, 0.00847714301198721, 0.0066450792364776134, 0.005319397430866957, 0.0040586646646261215, 0.003198223654180765, 0.1504276841878891, 0.002739130752161145, 0.1328490823507309, 0.0035971503239125013, 0.10144443809986115, 0.3709612786769867, 0.008881946094334126, 0.01328958012163639, 0.017969001084566116, 0.02180134318768978, 0.02467774972319603, 0.10545215755701065, 0.027579542249441147, 0.025834888219833374, 0.02401471696794033, 0.020740391686558723, 0.018101468682289124, 0.01473071426153183, 0.012262784875929356, 0.009635219350457191, 0.007210277020931244, 0.0055928900837898254, 0.004275447688996792, 0.0032434340100735426, 0.002450213534757495, 0.0018664777744561434, 0.0014183537568897009, 0.0010998870711773634, 0.0008536482346244156, 0.0006752828485332429, 0.0005353763117454946, 0.00043390094651840627, 0.0003557975869625807, 0.19303911924362183, 0.0003484344342723489, 0.00040918547892943025, 0.00047460224595852196, 0.0005164789035916328, 0.0005504717119038105, 0.0005970054771751165, 0.0006366253946907818, 0.000663233338855207, 0.0007905294187366962, 0.0007763706962577999, 0.0007387209334410727, 0.000750198436435312, 0.17805810272693634, 0.0009244927787221968, 0.0011720102047547698, 0.0012585276272147894, 0.0014247385552152991, 0.0016838378505781293, 0.0018426076276227832, 0.0018040158320218325, 0.0019550174474716187, 0.0019667199812829494, 0.12379501014947891, 0.0024421268608421087, 0.00245105498470366, 0.002642947481945157, 0.0029306947253644466, 0.12105247378349304, 0.003276512026786804, 0.003768804483115673, 0.13188907504081726, 0.004323374480009079, 0.00493048457428813, 0.004955344833433628, 0.005263959988951683, 0.005374131258577108, 0.005487170070409775, 0.005380059592425823, 0.00557505851611495, 0.005171183496713638, 0.0046943179331719875, 0.13237208127975464, 0.0044463821686804295, 0.004306018352508545, 0.004180282354354858, 0.004045211710035801, 0.0038428923580795527, 0.0037407907657325268, 0.0034270859323441982, 0.0032216343097388744, 0.13018034398555756, 0.0030391847249120474, 0.0030496150720864534, 0.0030334414914250374, 0.002947005908936262, 0.002820259891450405, 0.002708135638386011, 0.0025974430609494448, 0.0025215151254087687, 0.002412013243883848, 0.002291294513270259, 0.002101240446791053, 0.10647609084844589, 0.002092741196975112, 0.0020951104816049337, 0.0021467842161655426, 0.002121147932484746, 0.002086311113089323, 0.0021979836747050285, 0.11710107326507568, 0.0021146005019545555, 0.0022201756946742535, 0.002375439740717411, 0.002385784639045596, 0.0025158743374049664, 0.45902547240257263, 0.0033836879301816225, 0.00449706707149744, 0.0057364520616829395, 0.006724232342094183, 0.007481618318706751, 0.008426209911704063, 0.009313995018601418, 0.12400815635919571, 0.011478956788778305, 0.011218424886465073, 0.011308223940432072, 0.011954654939472675, 0.011531914584338665, 0.01074098702520132, 0.010294267907738686, 0.009399534203112125, 0.008908029645681381, 0.008029477670788765, 0.0072312671691179276, 0.12928150594234467, 0.006237142253667116, 0.005857024807482958, 0.005406747106462717, 0.005070139653980732, 0.004675998352468014, 0.11097262054681778, 0.10024989396333694, 0.004190858453512192, 0.004165190272033215, 0.00408865325152874, 0.11219088733196259, 0.11092628538608551, 0.004230565391480923, 0.004404302220791578, 0.004497944377362728, 0.0045788767747581005, 0.00457766093313694, 0.004583880305290222, 0.00449761375784874, 0.004430034197866917, 0.004229065962135792, 0.004107908345758915, 0.003955082036554813, 0.003822223749011755, 0.0036058116238564253, 0.45156338810920715, 0.004157388582825661, 0.004848623648285866, 0.005423030350357294, 0.006002557463943958, 0.13531829416751862, 0.007186416536569595, 0.0077755386009812355, 0.0963527113199234, 0.009274487383663654, 0.009005844593048096, 0.08253460377454758, 0.009856991469860077, 0.3600880205631256, 0.011861413717269897, 0.03796045482158661, 0.01398768275976181, 0.015076114796102047, 0.01704949513077736, 0.021758345887064934, 0.17829471826553345, 0.018632154911756516, 0.015493826009333134, 0.014871065504848957, 0.018931306898593903, 0.012847121804952621, 0.01269516907632351, 0.011092323809862137, 0.010712698101997375, 0.009958409704267979, 0.10863760113716125, 0.008021483197808266, 0.007443043868988752, 0.007182873785495758, 0.006818120367825031, 0.006307516247034073, 0.005421917885541916, 0.005010080058127642, 0.004717040807008743, 0.004165350925177336, 0.0038838470354676247, 0.0037559063639491796, 0.003237401833757758, 0.003114622551947832, 0.08795493841171265, 0.003235800424590707, 0.0026417484041303396, 0.003003935795277357, 0.0026540295220911503, 0.002305293222889304, 0.17499960958957672, 0.0030012631323188543, 0.002600249834358692, 0.0031791245564818382, 0.0031174139585345984, 0.002910423558205366, 0.1009279415011406, 0.0027182581834495068, 0.002971758833155036, 0.0025666295550763607, 0.0028030883986502886, 0.002542011206969619, 0.0028775592800229788, 0.002626001602038741, 0.002406435552984476, 0.003024464938789606, 0.0022701884154230356, 0.1323201209306717, 0.0030959350988268852, 0.0024814095813781023, 0.0025200035888701677, 0.0022748373448848724, 0.18502815067768097, 0.002522627357393503, 0.002499349880963564, 0.002874124562367797, 0.00284063839353621, 0.0027392886113375425, 0.003459450090304017, 0.002650737762451172, 0.0026842341758310795, 0.002803321462124586, 0.0028312387876212597, 0.002633314114063978, 0.002684046281501651, 0.002710578730329871, 0.0027639600448310375, 0.0026151053607463837, 0.002356736920773983, 0.14390283823013306, 0.002304806374013424, 0.0022168667055666447, 0.0023939800448715687, 0.002344517270103097, 0.0023210360668599606, 0.1426367163658142, 0.002540932036936283, 0.003855291288346052, 0.0036169211380183697, 0.005563193932175636, 0.007141205482184887, 0.003506541019305587, 0.003434814279899001, 0.0033774515613913536, 0.32143303751945496, 0.0029382207430899143, 0.2658742070198059, 0.002717752708122134, 0.002859551226720214, 0.003734528087079525, 0.0030719980131834745, 0.004483202937990427, 0.003804001957178116, 0.004559140186756849, 0.004158657975494862, 0.0034396115224808455, 0.0032077895011752844, 0.003497942117974162, 0.20613501965999603, 0.0032315594144165516, 0.003756041871383786, 0.0035965638235211372, 0.004001847002655268, 0.003694611368700862, 0.0035967270378023386, 0.0033189281821250916, 0.003264962462708354, 0.12359073013067245, 0.003170501906424761, 0.00320459995418787, 0.003960511181503534, 0.0036172810941934586, 0.1719096153974533, 0.0032036309130489826, 0.003689813893288374, 0.0033263997174799442, 0.1358010470867157, 0.0038202141877263784, 0.00369737995788455, 0.0037137053441256285, 0.003955031279474497, 0.0038157885428518057, 0.004184283781796694, 0.004164692480117083, 0.003688050666823983, 0.004177473951131105, 0.004103036597371101, 0.0038508628495037556, 0.003373414743691683, 0.0033434403594583273, 0.003186709014698863, 0.0032519411761313677, 0.09956452250480652, 0.0030714573804289103, 0.1428697258234024, 0.00343911349773407, 0.003397466382011771, 0.0031333796214312315, 0.0033639546018093824, 0.12156044691801071, 0.1058133989572525, 0.11698365956544876, 0.0037031127139925957, 0.1759231835603714, 0.004404719453305006, 0.0048055872321128845, 0.005030513275414705, 0.0052876826375722885, 0.005503992550075054, 0.00566059211269021, 0.005671828985214233, 0.005985798314213753, 0.0056871832348406315, 0.005656472872942686, 0.167583167552948, 0.00558400247246027, 0.005563977174460888, 0.005706742871552706, 0.0053915600292384624, 0.005270263645797968, 0.0051368349231779575, 0.005064724013209343, 0.4184717833995819, 0.12311827391386032, 0.006151148583739996, 0.006930035538971424, 0.09805511683225632, 0.008207458071410656, 0.00880997721105814, 0.009091957472264767, 0.009505229070782661, 0.00967689510434866, 0.00991551112383604, 0.0096259955316782, 0.009503149427473545, 0.009459800086915493, 0.1203279048204422, 0.008949934504926205, 0.008638730272650719, 0.008357491344213486, 0.008067965507507324, 0.0931503102183342, 0.12265516072511673, 0.007492624688893557, 0.007435837760567665, 0.007288243621587753, 0.00693043228238821, 0.006620972417294979, 0.006345811299979687, 0.0060860151425004005, 0.005865536164492369, 0.005726035684347153, 0.005200020037591457, 0.00501656299456954, 0.004694622475653887, 0.004407625645399094, 0.004167293198406696, 0.003975128289312124, 0.0037569929845631123, 0.003531639464199543, 0.0033282157965004444, 0.0031224056147038937, 0.1277414858341217, 0.0028838233556598425, 0.002857592888176441, 0.0028237435035407543, 0.002795987529680133, 0.002614550059661269, 0.0025674740318208933, 0.11615308374166489, 0.11838867515325546, 0.002605442190542817, 0.002647648798301816, 0.0028686351142823696, 0.0028275642544031143, 0.0028378369752317667, 0.0028870028909295797, 0.0028566287364810705, 0.0030209971591830254, 0.1511421799659729, 0.12032819539308548, 0.003029564628377557, 0.003255281364545226, 0.0032740323804318905, 0.0034482087939977646, 0.1494600772857666, 0.15126577019691467, 0.003969637211412191, 0.004196854308247566, 0.004302417393773794, 0.0045587546192109585, 0.12711846828460693, 0.0048521836288273335, 0.005033445078879595, 0.00516441510990262, 0.005340576637536287, 0.12195442616939545, 0.10591229796409607, 0.005581941455602646, 0.005797989666461945, 0.005946403369307518, 0.0059570674784481525, 0.006054647266864777, 0.0059684994630515575, 0.41156089305877686, 0.006573268678039312, 0.007255755364894867, 0.0077943820506334305, 0.008315601386129856, 0.008634397760033607, 0.008988519199192524, 0.009157748892903328, 0.009192042984068394, 0.009293417446315289, 0.11356642097234726, 0.009202106855809689, 0.009006314910948277, 0.008904104121029377, 0.10868380963802338, 0.008600716479122639, 0.008374221622943878, 0.008297818712890148, 0.008004159666597843, 0.007821332663297653, 0.007383684162050486, 0.007128539495170116, 0.006736598908901215, 0.006450984627008438, 0.0060692597180604935, 0.005821610800921917, 0.005474896635860205, 0.0052032615058124065, 0.004870834294706583, 0.004574712365865707, 0.0043686931021511555, 0.004159034229815006, 0.0038240402936935425, 0.003613036824390292, 0.003414408303797245, 0.003231728682294488, 0.00305037759244442, 0.002929816721007228, 0.12368666380643845, 0.0027195282746106386, 0.002647573361173272, 0.0026144429575651884, 0.002529109362512827, 0.002466123318299651, 0.5153621435165405, 0.002811891259625554, 0.0032052858732640743, 0.0035156363155692816, 0.003927283920347691, 0.004193260334432125, 0.004537335131317377, 0.004736852366477251, 0.0048854718916118145, 0.005041620694100857, 0.005170041229575872, 0.005243295803666115, 0.005293810740113258, 0.005337918177247047, 0.005314304959028959, 0.0052513424307107925, 0.005210211966186762, 0.0050932010635733604, 0.004988459870219231, 0.11576958000659943, 0.0048647429794073105, 0.10717121511697769, 0.004863194189965725, 0.004851987585425377, 0.004844453185796738, 0.004806389566510916, 0.004753028508275747, 0.004690502304583788, 0.004576778504997492, 0.004484192933887243, 0.004391140304505825, 0.004247651901096106, 0.11456459015607834, 0.004084475804120302, 0.1179630383849144, 0.00408998504281044, 0.004126552492380142, 0.00409664586186409, 0.11460921913385391, 0.004152180161327124, 0.004204064141958952, 0.004215957596898079, 0.004237181041389704, 0.004156066570430994, 0.0041463254019618034, 0.004100702702999115, 0.0040521505288779736, 0.003925771918147802, 0.11891413480043411, 0.003830574220046401, 0.003839316312223673, 0.0038214940577745438, 0.0037355029489845037, 0.11570239067077637, 0.11507895588874817, 0.0038256088737398386, 0.003926453646272421, 0.004020968917757273, 0.004016572143882513, 0.00405815988779068, 0.003992308396846056, 0.0992550402879715, 0.00407217163592577, 0.00406008493155241, 0.004090303089469671, 0.004067842848598957, 0.004050555173307657, 0.004063034895807505, 0.003920331131666899, 0.00387579551897943, 0.0038313092663884163, 0.003747327486053109, 0.003674096893519163, 0.0035479639191180468, 0.003485948545858264, 0.0033683988731354475, 0.0032117750961333513, 0.003142992500215769, 0.003030553925782442, 0.1501859873533249, 0.13365676999092102, 0.0030007492750883102, 0.1270514875650406, 0.0031939928885549307, 0.0032896820921450853, 0.003375027095898986, 0.12782296538352966, 0.0035871088039129972, 0.12853139638900757, 0.003948663827031851, 0.004066649824380875, 0.004199851304292679, 0.004305338021367788, 0.004395199008285999, 0.004426638130098581, 0.0044553400948643684, 0.00445711612701416, 0.004442666191607714, 0.004388267640024424, 0.004370145499706268, 0.004319880157709122, 0.004184657242149115, 0.004086929839104414, 0.003989109769463539, 0.003909355495125055, 0.003801084356382489, 0.0036703268997371197, 0.003578680567443371, 0.0034498549066483974, 0.00333023932762444, 0.0032175329979509115, 0.0031120367348194122, 0.0030138560105115175, 0.002908913418650627, 0.002807609038427472, 0.0027202090714126825, 0.0026147428434342146, 0.002525804564356804, 0.002451160689815879, 0.0023567834869027138, 0.0023081861436367035, 0.002189359162002802, 0.0021250019781291485, 0.12275142967700958, 0.002070851856842637, 0.11931341886520386, 0.0021088195499032736, 0.0021688411943614483, 0.0022163826506584883, 0.0022608479484915733, 0.13953983783721924, 0.0023644131142646074, 0.0024594555143266916, 0.002501024631783366, 0.0025760922580957413, 0.4533957540988922, 0.0029967266600579023, 0.003476779442280531, 0.0038814509753137827, 0.004286474548280239, 0.004594044294208288, 0.0049593690782785416, 0.005230093840509653, 0.005439617671072483, 0.005735380575060844, 0.09957505017518997, 0.006106496322900057, 0.006203618831932545, 0.006507168523967266, 0.006445000879466534, 0.00657438300549984, 0.006345353089272976, 0.006291617173701525, 0.006428667344152927, 0.0061985827051103115, 0.00600397726520896, 0.005952263716608286, 0.005701368674635887, 0.10573329031467438, 0.00562460208311677, 0.0053781610913574696, 0.1366230547428131, 0.36582571268081665, 0.005969705060124397, 0.006232724990695715, 0.10052622854709625, 0.007211458869278431, 0.007483020890504122, 0.008051663637161255, 0.008307449519634247, 0.008222959004342556, 0.008347838185727596, 0.008527098223567009, 0.008461006917059422, 0.00844372995197773, 0.00824446976184845, 0.09584014862775803, 0.008063677698373795, 0.09367483854293823, 0.00788002647459507, 0.007898300886154175, 0.1192677915096283, 0.0077253906056284904, 0.10111896693706512, 0.007525057066231966, 0.007654216140508652, 0.1332038789987564, 0.0072660441510379314, 0.007308468222618103, 0.0072819823399186134, 0.007104211021214724, 0.11769696325063705, 0.0069403997622430325, 0.006978098768740892, 0.00672517204657197, 0.006553124636411667, 0.006509815342724323, 0.00613775197416544, 0.005912096239626408, 0.1249813362956047, 0.0057249790988862514, 0.005623278673738241, 0.11558716744184494, 0.005399809218943119, 0.00541464239358902, 0.005369238089770079, 0.005163159221410751, 0.005173512268811464, 0.004974994342774153, 0.004759682342410088, 0.1057950034737587, 0.004666462540626526, 0.004617131315171719, 0.0045431144535541534, 0.39782819151878357, 0.0049580479972064495, 0.005178511142730713, 0.0054649957455694675, 0.005846885032951832, 0.006010338664054871, 0.0062673320062458515, 0.00631148973479867, 0.006551576778292656, 0.006401859223842621, 0.006487864535301924, 0.006563699804246426, 0.006410949397832155, 0.0064080399461090565, 0.006067811045795679, 0.005961115937680006, 0.005840891506522894, 0.005713439080864191, 0.005610850639641285, 0.005521257407963276, 0.12231820821762085, 0.005186683498322964, 0.0051053068600595, 0.005081898532807827, 0.004907542839646339, 0.00468041468411684, 0.004707982297986746, 0.00463520735502243, 0.0043500689789652824, 0.004215998109430075, 0.004172204993665218, 0.00393045786768198, 0.0038485820405185223, 0.09561625868082047, 0.0037154974415898323, 0.003624827368184924, 0.003474853467196226, 0.003393367398530245, 0.0033450203482061625, 0.14408883452415466, 0.11878273636102676, 0.003334892215207219, 0.003497689962387085, 0.0035750416573137045, 0.0034981209319084883, 0.003555133007466793, 0.003439069725573063, 0.0034999228082597256, 0.003360547125339508, 0.0034843478351831436, 0.0033998661674559116, 0.003263315884396434, 0.0033384207636117935, 0.10271602869033813, 0.003284978913143277, 0.0032729667145758867, 0.003160802647471428, 0.0031093519646674395, 0.003080366412177682, 0.003035428933799267, 0.1311919093132019, 0.11552616208791733, 0.0031621658708900213, 0.1480695754289627, 0.0034537685569375753, 0.0034244440030306578, 0.0036759364884346724, 0.0036723597440868616, 0.0036398638039827347, 0.003834165632724762, 0.0037137861363589764, 0.0037209447473287582, 0.00385315902531147, 0.0037026884965598583, 0.003660827176645398, 0.003619222203269601, 0.12548600137233734, 0.003571153385564685, 0.12576617300510406, 0.0036478715483099222, 0.0037180304061621428, 0.003866793354973197, 0.003882267279550433, 0.0038200055714696646, 0.0038444637320935726, 0.0037658624351024628, 0.0037124839145690203, 0.0037124152295291424, 0.003667967626824975, 0.0036491062492132187, 0.003582986770197749, 0.0035177841782569885, 0.0034112161956727505, 0.003306917380541563, 0.107682004570961, 0.003273569978773594, 0.0032462922390550375, 0.0031830817461013794, 0.10613247007131577, 0.0031582822557538748, 0.0032034716568887234, 0.0031784444581717253, 0.0031856766436249018, 0.0032121934927999973, 0.003146784845739603, 0.1500200629234314, 0.0032193579245358706, 0.003220298793166876, 0.11292038857936859, 0.1377682387828827, 0.0034884882625192404, 0.003552771173417568, 0.003621088806539774, 0.00370595371350646, 0.0038424136582762003, 0.003915663808584213, 0.1106119230389595, 0.003955597057938576, 0.15578950941562653, 0.11273173242807388, 0.09634880721569061, 0.004542801063507795, 0.00477544404566288, 0.4275168478488922, 0.005663001909852028, 0.006216986570507288, 0.10744351148605347, 0.0073982481844723225, 0.00803160760551691, 0.00844503752887249, 0.10432284325361252, 0.009333964437246323, 0.009696559980511665, 0.009893374517560005, 0.10611636936664581, 0.1020260825753212, 0.010603588074445724, 0.010778523050248623, 0.010788376443088055, 0.010752009227871895, 0.08847889304161072, 0.08231256157159805, 0.07119660079479218, 0.01113222911953926, 0.0110110929235816, 0.010552323423326015, 0.30824267864227295, 0.10764788091182709, 0.011285429820418358, 0.011693054810166359, 0.09677139669656754, 0.012181517668068409, 0.012437121011316776, 0.012513846158981323, 0.11783090978860855, 0.012326916679739952, 0.012250188738107681, 0.012249316088855267, 0.01187907624989748, 0.011545409448444843, 0.011229523457586765, 0.010819424875080585, 0.010566147975623608, 0.010016578249633312, 0.009654455818235874, 0.009145821444690228, 0.008724457584321499, 0.00829668901860714, 0.007875720970332623, 0.09313173592090607, 0.007211894262582064, 0.006911894306540489, 0.006685514003038406, 0.12360388785600662, 0.006329716183245182, 0.0059913694858551025, 0.00589163089171052, 0.0056941937655210495, 0.10321997851133347, 0.005412170197814703, 0.005177606362849474, 0.005070593673735857, 0.128762349486351, 0.00488203763961792, 0.12128392606973648, 0.11695674061775208, 0.004932461306452751, 0.004975371528416872, 0.004986300133168697, 0.005051704589277506, 0.09420502930879593, 0.004998230841010809, 0.005019394215196371, 0.005001244135200977, 0.004989058244973421, 0.005015058908611536, 0.004871298559010029, 0.0048148357309401035, 0.004774549510329962, 0.004667163360863924, 0.004555181600153446, 0.004478691145777702, 0.004348649177700281, 0.004280637018382549, 0.004101510159671307, 0.11491367965936661, 0.003944974392652512, 0.003938108682632446, 0.0038943542167544365, 0.003795732045546174, 0.0037301855627447367, 0.12309687584638596, 0.0036710193380713463, 0.003681694623082876, 0.11852768063545227, 0.003667351556941867, 0.0036950893700122833, 0.003711151657626033, 0.003705722978338599, 0.003669530851766467, 0.003637785790488124, 0.0036349259316921234, 0.0035776160657405853, 0.0035380045883357525, 0.0034921139013022184, 0.12573763728141785, 0.0034539219923317432, 0.0034556188620626926, 0.0034106180537492037, 0.0033810930326581, 0.003364556236192584, 0.003324254183098674, 0.0032784179784357548, 0.0032225092872977257, 0.0031796700786799192, 0.003118861233815551, 0.003070620819926262, 0.003025188809260726, 0.0029312227852642536, 0.00287102279253304, 0.002813757862895727, 0.0027518265414983034, 0.0026940866373479366, 0.0026257745921611786, 0.002563284244388342, 0.002524847164750099, 0.002440298441797495, 0.002373051829636097, 0.002334511373192072, 0.002266023075208068, 0.002224711934104562, 0.002184591256082058, 0.0021167732775211334, 0.00206838920712471, 0.13526996970176697, 0.0020157243125140667, 0.0020413424354046583, 0.0020414458122104406, 0.00202061771415174, 0.002028882736340165, 0.0020186910405755043, 0.1564135104417801, 0.002033831086009741, 0.12969136238098145, 0.13240492343902588, 0.002273035701364279, 0.0024047892075031996, 0.0025054330471903086, 0.0025990684516727924, 0.14281617105007172, 0.0027999132871627808, 0.00296353199519217, 0.003051169915124774, 0.00311255338601768, 0.0032003135420382023, 0.0032366544473916292, 0.003279242431744933, 0.00332372123375535, 0.003339646849781275, 0.1183764636516571, 0.003398048458620906, 0.0034282614942640066, 0.003475029254332185, 0.0034893075935542583, 0.003482083324342966, 0.13122142851352692, 0.43197640776634216, 0.00401074904948473, 0.004376875702291727, 0.00476791150867939, 0.0050964062102139, 0.005387807264924049, 0.0056627593003213406, 0.005884456913918257, 0.006086269393563271, 0.006237268913537264, 0.006360982544720173, 0.09732894599437714, 0.006523733027279377, 0.0065934788435697556, 0.006642645690590143, 0.11443576216697693, 0.006719804834574461, 0.0067240395583212376, 0.00679796515032649, 0.006694210693240166, 0.006607996765524149, 0.006548048462718725, 0.006394601427018642, 0.006287026684731245, 0.00614312756806612, 0.005992107093334198, 0.005874663591384888, 0.00570694450289011, 0.005546925589442253, 0.005361082497984171, 0.005197444930672646, 0.0050038970075547695, 0.00483344029635191, 0.1256488561630249, 0.0046189529821276665, 0.004526485223323107, 0.11716509610414505, 0.004430903121829033, 0.004345250315964222, 0.004297181963920593, 0.11118786036968231, 0.004252197686582804, 0.004259228240698576, 0.0042257909663021564, 0.004196024965494871, 0.0041760788299143314, 0.0041192444041371346, 0.004027275834232569, 0.004006090573966503, 0.003902201075106859, 0.0038400148041546345, 0.003750935196876526, 0.0036756752524524927, 0.003567968960851431, 0.003484985325485468, 0.003449304262176156, 0.003330768784508109, 0.003240011166781187, 0.003167754039168358, 0.003067432204261422, 0.1364831030368805, 0.002988195512443781, 0.0030002817511558533, 0.12994548678398132, 0.0029945645947009325, 0.0030122697353363037, 0.003041780786588788, 0.12964852154254913, 0.1166287362575531, 0.0031970220152288675, 0.0032884308602660894, 0.0033634889405220747, 0.0034196036867797375, 0.12697245180606842, 0.0035746523644775152, 0.0036751842126250267, 0.003713048994541168, 0.0037479104939848185, 0.0037583850789815187, 0.0038010256830602884, 0.003795779775828123, 0.12747634947299957, 0.0038723440375179052, 0.0038827918469905853, 0.0038958806544542313, 0.003893979825079441, 0.0038929020520299673, 0.0038679533172398806, 0.1141863465309143, 0.12987959384918213, 0.003940007649362087, 0.12720385193824768, 0.004165664780884981, 0.0042679873295128345, 0.11721136420965195, 0.004446547478437424, 0.004575931467115879, 0.004641955252736807, 0.004684168379753828, 0.42641523480415344, 0.005200106650590897, 0.0056112646125257015, 0.0059849536046385765, 0.0063112410716712475, 0.006580914370715618, 0.00679941987618804, 0.007011648267507553, 0.0071215517818927765, 0.007236001547425985, 0.007255441043525934, 0.0072770374827086926, 0.007271665148437023, 0.007284641731530428, 0.007126259617507458, 0.007007364183664322, 0.0068737296387553215, 0.006710224784910679, 0.0066122813150286674, 0.006443118676543236, 0.0062941075302660465, 0.0060875616036355495, 0.005886888597160578, 0.3595897853374481, 0.005925318691879511, 0.006163311656564474, 0.006327440962195396, 0.006456925068050623, 0.006542508956044912, 0.006583784706890583, 0.006612950935959816, 0.0066307540982961655, 0.10866215825080872, 0.10139193385839462, 0.006729731336236, 0.10446369647979736, 0.00677163852378726, 0.10702837258577347, 0.006944957654923201, 0.007015518844127655, 0.007000329904258251, 0.007028754334896803, 0.006941480096429586, 0.006915363483130932, 0.0067824190482497215, 0.006695222109556198, 0.006633811164647341, 0.006473769433796406, 0.006282384507358074, 0.10670637339353561, 0.00602469127625227, 0.006029902957379818, 0.10481347888708115, 0.005766970105469227, 0.005726750940084457, 0.005619661416858435, 0.005549481604248285, 0.005420425441116095, 0.0053255329839885235, 0.005211642477661371, 0.005092808976769447, 0.00494613591581583, 0.004826323129236698, 0.004731985740363598, 0.1228874921798706, 0.004505692981183529, 0.004460183437913656, 0.004377849865704775, 0.004301230423152447, 0.004217298701405525, 0.004109092988073826, 0.0040363771840929985, 0.003972055856138468, 0.0038480712100863457, 0.10831718891859055, 0.13395331799983978, 0.10969658941030502, 0.0038457713089883327, 0.0038919018115848303, 0.003968722652643919, 0.13980510830879211, 0.004046292044222355, 0.004133050795644522, 0.00417607044801116, 0.12463867664337158, 0.004280241671949625, 0.004342959262430668, 0.004392798058688641, 0.004394098650664091, 0.004429347347468138, 0.004412931390106678, 0.11529339104890823, 0.004434753675013781, 0.004441810771822929, 0.004455676767975092, 0.00445532938465476, 0.11575955897569656, 0.004477119538933039, 0.004469909705221653, 0.004530525766313076, 0.0044410256668925285, 0.004431415349245071, 0.004401680082082748, 0.004311363212764263, 0.10348373651504517, 0.004240119829773903, 0.0042406306602060795, 0.004240699578076601, 0.004184627905488014, 0.004140323027968407, 0.004136806819587946, 0.13157841563224792, 0.1395394653081894, 0.00411462364718318, 0.004148022271692753, 0.0041761682368814945, 0.1235317587852478, 0.004239651374518871, 0.0043105692602694035, 0.0043451860547065735, 0.004360326565802097, 0.004333242308348417, 0.004313669167459011, 0.004295307211577892, 0.004272477701306343, 0.004249039571732283, 0.004222320392727852, 0.004096572287380695, 0.004045131616294384, 0.003965301904827356, 0.0038955891504883766, 0.0037886758800596, 0.0037408063653856516, 0.003630830440670252, 0.0035472630988806486, 0.0034858915023505688, 0.0034103826619684696, 0.0033119679428637028, 0.0032240075524896383, 0.0031541942153126, 0.0030937623232603073, 0.0029895438347011805, 0.0029163926374167204, 0.0028734097722917795, 0.12451934069395065, 0.002759224735200405, 0.0027723191305994987, 0.11750539392232895, 0.0027466535102576017, 0.12511345744132996, 0.002873143879696727, 0.0028957691974937916, 0.0029390661511570215, 0.0029664216563105583, 0.003024526871740818, 0.11174073815345764, 0.0031034417916089296, 0.0031120525673031807, 0.0031768609769642353, 0.003172303084284067, 0.0032079354859888554, 0.0032203681766986847, 0.1267102211713791, 0.003278618212789297, 0.0032836548052728176, 0.003317233407869935, 0.0033074792008847, 0.13828188180923462, 0.003404004964977503, 0.0034133295994251966, 0.12509389221668243, 0.0035202393773943186, 0.13202516734600067, 0.10645074397325516, 0.003904783632606268, 0.004033492878079414, 0.004161076620221138, 0.004224442411214113, 0.004302658140659332, 0.004362300969660282, 0.004385561216622591, 0.004389147739857435, 0.004473864566534758, 0.0043876441195607185, 0.004385450854897499, 0.0043253363110125065, 0.004266267642378807, 0.004220845643430948, 0.0041550216265022755, 0.004086929839104414, 0.0040535470470786095, 0.00394427077844739, 0.003908270504325628, 0.0037852313835173845, 0.0037331408821046352, 0.003629575250670314, 0.12799854576587677, 0.0035235327668488026, 0.003481213003396988, 0.13000217080116272, 0.0034783182200044394, 0.0035045624244958162, 0.003493721829727292, 0.00348033313639462, 0.0034783435985445976, 0.0034394485410302877, 0.0034582617226988077, 0.0034253138583153486, 0.11483389139175415, 0.00333866523578763, 0.1307336837053299, 0.11409810930490494, 0.003551836358383298, 0.13586130738258362, 0.0037209289148449898, 0.1291668713092804, 0.12510856986045837, 0.004255538806319237, 0.004448728635907173, 0.004567994736135006, 0.09994740784168243, 0.004913087002933025, 0.005046299193054438, 0.005114664323627949, 0.005224582739174366, 0.005271265283226967, 0.005302872508764267, 0.005311116110533476, 0.005299446173012257, 0.005283357109874487, 0.005259546916931868, 0.12326782196760178, 0.005171423777937889, 0.0052093262784183025, 0.0051719113253057, 0.00511904526501894, 0.1229352056980133, 0.12141735106706619, 0.0050983126275241375, 0.005144553724676371, 0.0051436093635857105, 0.0051585170440375805, 0.005140608176589012, 0.005080015864223242, 0.005057351663708687, 0.004975491669028997, 0.004899309016764164, 0.0048230476677417755, 0.004730490036308765, 0.004640782717615366, 0.00451523857191205, 0.12030046433210373, 0.004377077799290419, 0.004356735851615667, 0.0042834156192839146, 0.004247454460710287, 0.11544177681207657, 0.004141396377235651, 0.00412767892703414, 0.004102151375263929, 0.0040710074827075005, 0.11910273134708405, 0.10986053943634033, 0.004103684797883034, 0.004177606664597988, 0.004209562670439482, 0.004194336012005806, 0.004173757042735815, 0.004171328153461218, 0.004152490291744471, 0.004093954339623451, 0.00409131171181798, 0.004031268414109945, 0.003970792982727289, 0.0038910438306629658, 0.0038276195991784334, 0.0037831198424100876, 0.0036666598170995712, 0.11725764721632004, 0.0036023191642016172, 0.0035707824863493443, 0.00352739030495286, 0.12289319187402725, 0.003528137458488345, 0.003520204219967127, 0.12676334381103516, 0.0035657265689224005, 0.1354241818189621, 0.0036975573748350143, 0.10826887935400009, 0.11421564966440201, 0.004055391065776348, 0.004203367047011852, 0.004313880577683449, 0.0044214497320353985, 0.004528926219791174, 0.004521586932241917, 0.004571367055177689, 0.10868276655673981, 0.004631351213902235, 0.004722147714346647, 0.004683561623096466, 0.004691950511187315, 0.004676134325563908, 0.10583009570837021, 0.004679280333220959, 0.00471185939386487, 0.004699139390140772, 0.0046677314676344395, 0.004652682691812515, 0.004590843804180622, 0.004597196821123362, 0.0045196861028671265, 0.004392687696963549, 0.0043397811241447926, 0.004233952611684799, 0.0041447295807302, 0.004092768300324678, 0.004011078737676144, 0.0039116800762712955, 0.00382629269734025, 0.0037147710099816322, 0.0036266944371163845, 0.0035257372073829174, 0.00342295179143548, 0.003387044183909893, 0.11301148682832718, 0.0032262280583381653, 0.10093600302934647, 0.11399751156568527, 0.00332440878264606, 0.0033529922366142273, 0.11845944821834564, 0.0035003931261599064, 0.11978467553853989, 0.0036393546033650637, 0.003729036543518305, 0.0038393486756831408, 0.00394743587821722, 0.003932144492864609, 0.003955767955631018, 0.003979488741606474, 0.0039558918215334415, 0.004004825372248888, 0.13345465064048767, 0.004002929199486971, 0.004040504340082407, 0.0040054465644061565, 0.004002514760941267, 0.004023343790322542, 0.0039649419486522675, 0.1370743215084076, 0.003947879653424025, 0.003950930200517178, 0.12081509828567505, 0.004018475767225027, 0.004095594398677349, 0.004075904376804829, 0.004105145577341318, 0.004095322918146849, 0.004059030208736658, 0.004085148684680462, 0.004033996257930994, 0.10636399686336517, 0.12723548710346222, 0.004091364331543446, 0.004081298131495714, 0.004094250500202179, 0.004150404594838619, 0.0041685644537210464, 0.004122708924114704, 0.004111794754862785, 0.004067196976393461, 0.004025996197015047, 0.0039828685112297535, 0.003940106835216284, 0.00383547181263566, 0.12898868322372437, 0.00377862760797143, 0.003759021870791912, 0.0037636759225279093, 0.003733163932338357, 0.0036756033077836037, 0.0036302274093031883, 0.003612713422626257, 0.0035691692028194666, 0.129410058259964, 0.0034693051129579544, 0.40595731139183044, 0.003798032645136118, 0.004075624980032444, 0.004305180162191391, 0.004608146380633116, 0.004750361200422049, 0.004906667862087488, 0.005060058552771807, 0.005168781615793705, 0.005255010910332203, 0.005341768730431795, 0.005363859236240387, 0.0053604980930686, 0.10821601748466492, 0.13356372714042664, 0.10870138555765152, 0.005688868463039398, 0.005805510096251965, 0.005940251983702183, 0.11264006793498993, 0.005989549681544304, 0.006049530580639839, 0.006098092067986727, 0.0061661358922719955, 0.00615251949056983, 0.006040759850293398, 0.006026164162904024, 0.0059635331854224205, 0.005860783625394106, 0.1394578069448471, 0.00573961716145277, 0.11200489848852158, 0.005699296947568655, 0.005686062388122082, 0.005635309033095837, 0.005657610483467579, 0.005511249415576458, 0.005436071194708347, 0.005352856125682592, 0.005256001837551594, 0.00523998262360692, 0.005120115354657173, 0.004972108639776707, 0.004845520481467247, 0.004723991267383099, 0.00462926272302866, 0.0045003751292824745, 0.004387556109577417, 0.004246758297085762, 0.004146385006606579, 0.004031430929899216, 0.003914117347449064, 0.003824526211246848, 0.0037072072736918926, 0.0036066544707864523, 0.003507581539452076, 0.003424027469009161, 0.003306420287117362, 0.003239561803638935, 0.0031516419257968664, 0.0030414045322686434, 0.1098816841840744, 0.0029321431647986174, 0.00289827398955822, 0.0028669042512774467, 0.0028533737640827894, 0.1195935532450676, 0.002809469820931554, 0.0028464121278375387, 0.002815886167809367, 0.002802786650136113, 0.002781467977911234, 0.11112311482429504, 0.0028128318954259157, 0.0028114463202655315, 0.0028194934129714966, 0.0028228447772562504, 0.002865780843421817, 0.0028171578887850046, 0.002798912813887, 0.0028056763112545013, 0.0027586703654378653, 0.0027333491016179323, 0.0027136027347296476, 0.0026795105077326298, 0.002629767404869199, 0.0026056035421788692, 0.12212952226400375, 0.002599159022793174, 0.0025971021968871355, 0.0026081812102347612, 0.0025717143435031176, 0.002589416690170765, 0.0025475032161921263, 0.002548879012465477, 0.002515493892133236, 0.0025016230065375566, 0.0025066854432225227, 0.0024251120630651712, 0.11962458491325378, 0.00241278437897563, 0.002442593453451991, 0.0024410290643572807, 0.0024297700729221106, 0.0024544119369238615, 0.0024074590764939785, 0.0024173296988010406, 0.0024192091077566147, 0.13895027339458466, 0.002431681379675865, 0.0024254657328128815, 0.002434531459584832, 0.002445179270580411, 0.002435913309454918, 0.0024456174578517675, 0.002433154499158263, 0.0024405098520219326, 0.002408752916380763, 0.0023979381658136845, 0.002354022115468979, 0.002354859374463558, 0.0023052734322845936, 0.002297687577083707, 0.1155342236161232, 0.002277818974107504, 0.0022982812952250242, 0.002293214900419116, 0.0023078909143805504, 0.002312059048563242, 0.0022855878341943026, 0.0023000771179795265, 0.002260182984173298, 0.14397957921028137, 0.12877468764781952, 0.46104660630226135, 0.0026401514187455177, 0.0029635196551680565, 0.0032306539360433817, 0.0035505041014403105, 0.11507033556699753, 0.004041461739689112, 0.0043395389802753925, 0.0045852744951844215, 0.004799611400812864, 0.005004793405532837, 0.005118997301906347, 0.005241445731371641, 0.005401848815381527, 0.005429329816251993, 0.005482259206473827, 0.10771091282367706, 0.005558115430176258]\n",
            "Val loss 0.02383671643377598\n",
            "Val auc roc 0.5193816860104911\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93e7d0f7525d4964bb6bbcfafaae166f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1629.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0257\n",
            "Train Losses : [0.00559870945289731, 0.005576999392360449, 0.005592718254774809, 0.11645656079053879, 0.005591819528490305, 0.005613961257040501, 0.005629078950732946, 0.005611197557300329, 0.005532963201403618, 0.00547006307169795, 0.00541550712659955, 0.005313638132065535, 0.1065533310174942, 0.00521463667973876, 0.00517222611233592, 0.005105938296765089, 0.0050796158611774445, 0.0049955458380281925, 0.004912571981549263, 0.004846564028412104, 0.004727778024971485, 0.0046015395782887936, 0.004524237476289272, 0.0044594453647732735, 0.11236804723739624, 0.004306926857680082, 0.004288761410862207, 0.14466546475887299, 0.1368284970521927, 0.13395437598228455, 0.004383809398859739, 0.004461623262614012, 0.004531289450824261, 0.11069303005933762, 0.004682892467826605, 0.004711865447461605, 0.12817886471748352, 0.004817093722522259, 0.00494740903377533, 0.10964835435152054, 0.11049642413854599, 0.11541962623596191, 0.005432331934571266, 0.13160927593708038, 0.005729899276047945, 0.005819845478981733, 0.00600951723754406, 0.10689695179462433, 0.006163276266306639, 0.09126880764961243, 0.006440988276153803, 0.006514193024486303, 0.0065519725903868675, 0.00658400496467948, 0.006612554658204317, 0.0065782335586845875, 0.11601107567548752, 0.006519480608403683, 0.006522852927446365, 0.006547047756612301, 0.00639805942773819, 0.11174160242080688, 0.006336954887956381, 0.006355426274240017, 0.006241830065846443, 0.10970150679349899, 0.006166869308799505, 0.0060971383936703205, 0.11271840333938599, 0.006025399547070265, 0.00604051211848855, 0.005959088448435068, 0.005940150935202837, 0.005889267195016146, 0.005819555837661028, 0.005649853032082319, 0.005571156740188599, 0.005525447893887758, 0.005420656409114599, 0.005246307700872421, 0.005096728447824717, 0.004957384895533323, 0.004866560455411673, 0.004742775112390518, 0.004587741103023291, 0.004467672202736139, 0.004363071173429489, 0.1367875635623932, 0.0041927192360162735, 0.0041269720532000065, 0.004079151898622513, 0.004033134784549475, 0.003930584993213415, 0.0038847634568810463, 0.0037943252827972174, 0.0037187428679317236, 0.0036596436984837055, 0.0035803106147795916, 0.0035140840336680412, 0.0034900116734206676, 0.003368029370903969, 0.0032878750935196877, 0.0032071333844214678, 0.0031602007802575827, 0.003064225660637021, 0.0029845263343304396, 0.0029258416034281254, 0.002872333163395524, 0.002818762557581067, 0.4536997973918915, 0.0029238532297313213, 0.13524146378040314, 0.12063867598772049, 0.11361491680145264, 0.11160384118556976, 0.004221621435135603, 0.004493091721087694, 0.004727955441921949, 0.00500917062163353, 0.005154490936547518, 0.005322671495378017, 0.005466763395816088, 0.005619046278297901, 0.005686032585799694, 0.12878187000751495, 0.005857632029801607, 0.005922900512814522, 0.005981097463518381, 0.09689696878194809, 0.00608394481241703, 0.006136407144367695, 0.11370174586772919, 0.006196758709847927, 0.006229612044990063, 0.11981602013111115, 0.006284166127443314, 0.006335332524031401, 0.006325516849756241, 0.0063588363118469715, 0.3472153842449188, 0.11848852783441544, 0.006940919905900955, 0.007306814659386873, 0.007469230331480503, 0.007655499503016472, 0.007808193564414978, 0.007959648966789246, 0.008004413917660713, 0.008069954812526703, 0.008025707677006721, 0.008024396374821663, 0.00794823095202446, 0.007835162803530693, 0.10637445002794266, 0.007739584427326918, 0.007625408470630646, 0.007522468455135822, 0.007509532850235701, 0.007273530587553978, 0.007125300820916891, 0.006966131739318371, 0.006871279329061508, 0.09858248382806778, 0.006534863263368607, 0.006493581924587488, 0.006335529498755932, 0.006242731120437384, 0.1272103488445282, 0.005992289166897535, 0.005900121759623289, 0.005840793251991272, 0.10460489243268967, 0.005662207957357168, 0.005613690707832575, 0.005544626619666815, 0.005512813106179237, 0.005354929715394974, 0.11405529826879501, 0.005266337189823389, 0.005289728287607431, 0.0051357378251850605, 0.005075883585959673, 0.1020577996969223, 0.004959750454872847, 0.004998642019927502, 0.004907602909952402, 0.004918170627206564, 0.0047763315960764885, 0.004680958576500416, 0.10744568705558777, 0.004630923271179199, 0.004583440721035004, 0.004558807238936424, 0.004462096840143204, 0.10581553727388382, 0.004430127330124378, 0.004423629026859999, 0.004353711381554604, 0.004323872737586498, 0.0043023317120969296, 0.004300744738429785, 0.0992358848452568, 0.004168356768786907, 0.10771358758211136, 0.004185803700238466, 0.00422236043959856, 0.004217603709548712, 0.004239071160554886, 0.00419020326808095, 0.004206182435154915, 0.004148623440414667, 0.004061257466673851, 0.10346972197294235, 0.004014937207102776, 0.1344904601573944, 0.004062995780259371, 0.004089766182005405, 0.004069743677973747, 0.004097775090485811, 0.00406313082203269, 0.12441352009773254, 0.004068440292030573, 0.004088487941771746, 0.12203428149223328, 0.0041655576787889, 0.0041786786168813705, 0.004174777772277594, 0.004261947236955166, 0.12413015216588974, 0.004266566596925259, 0.004248980898410082, 0.0042710453271865845, 0.0042946054600179195, 0.004271334037184715, 0.004242835100740194, 0.004233258776366711, 0.004198203794658184, 0.004129239358007908, 0.004105469211935997, 0.00404371740296483, 0.004035061225295067, 0.1223101019859314, 0.003970286343246698, 0.003915051463991404, 0.0038827634416520596, 0.0038555236533284187, 0.0037925776559859514, 0.0037905098870396614, 0.0037024326156824827, 0.003669621190056205, 0.0036050837952643633, 0.41196751594543457, 0.003777578705921769, 0.003962274175137281, 0.004146340303122997, 0.004277309402823448, 0.10561078041791916, 0.12695109844207764, 0.00476381741464138, 0.00489448569715023, 0.10857509821653366, 0.005251825321465731, 0.005362559109926224, 0.00544153805822134, 0.005565294995903969, 0.005674374755471945, 0.0056402068585157394, 0.10798592865467072, 0.005737776402384043, 0.005826429929584265, 0.1099727675318718, 0.1058422401547432, 0.005932498257607222, 0.006016701925545931, 0.0060447691939771175, 0.1017666682600975, 0.006140331272035837, 0.006202484481036663, 0.00622542854398489, 0.00616447115316987, 0.006138547323644161, 0.10092134773731232, 0.006092178635299206, 0.006066285539418459, 0.006080930121243, 0.006001593545079231, 0.005925093777477741, 0.005864524282515049, 0.005753116682171822, 0.005709148943424225, 0.0055839610286056995, 0.13003462553024292, 0.00542723061516881, 0.740971565246582, 0.005934692453593016, 0.11210772395133972, 0.007091120351105928, 0.007537239696830511, 0.008026820607483387, 0.00843044463545084, 0.008761021308600903, 0.00908572692424059, 0.009329847060143948, 0.009525706991553307, 0.009625471197068691, 0.009736437350511551, 0.009812743403017521, 0.009765086695551872, 0.009754294529557228, 0.009690203703939915, 0.009572189301252365, 0.00949318427592516, 0.009333684109151363, 0.10556940734386444, 0.009058477357029915, 0.00891015212982893, 0.008789651095867157, 0.008631610311567783, 0.008419889025390148, 0.008248576894402504, 0.008056829683482647, 0.007886379025876522, 0.0077033876441419125, 0.007498201448470354, 0.10084671527147293, 0.11080550402402878, 0.007030252367258072, 0.006961224600672722, 0.00681364256888628, 0.006706056650727987, 0.006616145372390747, 0.11847871541976929, 0.006397000048309565, 0.00630823103711009, 0.006210156250745058, 0.1173078641295433, 0.006103905383497477, 0.006059529259800911, 0.0059502581134438515, 0.10599099099636078, 0.3744231164455414, 0.006073722615838051, 0.006278806831687689, 0.006503068841993809, 0.006636999547481537, 0.00682451156899333, 0.006890879012644291, 0.006956677418202162, 0.00703576160594821, 0.0069749075919389725, 0.0069849370047450066, 0.006926810368895531, 0.006881136447191238, 0.006846755743026733, 0.006762717850506306, 0.006696625147014856, 0.12445670366287231, 0.0064996681176126, 0.10380464047193527, 0.006420417223125696, 0.0064015015959739685, 0.0063612195663154125, 0.006301297340542078, 0.006242154166102409, 0.0061785499565303326, 0.006103643216192722, 0.005946245510131121, 0.00585962925106287, 0.005756320897489786, 0.005691956263035536, 0.00554327666759491, 0.005469577386975288, 0.10664903372526169, 0.005245023872703314, 0.005204066168516874, 0.005096621811389923, 0.005028120242059231, 0.004951699636876583, 0.004852983634918928, 0.004790807142853737, 0.004710569977760315, 0.00461498973891139, 0.0045057665556669235, 0.004458324983716011, 0.004371961113065481, 0.004292197525501251, 0.004175503738224506, 0.00408140430226922, 0.004025901667773724, 0.003949132282286882, 0.0038378580939024687, 0.003801388666033745, 0.0036891410127282143, 0.003619600785896182, 0.0035415294114500284, 0.003448295872658491, 0.0034043726045638323, 0.00336313433945179, 0.00324440049007535, 0.0032070064917206764, 0.0031277763191610575, 0.003061132738366723, 0.11720658838748932, 0.0029830955900251865, 0.0029974004719406366, 0.002965423045679927, 0.002920388476923108, 0.002883403329178691, 0.12224152684211731, 0.11135680228471756, 0.0029124286957085133, 0.0029599620029330254, 0.0029759572353214025, 0.0030021248385310173, 0.10795295238494873, 0.003051670966669917, 0.0030639439355582, 0.0031116981990635395, 0.003134879982098937, 0.003112729638814926, 0.11802513897418976, 0.0031678753439337015, 0.0032089913729578257, 0.14020761847496033, 0.0032618679106235504, 0.003340788185596466, 0.0033858660608530045, 0.00338812917470932, 0.0033905976451933384, 0.0034138078335672617, 0.0034098660107702017, 0.003474990837275982, 0.003433406352996826, 0.003375507425516844, 0.0033893371000885963, 0.003355475841090083, 0.0033163847401738167, 0.12786179780960083, 0.09847371280193329, 0.0033418843522667885, 0.0033537475392222404, 0.0033758508507162333, 0.003396918997168541, 0.0033994391560554504, 0.003388873767107725, 0.003383336355909705, 0.003394103841856122, 0.003365203505381942, 0.003343448508530855, 0.0033330644946545362, 0.11085323989391327, 0.003288831328973174, 0.0032868264243006706, 0.14112414419651031, 0.0033505670726299286, 0.0033581231255084276, 0.003368936013430357, 0.12676003575325012, 0.12019121646881104, 0.0034981267526745796, 0.003591735614463687, 0.003649423597380519, 0.0036553367972373962, 0.11761429160833359, 0.003802831517532468, 0.003800384933128953, 0.0038413330912590027, 0.0039026939775794744, 0.11590263247489929, 0.004007383249700069, 0.0039844391867518425, 0.0040033841505646706, 0.004060292150825262, 0.004031192045658827, 0.004034696612507105, 0.00402717525139451, 0.12349182367324829, 0.004012747667729855, 0.004092566203325987, 0.1010422483086586, 0.0041011604480445385, 0.004100486170500517, 0.004088391084223986, 0.1256347894668579, 0.0041370452381670475, 0.004211010877043009, 0.004201078787446022, 0.004203273449093103, 0.004220418632030487, 0.11062341183423996, 0.0042256517335772514, 0.004223437514156103, 0.004216540604829788, 0.004236823879182339, 0.004217327572405338, 0.00417492026463151, 0.004183162469416857, 0.12505221366882324, 0.0041185286827385426, 0.004125757608562708, 0.0041350251995027065, 0.004122198559343815, 0.004122560378164053, 0.12516944110393524, 0.004049703013151884, 0.10728035867214203, 0.0041405800729990005, 0.004108976107090712, 0.004123032093048096, 0.004154201131314039, 0.004132923204451799, 0.0041382331401109695, 0.004153330344706774, 0.004114880692213774, 0.004067046567797661, 0.004011653363704681, 0.003954197280108929, 0.003929916303604841, 0.0039178067818284035, 0.13202102482318878, 0.003861798672005534, 0.0038016699254512787, 0.003831874579191208, 0.0037593774031847715, 0.0037171151489019394, 0.0036788308061659336, 0.0036519316490739584, 0.0036366316489875317, 0.003581668483093381, 0.0035434614401310682, 0.003523578168824315, 0.0034618498757481575, 0.0033944305032491684, 0.003376386361196637, 0.0032870243303477764, 0.1303347945213318, 0.003238873789086938, 0.0032084365375339985, 0.12490925192832947, 0.003239487065002322, 0.003245663596317172, 0.003249992849305272, 0.0032627105247229338, 0.003246111096814275, 0.003238385310396552, 0.003210691502317786, 0.0031738928519189358, 0.0031817324925214052, 0.003177966922521591, 0.0031336210668087006, 0.003082391107454896, 0.003059865441173315, 0.003003673395141959, 0.0030011506751179695, 0.0029618588741868734, 0.002928805770352483, 0.002884823828935623, 0.0028130377177149057, 0.0027997042052447796, 0.14697112143039703, 0.13521204888820648, 0.002800609450787306, 0.0028028434608131647, 0.002834743820130825, 0.0028435536660254, 0.002851199358701706, 0.0028960024937987328, 0.00286501063965261, 0.0028725783340632915, 0.002849251963198185, 0.1329856514930725, 0.002858393592759967, 0.0028861972969025373, 0.0029141816776245832, 0.002880306448787451, 0.0029155134689062834, 0.0028834266122430563, 0.12385684996843338, 0.0028912967536598444, 0.002941505517810583, 0.002946147695183754, 0.002960420213639736, 0.0029357392340898514, 0.0029412521980702877, 0.00294662662781775, 0.0029205994214862585, 0.1280984878540039, 0.1352415829896927, 0.002968044485896826, 0.0030349870212376118, 0.0030456832610070705, 0.003096715547144413, 0.0031064320355653763, 0.0031230899039655924, 0.0031255155336111784, 0.12184707075357437, 0.0031778255943208933, 0.0032043305691331625, 0.11952713131904602, 0.003242200706154108, 0.0033106824848800898, 0.003317098366096616, 0.11417276412248611, 0.003429797478020191, 0.003441580105572939, 0.0034929956309497356, 0.0035171767231076956, 0.0035304746124893427, 0.0035217676777392626, 0.003533285576850176, 0.0035393729340285063, 0.003515469841659069, 0.003497542580589652, 0.0034877178259193897, 0.003433133475482464, 0.0034269611351191998, 0.0033889207988977432, 0.003336428664624691, 0.003294846974313259, 0.0032757818698883057, 0.0032242562156170607, 0.0031768891494721174, 0.003146059811115265, 0.0031344154849648476, 0.003076322143897414, 0.0030310042202472687, 0.0029633117374032736, 0.002927396446466446, 0.0028767220210283995, 0.0028494263533502817, 0.0027980292215943336, 0.439098984003067, 0.12370264530181885, 0.0030829133465886116, 0.0032377925235778093, 0.0033757358323782682, 0.12544888257980347, 0.00365134933963418, 0.0038160383701324463, 0.1052691861987114, 0.0040615457110106945, 0.1330406367778778, 0.004381850361824036, 0.10474905371665955, 0.004637392237782478, 0.00475479569286108, 0.004881484899669886, 0.004960563965141773, 0.0050333719700574875, 0.005097759887576103, 0.09945443272590637, 0.10048277676105499, 0.005291665438562632, 0.00537740858271718, 0.005453010555356741, 0.0054952786304056644, 0.3734061121940613, 0.005746940150856972, 0.0059758261777460575, 0.006227919366210699, 0.1047159731388092, 0.006577984895557165, 0.10118063539266586, 0.006929764989763498, 0.007057307753711939, 0.12057657539844513, 0.007379532791674137, 0.09860561788082123, 0.007607459556311369, 0.007719691842794418, 0.007717848755419254, 0.007800563238561153, 0.007807533256709576, 0.00774215767160058, 0.007756524719297886, 0.11921539157629013, 0.007700917311012745, 0.007651446387171745, 0.007615227252244949, 0.007477871607989073, 0.007436067797243595, 0.007343325763940811, 0.11983978003263474, 0.0072055840864777565, 0.007093306165188551, 0.007017961237579584, 0.006950316950678825, 0.006834771949797869, 0.006719703786075115, 0.00662697060033679, 0.006463687401264906, 0.006401303689926863, 0.006249713245779276, 0.11266279965639114, 0.1202634796500206, 0.005963594187051058, 0.005961517803370953, 0.005924718454480171, 0.005855448078364134, 0.005770313553512096, 0.005709423683583736, 0.00560130225494504, 0.005543762817978859, 0.0054387585259974, 0.10971403121948242, 0.005328139755874872, 0.00521225668489933, 0.005198993720114231, 0.005158619489520788, 0.0050210910849273205, 0.00496883038431406, 0.004932318348437548, 0.0048217130824923515, 0.004728308878839016, 0.004665355663746595, 0.004588685929775238, 0.004497071262449026, 0.004463300108909607, 0.004362666048109531, 0.004300076514482498, 0.004202578216791153, 0.0041239033453166485, 0.0040100147016346455, 0.0039471047930419445, 0.0038701703306287527, 0.0038189927581697702, 0.0037811552174389362, 0.003653276478871703, 0.0035864929668605328, 0.13789565861225128, 0.003526056418195367, 0.003474636236205697, 0.0034567860420793295, 0.0034121181815862656, 0.003387745935469866, 0.0033549342770129442, 0.0033216977026313543, 0.0032778680324554443, 0.0032262227032333612, 0.0032011617440730333, 0.0031365621834993362, 0.0031254489440470934, 0.1326417177915573, 0.003056153655052185, 0.0030518588609993458, 0.0030545515473932028, 0.003035046625882387, 0.12145526707172394, 0.0030037236865609884, 0.0030141070019453764, 0.003021307522431016, 0.003036266891285777, 0.0030340005178004503, 0.003010703017935157, 0.0029858541674911976, 0.12320894747972488, 0.00302079226821661, 0.002992626279592514, 0.003005370032042265, 0.12718434631824493, 0.0030317087657749653, 0.1053176149725914, 0.003107562894001603, 0.0031642690300941467, 0.0032063499093055725, 0.0032269898802042007, 0.003222513711079955, 0.0032434009481221437, 0.0032490508165210485, 0.0032437974587082863, 0.003249869914725423, 0.0032265859190374613, 0.00323873246088624, 0.003212516661733389, 0.0031938166357576847, 0.003149852156639099, 0.0031446253415197134, 0.0031011421233415604, 0.0030593271367251873, 0.003043736796826124, 0.12844081223011017, 0.003009048756211996, 0.002995256567373872, 0.12983956933021545, 0.11770303547382355, 0.11639603227376938, 0.003153468482196331, 0.0032471190206706524, 0.0032901775557547808, 0.003363068914040923, 0.003407455049455166, 0.003411959856748581, 0.0034446732606738806, 0.0034605665132403374, 0.003446637187153101, 0.003444020403549075, 0.003453661222010851, 0.1174020990729332, 0.0034635576885193586, 0.003479522420093417, 0.0034738481044769287, 0.003479983191937208, 0.003481766441836953, 0.0034739437978714705, 0.003441004315391183, 0.0034218565560877323, 0.003427201882004738, 0.0033682927023619413, 0.0033653611317276955, 0.003303235862404108, 0.003297790652140975, 0.0032449786085635424, 0.0032166284509003162, 0.00317107024602592, 0.0031456563156098127, 0.003113726619631052, 0.12117183208465576, 0.00303718657232821, 0.0030508399941027164, 0.11914319545030594, 0.0030532353557646275, 0.0030614419374614954, 0.0030845256987959146, 0.0030615648720413446, 0.003061378374695778, 0.0030691686552017927, 0.0030701046343892813, 0.0030414650682359934, 0.003042273921892047, 0.0029910628218203783, 0.00296298461034894, 0.0029666260816156864, 0.13288789987564087, 0.002950008725747466, 0.0029437867924571037, 0.002945401007309556, 0.0029437632765620947, 0.0029337143059819937, 0.0028986397664994, 0.12112486362457275, 0.002905357163399458, 0.10862322151660919, 0.002985678380355239, 0.002988901687785983, 0.0030097304843366146, 0.003027394413948059, 0.12289793789386749, 0.003107599448412657, 0.0031131687574088573, 0.11495638638734818, 0.13380149006843567, 0.11877089738845825, 0.0034044997300952673, 0.1165425032377243, 0.0036017901729792356, 0.0037023802287876606, 0.12044522911310196, 0.003923747222870588, 0.10914742946624756, 0.1362503618001938, 0.004304904956370592, 0.11843021959066391, 0.004577756859362125, 0.004728961270302534, 0.004835506435483694, 0.00487747136503458, 0.004970722831785679, 0.005026109982281923, 0.0050873043946921825, 0.0050846426747739315, 0.0051289391703903675, 0.005131639074534178, 0.005084040574729443, 0.005121349822729826, 0.005034858360886574, 0.005001694895327091, 0.004985140636563301, 0.004902729764580727, 0.0048498911783099174, 0.004839298315346241, 0.11358599364757538, 0.00471822964027524, 0.004720871336758137, 0.0047102984972298145, 0.004647921770811081, 0.004567343275994062, 0.004546326585114002, 0.004476502072066069, 0.004425165243446827, 0.12561972439289093, 0.004359749145805836, 0.004305396229028702, 0.004262532573193312, 0.004235777538269758, 0.00419241888448596, 0.1315288096666336, 0.00414311233907938, 0.11935273557901382, 0.1264658123254776, 0.004228182602673769, 0.0042723678052425385, 0.004301220644265413, 0.004307417199015617, 0.004335631616413593, 0.0043128603138029575, 0.004336212296038866, 0.0042850845493376255, 0.004279057029634714, 0.11574123054742813, 0.004245263524353504, 0.004255356732755899, 0.004247981123626232, 0.004217862151563168, 0.004223180469125509, 0.4232496917247772, 0.004383652005344629, 0.11413121223449707, 0.004728516563773155, 0.0049202111549675465, 0.00503188231959939, 0.005178873892873526, 0.005276069510728121, 0.005330387968569994, 0.0054070050828158855, 0.0054554408416152, 0.0055057392455637455, 0.005514425691217184, 0.0055322591215372086, 0.005487733520567417, 0.11307664215564728, 0.005487442947924137, 0.005459046456962824, 0.00544670270755887, 0.005430353805422783, 0.0053854770958423615, 0.11594478040933609, 0.13084040582180023, 0.005411688704043627, 0.005401016212999821, 0.10498825460672379, 0.1294904202222824, 0.0055403695441782475, 0.00559642119333148, 0.005610097199678421, 0.005603131838142872, 0.005586405750364065, 0.00560053251683712, 0.005568389315158129, 0.12772129476070404, 0.0055595096200704575, 0.005538188852369785, 0.00552466232329607, 0.005500186234712601, 0.005446255207061768, 0.005437084008008242, 0.005363072734326124, 0.005288568791002035, 0.005265531130135059, 0.005150262266397476, 0.005121421068906784, 0.005009396001696587, 0.11620234698057175, 0.004902144428342581, 0.0048807463608682156, 0.126974418759346, 0.004813151899725199, 0.004793341271579266, 0.004773132037371397, 0.40211349725723267, 0.0049371132627129555, 0.005122562870383263, 0.005235338117927313, 0.005360976327210665, 0.005489928647875786, 0.11357846111059189, 0.005668490659445524, 0.005729541182518005, 0.005820746999233961, 0.0058332220651209354, 0.005884363781660795, 0.005911398213356733, 0.005890574771910906, 0.10713949054479599, 0.005888438783586025, 0.005890293512493372, 0.0059089395217597485, 0.005843695253133774, 0.005830236244946718, 0.005819679703563452, 0.005766988266259432, 0.0056727793999016285, 0.005612565204501152, 0.005554789211601019, 0.005491361487656832, 0.00539748277515173, 0.005312912631779909, 0.10168768465518951, 0.005200790707021952, 0.12204015254974365, 0.005133653525263071, 0.0050981431268155575, 0.11127163469791412, 0.005091991741210222, 0.0051308549009263515, 0.005065111443400383, 0.005069933366030455, 0.005003979429602623, 0.005035266280174255, 0.004998631309717894, 0.004931413568556309, 0.004872505087405443, 0.00479824049398303, 0.004730186890810728, 0.004657558631151915, 0.004599280655384064, 0.004543739836663008, 0.0045301043428480625, 0.004398975055664778, 0.1130201518535614, 0.004346811678260565, 0.004264418967068195, 0.004270502366125584, 0.004163586068898439, 0.004239656496793032, 0.004158827941864729, 0.004111793823540211, 0.004041383042931557, 0.12591272592544556, 0.003952688071876764, 0.11081160604953766, 0.004014400765299797, 0.003988801501691341, 0.1306266486644745, 0.10321264714002609, 0.00399808818474412, 0.004097518511116505, 0.004055837634950876, 0.004111682064831257, 0.004096432588994503, 0.0041392482817173, 0.004201759584248066, 0.1112576574087143, 0.004109976813197136, 0.004161561373621225, 0.14040420949459076, 0.004154882859438658, 0.004213663749396801, 0.10957805812358856, 0.0042768255807459354, 0.00432731444016099, 0.004336164798587561, 0.1070079356431961, 0.004392960108816624, 0.0044482615776360035, 0.11215300112962723, 0.004484449978917837, 0.14162902534008026, 0.0045998189598321915, 0.004755890928208828, 0.00461317365989089, 0.004706600680947304, 0.0047022392973303795, 0.004717515781521797, 0.0047430540435016155, 0.004764007870107889, 0.004677319899201393, 0.004638474900275469, 0.004566674120724201, 0.004591184668242931, 0.004533690400421619, 0.004551438149064779, 0.004408468492329121, 0.004337399732321501, 0.004386435262858868, 0.004288014490157366, 0.004268691875040531, 0.004168505780398846, 0.004146987572312355, 0.12403664737939835, 0.003969389945268631, 0.003986007068306208, 0.003969445824623108, 0.0038889488205313683, 0.003967977128922939, 0.13595199584960938, 0.0038369144313037395, 0.003907454200088978, 0.0037868840154260397, 0.003771107876673341, 0.1272168606519699, 0.0038538749795407057, 0.0038295506965368986, 0.0038205732125788927, 0.11693663150072098, 0.003901422955095768, 0.12943539023399353, 0.003923036623746157, 0.003872540546581149, 0.003930557519197464, 0.00397875253111124, 0.1305665522813797, 0.004013073164969683, 0.00398343475535512, 0.004021988715976477, 0.003997099120169878, 0.004109418485313654, 0.004058154299855232, 0.004037925507873297, 0.0039971559308469296, 0.003990817349404097, 0.003962933085858822, 0.00389957707375288, 0.003908011596649885, 0.0039024169091135263, 0.0038054874166846275, 0.10953500866889954, 0.003808705834671855, 0.0038125941064208746, 0.003796712262555957, 0.0037511580158025026, 0.0037248984444886446, 0.003663281910121441, 0.003643969539552927, 0.0036119476426392794, 0.003611208638176322, 0.0035157492384314537, 0.0035010145511478186, 0.003476113313809037, 0.0034235049970448017, 0.0034166653640568256, 0.11663211137056351, 0.003325548255816102, 0.003411990823224187, 0.0032931191381067038, 0.0032604471780359745, 0.0032575682271271944, 0.003219000529497862, 0.003208129433915019, 0.0032445378601551056, 0.0031422453466802835, 0.0031496905721724033, 0.0030905427411198616, 0.003031535306945443, 0.0030046480242162943, 0.002981665078550577, 0.002969693159684539, 0.002930015791207552, 0.00290303654037416, 0.0028318334370851517, 0.0028930690605193377, 0.0027771948371082544, 0.0027773987967520952, 0.002731156302616, 0.002724084537476301, 0.002665891544893384, 0.002617791062220931, 0.00260104495100677, 0.0025393704418092966, 0.0025076104793697596, 0.00251210480928421, 0.0024987144861370325, 0.002425959799438715, 0.002387637970969081, 0.0023441738449037075, 0.12574328482151031, 0.002368743298575282, 0.11818879097700119, 0.0023590114433318377, 0.0023808313999325037, 0.002389008877798915, 0.0023858745116740465, 0.1438305824995041, 0.0024321163073182106, 0.0024885039310902357, 0.002485129050910473, 0.002601233310997486, 0.0025023953057825565, 0.002526037162169814, 0.0025363836903125048, 0.0025528857950121164, 0.0025254948996007442, 0.002567537594586611, 0.002523579867556691, 0.0025022716727107763, 0.00249195727519691, 0.0024757888168096542, 0.0024732365272939205, 0.15083114802837372, 0.0024932350497692823, 0.13410161435604095, 0.0025542196817696095, 0.00259756064042449, 0.0025731234345585108, 0.0025880681350827217, 0.002618831116706133, 0.0026064603589475155, 0.002630056580528617, 0.10340889543294907, 0.002687734318897128, 0.13476596772670746, 0.14733822643756866, 0.002798532834276557, 0.002831672551110387, 0.0029624495655298233, 0.003026828169822693, 0.0029698216821998358, 0.003030108753591776, 0.003039998933672905, 0.0030600845348089933, 0.0030878176912665367, 0.003066891571506858, 0.1557595133781433, 0.003106946125626564, 0.0031125338282436132, 0.003149026306346059, 0.14468395709991455, 0.003215599339455366, 0.0032126528676599264, 0.0032810394186526537, 0.0032864129170775414, 0.0033210967667400837, 0.0032982260454446077, 0.0033164401538670063, 0.003285551443696022, 0.0032792550045996904, 0.0032882841769605875, 0.0033517780248075724, 0.0033188732340931892, 0.10673702508211136, 0.0032385133672505617, 0.003235063049942255, 0.003225545398890972, 0.13247409462928772, 0.13273829221725464, 0.003314168192446232, 0.10560092329978943, 0.00338724791072309, 0.003459556493908167, 0.003500143066048622, 0.003536027856171131, 0.003550094785168767, 0.003602957585826516, 0.15407614409923553, 0.0036215290892869234, 0.003699572989717126, 0.0037385921459645033, 0.0037657474167644978, 0.11991112679243088, 0.10631493479013443, 0.0038177657406777143, 0.00386635959148407, 0.003912554122507572, 0.003980277571827173, 0.0039950343780219555, 0.003964819945394993, 0.12379288673400879, 0.004001560155302286, 0.09772109240293503, 0.004057093523442745, 0.004112965893000364, 0.004290776327252388, 0.004146751947700977, 0.004154505208134651, 0.004157192073762417, 0.004285200033336878, 0.00416641915217042, 0.13015414774417877, 0.004126991610974073, 0.004176669288426638, 0.0041548265144228935, 0.10875667631626129, 0.004151878412812948, 0.12797604501247406, 0.004309777170419693, 0.0042570969089865685, 0.004360437858849764, 0.004365535918623209, 0.004335481207817793, 0.0042798323556780815, 0.004237974528223276, 0.004249392077326775, 0.0042184400372207165, 0.00416573416441679, 0.004180369433015585, 0.004136490635573864, 0.004091970156878233, 0.004079828970134258, 0.004031949210911989, 0.003968289587646723, 0.4018637239933014, 0.004060851875692606, 0.004164718557149172, 0.004288320429623127, 0.004363343119621277, 0.004555841907858849, 0.004651607945561409, 0.0045414092019200325, 0.004609518684446812, 0.004707853309810162, 0.004734196700155735, 0.004606923088431358, 0.004595712758600712, 0.004602082539349794, 0.13052096962928772, 0.004630507901310921, 0.004629906732589006, 0.0046303593553602695, 0.004562309477478266, 0.004582608584314585, 0.004566724877804518, 0.004575397353619337, 0.00447828508913517, 0.004458255134522915, 0.10283617675304413, 0.004390972666442394, 0.004468853585422039, 0.004420461133122444, 0.0043465448543429375, 0.004331238102167845, 0.004260716959834099, 0.004245501942932606, 0.004226191900670528, 0.004147323779761791, 0.004168857354670763, 0.004135276190936565, 0.004013283643871546, 0.0040053874254226685, 0.0038996939547359943, 0.0038442984223365784, 0.003931844606995583, 0.0037689877208322287, 0.0036985729821026325, 0.0036806189455091953, 0.0036426614969968796, 0.11996084451675415, 0.0035773864947259426, 0.003562298836186528, 0.0034985074307769537, 0.10086984187364578, 0.0034832414239645004, 0.0035195581149309874, 0.003499278100207448, 0.0034537205938249826, 0.0034374322276562452, 0.12164179980754852, 0.0034492292907088995, 0.12243082374334335, 0.0035215707030147314, 0.1192951425909996, 0.0035465110559016466, 0.00357425631955266, 0.00363904912956059, 0.42572981119155884, 0.003791659837588668, 0.12928150594234467, 0.004203458316624165, 0.004310725722461939, 0.004526013508439064, 0.004566687624901533, 0.13756950199604034, 0.0048927608877420425, 0.004992024041712284, 0.0050241476856172085, 0.005154591053724289, 0.005263993050903082, 0.005319169256836176, 0.005316775757819414, 0.12281028181314468, 0.005396041553467512, 0.00542087946087122, 0.11297755688428879, 0.005437484942376614, 0.005454402416944504, 0.005653157364577055, 0.10726325213909149, 0.00558025436475873, 0.005507160909473896, 0.005533292423933744, 0.005546238739043474, 0.0054961615242064, 0.11174673587083817, 0.005501636303961277, 0.005473833065479994, 0.0056147282011806965, 0.005424799397587776, 0.005452349316328764, 0.0053666322492063046, 0.00534503348171711, 0.005338125862181187, 0.005266109015792608, 0.005176135804504156, 0.0051821074448525906, 0.005123286508023739, 0.005055216606706381, 0.005012140609323978, 0.004906609188765287, 0.004833409562706947, 0.004843296483159065, 0.00468569016084075, 0.004625904373824596, 0.10387711971998215, 0.37760454416275024, 0.004675122443586588, 0.004781069699674845, 0.10803930461406708, 0.1190948337316513, 0.0051051052287220955, 0.0052526299841701984, 0.005328441504389048, 0.005502661690115929, 0.12034212052822113, 0.005591741297394037, 0.005701654590666294, 0.005808383692055941, 0.005898496601730585, 0.09735659509897232, 0.00586165115237236, 0.005799571517854929, 0.0058110360987484455, 0.005816291086375713, 0.0058058989234268665, 0.11347546428442001, 0.005881516728550196, 0.005852901842445135, 0.005822564475238323, 0.005821050610393286, 0.1421166956424713, 0.005898083560168743, 0.1204904243350029, 0.005766461603343487, 0.005849221721291542, 0.0058005573228001595, 0.005769153591245413, 0.005873902700841427, 0.005740205757319927, 0.11260591447353363, 0.005770683754235506, 0.005685308016836643, 0.005670506041496992, 0.005653677508234978, 0.005578150972723961, 0.00547107495367527, 0.005469910334795713, 0.005372585263103247, 0.10398342460393906, 0.42390677332878113, 0.005482435692101717, 0.005719273816794157, 0.005731934681534767, 0.005917190574109554, 0.005897922907024622, 0.0061077396385371685, 0.006016627885401249, 0.006056316662579775, 0.006104886997491121, 0.006164921447634697, 0.006139506120234728, 0.00606853561475873, 0.006052122917026281, 0.10525476187467575, 0.006003098096698523, 0.006123692728579044, 0.005995169747620821, 0.006003148853778839, 0.0059656607918441296, 0.1001051738858223, 0.005846340209245682, 0.005828579422086477, 0.005756616126745939, 0.11727204918861389, 0.005715681239962578, 0.00567445857450366, 0.10461040586233139, 0.10711900889873505, 0.005679119378328323, 0.0057782516814768314, 0.005737354978919029, 0.005711756646633148, 0.0056681218557059765, 0.005673393607139587, 0.005615027155727148, 0.005598363466560841, 0.005532099399715662, 0.11484630405902863, 0.005581430159509182, 0.0055133807472884655, 0.005385995842516422, 0.005497532431036234, 0.0053903814405202866, 0.005281568039208651, 0.0052787018939852715, 0.005222429521381855, 0.005181047599762678, 0.00514346593990922, 0.0050630588084459305, 0.004929764196276665, 0.1094207838177681, 0.004903828259557486, 0.004857887048274279, 0.127911776304245, 0.004756776150316, 0.004790153354406357, 0.0047233025543391705, 0.004758789669722319, 0.004657594487071037, 0.004658553283661604, 0.004669019486755133, 0.004634954500943422, 0.004533992614597082, 0.004515819251537323, 0.0045037828385829926, 0.00436310563236475, 0.0043664840050041676, 0.004288885742425919, 0.004264538642019033, 0.004192723892629147, 0.004154357593506575, 0.0041647483594715595, 0.11495305597782135, 0.004017021507024765, 0.0039962055161595345, 0.003986693453043699, 0.003958612214773893, 0.0039513735100626945, 0.003866941435262561, 0.0038315700367093086, 0.003822223050519824, 0.0037957550957798958, 0.0037151509895920753, 0.003688044613227248, 0.0036889647599309683, 0.0036174554843455553, 0.0035567828454077244, 0.003590998938307166, 0.0034912144765257835, 0.0034815038088709116, 0.003439866006374359, 0.0034193084575235844, 0.003358102636411786, 0.0032948420848697424, 0.003243033541366458, 0.0032929610460996628, 0.0032008630223572254, 0.003201435785740614, 0.0031430486124008894, 0.003057406283915043, 0.003108950098976493, 0.0030440904665738344, 0.0030303189996629953, 0.0029242336750030518, 0.0029336188454180956, 0.002891634590923786, 0.0028230473399162292, 0.11705046892166138, 0.0027825715951621532, 0.00283616129308939, 0.10600189119577408, 0.0027977891732007265, 0.0027968750800937414, 0.002833646722137928, 0.105351023375988, 0.002859654603525996, 0.0028525306843221188, 0.002850230783224106, 0.0028449618257582188, 0.00288784084841609, 0.002903977409005165, 0.0028896532021462917, 0.1124948188662529, 0.0029359643813222647, 0.11458776146173477, 0.9349626302719116, 0.12853974103927612, 0.0034204500261694193, 0.0035975307691842318, 0.003874359652400017, 0.004057589918375015, 0.00428619422018528, 0.004350739531219006, 0.004507716745138168, 0.0045978301204741, 0.004885506816208363, 0.004851028323173523, 0.0049302540719509125, 0.005013921298086643, 0.005052853375673294, 0.005209921393543482, 0.00517927436158061, 0.005217715632170439, 0.00510057620704174, 0.11034514755010605, 0.0052773114293813705, 0.005202992353588343, 0.005204705521464348, 0.45379605889320374, 0.005381049122661352, 0.0054621524177491665, 0.005636321380734444, 0.005739219021052122, 0.005764584988355637, 0.005890966858714819, 0.005911672487854958, 0.005945907905697823, 0.13327744603157043, 0.006153819616883993, 0.1336393654346466, 0.0061242422088980675, 0.006167948246002197, 0.10697806626558304, 0.006243204232305288, 0.006256127264350653, 0.006284624338150024, 0.0062435041181743145, 0.006223875563591719, 0.0063689216040074825, 0.00624871626496315, 0.11510948091745377, 0.006216662935912609, 0.0061652022413909435, 0.0061848280020058155, 0.10067018121480942, 0.006172408815473318, 0.006302377674728632, 0.0061415559612214565, 0.006123116705566645, 0.006061126943677664, 0.006004247348755598, 0.005952909123152494, 0.005895543377846479]\n",
            "Val loss 0.023050585836217982\n",
            "Val auc roc 0.5691496163682864\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f07a997eb8c43fdb79758f2de8a7a9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1629.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0246\n",
            "Train Losses : [0.005819377023726702, 0.11019498854875565, 0.0057474891655147076, 0.0057107931934297085, 0.11892405897378922, 0.005698865279555321, 0.005717066582292318, 0.005674040410667658, 0.12039310485124588, 0.005612967535853386, 0.005582691170275211, 0.10061509162187576, 0.0055886381305754185, 0.005573876202106476, 0.005611505825072527, 0.005544174462556839, 0.005455854814499617, 0.005447929259389639, 0.005427901167422533, 0.005387666169553995, 0.005517268553376198, 0.005275113042443991, 0.005249082576483488, 0.005302873440086842, 0.09513087570667267, 0.005107739940285683, 0.0050568594597280025, 0.005029610358178616, 0.005060951225459576, 0.004963855724781752, 0.004918798804283142, 0.10429105162620544, 0.3753775954246521, 0.004959153011441231, 0.13169360160827637, 0.11613525450229645, 0.10713531821966171, 0.005475501995533705, 0.00568619929254055, 0.005666831973940134, 0.005763154476881027, 0.11189258098602295, 0.005896203685551882, 0.006043971981853247, 0.0060435207560658455, 0.00607164204120636, 0.006082850508391857, 0.11820545047521591, 0.11730344593524933, 0.006190743762999773, 0.006238448899239302, 0.0063769156113266945, 0.10288532823324203, 0.006327303592115641, 0.006382316816598177, 0.0063221449963748455, 0.006364133674651384, 0.0062609585002064705, 0.09915633499622345, 0.006236901506781578, 0.006344058085232973, 0.006270052865147591, 0.006316087674349546, 0.006205176003277302, 0.006203620228916407, 0.006185798905789852, 0.006045358721166849, 0.005993535742163658, 0.00597219867631793, 0.12950445711612701, 0.005923935677856207, 0.005884327460080385, 0.005795964039862156, 0.005892136599868536, 0.005744291469454765, 0.005678724497556686, 0.005697461310774088, 0.1130777969956398, 0.005654705688357353, 0.005487893708050251, 0.005578191019594669, 0.09133462607860565, 0.005412482190877199, 0.005507909692823887, 0.005385390017181635, 0.005417075008153915, 0.005379815120249987, 0.0052312505431473255, 0.00533404853194952, 0.0051637268625199795, 0.005116466898471117, 0.005170217249542475, 0.09832536429166794, 0.005019404459744692, 0.004994306713342667, 0.004969450179487467, 0.14811941981315613, 0.0049223946407437325, 0.004862504545599222, 0.004869709722697735, 0.0048361411318182945, 0.004817630629986525, 0.004772544838488102, 0.004768943879753351, 0.004801824223250151, 0.12867264449596405, 0.14780643582344055, 0.004748350940644741, 0.0046852221712470055, 0.004679152276366949, 0.004694386385381222, 0.12360476702451706, 0.004722670651972294, 0.1362488865852356, 0.1257360875606537, 0.10953538864850998, 0.004819998051971197, 0.004933503922075033, 0.004891340155154467, 0.004923210013657808, 0.004923050291836262, 0.0049493033438920975, 0.004962913691997528, 0.004942191764712334, 0.004946200642734766, 0.004936647601425648, 0.004929158370941877, 0.004881813190877438, 0.0048919604159891605, 0.004841023124754429, 0.004795835819095373, 0.004819696769118309, 0.00475502572953701, 0.004741284064948559, 0.004668234847486019, 0.004661636892706156, 0.004586126655340195, 0.004543588031083345, 0.004484967794269323, 0.004445306491106749, 0.004422992933541536, 0.0044102449901402, 0.004348866641521454, 0.004280357621610165, 0.10966791957616806, 0.10859893262386322, 0.11545000970363617, 0.004227736033499241, 0.004253011662513018, 0.0042369733564555645, 0.004235801286995411, 0.11962439119815826, 0.11805608868598938, 0.0043269130401313305, 0.004359613172709942, 0.004342383239418268, 0.004346062429249287, 0.004369521513581276, 0.0044104657135903835, 0.004356937948614359, 0.004345943219959736, 0.004385332111269236, 0.0043076397851109505, 0.004310484975576401, 0.004305502399802208, 0.004250009078532457, 0.004239343572407961, 0.0041880179196596146, 0.004188804421573877, 0.0041856118477880955, 0.38336071372032166, 0.004174119792878628, 0.004250049591064453, 0.004316041711717844, 0.10053572803735733, 0.004460055381059647, 0.004494307097047567, 0.004566439427435398, 0.11935883015394211, 0.004642672371119261, 0.004673221614211798, 0.004738952964544296, 0.004769823048263788, 0.004744093865156174, 0.004794204607605934, 0.00476117804646492, 0.11989101767539978, 0.004766116384416819, 0.004766578320413828, 0.00478803226724267, 0.12584318220615387, 0.004794081207364798, 0.004847506992518902, 0.004789111204445362, 0.0048593091778457165, 0.004821316804736853, 0.004788993392139673, 0.00480597373098135, 0.004800213500857353, 0.004772708285599947, 0.004712929483503103, 0.37335827946662903, 0.0047620488330721855, 0.004891702439635992, 0.3894975781440735, 0.005080267786979675, 0.00529703451320529, 0.005436418112367392, 0.117162324488163, 0.0056706881150603294, 0.005861104466021061, 0.005859057884663343, 0.005972579121589661, 0.006120624486356974, 0.006144326645880938, 0.006161055527627468, 0.0061916932463645935, 0.006180962081998587, 0.006242834497243166, 0.006195570807904005, 0.006155490409582853, 0.006148274056613445, 0.006178497336804867, 0.006204360630363226, 0.006067157257348299, 0.006091318558901548, 0.005993606057018042, 0.005982235074043274, 0.005968512501567602, 0.005900129210203886, 0.0058205933310091496, 0.005805178079754114, 0.0057906475849449635, 0.005702826660126448, 0.00565483421087265, 0.005539231467992067, 0.10154534131288528, 0.11482398957014084, 0.005448011215776205, 0.005456296727061272, 0.005547917913645506, 0.12823890149593353, 0.1324431300163269, 0.00538916839286685, 0.005399561952799559, 0.00539930397644639, 0.1287667155265808, 0.005488966125994921, 0.0054396684281528, 0.13106347620487213, 0.005413851700723171, 0.005470937583595514, 0.11435702443122864, 0.005506216082721949, 0.005486379377543926, 0.005477074068039656, 0.0055129616521298885, 0.0055003403685987, 0.005438190419226885, 0.0054077040404081345, 0.0054076798260211945, 0.1331450194120407, 0.005380012560635805, 0.005399755667895079, 0.005318602547049522, 0.005340260453522205, 0.11854526400566101, 0.005334416404366493, 0.005314338952302933, 0.005240990314632654, 0.005271643400192261, 0.11401687562465668, 0.0052131544798612595, 0.005209246650338173, 0.12444459646940231, 0.005200844258069992, 0.005193089600652456, 0.005184974055737257, 0.00517302704975009, 0.005177964456379414, 0.005178059916943312, 0.0051344470120966434, 0.005097279325127602, 0.00509472843259573, 0.005068697035312653, 0.005036193877458572, 0.004987549502402544, 0.004915807396173477, 0.004877520725131035, 0.11128685623407364, 0.0048430124297738075, 0.004857691470533609, 0.11825300008058548, 0.00478743389248848, 0.1319875866174698, 0.00476456992328167, 0.004773606080561876, 0.00482075335457921, 0.00479315547272563, 0.004792341496795416, 0.004748523700982332, 0.0047419811598956585, 0.004763532429933548, 0.004685911349952221, 0.004702368285506964, 0.004634258337318897, 0.004624212626367807, 0.004567081108689308, 0.11804061383008957, 0.004538629669696093, 0.004582702647894621, 0.004502888303250074, 0.004467415157705545, 0.0044532655738294125, 0.0044387164525687695, 0.004399683326482773, 0.004426731262356043, 0.004346493165940046, 0.004300463944673538, 0.004301528912037611, 0.12649500370025635, 0.1298779845237732, 0.0042619239538908005, 0.00424233777448535, 0.004246997646987438, 0.004247058182954788, 0.0042623719200491905, 0.004217560403048992, 0.11821313947439194, 0.004227695986628532, 0.0042008026503026485, 0.004199233837425709, 0.00419916957616806, 0.004196997731924057, 0.004164679441601038, 0.11141297221183777, 0.0041638026013970375, 0.004153556190431118, 0.00413998169824481, 0.004118753131479025, 0.004147001076489687, 0.004093285650014877, 0.004092668183147907, 0.004094976931810379, 0.004035757388919592, 0.11674068868160248, 0.09359234571456909, 0.004029282834380865, 0.004023562651127577, 0.004071865696460009, 0.13386346399784088, 0.10677037388086319, 0.004080906044691801, 0.00412264559417963, 0.004132469650357962, 0.004137125797569752, 0.004130289889872074, 0.004133265931159258, 0.004128721542656422, 0.004149333108216524, 0.004162381403148174, 0.004127888474613428, 0.10692207515239716, 0.004106923006474972, 0.004136162810027599, 0.00409458065405488, 0.004125009290874004, 0.004073007497936487, 0.004082038067281246, 0.004061392042785883, 0.09259703755378723, 0.12283585220575333, 0.004025987349450588, 0.004054186400026083, 0.004107640124857426, 0.0040671080350875854, 0.004070683848112822, 0.004080486949533224, 0.004065465182065964, 0.004029478877782822, 0.004006834235042334, 0.004024454392492771, 0.003990604542195797, 0.13490432500839233, 0.0039565684273839, 0.11519451439380646, 0.00403571454808116, 0.003993670456111431, 0.003975246101617813, 0.003975651226937771, 0.11266851425170898, 0.13148190081119537, 0.0040147132240235806, 0.004031043965369463, 0.004083192907273769, 0.11455895006656647, 0.12959665060043335, 0.004142999183386564, 0.004149411339312792, 0.004189189989119768, 0.0042566261254251, 0.11585967242717743, 0.004257043823599815, 0.004296945407986641, 0.004280440043658018, 0.0043183304369449615, 0.11783953011035919, 0.004331591539084911, 0.004384166095405817, 0.004347370937466621, 0.004382986109703779, 0.12611424922943115, 0.004362063016742468, 0.004383319988846779, 0.12599840760231018, 0.004512907471507788, 0.12097632139921188, 0.004457429517060518, 0.004480135627090931, 0.00450963294133544, 0.00449388986453414, 0.13425634801387787, 0.004517753608524799, 0.004544052295386791, 0.0045859068632125854, 0.004571930970996618, 0.1292589157819748, 0.004590299446135759, 0.004586747381836176, 0.0046011595986783504, 0.004587014205753803, 0.0045723868533968925, 0.004571517463773489, 0.10575469583272934, 0.004630084149539471, 0.00460424879565835, 0.004613021854311228, 0.004565947689116001, 0.00462332321330905, 0.004543447867035866, 0.10724341869354248, 0.00452203257009387, 0.004512980114668608, 0.00454222597181797, 0.0045289513655006886, 0.0044767605140805244, 0.004505801480263472, 0.004428898449987173, 0.004410256631672382, 0.0044351969845592976, 0.12508773803710938, 0.004358102101832628, 0.004413526970893145, 0.0043668076395988464, 0.004308371338993311, 0.004321590531617403, 0.004310816526412964, 0.004314899910241365, 0.0042383973486721516, 0.004214806482195854, 0.004249889869242907, 0.004162732511758804, 0.0042112539522349834, 0.004120504017919302, 0.004075450357049704, 0.004056069068610668, 0.0040122042410075665, 0.003979969769716263, 0.10998718440532684, 0.1389027088880539, 0.0039558191783726215, 0.003964492119848728, 0.0040029059164226055, 0.003938563168048859, 0.003943435847759247, 0.00393981859087944, 0.003912563435733318, 0.003910501021891832, 0.12209468334913254, 0.0038968774024397135, 0.003912061452865601, 0.12141136825084686, 0.0039406344294548035, 0.003910441417247057, 0.0039008266758173704, 0.0038873711600899696, 0.0038903101813048124, 0.003893244080245495, 0.003889303421601653, 0.0038754132110625505, 0.0038978843949735165, 0.1180134266614914, 0.003857012139633298, 0.11799938231706619, 0.003846561536192894, 0.1175038143992424, 0.0038912156596779823, 0.0038882598746567965, 0.003899375209584832, 0.003927174955606461, 0.003931148909032345, 0.13513019680976868, 0.003933464176952839, 0.003994486294686794, 0.003972085192799568, 0.003948564175516367, 0.003957760985940695, 0.003953265957534313, 0.00400384608656168, 0.003931208048015833, 0.10601933300495148, 0.003943136427551508, 0.003940969239920378, 0.003934662323445082, 0.0039632017724215984, 0.003952850587666035, 0.003923026844859123, 0.003877895651385188, 0.003906299825757742, 0.003858733456581831, 0.0038428495172411203, 0.00387234496884048, 0.0037954512517899275, 0.0037674724590033293, 0.0037697055377066135, 0.12750974297523499, 0.0037594460882246494, 0.0037339653354138136, 0.003718435065820813, 0.003690240904688835, 0.0036862781271338463, 0.00367982336319983, 0.1050967276096344, 0.003646289464086294, 0.11537128686904907, 0.0036816445644944906, 0.0036754810716956854, 0.0036997441202402115, 0.0036688167601823807, 0.003720371751114726, 0.0036736514884978533, 0.0036818429362028837, 0.0036527328193187714, 0.0036318516358733177, 0.003618787042796612, 0.003607330145314336, 0.003596350783482194, 0.0035821094643324614, 0.12774090468883514, 0.0035620650742202997, 0.11061608791351318, 0.00358428992331028, 0.003565246472135186, 0.0035767608787864447, 0.003608117112889886, 0.38381776213645935, 0.003664239775389433, 0.0037700410466641188, 0.003787175053730607, 0.0038797163870185614, 0.0038887797854840755, 0.003955064341425896, 0.10899729281663895, 0.003994900733232498, 0.004035550635308027, 0.00408258056268096, 0.10604869574308395, 0.004144079051911831, 0.004144346807152033, 0.004178810864686966, 0.004252152983099222, 0.00418954249471426, 0.004202321637421846, 0.004236765205860138, 0.004208426922559738, 0.00421537971124053, 0.12693068385124207, 0.004224211908876896, 0.004302206914871931, 0.004219117574393749, 0.10680224001407623, 0.004209815990179777, 0.004233967047184706, 0.004207503981888294, 0.004205317236483097, 0.004217753652483225, 0.0042021507397294044, 0.0042230817489326, 0.004188333172351122, 0.004207371734082699, 0.12108635902404785, 0.0041823601350188255, 0.004178978968411684, 0.004158949479460716, 0.004151823464781046, 0.004131134133785963, 0.004119229502975941, 0.004135469440370798, 0.004117866512387991, 0.11998922377824783, 0.10610326379537582, 0.0040804618038237095, 0.0040723890997469425, 0.004068994428962469, 0.004078059922903776, 0.004088937770575285, 0.004119881894439459, 0.004149126820266247, 0.004036519676446915, 0.004065700341016054, 0.10537530481815338, 0.004045958165079355, 0.004014601465314627, 0.1119430884718895, 0.10930996388196945, 0.004037883132696152, 0.0040404764004051685, 0.004063632804900408, 0.11034106463193893, 0.004090921487659216, 0.0041045923717319965, 0.004143580794334412, 0.13518166542053223, 0.004166080616414547, 0.004136324860155582, 0.004154236521571875, 0.004157127812504768, 0.004150792025029659, 0.004149405751377344, 0.00420253025367856, 0.004147989209741354, 0.12745413184165955, 0.004132074303925037, 0.004149219486862421, 0.13220682740211487, 0.004163831938058138, 0.004229906480759382, 0.00416188407689333, 0.004164674319326878, 0.004171001259237528, 0.004179488867521286, 0.00419513788074255, 0.004176762420684099, 0.004144204314798117, 0.004165189806371927, 0.00418013334274292, 0.0040716612711548805, 0.00410199910402298, 0.004088579677045345, 0.004023089073598385, 0.004012594930827618, 0.003990805242210627, 0.0039993766695261, 0.003973907791078091, 0.003917295020073652, 0.0038725300692021847, 0.003911999985575676, 0.0038484109099954367, 0.003836690913885832, 0.0037788627669215202, 0.0037942829076200724, 0.0037433654069900513, 0.0037289627362042665, 0.0036829779855906963, 0.0036711315624415874, 0.12818096578121185, 0.0036291792057454586, 0.003635238856077194, 0.003607255406677723, 0.003618803806602955, 0.0035696944687515497, 0.11662706732749939, 0.003588418010622263, 0.003551153466105461, 0.0035601623822003603, 0.1112721636891365, 0.12005206942558289, 0.003577752271667123, 0.0035841981880366802, 0.003613474080339074, 0.0036083166487514973, 0.0035971663892269135, 0.003639421658590436, 0.135786235332489, 0.003613285254687071, 0.11502469331026077, 0.0037430196534842253, 0.00367490341886878, 0.11541291326284409, 0.003731885924935341, 0.0037356624379754066, 0.0037528881803154945, 0.0037812371738255024, 0.00378402229398489, 0.00378390378318727, 0.003757923375815153, 0.0038003523368388414, 0.003737623570486903, 0.0037675683852285147, 0.0037307271268218756, 0.003743037348613143, 0.003728171344846487, 0.003729267045855522, 0.003735315054655075, 0.003663628827780485, 0.0037160441279411316, 0.0036520622670650482, 0.0036336234770715237, 0.003602926852181554, 0.003574520815163851, 0.0035787837114185095, 0.003572370857000351, 0.00351646076887846, 0.003516722470521927, 0.003484465181827545, 0.0034529874101281166, 0.003444046014919877, 0.003450767369940877, 0.0034079302567988634, 0.0033901624847203493, 0.003372610080987215, 0.0033346437849104404, 0.0033224942162632942, 0.0033022831194102764, 0.0033050337806344032, 0.12224163860082626, 0.003282589139416814, 0.12062746286392212, 0.0032524624839425087, 0.003269977169111371, 0.003247682237997651, 0.0032654679380357265, 0.00326016778126359, 0.14032888412475586, 0.003256025491282344, 0.0032668549101799726, 0.0032569454051554203, 0.10766313970088959, 0.003280793549492955, 0.00329072424210608, 0.003322373842820525, 0.003293081186711788, 0.003320748684927821, 0.0033045492600649595, 0.1264897584915161, 0.003338256385177374, 0.0033357366919517517, 0.0033330058213323355, 0.0033368293661624193, 0.0033470774069428444, 0.0033235731534659863, 0.0033045278396457434, 0.003317422466352582, 0.003353046951815486, 0.003293114947155118, 0.0032727473881095648, 0.003260210854932666, 0.12335735559463501, 0.0032648248597979546, 0.0032776074949651957, 0.0032692127861082554, 0.003267834661528468, 0.0032518338412046432, 0.0032557607628405094, 0.1096823513507843, 0.003248542081564665, 0.0032487818971276283, 0.12167574465274811, 0.0032500179950147867, 0.0032466403208673, 0.0032576031517237425, 0.003286017570644617, 0.10396506637334824, 0.003337792819365859, 0.0032866497058421373, 0.003302808618173003, 0.00332230469211936, 0.003294519614428282, 0.003309081308543682, 0.0033193896524608135, 0.10251841694116592, 0.0033022661227732897, 0.003334037261083722, 0.003371642204001546, 0.0033025159500539303, 0.0032890846487134695, 0.0033196781296283007, 0.003302921773865819, 0.0032830843701958656, 0.0033183188643306494, 0.0032816659659147263, 0.12401635944843292, 0.0032784228678792715, 0.4093046188354492, 0.003319572890177369, 0.003378648543730378, 0.0034253487829118967, 0.0034943614155054092, 0.003536067670211196, 0.003554758382961154, 0.0035915938206017017, 0.0036218222230672836, 0.003605994861572981, 0.11481271684169769, 0.0036797018256038427, 0.003687115851789713, 0.0037199510261416435, 0.0037352468352764845, 0.0037528183311223984, 0.0037799905985593796, 0.0037102445494383574, 0.1354113072156906, 0.003740609623491764, 0.0037559603806585073, 0.003751769894734025, 0.003794086165726185, 0.003750063478946686, 0.003743397071957588, 0.0037798993289470673, 0.003777027828618884, 0.003751144278794527, 0.0037274500355124474, 0.003713307436555624, 0.003728931536898017, 0.1300269216299057, 0.003695528954267502, 0.003695523599162698, 0.003664720105007291, 0.0036837889347225428, 0.0036674411967396736, 0.003697099629789591, 0.003654379630461335, 0.1176978126168251, 0.003658257657662034, 0.0036545544862747192, 0.003630258608609438, 0.0036745371762663126, 0.0036423136480152607, 0.10187235474586487, 0.0036964542232453823, 0.003640086157247424, 0.003636177396401763, 0.10743516683578491, 0.0036464480217546225, 0.11836238950490952, 0.003675220999866724, 0.0036892732605338097, 0.003672619815915823, 0.003670223755761981, 0.0036754582542926073, 0.003684892551973462, 0.11622660607099533, 0.0036619172897189856, 0.0036772771272808313, 0.003740878077223897, 0.003712356323376298, 0.0036744314711540937, 0.003711913013830781, 0.0037378219421952963, 0.11350982636213303, 0.00366192776709795, 0.0037198441568762064, 0.003690093057230115, 0.003685559844598174, 0.003652151208370924, 0.0037034833803772926, 0.10953503102064133, 0.003677579341456294, 0.003679909510537982, 0.10782941430807114, 0.003653652500361204, 0.12055298686027527, 0.003694087266921997, 0.0037245198618620634, 0.0037365953903645277, 0.003720022039487958, 0.0037563478108495474, 0.0037359436973929405, 0.1054978221654892, 0.0037057860754430294, 0.0037448385264724493, 0.0037389248609542847, 0.0037163987290114164, 0.0037227554712444544, 0.003716281382367015, 0.0037394026294350624, 0.0037099779583513737, 0.003696274245157838, 0.0037124641239643097, 0.0036964495666325092, 0.10970229655504227, 0.11043895781040192, 0.0036860755644738674, 0.4297375977039337, 0.003746054135262966, 0.003812723094597459, 0.0038662615697830915, 0.0039693512953817844, 0.003954619634896517, 0.00409708172082901, 0.004036372061818838, 0.004030372481793165, 0.09612030535936356, 0.00411075446754694, 0.004101300612092018, 0.004128545988351107, 0.004136789124459028, 0.10005579143762589, 0.004176237620413303, 0.004191612359136343, 0.004200379364192486, 0.004217387177050114, 0.0041958922520279884, 0.1183001771569252, 0.004209010861814022, 0.004257107153534889, 0.004265849944204092, 0.004232042469084263, 0.00431018928065896, 0.0042199729941785336, 0.004225042648613453, 0.12811926007270813, 0.00425062607973814, 0.13035999238491058, 0.0042450688779354095, 0.004256670828908682, 0.004281945992261171, 0.00421678414568305, 0.004244052805006504, 0.004313528072088957, 0.004212894011288881, 0.12749135494232178, 0.11575870215892792, 0.10862676054239273, 0.004323028959333897, 0.1089557483792305, 0.004343848209828138, 0.004322289954870939, 0.004375544376671314, 0.11202435940504074, 0.004372695926576853, 0.13010872900485992, 0.004401868209242821, 0.004421551711857319, 0.004451961722224951, 0.0044832248240709305, 0.11942584812641144, 0.00446462444961071, 0.004481051582843065, 0.004527844488620758, 0.004487758502364159, 0.12066792696714401, 0.11605977267026901, 0.00451852660626173, 0.004529263824224472, 0.0047210752964019775, 0.004551109857857227, 0.004570886492729187, 0.004572034813463688, 0.004570811986923218, 0.004614715464413166, 0.00454779714345932, 0.004535410553216934, 0.004583259113132954, 0.004580995067954063, 0.0045375218614935875, 0.004562042653560638, 0.0044891368597745895, 0.004525326658040285, 0.004530817735940218, 0.0044884029775857925, 0.0044924006797373295, 0.004415952600538731, 0.004379631951451302, 0.004402707330882549, 0.004354536067694426, 0.004311312921345234, 0.004308050964027643, 0.004315678961575031, 0.10374196618795395, 0.004317461047321558, 0.004326766822487116, 0.11328469961881638, 0.004207221325486898, 0.0042474884539842606, 0.11976061016321182, 0.004223779309540987, 0.004229356534779072, 0.004250704776495695, 0.004222760442644358, 0.004264517687261105, 0.004249620717018843, 0.004177823197096586, 0.004173762630671263, 0.004166998900473118, 0.004229217302054167, 0.0042497250251472, 0.004184695892035961, 0.004105319269001484, 0.004112615250051022, 0.0041014812886714935, 0.0040871864184737206, 0.00409720791503787, 0.004066318739205599, 0.004059024155139923, 0.10043080896139145, 0.004016160033643246, 0.003987357020378113, 0.003992742393165827, 0.004032101482152939, 0.4270676374435425, 0.004032332915812731, 0.004092303104698658, 0.004065017215907574, 0.004128497093915939, 0.004135864786803722, 0.004172137938439846, 0.004203590098768473, 0.10497501492500305, 0.004200122319161892, 0.0042105731554329395, 0.0042252251878380775, 0.1300116926431656, 0.004273509606719017, 0.12190766632556915, 0.00432461267337203, 0.004377412609755993, 0.004351670853793621, 0.004335329402238131, 0.004317326005548239, 0.00433153985068202, 0.0043517653830349445, 0.004350715316832066, 0.0043771835044026375, 0.004346245434135199, 0.004328387789428234, 0.1044960767030716, 0.004311740398406982, 0.004313232842832804, 0.0043045394122600555, 0.004304714500904083, 0.004304378293454647, 0.004368494730442762, 0.00424985634163022, 0.004248667508363724, 0.004284423775970936, 0.004275028593838215, 0.004215783439576626, 0.11284399777650833, 0.004229449667036533, 0.004212669562548399, 0.00420363387092948, 0.10949481278657913, 0.0042237788438797, 0.004226478282362223, 0.004174715373665094, 0.004154171794652939, 0.004204527474939823, 0.09994497895240784, 0.10622485727071762, 0.0041764965280890465, 0.004260945599526167, 0.0041938587091863155, 0.004230021499097347, 0.004218411166220903, 0.004187313839793205, 0.004183731507509947, 0.0042759450152516365, 0.004160165321081877, 0.004178821574896574, 0.0041490704752504826, 0.004169297870248556, 0.127544566988945, 0.00410132622346282, 0.0041115907952189445, 0.12212386727333069, 0.0041378033347427845, 0.004114069975912571, 0.0041019306518137455, 0.1044854000210762, 0.12525640428066254, 0.1286597102880478, 0.004146370105445385, 0.004320182837545872, 0.004186865407973528, 0.004181080963462591, 0.004208901897072792, 0.10507794469594955, 0.004272492602467537, 0.004216447006911039, 0.004258393310010433, 0.004232378676533699, 0.004213630221784115, 0.004204612225294113, 0.004258143715560436, 0.004281660541892052, 0.0042318496853113174, 0.1133970394730568, 0.004231116268783808, 0.004270631354302168, 0.004201440140604973, 0.004179013427346945, 0.0042049577459692955, 0.004245998803526163, 0.0042038122192025185, 0.004148658365011215, 0.10404406487941742, 0.004187044221907854, 0.004183752462267876, 0.004163193982094526, 0.004133524373173714, 0.1282559186220169, 0.13735470175743103, 0.004154161550104618, 0.004144340753555298, 0.004153051879256964, 0.004182660486549139, 0.1162397488951683, 0.004158858675509691, 0.1258774995803833, 0.004265332128852606, 0.1133439838886261, 0.004212838131934404, 0.00420769676566124, 0.00421977648511529, 0.0042225634679198265, 0.004226630553603172, 0.004315135534852743, 0.13600333034992218, 0.11622639000415802, 0.004279786720871925, 0.1179753839969635, 0.004294540733098984, 0.004283875226974487, 0.004351589363068342, 0.004296335391700268, 0.004328382201492786, 0.004358693957328796, 0.004312789998948574, 0.004330885596573353, 0.11702188849449158, 0.00433481065556407, 0.004314441699534655, 0.004332477226853371, 0.0043755448423326015, 0.004346568137407303, 0.004367592744529247, 0.004315156489610672, 0.0043893856927752495, 0.004336766432970762, 0.004305316135287285, 0.004270835779607296, 0.004255271982401609, 0.0043155779130756855, 0.004226866643875837, 0.004224721342325211, 0.004223067779093981, 0.004282720386981964, 0.004220578353852034, 0.004189804196357727, 0.004187959246337414, 0.004138011950999498, 0.004146176390349865, 0.12913328409194946, 0.004134835675358772, 0.004128926433622837, 0.004158542957156897, 0.004123265389353037, 0.0041160909458994865, 0.004070298280566931, 0.11656343936920166, 0.004143586382269859, 0.004111226182430983, 0.004058607388287783, 0.004087911918759346, 0.004084194544702768, 0.004103765357285738, 0.0040251933969557285, 0.004039112478494644, 0.004025493748486042, 0.003985042683780193, 0.003998674917966127, 0.004000779241323471, 0.12111454457044601, 0.003949212841689587, 0.003961046691983938, 0.003958372864872217, 0.1367432028055191, 0.0039649042300879955, 0.0039754025638103485, 0.003941257018595934, 0.003969570156186819, 0.004017696250230074, 0.10304807871580124, 0.003932945430278778, 0.1216786727309227, 0.003978552762418985, 0.11423783004283905, 0.003984172362834215, 0.003974074497818947, 0.003999955486506224, 0.10574118793010712, 0.00397759722545743, 0.1234670877456665, 0.004028888884931803, 0.004027893301099539, 0.004017945844680071, 0.004114982206374407, 0.004066714085638523, 0.1219165027141571, 0.004030006937682629, 0.11204999685287476, 0.004058274440467358, 0.004106360021978617, 0.004073634278029203, 0.11972574144601822, 0.004121279343962669, 0.004112796857953072, 0.004081691149622202, 0.0041018216870725155, 0.0040977331809699535, 0.00409443536773324, 0.004125140607357025, 0.00412160437554121, 0.004112449940294027, 0.004092734772711992, 0.004112450405955315, 0.004095084965229034, 0.13551278412342072, 0.004093427211046219, 0.13365556299686432, 0.004076140001416206, 0.004068252630531788, 0.004086619708687067, 0.004115878604352474, 0.004130084533244371, 0.004130077548325062, 0.4201993942260742, 0.004103019367903471, 0.004153120331466198, 0.004166979808360338, 0.1230391338467598, 0.004190394654870033, 0.0042747389525175095, 0.004332990385591984, 0.004268916789442301, 0.004286268725991249, 0.10581275820732117, 0.4043462574481964, 0.004351973533630371, 0.004397294949740171, 0.11285916715860367, 0.004449959844350815, 0.004529342986643314, 0.004498234484344721, 0.004567993804812431, 0.004556835629045963, 0.004599899984896183, 0.004618425853550434, 0.004599976819008589, 0.10942687839269638, 0.004652851726859808, 0.004658665508031845, 0.004640708677470684, 0.004643825814127922, 0.00473072100430727, 0.11738588660955429, 0.004719930235296488, 0.004675439093261957, 0.004655069205909967, 0.00466910982504487, 0.004685433115810156, 0.004702938720583916, 0.11543954908847809, 0.004650237504392862, 0.11533192545175552, 0.004669996909797192, 0.004654983524233103, 0.004664216190576553, 0.0046590836718678474, 0.004679386503994465, 0.004722632933408022, 0.004683163948357105, 0.0046403720043599606, 0.12743344902992249, 0.004757308401167393, 0.004641548730432987, 0.0046421196311712265, 0.004653274547308683, 0.004655475728213787, 0.004696304444223642, 0.0046371715143322945, 0.004629442002624273, 0.004627380054444075, 0.004651011899113655, 0.004637011326849461, 0.004611048381775618, 0.0046272967010736465, 0.004597904160618782, 0.0045590754598379135, 0.0045584822073578835, 0.122314453125, 0.004530387464910746, 0.0045286198146641254, 0.004528115037828684, 0.13865716755390167, 0.004512633662670851, 0.004496487323194742, 0.004574629012495279, 0.004518033005297184, 0.0045103272423148155, 0.004475447349250317, 0.004552281927317381, 0.004464779049158096, 0.0044938428327441216, 0.004450533539056778, 0.004480546806007624, 0.004493852611631155, 0.14239946007728577, 0.004445286467671394, 0.1187439113855362, 0.004426530096679926, 0.004435775801539421, 0.004483115393668413, 0.004454378969967365, 0.0044366056099534035, 0.004413282033056021, 0.004401634447276592, 0.004412027541548014, 0.13213160634040833, 0.004389593377709389, 0.004390063229948282, 0.004388059955090284, 0.004434511996805668, 0.004399181343615055, 0.004419028293341398, 0.0044029392302036285, 0.004438654985278845, 0.004399063065648079, 0.004363055806607008, 0.004383722320199013, 0.004342842381447554, 0.004352848045527935, 0.11563204228878021, 0.004316528327763081, 0.004358758684247732, 0.0043370588682591915, 0.004323914647102356, 0.004310152493417263, 0.1404733955860138, 0.004292768891900778, 0.0043310318142175674, 0.0042972806841135025, 0.12085641175508499, 0.004317648708820343, 0.004284149967133999, 0.004289795178920031, 0.004333672113716602, 0.0043294248171150684, 0.10415326803922653, 0.108106829226017, 0.004319281317293644, 0.004290606826543808, 0.004311916884034872, 0.004306023009121418, 0.10942333191633224, 0.004302166868001223, 0.004323330707848072, 0.004311223514378071, 0.004309274721890688, 0.004326057620346546, 0.004301575943827629, 0.11570555716753006, 0.0042942301370203495, 0.10713068395853043, 0.004293098114430904, 0.0043114409781992435, 0.004348271060734987, 0.004298328887671232, 0.004323601722717285, 0.0043282415717840195, 0.004305964335799217, 0.004341310355812311, 0.00431486451998353, 0.004285888746380806, 0.004313141573220491, 0.004298192448914051, 0.12655241787433624, 0.00427008792757988, 0.004277304746210575, 0.004352022893726826, 0.004254504106938839, 0.004276246298104525, 0.004262079019099474, 0.004289995413273573, 0.004240147769451141, 0.004246470984071493, 0.004264622926712036, 0.0042504677549004555, 0.0042306347750127316, 0.004225537646561861, 0.00424105254933238, 0.004198097158223391, 0.11828909814357758, 0.004229454789310694, 0.004219821188598871, 0.004207983613014221, 0.0042276522144675255, 0.12791107594966888, 0.004183775745332241, 0.0042018829844892025, 0.11557032912969589, 0.004226705990731716, 0.004169152118265629, 0.004167960025370121, 0.00429394468665123, 0.395734041929245, 0.0042225755751132965, 0.10681968182325363, 0.004244095645844936, 0.004244042094796896, 0.004292949102818966, 0.004342148080468178, 0.12430987507104874, 0.10378959774971008, 0.004291853401809931, 0.10912410169839859, 0.004337536171078682, 0.004316040780395269, 0.004323168192058802, 0.13914605975151062, 0.00435621989890933, 0.004376509692519903, 0.11513057351112366, 0.004409817513078451, 0.004391918424516916, 0.004398837685585022, 0.004397377837449312, 0.004402637016028166, 0.11044828593730927, 0.004437492229044437, 0.10228434950113297, 0.004415273200720549, 0.10878812521696091, 0.13049745559692383, 0.00449127983301878, 0.004448490682989359, 0.004444936756044626, 0.004460815340280533, 0.004432260058820248, 0.004464178811758757, 0.004481470678001642, 0.004471357446163893, 0.0044870623387396336, 0.12028571218252182, 0.004454562906175852, 0.0044401949271559715, 0.004495969973504543, 0.004488733131438494, 0.004463144578039646, 0.004452750086784363, 0.004440631717443466, 0.11690684407949448, 0.004439987242221832, 0.004429676104336977, 0.004493474494665861, 0.004442077595740557, 0.004465683829039335, 0.004456027410924435, 0.004434880800545216, 0.0044680628925561905, 0.004483250435441732, 0.004488486330956221, 0.004416808485984802, 0.004489520564675331, 0.004438573028892279, 0.004443250596523285, 0.0044164820574223995, 0.004393210168927908, 0.004382772371172905, 0.004373486153781414, 0.0043991440907120705, 0.0043832361698150635, 0.004358809906989336, 0.004424910061061382, 0.004393510986119509, 0.004374703858047724, 0.0043638101778924465, 0.004352000076323748, 0.004335146863013506, 0.1177290603518486, 0.004369295667856932, 0.1109958365559578, 0.004322881810367107, 0.004340599291026592, 0.004327768925577402, 0.004395667463541031, 0.004324126522988081, 0.10997941344976425, 0.004348565358668566, 0.004331285133957863, 0.004343354143202305, 0.004354221746325493, 0.004337399732321501, 0.0043701413087546825, 0.0043264348059892654, 0.00436045927926898, 0.004302979446947575, 0.004319080151617527, 0.10692160576581955, 0.13162894546985626, 0.004310701508074999, 0.004333993885666132, 0.004330850206315517, 0.004316969774663448, 0.004328694194555283, 0.004301866982132196, 0.004313895478844643, 0.12766176462173462, 0.004318246617913246, 0.00430324487388134, 0.004328659735620022, 0.004289980977773666, 0.004298328887671232, 0.004307117313146591, 0.00429935147985816, 0.0043136500753462315, 0.00432134373113513, 0.004295832943171263, 0.0042961351573467255, 0.004317314829677343, 0.0042740339413285255, 0.004306379240006208, 0.004299545660614967, 0.004316482227295637, 0.004292834084481001, 0.00429140729829669, 0.004329892806708813, 0.004259766079485416, 0.004272138699889183, 0.004247395787388086, 0.00426936661824584, 0.004284840542823076, 0.004240341018885374, 0.1062721386551857, 0.004250416532158852, 0.004260539077222347, 0.13194258511066437, 0.0042587765492498875, 0.00424014450982213, 0.004267440177500248, 0.004234603140503168, 0.004292408470064402, 0.004231661558151245, 0.004245392978191376, 0.0042412360198795795, 0.13008128106594086, 0.004247021861374378, 0.004233490210026503, 0.004241081885993481, 0.004237418994307518, 0.004217076115310192, 0.004248225595802069, 0.0042190649546682835, 0.0042841206304728985, 0.00423362897709012, 0.004229476675391197, 0.004232861567288637, 0.0042481450363993645, 0.004288335330784321, 0.004214979708194733, 0.004205253906548023, 0.004219336900860071, 0.004210199695080519, 0.1295129805803299, 0.11951233446598053, 0.004222979303449392, 0.12635116279125214, 0.004212641157209873, 0.10986235737800598, 0.004214172717183828, 0.0042144060134887695, 0.11410254240036011, 0.00423297518864274, 0.004213413223624229, 0.004202495329082012, 0.13397324085235596, 0.004207885358482599, 0.004216129891574383, 0.004236440174281597, 0.004215572495013475, 0.004215453751385212, 0.004247091710567474, 0.0042256759479641914, 0.004214154556393623, 0.12199968099594116, 0.004302604123950005, 0.004254086408764124, 0.0042170071974396706, 0.004225543700158596, 0.004229026380926371, 0.004231382627040148, 0.00421634316444397, 0.004252410959452391, 0.00430130073800683, 0.004208763130009174, 0.004231448285281658, 0.004209414590150118, 0.004215818829834461, 0.004208284430205822, 0.12764781713485718, 0.00424386840313673, 0.004197988659143448, 0.004215257242321968, 0.004205790814012289, 0.004229684825986624, 0.004207514692097902, 0.12396075576543808, 0.11998385190963745, 0.004271982703357935, 0.004237174987792969, 0.004199901595711708, 0.11456972360610962, 0.004222447052598, 0.0042297677136957645, 0.004246344789862633, 0.004211065359413624, 0.004226073157042265, 0.004199083428829908]\n",
            "Val loss 0.022893508529207976\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd304c8-3b06-471f-97a7-04c968e0b383"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}