{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_Once_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28503cf540374a9c88f95b70564336e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_311b16dab2d046b99672e719b26ca2fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff05e72e507f4e929ad8930323594772",
              "IPY_MODEL_e3547d0eae434ede8b4687a72620f7bc"
            ]
          }
        },
        "311b16dab2d046b99672e719b26ca2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff05e72e507f4e929ad8930323594772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82ef28ee37e645eda6bb047bd3819d67",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1630,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1630,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bd935bcb574495f800c731832e8a20d"
          }
        },
        "e3547d0eae434ede8b4687a72620f7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9040d2910a4648558351b23cab388149",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1630/1630 [15:15&lt;00:00,  1.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcee838267e540b190e14b383c50971e"
          }
        },
        "82ef28ee37e645eda6bb047bd3819d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bd935bcb574495f800c731832e8a20d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9040d2910a4648558351b23cab388149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcee838267e540b190e14b383c50971e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df0dbbbcddbe4baba8b5f94c8b12ff6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5aa1066677648279a1da7c294fb5576",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc6fa0593546408eb2e9a4ed8bf480e4",
              "IPY_MODEL_8bd61c50da6b4bdf8e3ebccc7c282c4b"
            ]
          }
        },
        "f5aa1066677648279a1da7c294fb5576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc6fa0593546408eb2e9a4ed8bf480e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1850632b7eb64ea0a00c2de427e55507",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1630,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1630,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe751aebf87d40a2a3839eeaaaa2c86c"
          }
        },
        "8bd61c50da6b4bdf8e3ebccc7c282c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d26fc430158142a2aefb07f9eb5d401f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1630/1630 [15:01&lt;00:00,  1.81it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2d399df65bc4793b5413357be8bf48e"
          }
        },
        "1850632b7eb64ea0a00c2de427e55507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe751aebf87d40a2a3839eeaaaa2c86c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d26fc430158142a2aefb07f9eb5d401f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2d399df65bc4793b5413357be8bf48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d7ccfe17e8f46868abfca214b4810e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d211344c8494b3a85141ca63720262b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77524dcc559a4ea79c3347ecc097119d",
              "IPY_MODEL_0bdba3107332442191344c0ac8815664"
            ]
          }
        },
        "0d211344c8494b3a85141ca63720262b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77524dcc559a4ea79c3347ecc097119d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ff29bc5ca9140d1adf2397ac88e8a47",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1630,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1630,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e388fb0a1b64f63bfed148950e2e0fd"
          }
        },
        "0bdba3107332442191344c0ac8815664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7974ad4c37bc45518bcc01e5ebc223b7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1630/1630 [14:57&lt;00:00,  1.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8ae82fc68174bc9a985037029e4e717"
          }
        },
        "7ff29bc5ca9140d1adf2397ac88e8a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e388fb0a1b64f63bfed148950e2e0fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7974ad4c37bc45518bcc01e5ebc223b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8ae82fc68174bc9a985037029e4e717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bc1d892d-fa00-4ff7-8be2-89b2637e51be"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 7.29 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8df0f7a-54d6-483e-bc54-753cf4ab846f"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db1f187c-1597-4952-989c-1e8ace23acd9"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7522b159-24e5-4490-a6c2-0f7472b4f20f"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "27d2b2a1-12c1-404e-e609-35a848f442df"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "7d6e452d-a2c7-4122-9671-ac634df3b030"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a9872ee5-3c04-4720-8519-8bfe87b664af"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23c9e425-4c80-4afe-e63b-b3a55fc06407"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "4105d2d2-426e-4c51-bf31-f7595a392c28"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "170e8999-9b6a-44d1-80c5-c9bab59bc652"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    #MeToo: Sruthi Hariharan finds supporters in P...\n",
              "1                                        ption - no:2 \n",
              "2    College Student Raped Woman After She Attended...\n",
              "3    #MeToo In India Is Just A Tip Of An Iceberg An...\n",
              "4     Four-member panel of retired judges to conduc...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0faae0f1-e20e-490d-d155-8bd6c4c977dc"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:14<00:00, 28192577.77B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab68d7ed-4339-470e-ff21-d37420d8b911"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 668557.05B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c995e602-4122-4a46-ae24-c3c8cfceead3"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55c38a7b-f6d3-421a-f97e-1be83c627b2f"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203e6a42-835f-4676-929f-2f915a905eaf"
      },
      "source": [
        "col_name = \"Sarcasm\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "28503cf540374a9c88f95b70564336e1",
            "311b16dab2d046b99672e719b26ca2fa",
            "ff05e72e507f4e929ad8930323594772",
            "e3547d0eae434ede8b4687a72620f7bc",
            "82ef28ee37e645eda6bb047bd3819d67",
            "1bd935bcb574495f800c731832e8a20d",
            "9040d2910a4648558351b23cab388149",
            "bcee838267e540b190e14b383c50971e",
            "df0dbbbcddbe4baba8b5f94c8b12ff6a",
            "f5aa1066677648279a1da7c294fb5576",
            "cc6fa0593546408eb2e9a4ed8bf480e4",
            "8bd61c50da6b4bdf8e3ebccc7c282c4b",
            "1850632b7eb64ea0a00c2de427e55507",
            "fe751aebf87d40a2a3839eeaaaa2c86c",
            "d26fc430158142a2aefb07f9eb5d401f",
            "d2d399df65bc4793b5413357be8bf48e",
            "0d7ccfe17e8f46868abfca214b4810e0",
            "0d211344c8494b3a85141ca63720262b",
            "77524dcc559a4ea79c3347ecc097119d",
            "0bdba3107332442191344c0ac8815664",
            "7ff29bc5ca9140d1adf2397ac88e8a47",
            "4e388fb0a1b64f63bfed148950e2e0fd",
            "7974ad4c37bc45518bcc01e5ebc223b7",
            "d8ae82fc68174bc9a985037029e4e717"
          ]
        },
        "outputId": "7f3d6731-a44a-4a7b-e013-3e3cf2bdabfc"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 6520\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28503cf540374a9c88f95b70564336e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1630.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0264\n",
            "Train Losses : [0.004475286230444908, 0.14126940071582794, 0.003974790219217539, 0.006037592887878418, 0.0076980614103376865, 0.3763190805912018, 0.01314375177025795, 0.0843273252248764, 0.026671098545193672, 0.15030094981193542, 0.04379262030124664, 0.06804273277521133, 0.03323616087436676, 0.061879873275756836, 0.13422904908657074, 0.017911404371261597, 0.015014879405498505, 0.010550916194915771, 0.10845867544412613, 0.004274772945791483, 0.00380760058760643, 0.0037844316102564335, 0.002426910912618041, 0.08338859677314758, 0.14035965502262115, 0.0020967749878764153, 0.002358761616051197, 0.003438682993873954, 0.0030613201670348644, 0.0022902064956724644, 0.002931456547230482, 0.002556653693318367, 0.002406004350632429, 0.00359511561691761, 0.002061626175418496, 0.0035813499707728624, 0.002009121235460043, 0.0016446345252916217, 0.001588369021192193, 0.0014415639452636242, 0.6422521471977234, 0.002484777709469199, 0.003225984750315547, 0.0057030292227864265, 0.06613178551197052, 0.15089350938796997, 0.008697086945176125, 0.01539977453649044, 0.01270210649818182, 0.13807515799999237, 0.02046818844974041, 0.01971728727221489, 0.014865269884467125, 0.21251527965068817, 0.015808256343007088, 0.0178030077368021, 0.011469951830804348, 0.18187008798122406, 0.009821968153119087, 0.010217818431556225, 0.007785756140947342, 0.007425345946103334, 0.004869102966040373, 0.004492105450481176, 0.0047686053439974785, 0.0032786892261356115, 0.0031279807444661856, 0.0027178123127669096, 0.002198649337515235, 0.0017893101321533322, 0.0015181408962234855, 0.0013256384991109371, 0.0011660116724669933, 0.0010189979802817106, 0.0008811658481135964, 0.16873350739479065, 0.0008938858518376946, 0.0011031527537852526, 0.0010001723421737552, 0.0011894683120772243, 0.0010437980527058244, 0.000995422713458538, 0.16840483248233795, 0.0013390143867582083, 0.0016175196506083012, 0.0015461711445823312, 0.002484072232618928, 0.0019010405521839857, 0.0035199460107833147, 0.0017577228136360645, 0.00249928398989141, 0.0021683555096387863, 0.0019465943332761526, 0.0018348876619711518, 0.0018235382158309221, 0.0017883796244859695, 0.0018518990837037563, 0.0017436199123039842, 0.18101346492767334, 0.002114806557074189, 0.0019547471310943365, 0.0022538246121257544, 0.10829412937164307, 0.002975634066388011, 0.002684985054656863, 0.057219523936510086, 0.0035530519671738148, 0.0037692487239837646, 0.003493643831461668, 0.003843949642032385, 0.00453174626454711, 0.003962535411119461, 0.004593572113662958, 0.0038389128167182207, 0.004436365328729153, 0.0034107051324099302, 0.003836723743006587, 0.11719730496406555, 0.003445990849286318, 0.003617214970290661, 0.0036181542091071606, 0.17205648124217987, 0.003254225244745612, 0.0036028900649398565, 0.004520526621490717, 0.0034837096463888884, 0.004875346086919308, 0.00368307763710618, 0.003459859173744917, 0.0036503414157778025, 0.00366754992865026, 0.0035530361346900463, 0.0029021897353231907, 0.40746504068374634, 0.0036552129313349724, 0.005127653479576111, 0.005547109991312027, 0.13370023667812347, 0.06428288668394089, 0.009212736040353775, 0.01081142295151949, 0.01165180653333664, 0.016442464664578438, 0.01289433054625988, 0.012608340941369534, 0.01126059703528881, 0.011427962221205235, 0.011532836593687534, 0.010931147262454033, 0.008694681338965893, 0.008689547888934612, 0.007150724064558744, 0.008290828205645084, 0.060946810990571976, 0.0057527851313352585, 0.004834820982068777, 0.004672472830861807, 0.004114043433219194, 0.003982659429311752, 0.0035550021566450596, 0.0038502621464431286, 0.002744607161730528, 0.0024685016833245754, 0.003130879020318389, 0.0023421538062393665, 0.0025634141638875008, 0.15689952671527863, 0.0019036184530705214, 0.0019074457231909037, 0.0020480933599174023, 0.002066768007352948, 0.09792396426200867, 0.00335791171528399, 0.002249989192932844, 0.001976525643840432, 0.19605405628681183, 0.0038918969221413136, 0.0032420335337519646, 0.1699858456850052, 0.0024911772925406694, 0.003173958510160446, 0.0032019810751080513, 0.11349164694547653, 0.004176306538283825, 0.004070446826517582, 0.004160679876804352, 0.004340977407991886, 0.005650028120726347, 0.004705585539340973, 0.005714280065149069, 0.0048951636999845505, 0.16469408571720123, 0.0049358876422047615, 0.0048467013984918594, 0.004926973022520542, 0.004857027903199196, 0.0043785604648292065, 0.004422586411237717, 0.004572358913719654, 0.004537657834589481, 0.12867456674575806, 0.003892181208357215, 0.0037882444448769093, 0.0037869804073125124, 0.0037772024516016245, 0.0037029858212918043, 0.003717704676091671, 0.0035331202670931816, 0.0943617969751358, 0.17904195189476013, 0.003651386359706521, 0.003913396969437599, 0.0037932582199573517, 0.0038803520146757364, 0.004031425807625055, 0.0037008929066359997, 0.0037540916819125414, 0.0035815408919006586, 0.0035245169419795275, 0.0034310724586248398, 0.003569388994947076, 0.003160639898851514, 0.0032117702066898346, 0.002926533343270421, 0.002779200440272689, 0.0027311125304549932, 0.002516804961487651, 0.09251654148101807, 0.13064970076084137, 0.0025570157449692488, 0.0027812558691948652, 0.0027529189828783274, 0.002863941015675664, 0.0029464333783835173, 0.14277693629264832, 0.0031862480100244284, 0.12769462168216705, 0.1153930053114891, 0.13321156799793243, 0.004345906898379326, 0.073304682970047, 0.005518822465091944, 0.0063702818006277084, 0.007008221000432968, 0.0074640861712396145, 0.007715394254773855, 0.007725436706095934, 0.007875507697463036, 0.00749964127317071, 0.00748391542583704, 0.007202835753560066, 0.1032315269112587, 0.006913979537785053, 0.006838263478130102, 0.0063180276192724705, 0.006166630890220404, 0.006172214634716511, 0.005300766322761774, 0.005063781514763832, 0.0047736926935613155, 0.004515777807682753, 0.004188661463558674, 0.1167006641626358, 0.003650610102340579, 0.003793341340497136, 0.0037579666823148727, 0.11393250524997711, 0.10936903953552246, 0.0035789604298770428, 0.0034640789963304996, 0.0037549342960119247, 0.003643553238362074, 0.003642250318080187, 0.003535530297085643, 0.0035370097029954195, 0.0035414337180554867, 0.13335685431957245, 0.1063302680850029, 0.0038887043483555317, 0.14988943934440613, 0.004098556935787201, 0.004573074635118246, 0.004721768666058779, 0.0050738113932311535, 0.13066796958446503, 0.005506922025233507, 0.0053816731087863445, 0.0053399475291371346, 0.09727322310209274, 0.005852981004863977, 0.006020821165293455, 0.005741746164858341, 0.006071851123124361, 0.10254815220832825, 0.005933829117566347, 0.005948696751147509, 0.005893786903470755, 0.005823682062327862, 0.005492431111633778, 0.005451894365251064, 0.004944667685776949, 0.005076751112937927, 0.12941351532936096, 0.0046731033362448215, 0.11935528367757797, 0.004465731792151928, 0.004587690811604261, 0.004528691992163658, 0.004465489648282528, 0.004487970843911171, 0.004177087917923927, 0.0040719169192016125, 0.003964496776461601, 0.0040209018625319, 0.0037001590244472027, 0.003399375593289733, 0.11194059252738953, 0.0033105004113167524, 0.0031746511813253164, 0.0031716637313365936, 0.00300329620949924, 0.0031612087041139603, 0.0031390059739351273, 0.002747574122622609, 0.0026007427368313074, 0.002555923303589225, 0.0024473832454532385, 0.002283173380419612, 0.12956438958644867, 0.0023050399031490088, 0.5761715769767761, 0.13597938418388367, 0.003998938016593456, 0.00490965461358428, 0.005972692277282476, 0.006974536459892988, 0.007768487557768822, 0.008850295096635818, 0.009475712664425373, 0.00988489668816328, 0.010448692366480827, 0.010679262690246105, 0.010860257782042027, 0.010665680281817913, 0.010367472656071186, 0.010170838795602322, 0.07818084955215454, 0.009352858178317547, 0.10295754671096802, 0.008717123419046402, 0.008572042919695377, 0.008151093497872353, 0.00784089881926775, 0.007298558950424194, 0.006897104438394308, 0.08771658688783646, 0.006189238280057907, 0.005980525631457567, 0.005682246293872595, 0.005384182557463646, 0.005034909583628178, 0.0047873095609247684, 0.004586546216160059, 0.004291572608053684, 0.0039045969024300575, 0.0037438080180436373, 0.12302237004041672, 0.0034799936693161726, 0.0033549226354807615, 0.003269149223342538, 0.0030930519569665194, 0.002959361532703042, 0.0028476465959101915, 0.002735306741669774, 0.148179829120636, 0.0026724517811089754, 0.002739877672865987, 0.0026451738085597754, 0.002812338061630726, 0.0026086538564413786, 0.002690842840820551, 0.002578438725322485, 0.002492836443707347, 0.0024141829926520586, 0.0023833662271499634, 0.0022436934523284435, 0.002244120929390192, 0.002191216452047229, 0.002145485719665885, 0.002048075431957841, 0.001935146632604301, 0.001949039287865162, 0.16676746308803558, 0.0019065922824665904, 0.001873563858680427, 0.0019376195268705487, 0.001978148240596056, 0.001975993625819683, 0.0019568921998143196, 0.0018835996743291616, 0.17005886137485504, 0.001981985755264759, 0.002051530871540308, 0.0021153551060706377, 0.11772730946540833, 0.0023271851241588593, 0.12529373168945312, 0.002630629576742649, 0.11262258887290955, 0.003123999573290348, 0.0034045909997075796, 0.003645793069154024, 0.41038328409194946, 0.004854885395616293, 0.005822333972901106, 0.006607599090784788, 0.007523966953158379, 0.00831461139023304, 0.009047523140907288, 0.00976498145610094, 0.009988412261009216, 0.0934654027223587, 0.01061396487057209, 0.01100603025406599, 0.10872256755828857, 0.01098423171788454, 0.011059090495109558, 0.011056584306061268, 0.010704143904149532, 0.01033109426498413, 0.11134946346282959, 0.00938152614980936, 0.00948676373809576, 0.08505672216415405, 0.008443431928753853, 0.008280549198389053, 0.007736900821328163, 0.09931498020887375, 0.007373295724391937, 0.1275293380022049, 0.0068185050040483475, 0.006742495112121105, 0.006420477293431759, 0.006337813567370176, 0.00611258577555418, 0.005760946776717901, 0.005505698267370462, 0.005227403249591589, 0.004956589080393314, 0.004775374662131071, 0.12468086928129196, 0.12571901082992554, 0.004309976939111948, 0.004257649183273315, 0.004222423303872347, 0.004282909911125898, 0.004217826761305332, 0.004080848768353462, 0.4608531594276428, 0.004508540499955416, 0.005055266432464123, 0.005527269560843706, 0.006134453229606152, 0.006606962997466326, 0.0068167890422046185, 0.00703471852466464, 0.00710686482489109, 0.0071961903013288975, 0.007433213293552399, 0.007174843922257423, 0.00719419727101922, 0.007094842381775379, 0.007033098489046097, 0.006673520430922508, 0.08137067407369614, 0.0062663559801876545, 0.006132778245955706, 0.006062023341655731, 0.005814855452626944, 0.3843808174133301, 0.15270036458969116, 0.00674721272662282, 0.00728929229080677, 0.007767664734274149, 0.008404695428907871, 0.14584629237651825, 0.009048561565577984, 0.009269674308598042, 0.009250281378626823, 0.009320230223238468, 0.09424816071987152, 0.009291051886975765, 0.00933066289871931, 0.009211540222167969, 0.008987702429294586, 0.008724975399672985, 0.008467985317111015, 0.008143044076859951, 0.0076731229200959206, 0.007334341760724783, 0.0070078629069030285, 0.006575452629476786, 0.006215470843017101, 0.005873719695955515, 0.005548712331801653, 0.005219773389399052, 0.004950711503624916, 0.004592500161379576, 0.004324632231146097, 0.004075644072145224, 0.0038309702649712563, 0.003587219398468733, 0.003386657452210784, 0.0032032711897045374, 0.003032972104847431, 0.11162217706441879, 0.0028217153158038855, 0.0027180130127817392, 0.0026599776465445757, 0.13767686486244202, 0.0026089672464877367, 0.002662661951035261, 0.002661589067429304, 0.0026082792319357395, 0.0025988121051341295, 0.00263986736536026, 0.0025295577943325043, 0.002525407588109374, 0.002548287156969309, 0.002409490291029215, 0.002401640871539712, 0.002322015818208456, 0.0023430557921528816, 0.002254283055663109, 0.002241886919364333, 0.0021628444083034992, 0.0021439664997160435, 0.0020148504991084337, 0.0019308584742248058, 0.1274246871471405, 0.0019650375470519066, 0.001923171803355217, 0.0019524728413671255, 0.0019402285106480122, 0.0019913644064217806, 0.15381565690040588, 0.0021000131964683533, 0.002067900262773037, 0.09499719738960266, 0.0021966169588267803, 0.002369445515796542, 0.0024106535129249096, 0.002541898749768734, 0.002629960421472788, 0.00246794824488461, 0.15137998759746552, 0.0027580205351114273, 0.0027142094913870096, 0.12329041957855225, 0.002952482318505645, 0.0031448875088244677, 0.0031914114952087402, 0.12087787687778473, 0.0034361560828983784, 0.003731959266588092, 0.0036783977411687374, 0.003957969136536121, 0.003952821251004934, 0.0039709643460810184, 0.004106649663299322, 0.003961270209401846, 0.0040900991298258305, 0.004077103454619646, 0.003739025676622987, 0.16746678948402405, 0.004134573508054018, 0.0037970568519085646, 0.0040449961088597775, 0.003915475681424141, 0.003806585446000099, 0.0037178369238972664, 0.0035948853474110365, 0.00357072614133358, 0.0034568640403449535, 0.18894433975219727, 0.003560814308002591, 0.09265272319316864, 0.003661572001874447, 0.0036118589341640472, 0.0037226646672934294, 0.0036034719087183475, 0.003691329387947917, 0.10947714000940323, 0.0036836492363363504, 0.11316637694835663, 0.15824276208877563, 0.004095991607755423, 0.00451231561601162, 0.004540590103715658, 0.0047823102213442326, 0.0046963198110461235, 0.0048708561807870865, 0.004790055565536022, 0.004711246117949486, 0.004751695320010185, 0.004796721041202545, 0.004655936267226934, 0.004394008312374353, 0.004444056190550327, 0.004290311597287655, 0.004090458154678345, 0.003978443332016468, 0.003899815259501338, 0.003741219872608781, 0.0036064765881747007, 0.003425224218517542, 0.003392841899767518, 0.003181320847943425, 0.0030470946803689003, 0.00291599053889513, 0.0028475485742092133, 0.1279717981815338, 0.002773361513391137, 0.0027284177485853434, 0.002643589861690998, 0.0025777495466172695, 0.0026257450226694345, 0.002553192898631096, 0.002526444150134921, 0.002425340237095952, 0.0024131149984896183, 0.002301560016348958, 0.00223273946903646, 0.0022135931067168713, 0.0021124640479683876, 0.002085236832499504, 0.0019928147085011005, 0.001975366845726967, 0.0019669081084430218, 0.0018294977489858866, 0.0018132688710466027, 0.0017661526799201965, 0.13438090682029724, 0.0018006268655881286, 0.001810949994251132, 0.0017225464107468724, 0.0017877205973491073, 0.0017531431512907147, 0.001782381790690124, 0.0017587865004315972, 0.0018059577560052276, 0.0017190630314871669, 0.0017133711371570826, 0.0016987710259854794, 0.0016489516710862517, 0.0016532326117157936, 0.1715519279241562, 0.0016280623385682702, 0.0016715978272259235, 0.0017357936594635248, 0.1552160233259201, 0.001872481545433402, 0.0020190011709928513, 0.13950763642787933, 0.0022261328995227814, 0.002325559500604868, 0.002394262468442321, 0.002502291463315487, 0.002617972670122981, 0.002698951866477728, 0.0028466915246099234, 0.0027400911785662174, 0.0027589190285652876, 0.002775510074570775, 0.002838084939867258, 0.00278259813785553, 0.002779943635687232, 0.0027305830735713243, 0.1455584615468979, 0.002740156836807728, 0.0028398996219038963, 0.0028914608992636204, 0.0028718472458422184, 0.0029078468214720488, 0.13527068495750427, 0.002926419023424387, 0.0030195650178939104, 0.003005906706675887, 0.11038263142108917, 0.003111450932919979, 0.13005949556827545, 0.0033705872483551502, 0.003509216010570526, 0.003693407168611884, 0.003683621995151043, 0.003820778103545308, 0.0038700869772583246, 0.0038230433128774166, 0.003827347420156002, 0.0038780642207711935, 0.003751660231500864, 0.0037137006875127554, 0.0036426300648599863, 0.0035992476623505354, 0.003502377076074481, 0.0034283683635294437, 0.11158467084169388, 0.0033243924845010042, 0.0033318367786705494, 0.003309555584564805, 0.003297769697383046, 0.0032085583079606295, 0.0031565241515636444, 0.10517950356006622, 0.4060157537460327, 0.0036215907894074917, 0.004197890404611826, 0.00460988050326705, 0.1087099090218544, 0.005536212120205164, 0.09904222935438156, 0.006556766573339701, 0.006954093463718891, 0.007350614760071039, 0.13662877678871155, 0.008216381072998047, 0.00843212939798832, 0.008585048839449883, 0.008672496303915977, 0.008876950480043888, 0.008749288506805897, 0.1548869013786316, 0.008763391524553299, 0.00856455322355032, 0.008456793613731861, 0.008411874063313007, 0.008069457486271858, 0.00800186488777399, 0.007487358059734106, 0.1300843060016632, 0.007058997638523579, 0.0068396576680243015, 0.006733921356499195, 0.006505821365863085, 0.006271782796829939, 0.006002029869705439, 0.005609971005469561, 0.10689204186201096, 0.005222017876803875, 0.0050972397439181805, 0.004908618051558733, 0.106238454580307, 0.1179441437125206, 0.0047205351293087006, 0.004744602367281914, 0.00468782614916563, 0.004631161689758301, 0.004597689025104046, 0.004497801419347525, 0.004430355038493872, 0.004294435493648052, 0.004174837376922369, 0.004071051720529795, 0.003971943166106939, 0.1256091594696045, 0.003797610057517886, 0.00376735907047987, 0.0037032298278063536, 0.003684848081320524, 0.11378331482410431, 0.0036008437164127827, 0.003602611366659403, 0.003586023347452283, 0.0035513704642653465, 0.003528681118041277, 0.003479201579466462, 0.003389789490029216, 0.003369526704773307, 0.003299812786281109, 0.00318887154571712, 0.0031267297454178333, 0.0030301602091640234, 0.002940340666100383, 0.0028612264432013035, 0.002786265918985009, 0.00270589767023921, 0.002627194859087467, 0.0025489465333521366, 0.002473678672686219, 0.0023894584737718105, 0.0023298386950045824, 0.0022503274958580732, 0.002192893996834755, 0.0021111280657351017, 0.14954039454460144, 0.002069099573418498, 0.13261787593364716, 0.0021427390165627003, 0.0022172420285642147, 0.002258473541587591, 0.002295425161719322, 0.12944898009300232, 0.002432574052363634, 0.0025128049310296774, 0.002589561278000474, 0.002634989330545068, 0.4417440891265869, 0.0031298964750021696, 0.0036054428201168776, 0.004030549433082342, 0.004429128021001816, 0.00484821479767561, 0.005204889923334122, 0.09312383830547333, 0.3976486027240753, 0.00682804873213172, 0.007796413730829954, 0.008650225587189198, 0.11757867783308029, 0.01033797673881054, 0.011052818968892097, 0.011727669276297092, 0.012128950096666813, 0.012446312233805656, 0.012657872401177883, 0.012614483945071697, 0.012580628506839275, 0.012441731058061123, 0.012101350352168083, 0.01180086936801672, 0.011295311152935028, 0.010899828746914864, 0.010464214719831944, 0.009864241816103458, 0.00933142937719822, 0.00883191917091608, 0.008388672955334187, 0.12231341749429703, 0.007524512708187103, 0.12212158739566803, 0.007008839398622513, 0.11032753437757492, 0.12019606679677963, 0.13504579663276672, 0.006666294299066067, 0.006614676211029291, 0.006566415075212717, 0.006532543804496527, 0.006426478270441294, 0.006269564386457205, 0.006148844957351685, 0.005976494401693344, 0.005802731495350599, 0.005628677085042, 0.005457261111587286, 0.005268459673970938, 0.005051536951214075, 0.004874925594776869, 0.004666266031563282, 0.004520216956734657, 0.004311505239456892, 0.004147930536419153, 0.003974026534706354, 0.0038097677752375603, 0.003651565406471491, 0.003501889994367957, 0.003361825365573168, 0.0032477478962391615, 0.0031058823224157095, 0.0029795877635478973, 0.002871366450563073, 0.0027585260104388, 0.0026631366927176714, 0.0025686873123049736, 0.0024684069212526083, 0.0023834428284317255, 0.00230862433090806, 0.0022243750281631947, 0.002150740474462509, 0.002079838188365102, 0.0020308189559727907, 0.001956706400960684, 0.0018970558885484934, 0.13432809710502625, 0.0018453376833349466, 0.0018468074267730117, 0.0018421970307826996, 0.0018418552353978157, 0.0018283284734934568, 0.0018262008670717478, 0.0017989673651754856, 0.0017824849346652627, 0.0017649329965934157, 0.001744504552334547, 0.0017185620963573456, 0.001697268453426659, 0.0016767209162935615, 0.0016475300071761012, 0.0016202789265662432, 0.1315145045518875, 0.0016227663727477193, 0.001647765515372157, 0.0016625042771920562, 0.0016787140630185604, 0.0016840060707181692, 0.0016921849455684423, 0.0016880192561075091, 0.0016864696517586708, 0.1486215889453888, 0.13942420482635498, 0.0018351422622799873, 0.0019233974162489176, 0.0020111901685595512, 0.0020912974141538143, 0.002144573489204049, 0.002198177855461836, 0.0022354491520673037, 0.002269977005198598, 0.12303470075130463, 0.00239161285571754, 0.0024447001051157713, 0.0025066682137548923, 0.1280105859041214, 0.0026539352256804705, 0.002750638872385025, 0.0028250347822904587, 0.0028819935396313667, 0.0029337890446186066, 0.002969706431031227, 0.002990553854033351, 0.11127849668264389, 0.11799022555351257, 0.0032035349868237972, 0.00331664620898664, 0.0034049488604068756, 0.0034819142892956734, 0.003539960365742445, 0.0035747154615819454, 0.0035964755807071924, 0.0036028907634317875, 0.0035949235316365957, 0.00357512547634542, 0.12264876812696457, 0.11914536356925964, 0.0036985515616834164, 0.11278548836708069, 0.003930909559130669, 0.004046822432428598, 0.004146336577832699, 0.004242382012307644, 0.004273869097232819, 0.004297147039324045, 0.12290260195732117, 0.0043884809128940105, 0.004429108928889036, 0.11769145727157593, 0.004566092509776354, 0.11951249837875366, 0.004778536502271891, 0.12267426401376724, 0.005071620922535658, 0.12458985298871994, 0.005426129791885614, 0.005587172228842974, 0.005731366574764252, 0.005797781050205231, 0.005851867143064737, 0.005839631427079439, 0.11146513372659683, 0.0058784447610378265, 0.0059189461171627045, 0.005884980317205191, 0.005856373347342014, 0.005781073123216629, 0.005695860832929611, 0.11248618364334106, 0.00554642453789711, 0.11352644115686417, 0.11020174622535706, 0.005641655996441841, 0.005692089442163706, 0.005753716453909874, 0.00573109183460474, 0.00568637577816844, 0.005628082435578108, 0.005550593137741089, 0.11701621115207672, 0.005434451624751091, 0.38559553027153015, 0.10925301164388657, 0.006472878623753786, 0.006983921397477388, 0.007438419386744499, 0.007824939675629139, 0.008154078386723995, 0.008339718915522099, 0.008566169999539852, 0.11397315561771393, 0.008763955906033516, 0.00879545696079731, 0.10523128509521484, 0.11420097947120667, 0.00900326482951641, 0.009108547121286392, 0.00905127078294754, 0.008987069129943848, 0.008860954083502293, 0.008711745962500572, 0.008467406034469604, 0.09844958782196045, 0.00807702261954546, 0.1109265610575676, 0.007808354217559099, 0.10691491514444351, 0.007614621892571449, 0.007526869885623455, 0.007412729784846306, 0.007258523255586624, 0.007087102625519037, 0.0068751429207623005, 0.11398549377918243, 0.0065444461070001125, 0.11568399518728256, 0.006350873503834009, 0.006320093292742968, 0.006183963734656572, 0.006068489979952574, 0.0059099821373820305, 0.00576402572914958, 0.005597371608018875, 0.0054197837598621845, 0.005247167777270079, 0.005090540740638971, 0.004894453566521406, 0.004711231216788292, 0.00454377569258213, 0.11675731092691422, 0.0043166279792785645, 0.004199255723506212, 0.004117202013731003, 0.004012696444988251, 0.12131545692682266, 0.0038922522217035294, 0.00385934766381979, 0.003805655287578702, 0.003761156229302287, 0.003709375159814954, 0.0036309536080807447, 0.0035540421959012747, 0.003482976695522666, 0.003405964933335781, 0.0033284975215792656, 0.003242083126679063, 0.003166899085044861, 0.00307779130525887, 0.002990563167259097, 0.0029124587308615446, 0.002827801275998354, 0.0027484481688588858, 0.12903952598571777, 0.0026698431465774775, 0.0026544048450887203, 0.0026354994624853134, 0.002612869720906019, 0.0025855882558971643, 0.002552628517150879, 0.002513817511498928, 0.0024761436507105827, 0.11363963037729263, 0.002460737945511937, 0.1251295506954193, 0.0025403264444321394, 0.0025946132373064756, 0.0026381174102425575, 0.0026891056913882494, 0.0026957381051033735, 0.002711957087740302, 0.0027120576705783606, 0.002712141489610076, 0.0027012277860194445, 0.0026912600733339787, 0.0026606654282659292, 0.002634829143062234, 0.0026032649911940098, 0.11406215280294418, 0.12832999229431152, 0.13021372258663177, 0.0028188740834593773, 0.002944404724985361, 0.1349666714668274, 0.13168807327747345, 0.003452892415225506, 0.003672793973237276, 0.0038387218955904245, 0.004008150659501553, 0.004143556579947472, 0.00426531583070755, 0.004324255045503378, 0.004375241696834564, 0.004386485554277897, 0.004401816986501217, 0.12258677184581757, 0.004457033704966307, 0.004520336631685495, 0.004525706171989441, 0.004525550175458193, 0.0045053004287183285, 0.004473468288779259, 0.11497945338487625, 0.004428334999829531, 0.004448770545423031, 0.004409242421388626, 0.0043933275155723095, 0.004343040287494659, 0.004281465895473957, 0.004223368596285582, 0.10853848606348038, 0.004124610684812069, 0.004101469647139311, 0.004080745857208967, 0.0040434920229017735, 0.003995921462774277, 0.003910650499165058, 0.0038451501168310642, 0.003774813609197736, 0.003682947251945734, 0.0035992744378745556, 0.0035141303669661283, 0.003417505882680416, 0.0033455395605415106, 0.003236451419070363, 0.0031474921852350235, 0.0030934682581573725, 0.002974077593535185, 0.1343778669834137, 0.002884608693420887, 0.11923650652170181, 0.0029136594384908676, 0.0029406410176306963, 0.003001631237566471, 0.0029875675681978464, 0.12137265503406525, 0.11259885132312775, 0.0031604121904820204, 0.003261597827076912, 0.0033399960957467556, 0.0034180686343461275, 0.003452638629823923, 0.003491840325295925, 0.12780585885047913, 0.003587197745218873, 0.003645909484475851, 0.0037449290975928307, 0.0037405777256935835, 0.0037382924929261208, 0.003753130789846182, 0.003725652350112796, 0.1215994656085968, 0.0037301499396562576, 0.0037529333494603634, 0.003774372860789299, 0.003770558163523674, 0.0037490136455744505, 0.0037210399750620127, 0.003683044807985425, 0.11972858011722565, 0.003667158540338278, 0.0036705685779452324, 0.0036542268935590982, 0.0036370735615491867, 0.0036301524378359318, 0.003573960391804576, 0.0035306590143591166, 0.0034731982741504908, 0.11787902563810349, 0.0034320466220378876, 0.003438523504883051, 0.0034130127169191837, 0.0033851335756480694, 0.003365093842148781, 0.0033165914937853813, 0.13133122026920319, 0.003301425836980343, 0.0033233347348868847, 0.0033201766200363636, 0.0033303049858659506, 0.12934516370296478, 0.003355620428919792, 0.003376186126843095, 0.003392244456335902, 0.003394704544916749, 0.0033861082047224045, 0.0033725197426974773, 0.0033539426513016224, 0.0033170150127261877, 0.0032722693867981434, 0.0032344183418899775, 0.11789289116859436, 0.003204920794814825, 0.003224113257601857, 0.0031903970520943403, 0.12938368320465088, 0.42199453711509705, 0.003676881082355976, 0.004082018509507179, 0.004487164784222841, 0.004864583723247051, 0.005163283087313175, 0.005454822909086943, 0.005718535743653774, 0.005938212387263775, 0.006069544702768326, 0.006179883144795895, 0.006269714795053005, 0.12566602230072021, 0.00639796257019043, 0.006463734433054924, 0.006482409778982401, 0.006477536167949438, 0.1165337860584259, 0.006489410065114498, 0.006455035414546728, 0.006411504000425339, 0.10885883122682571, 0.006363735068589449, 0.006331936456263065, 0.0062645357102155685, 0.006229516584426165, 0.3818190097808838, 0.0065199341624975204, 0.0069129071198403835, 0.00721657183021307, 0.0989258661866188, 0.0077874381095170975, 0.008000634610652924, 0.10793249309062958, 0.008441836573183537, 0.09722492098808289, 0.10496911406517029, 0.008981331251561642, 0.009127221070230007, 0.009198768064379692, 0.009269035421311855, 0.00918174535036087, 0.009149371646344662, 0.008953452110290527, 0.008829806931316853, 0.008568748831748962, 0.008351658470928669, 0.09962392598390579, 0.007896644994616508, 0.007705892901867628, 0.00749177997931838, 0.00726466067135334, 0.11888542026281357, 0.006888910196721554, 0.12132405489683151, 0.006656595040112734, 0.006574398372322321, 0.006431081797927618, 0.00630836421623826, 0.006145100574940443, 0.005994456820189953, 0.005846019834280014, 0.005662649869918823, 0.00549890985712409, 0.005314785987138748, 0.005092808045446873, 0.004910419229418039, 0.004764806479215622, 0.004604683723300695, 0.004419271368533373, 0.004258975386619568, 0.004098266363143921, 0.00394028564915061, 0.11666114628314972, 0.003739921608939767, 0.003691952209919691, 0.003582799807190895, 0.0035082169342786074, 0.003447425551712513, 0.0033727711997926235, 0.003288793610408902, 0.0032102856785058975, 0.0031186810228973627, 0.0030339618679136038, 0.002953100949525833, 0.002877244260162115, 0.002796789398416877, 0.1262890249490738, 0.002737445756793022, 0.0027067847549915314, 0.0026913988403975964, 0.12883006036281586, 0.002707182429730892, 0.002731802174821496, 0.13248951733112335, 0.002835647901520133, 0.0028856827411800623, 0.002910487586632371, 0.002936126198619604, 0.1346609890460968, 0.003056825138628483, 0.12937477231025696, 0.0032345224171876907, 0.0033382903784513474, 0.0034309551119804382, 0.003490172093734145, 0.003553796326741576, 0.11443838477134705, 0.0036702416837215424, 0.003742243628948927, 0.0037989127449691296, 0.003825575113296509, 0.0038787999656051397, 0.003881125245243311, 0.00386436958797276, 0.12156012654304504, 0.003867039456963539, 0.1257559210062027, 0.00398727273568511, 0.004062161780893803, 0.004114092793315649, 0.004133956972509623, 0.004160238895565271, 0.004158626776188612, 0.004127517808228731, 0.0040910569950938225, 0.11683862656354904, 0.004063019063323736, 0.11022040247917175, 0.11931384354829788, 0.004270618315786123, 0.004376661963760853, 0.004452303983271122, 0.004485947545617819, 0.004523220006376505, 0.004535104613751173, 0.0045290603302419186, 0.004503037314862013, 0.004457033704966307, 0.004412144888192415, 0.11123231798410416, 0.004365504719316959, 0.38239172101020813, 0.004742893856018782, 0.00513578811660409, 0.005467171780765057, 0.005723253823816776, 0.006013254169374704, 0.11380670964717865, 0.006454625632613897, 0.006616809871047735, 0.11685933917760849, 0.10413011163473129, 0.007233334239572287, 0.007439217995852232, 0.00759356003254652, 0.007667700294405222, 0.0076956055127084255, 0.007717276457697153, 0.10861223191022873, 0.007639505434781313, 0.007608522195369005, 0.0075581870041787624, 0.007451682351529598, 0.00733470031991601, 0.09725779294967651, 0.007122400216758251, 0.007047922350466251, 0.00686646020039916, 0.006757652387022972, 0.006592441350221634, 0.006411615759134293, 0.0062911552377045155, 0.006033299490809441, 0.005872622597962618, 0.005699187982827425, 0.005465357098728418, 0.005257976241409779, 0.005076822824776173, 0.00489851413294673, 0.1267005056142807, 0.004684684332460165, 0.004559233784675598, 0.0044257077388465405, 0.004325912334024906, 0.0042293863371014595, 0.004127751104533672, 0.003990823403000832, 0.11493570357561111, 0.003845740342512727, 0.0038207133766263723, 0.003753683064132929, 0.003699052147567272, 0.0036653177812695503, 0.003549291053786874, 0.0034932999406009912, 0.003416266990825534, 0.003365444717928767, 0.00327938050031662, 0.003197941929101944, 0.003100481117144227, 0.12130749225616455, 0.003043056931346655, 0.003041207557544112, 0.1377267837524414, 0.0030370440799742937, 0.003042827593162656, 0.003069523489102721, 0.10580309480428696, 0.0031187133863568306, 0.0031883723568171263, 0.003190639428794384, 0.003213727381080389, 0.003237792057916522, 0.003223259234800935, 0.003238873789086938, 0.12856441736221313, 0.003217208432033658, 0.11229663342237473, 0.12754103541374207, 0.0034707789309322834, 0.11995763331651688, 0.0037271238397806883, 0.003896427573636174, 0.0040295799262821674, 0.0041212099604308605, 0.004177847411483526, 0.004216811619699001, 0.12833695113658905, 0.004368757363408804, 0.00444142147898674, 0.004493687767535448, 0.004509032238274813, 0.004530163947492838, 0.09327246248722076, 0.004560162778943777, 0.004600801505148411, 0.004605141934007406, 0.004588339477777481, 0.004555295221507549, 0.0044908965937793255, 0.00446159765124321, 0.004424633923918009, 0.004320434294641018, 0.004238341469317675, 0.00414452888071537, 0.004091175738722086, 0.004040370229631662, 0.11498136818408966, 0.003875818569213152, 0.0038142690900713205, 0.003804674604907632, 0.003764017252251506, 0.12896181643009186, 0.0036833349149674177, 0.003691424848511815, 0.12270121276378632, 0.00372791918925941, 0.003750397125259042, 0.003805705113336444, 0.0038095831405371428, 0.003768159309402108, 0.003757578320801258, 0.003750656731426716, 0.00370004097931087, 0.003630422754213214, 0.00359120755456388, 0.0035244813188910484, 0.0034982869401574135, 0.003402175148949027, 0.003346843644976616, 0.003253011964261532, 0.003219562815502286, 0.0031350250355899334, 0.003065520664677024, 0.002985004335641861, 0.0029106291476637125, 0.002877776511013508, 0.11840993165969849, 0.002795326290652156, 0.0027596126310527325, 0.002737445291131735, 0.0027172549162060022, 0.002694634487852454, 0.0026482203975319862, 0.0026188006158918142, 0.0025763961020857096, 0.0025492392014712095, 0.0025058062747120857, 0.002454022178426385, 0.002406809013336897, 0.0023861657828092575, 0.0023294989950954914, 0.0022784066386520863, 0.0022458448074758053, 0.0021975536365062, 0.002159039955586195, 0.0021183474455028772, 0.0020806516986340284, 0.12831644713878632, 0.0020647812634706497, 0.002049934584647417, 0.14052651822566986, 0.0021077152341604233, 0.0021442994475364685, 0.002172690350562334, 0.0022028302773833275, 0.12363699078559875, 0.1272203028202057, 0.0024115468841046095, 0.002501975977793336, 0.002593195764347911, 0.002676920499652624, 0.1138014942407608, 0.0028448342345654964, 0.002944787498563528, 0.0030309141147881746, 0.13419109582901, 0.12172091752290726, 0.003362532937899232, 0.003512635361403227, 0.0036807528231292963, 0.0037638675421476364, 0.0038416050374507904, 0.0039009687025099993, 0.003952999599277973, 0.004003573674708605, 0.004005975089967251, 0.004033882636576891, 0.003985787741839886, 0.003981085494160652, 0.003946178592741489, 0.003903831820935011, 0.0038719570729881525, 0.0037684787530452013, 0.0037338919937610626, 0.43343719840049744, 0.003961500711739063, 0.004223794210702181, 0.0044876872561872005, 0.004708408378064632, 0.004879515618085861, 0.00501466216519475, 0.00517994025722146, 0.0052402629517018795, 0.005317024886608124, 0.0053683496080338955, 0.11014454811811447, 0.005437836982309818, 0.005492981057614088, 0.0055012269876897335, 0.12140344828367233, 0.00559482304379344, 0.005578844342380762, 0.00558109488338232, 0.005585191771388054, 0.00555218942463398, 0.005499254912137985, 0.00539788044989109, 0.005299000535160303, 0.005207250360399485, 0.11996261030435562, 0.12683410942554474, 0.005124866031110287, 0.005131151061505079, 0.005129578523337841, 0.10717827081680298, 0.005113908555358648, 0.13323137164115906, 0.005177110433578491, 0.005252895876765251, 0.005280624609440565, 0.005260654259473085, 0.005230184644460678, 0.005184783600270748, 0.005152434576302767, 0.005077446810901165, 0.0049826097674667835, 0.004892359953373671, 0.004798314534127712, 0.004713431466370821, 0.3918057680130005, 0.0048890127800405025, 0.005138992797583342, 0.005387180019170046, 0.10870980471372604, 0.005780888255685568, 0.0059592826291918755, 0.006142946891486645, 0.006221185438334942, 0.006314917467534542, 0.006384651642292738, 0.09908769279718399, 0.006434090901166201, 0.11938668042421341, 0.006527224089950323, 0.10152380913496017, 0.006679124664515257, 0.006738144438713789, 0.006762984674423933, 0.006756418850272894, 0.006710108369588852, 0.1210750862956047, 0.006680014543235302, 0.006631958764046431, 0.006576898042112589, 0.006503434851765633, 0.006425616797059774, 0.006313788238912821, 0.006160227581858635, 0.36672714352607727, 0.12018029391765594, 0.0067018307745456696, 0.006982732098549604, 0.007229502312839031, 0.00741951959207654, 0.007575389929115772, 0.09990976750850677, 0.11418014764785767, 0.007998868823051453, 0.10345989465713501, 0.008253655396401882, 0.008404824882745743, 0.008448553271591663, 0.008427286520600319, 0.008393892087042332, 0.008346966467797756, 0.008237063884735107]\n",
            "Val loss 0.023245171146017724\n",
            "Val auc roc 0.48088144625005497\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df0dbbbcddbe4baba8b5f94c8b12ff6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1630.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0257\n",
            "Train Losses : [0.12048467248678207, 0.008055335842072964, 0.007964194752275944, 0.007821696810424328, 0.09777890890836716, 0.007575798314064741, 0.007471119984984398, 0.007342149969190359, 0.007211224175989628, 0.007019872777163982, 0.00687565328553319, 0.0066758207976818085, 0.006502355914562941, 0.006282422691583633, 0.006105020642280579, 0.005898497067391872, 0.10742663592100143, 0.005584951490163803, 0.005475475452840328, 0.0053609078750014305, 0.005222513806074858, 0.005087594967335463, 0.004948385991156101, 0.004801170434802771, 0.004657620098441839, 0.004535648971796036, 0.004394216928631067, 0.11717390269041061, 0.10664565861225128, 0.004204241558909416, 0.004179907497018576, 0.004164174199104309, 0.004122637212276459, 0.004082955420017242, 0.004024133086204529, 0.003963111899793148, 0.003894612193107605, 0.0038458004128187895, 0.003778718411922455, 0.12555719912052155, 0.003676102263852954, 0.0036671271082013845, 0.003645149292424321, 0.003595567774027586, 0.12331711500883102, 0.003563517238944769, 0.003580625168979168, 0.1134663000702858, 0.0036241759080439806, 0.003672598162665963, 0.0036540061701089144, 0.003663180163130164, 0.0036636684089899063, 0.0036513865925371647, 0.003629896556958556, 0.003592247609049082, 0.0035709154326468706, 0.1260935217142105, 0.0035529315937310457, 0.0035339167807251215, 0.003541798098012805, 0.0035101750399917364, 0.0035068325232714415, 0.46021318435668945, 0.0037512665148824453, 0.00403673155233264, 0.004298673942685127, 0.10865230858325958, 0.004764914512634277, 0.005002912133932114, 0.11654432862997055, 0.12361624836921692, 0.005705646239221096, 0.005925713572651148, 0.006137963850051165, 0.0063009727746248245, 0.0064183976501226425, 0.006502921227365732, 0.006561566144227982, 0.006592532619833946, 0.006571348290890455, 0.0065345363691449165, 0.00649546692147851, 0.00643908092752099, 0.006338348612189293, 0.006209059618413448, 0.006090879440307617, 0.0059576258063316345, 0.11130394041538239, 0.005759078543633223, 0.005676727741956711, 0.005588683765381575, 0.005500873550772667, 0.005373425781726837, 0.1194533258676529, 0.005217595491558313, 0.005152126774191856, 0.12160336971282959, 0.005075428169220686, 0.12966281175613403, 0.005083349067717791, 0.005091641563922167, 0.00508606992661953, 0.005064590368419886, 0.005029250867664814, 0.004974300041794777, 0.004913054872304201, 0.00485382741317153, 0.004772680811583996, 0.004679104778915644, 0.11542823165655136, 0.004562036134302616, 0.1162249818444252, 0.004545556381344795, 0.004549682606011629, 0.004531451500952244, 0.11171282827854156, 0.004535925108939409, 0.0045479475520551205, 0.004542473703622818, 0.004511597566306591, 0.004491323139518499, 0.004454460926353931, 0.004408298991620541, 0.12974362075328827, 0.004348801448941231, 0.00434462446719408, 0.004318974446505308, 0.004277704283595085, 0.11078867316246033, 0.004256071988493204, 0.004247672855854034, 0.004246027674525976, 0.413286030292511, 0.004532267339527607, 0.004812906961888075, 0.0050600143149495125, 0.005280665121972561, 0.11621488630771637, 0.005706968251615763, 0.005889927968382835, 0.0060330601409077644, 0.006153445225208998, 0.006231904495507479, 0.006301159504801035, 0.006310190539807081, 0.006325453519821167, 0.006288235541433096, 0.006260794121772051, 0.10575513541698456, 0.006168147549033165, 0.006150511093437672, 0.0061014676466584206, 0.006033033598214388, 0.0059376489371061325, 0.0058602807112038136, 0.12102825939655304, 0.0057034981437027454, 0.005664437543600798, 0.0055879270657896996, 0.005498826503753662, 0.11694687604904175, 0.11330609768629074, 0.005407867021858692, 0.005410117097198963, 0.005394464358687401, 0.0053687868639826775, 0.00532346498221159, 0.00525704538449645, 0.005185816902667284, 0.005116161424666643, 0.005032379645854235, 0.004927643109112978, 0.004839164204895496, 0.004741912242025137, 0.10918034613132477, 0.004596934653818607, 0.004529602359980345, 0.004474848508834839, 0.004414893686771393, 0.11062359064817429, 0.0043258354999125, 0.0042852130718529224, 0.004250254016369581, 0.0042032101191580296, 0.00416439026594162, 0.004100253339856863, 0.004038345534354448, 0.003967450466006994, 0.003902627620846033, 0.12296653538942337, 0.0038188276812434196, 0.11509371548891068, 0.003816688898950815, 0.003830601694062352, 0.0038361086044460535, 0.0038241527508944273, 0.1139564961194992, 0.003839170327410102, 0.003858668264001608, 0.0038704497274011374, 0.003872213652357459, 0.0038494414184242487, 0.1104966253042221, 0.003860961413010955, 0.11318764090538025, 0.003926510456949472, 0.003967303782701492, 0.004001346882432699, 0.004020443186163902, 0.0040309615433216095, 0.13579566776752472, 0.004050116054713726, 0.0040796780958771706, 0.004098812118172646, 0.004123395774513483, 0.004103616811335087, 0.004085287917405367, 0.004062578082084656, 0.004013363271951675, 0.003976225852966309, 0.003930632025003433, 0.003864762606099248, 0.12247305363416672, 0.11870214343070984, 0.11131086200475693, 0.0039445492438972, 0.004015693906694651, 0.11303775757551193, 0.0041648163460195065, 0.004237895365804434, 0.0043163420632481575, 0.004344574641436338, 0.0043610455468297005, 0.004373674746602774, 0.12199724465608597, 0.004408557899296284, 0.004452069289982319, 0.004463557619601488, 0.004453777801245451, 0.11986062675714493, 0.004494238644838333, 0.004507848527282476, 0.004534687846899033, 0.0045195105485618114, 0.118809774518013, 0.004541656468063593, 0.12247823923826218, 0.004612362943589687, 0.004659994505345821, 0.004667969886213541, 0.004685801919549704, 0.004691594280302525, 0.004680428188294172, 0.004655763041228056, 0.0045980722643435, 0.11933654546737671, 0.00454740971326828, 0.12548495829105377, 0.10880738496780396, 0.004682653583586216, 0.11587446928024292, 0.10295596718788147, 0.004995057359337807, 0.11548028141260147, 0.005290075670927763, 0.12140285223722458, 0.005590036977082491, 0.005735882557928562, 0.10922398418188095, 0.005993449129164219, 0.006115491036325693, 0.006185219623148441, 0.006260148715227842, 0.006272380705922842, 0.0062629529275000095, 0.0062586041167378426, 0.00619629817083478, 0.10626567155122757, 0.006141185760498047, 0.10103411227464676, 0.0061386278830468655, 0.006118642631918192, 0.00612194649875164, 0.006073938682675362, 0.0059975371696054935, 0.005924600642174482, 0.005832054186612368, 0.005719681736081839, 0.005620257463306189, 0.00550826545804739, 0.12876245379447937, 0.005346592981368303, 0.005279666278511286, 0.005199486389756203, 0.005115529056638479, 0.0050186472944915295, 0.004918055143207312, 0.004834707360714674, 0.0047348118387162685, 0.38721418380737305, 0.11385806649923325, 0.11171848326921463, 0.005411478225141764, 0.005714304279536009, 0.005912421736866236, 0.006121331360191107, 0.006275675259530544, 0.006381217390298843, 0.006466569844633341, 0.006536970846354961, 0.006580605637282133, 0.006573208142071962, 0.006554867140948772, 0.006491004955023527, 0.00640944205224514, 0.006363877095282078, 0.006281224545091391, 0.006148830987513065, 0.006010167300701141, 0.0059022908098995686, 0.005761782173067331, 0.12031808495521545, 0.005558914504945278, 0.0054864417761564255, 0.005404289346188307, 0.0053156097419559956, 0.0052070776000618935, 0.005114716477692127, 0.005006243474781513, 0.0048730624839663506, 0.119308702647686, 0.0047120521776378155, 0.00465053366497159, 0.004583540838211775, 0.004526438657194376, 0.004452715627849102, 0.0043637980706989765, 0.12179948389530182, 0.1169244647026062, 0.004300767555832863, 0.004284443333745003, 0.004292473196983337, 0.004282859154045582, 0.004240972455590963, 0.0042098406702280045, 0.10788418352603912, 0.105250783264637, 0.004247255623340607, 0.004283179994672537, 0.004286433570086956, 0.004316532518714666, 0.0042845201678574085, 0.004284325055778027, 0.004253983031958342, 0.1146712675690651, 0.004234754014760256, 0.1276031881570816, 0.004295043181627989, 0.004327143542468548, 0.004335999023169279, 0.1066853478550911, 0.11735697835683823, 0.00448425579816103, 0.10759425163269043, 0.11606061458587646, 0.004810800310224295, 0.004941809456795454, 0.10719882696866989, 0.005192485637962818, 0.005304215010255575, 0.00538868922740221, 0.005431229714304209, 0.005480131134390831, 0.005499919876456261, 0.00551257561892271, 0.005487430840730667, 0.005431554280221462, 0.005392384249716997, 0.005305663216859102, 0.005270874127745628, 0.005170291289687157, 0.0051010786555707455, 0.005017594434320927, 0.004885572008788586, 0.004790293052792549, 0.004694256000220776, 0.0046172295697033405, 0.004492064472287893, 0.004375763237476349, 0.004263691138476133, 0.004184972494840622, 0.004070741590112448, 0.003967592027038336, 0.003861847799271345, 0.0037617080379277468, 0.12930716574192047, 0.0036441609263420105, 0.0036143080797046423, 0.0035495609045028687, 0.12705165147781372, 0.003511018119752407, 0.003502107225358486, 0.003492833813652396, 0.0034699293319135904, 0.003475565928965807, 0.003433813573792577, 0.0033801665995270014, 0.003346123732626438, 0.0033255189191550016, 0.0032664923928678036, 0.0032051007729023695, 0.13269345462322235, 0.0031789783388376236, 0.003162620821967721, 0.0031583192758262157, 0.003143796231597662, 0.003100557252764702, 0.0030804970301687717, 0.0030681996140629053, 0.0030285168904811144, 0.002984223887324333, 0.0029575612861663103, 0.0029024379327893257, 0.0028703503776341677, 0.46073928475379944, 0.13383792340755463, 0.0032757853623479605, 0.12092413008213043, 0.003767408197745681, 0.003992852289229631, 0.004210199695080519, 0.0043901982717216015, 0.004587916657328606, 0.004706209991127253, 0.004824120085686445, 0.004937739111483097, 0.103009894490242, 0.10494398325681686, 0.005239606834948063, 0.0053686113096773624, 0.005445263814181089, 0.005537870340049267, 0.12355297803878784, 0.1128467544913292, 0.005777166225016117, 0.005887039005756378, 0.11025634407997131, 0.006048370618373156, 0.006125527434051037, 0.11617910861968994, 0.0062449611723423, 0.006313803140074015, 0.006345196161419153, 0.006362736225128174, 0.006351502146571875, 0.0063420245423913, 0.006300808861851692, 0.006230448838323355, 0.006160389631986618, 0.006026201415807009, 0.0059295943938195705, 0.12124907970428467, 0.005770019255578518, 0.005737496539950371, 0.0056331707164645195, 0.005573204718530178, 0.13001757860183716, 0.005436824634671211, 0.005393258761614561, 0.11785281449556351, 0.1055586040019989, 0.005370952654629946, 0.005391534883528948, 0.005382610484957695, 0.005377362947911024, 0.10935773700475693, 0.005369109567254782, 0.005359956528991461, 0.005348482169210911, 0.005316002760082483, 0.1082690879702568, 0.00529530830681324, 0.005285175051540136, 0.005258989054709673, 0.005213422235101461, 0.005155093967914581, 0.005093701649457216, 0.005037122871726751, 0.004966748878359795, 0.0048673138953745365, 0.004785063676536083, 0.004693843889981508, 0.004602436441928148, 0.004517827183008194, 0.004424945451319218, 0.004325089510530233, 0.004256980028003454, 0.12344523519277573, 0.004106036853045225, 0.12803716957569122, 0.0040656570345163345, 0.11410265415906906, 0.004106984939426184, 0.004127312917262316, 0.004154191818088293, 0.004136325791478157, 0.004129987675696611, 0.004120767116546631, 0.004087678622454405, 0.004063633736222982, 0.12007766962051392, 0.004033522680401802, 0.0040315608493983746, 0.12456481158733368, 0.004055188037455082, 0.0040626791305840015, 0.0040663438849151134, 0.0040696607902646065, 0.004049173090606928, 0.004043084569275379, 0.004017443396151066, 0.003971187397837639, 0.00393503624945879, 0.0038992580957710743, 0.0038357465527951717, 0.003782554529607296, 0.0037328905891627073, 0.003679513931274414, 0.0036229100078344345, 0.003549508284777403, 0.003481607185676694, 0.11607862263917923, 0.0034173501189798117, 0.0033854797948151827, 0.003357396926730871, 0.0033298800699412823, 0.003300662152469158, 0.0032628474291414022, 0.12346156686544418, 0.003232392715290189, 0.0032279968727380037, 0.0032151886261999607, 0.12009162455797195, 0.003240465186536312, 0.003258738899603486, 0.003267817199230194, 0.11691983789205551, 0.13043668866157532, 0.0033977809362113476, 0.0034616293851286173, 0.003524910658597946, 0.0035691556986421347, 0.0035927328281104565, 0.003623253433033824, 0.0036343138199299574, 0.003637320827692747, 0.003624275093898177, 0.003609165782108903, 0.0035914999898523092, 0.13246524333953857, 0.0035803820937871933, 0.003597092116251588, 0.0036035238299518824, 0.003589932806789875, 0.00356977223418653, 0.003552811685949564, 0.003526826621964574, 0.0034930214751511812, 0.003464702283963561, 0.0034209517762064934, 0.003380389651283622, 0.0033356458880007267, 0.0032923128455877304, 0.0032399999909102917, 0.0031906599178910255, 0.003141358494758606, 0.0030855939257889986, 0.0030346254352480173, 0.12811756134033203, 0.0029823696240782738, 0.0029702773317694664, 0.0029591750353574753, 0.002938780700787902, 0.0029170953202992678, 0.002888369606807828, 0.0028593551833182573, 0.002830847632139921, 0.002799245063215494, 0.002764707198366523, 0.13066035509109497, 0.0027387558948248625, 0.002741164993494749, 0.0027392846532166004, 0.0027293397579342127, 0.00271693360991776, 0.002705299761146307, 0.0026833994779735804, 0.002666665008291602, 0.002637326018884778, 0.002615658100694418, 0.0025856581050902605, 0.0025539977941662073, 0.0025285272859036922, 0.002495978493243456, 0.002459664363414049, 0.0024278280325233936, 0.0023960713297128677, 0.14004278182983398, 0.002370904665440321, 0.0023740404285490513, 0.00237447046674788, 0.0023741822224110365, 0.0023680704180151224, 0.0023578722029924393, 0.0023422297090291977, 0.002326756250113249, 0.0023124907165765762, 0.0022953564766794443, 0.0022690552286803722, 0.002249810378998518, 0.002222983166575432, 0.12652139365673065, 0.002216379391029477, 0.0022236767690628767, 0.002230740152299404, 0.0022309606429189444, 0.12762705981731415, 0.0022640754468739033, 0.00229284749366343, 0.0023150835186243057, 0.12684659659862518, 0.002378481673076749, 0.002427242463454604, 0.0024580752942711115, 0.0024922501761466265, 0.0025109334383159876, 0.1324111819267273, 0.002585288602858782, 0.002622986678034067, 0.0026625064201653004, 0.0026905995327979326, 0.002711314707994461, 0.002724986057728529, 0.002731709275394678, 0.0027306280098855495, 0.0027293353341519833, 0.0027233618311583996, 0.0027136472053825855, 0.002692427020519972, 0.13004592061042786, 0.002699387725442648, 0.0027156472206115723, 0.4679272770881653, 0.12250314652919769, 0.003217466874048114, 0.003471857402473688, 0.11942660063505173, 0.00396465789526701, 0.004197463858872652, 0.004418635740876198, 0.004610217642039061, 0.004767976701259613, 0.1125641018152237, 0.0050816345028579235, 0.005231177434325218, 0.0053472514264285564, 0.00543650658801198, 0.005517090670764446, 0.1191716343164444, 0.005640165414661169, 0.005709044635295868, 0.1158052459359169, 0.005825485568493605, 0.005879256408661604, 0.005918445996940136, 0.0059231421910226345, 0.10866394639015198, 0.0059521133080124855, 0.005961121525615454, 0.0059586563147604465, 0.005937546491622925, 0.005899537820369005, 0.005843257997184992, 0.0057784272357821465, 0.005698099732398987, 0.00561504578217864, 0.0055329385213553905, 0.005430517718195915, 0.00533062219619751, 0.10941066592931747, 0.00517805153504014, 0.005114485044032335, 0.005046152975410223, 0.0049710446037352085, 0.004899758845567703, 0.004806193057447672, 0.004727185238152742, 0.004635470453649759, 0.004542938899248838, 0.0044507551938295364, 0.0043595777824521065, 0.004275313578546047, 0.004173732828348875, 0.00408911844715476, 0.003998881671577692, 0.003904475597664714, 0.0038170593325048685, 0.003736193059012294, 0.0036460438277572393, 0.0035591667983680964, 0.003480991581454873, 0.0034002542961388826, 0.0033214695286005735, 0.0032479630317538977, 0.003174732206389308, 0.0031064278446137905, 0.0030304209794849157, 0.1293155699968338, 0.002944741165265441, 0.0029295370914041996, 0.0029044647235423326, 0.002874603495001793, 0.0028458803426474333, 0.0028167900163680315, 0.0027764926198869944, 0.0027440956328064203, 0.0027051500510424376, 0.0026629746425896883, 0.0026230360381305218, 0.0025792825035750866, 0.0025395764969289303, 0.002498603193089366, 0.002462098142132163, 0.1309538334608078, 0.002423722529783845, 0.0024230426643043756, 0.002417823299765587, 0.002412008587270975, 0.00240157637745142, 0.0023845115210860968, 0.0023686238564550877, 0.12763433158397675, 0.00237378291785717, 0.0023887907154858112, 0.0023998531978577375, 0.002406276995316148, 0.1245834082365036, 0.002448535989969969, 0.002481797942891717, 0.0025103844236582518, 0.002526544500142336, 0.0025395890697836876, 0.0025427283253520727, 0.0025452799163758755, 0.12900885939598083, 0.002582569606602192, 0.12952162325382233, 0.002677065087482333, 0.0027351698372513056, 0.13042253255844116, 0.0028642858378589153, 0.002937914803624153, 0.0029980679973959923, 0.0030522115994244814, 0.0030875469092279673, 0.0031162973027676344, 0.0031312566716223955, 0.12224581092596054, 0.003194184973835945, 0.11873886734247208, 0.003316552611067891, 0.003376823151484132, 0.003433643141761422, 0.0034646489657461643, 0.0034926272928714752, 0.12314683943986893, 0.1172020435333252, 0.0036615680437535048, 0.003742372849956155, 0.0038037793710827827, 0.0038527166470885277, 0.00388747057877481, 0.003908833954483271, 0.003927659243345261, 0.11883827298879623, 0.003972796257585287, 0.003995634149760008, 0.00402000593021512, 0.00403138343244791, 0.004025906790047884, 0.11867798119783401, 0.11818455904722214, 0.0041101472452282906, 0.004167764913290739, 0.0042052073404192924, 0.004235007334500551, 0.004240029491484165, 0.004246072378009558, 0.4087897539138794, 0.004495563451200724, 0.11120911687612534, 0.004992673639208078, 0.0052351877093315125, 0.005433296784758568, 0.00560454186052084, 0.00575066776946187, 0.005868714768439531, 0.005956735927611589, 0.00601381016895175, 0.006051382049918175, 0.006078547332435846, 0.006071413401514292, 0.006048009730875492, 0.11308099329471588, 0.006033552810549736, 0.0060219536535441875, 0.005993046797811985, 0.005945068318396807, 0.005902839358896017, 0.005838758312165737, 0.005755885038524866, 0.005667025689035654, 0.10778780281543732, 0.0055409446358680725, 0.005498678423464298, 0.005437549203634262, 0.005359175615012646, 0.005289122927933931, 0.005216142628341913, 0.005124961957335472, 0.005028689280152321, 0.004950153175741434, 0.004854396916925907, 0.0047518303617835045, 0.12076637148857117, 0.004609796684235334, 0.004570357035845518, 0.004509975202381611, 0.004450436681509018, 0.004387685097754002, 0.0043264757841825485, 0.004247400444000959, 0.004180497024208307, 0.004110424779355526, 0.004032302647829056, 0.003963141702115536, 0.0038879518397152424, 0.003814112860709429, 0.0037448147777467966, 0.0036762866657227278, 0.0035959482192993164, 0.0035313982516527176, 0.003460618667304516, 0.0033944055903702974, 0.0033235447481274605, 0.003260994330048561, 0.0031986047979444265, 0.003134374972432852, 0.0030782613903284073, 0.12012652307748795, 0.0030013611540198326, 0.002982695121318102, 0.13033735752105713, 0.002974260365590453, 0.002987687708809972, 0.002988962223753333, 0.002987524028867483, 0.0029824806842952967, 0.0029677196871489286, 0.0029572583734989166, 0.002934612100943923, 0.0029148501344025135, 0.002887364011257887, 0.0028589528519660234, 0.0028380390722304583, 0.0028080882038921118, 0.12036974728107452, 0.0027754162438213825, 0.0027785429265350103, 0.0027760665398091078, 0.0027681109495460987, 0.002757613779976964, 0.0027408660389482975, 0.0027221597265452147, 0.0027003868017345667, 0.00268213520757854, 0.002658975077793002, 0.0026278921868652105, 0.002602618420496583, 0.0025736915413290262, 0.12809044122695923, 0.12176763266324997, 0.002594134770333767, 0.002628801856189966, 0.0026587366592139006, 0.002673777984455228, 0.0026907327119261026, 0.0027039791457355022, 0.0027007723692804575, 0.0027002126444131136, 0.002692784648388624, 0.002682641614228487, 0.0026708340737968683, 0.002658276353031397, 0.002634405391290784, 0.002617573831230402, 0.002597041195258498, 0.0025732715148478746, 0.0025429693050682545, 0.002514476887881756, 0.46747517585754395, 0.0026560421101748943, 0.0028029324021190405, 0.0029430552385747433, 0.0030730057042092085, 0.0031741419807076454, 0.003282722784206271, 0.1193414032459259, 0.0034722983837127686, 0.0035684360191226006, 0.003649492282420397, 0.0037206297274678946, 0.0037713695783168077, 0.0038082136306911707, 0.0038426415994763374, 0.0038671933580189943, 0.003880861448124051, 0.0038715663831681013, 0.00387526024132967, 0.0038541897665709257, 0.00384058547206223, 0.003818873083218932, 0.003780857427045703, 0.0037448429502546787, 0.003709204727783799, 0.003668995574116707, 0.0036281528882682323, 0.0035844931844621897, 0.0035359628964215517, 0.0034877625294029713, 0.0034336522221565247, 0.0033890868071466684, 0.12505048513412476, 0.003328787861391902, 0.0033129907678812742, 0.0032920462545007467, 0.0032677375711500645, 0.0032422731164842844, 0.0032209481578320265, 0.0031877527944743633, 0.00315441214479506, 0.11741131544113159, 0.003120667999610305, 0.0031149201095104218, 0.0031091871205717325, 0.44542622566223145, 0.003289698390290141, 0.0034660189412534237, 0.0036185060162097216, 0.003769335802644491, 0.003890479216352105, 0.0039913044311106205, 0.004081884399056435, 0.004159396514296532, 0.11930209398269653, 0.004308887757360935, 0.004392274655401707, 0.004446245264261961, 0.004488827660679817, 0.004509373567998409, 0.10689425468444824, 0.12122675031423569, 0.004663235507905483, 0.004726099316030741, 0.12366334348917007, 0.004876686260104179, 0.0049187489785254, 0.0049683814868330956, 0.005005637649446726, 0.00501839118078351, 0.005022360477596521, 0.005018073599785566, 0.0049927192740142345, 0.004965031985193491, 0.12349702417850494, 0.004936310928314924, 0.3733360469341278, 0.0051641035825014114, 0.005377387627959251, 0.005566401407122612, 0.0057302191853523254, 0.005857159849256277, 0.005942029878497124, 0.006032319739460945, 0.006092242896556854, 0.006113676819950342, 0.006143742240965366, 0.11292506009340286, 0.006198620889335871, 0.1117132380604744, 0.10878657549619675, 0.35394591093063354, 0.006683850660920143, 0.0069877938367426395, 0.007256573531776667, 0.007476384751498699, 0.007650525309145451, 0.11765126883983612, 0.00810620840638876, 0.008157859556376934, 0.008212102577090263, 0.0084341149777174, 0.10150378942489624, 0.10616137832403183, 0.1282605528831482, 0.008593221195042133, 0.008731200359761715, 0.10898466408252716, 0.008723214268684387, 0.008810661733150482, 0.008728926070034504, 0.10206838697195053, 0.008699508383870125, 0.008687136694788933, 0.008647910319268703, 0.00856828037649393, 0.008472322486341, 0.00841325893998146, 0.008231334388256073, 0.008113797754049301, 0.007954936474561691, 0.007813094183802605, 0.00765133835375309, 0.0075097535736858845, 0.007330087013542652, 0.007168637588620186, 0.007019438780844212, 0.10287807136774063, 0.006735431496053934, 0.11788799613714218, 0.006541826296597719, 0.0064666736871004105, 0.006371582858264446, 0.006268635857850313, 0.00617624819278717, 0.006078745238482952, 0.005956935230642557, 0.10583993792533875, 0.005801264196634293, 0.005720133893191814, 0.005649865139275789, 0.005562973208725452, 0.0054825786501169205, 0.005392764694988728, 0.005302622448652983, 0.005198874045163393, 0.0051279435865581036, 0.005012080073356628, 0.0049331774935126305, 0.11151086539030075, 0.12194084376096725, 0.004768412560224533, 0.10417727380990982, 0.004780486226081848, 0.004774275701493025, 0.004761178512126207, 0.004759133327752352, 0.00472088111564517, 0.004699883051216602, 0.004658838268369436, 0.004620421677827835, 0.004575773142278194, 0.004533197730779648, 0.0044735088013112545, 0.004421956371515989, 0.004353981465101242, 0.004277658183127642, 0.004216402769088745, 0.004159768112003803, 0.12642501294612885, 0.004068480338901281, 0.11873935163021088, 0.004048725590109825, 0.40791916847229004, 0.004255958832800388, 0.004449888598173857, 0.004621725995093584, 0.004758602473884821, 0.12042048573493958, 0.005033914931118488, 0.00516889151185751, 0.005281195975840092, 0.11193030327558517, 0.005454986356198788, 0.11581229418516159, 0.11996916681528091, 0.005808704532682896, 0.005914743524044752, 0.006005590315908194, 0.006092186085879803, 0.0061131734400987625, 0.006160074379295111, 0.006169876083731651, 0.006154893431812525, 0.006135567091405392, 0.006115120369940996, 0.006068337243050337, 0.006009813863784075, 0.00595017010346055, 0.005869136657565832, 0.0057910350151360035, 0.10835646092891693, 0.1094672679901123, 0.00567028671503067, 0.005671578459441662, 0.005639153532683849, 0.005612690467387438, 0.005558681208640337, 0.005496766418218613, 0.00544553529471159, 0.0053920806385576725, 0.11015898734331131, 0.005278368946164846, 0.11573934555053711, 0.005240830592811108, 0.005225775763392448, 0.005200146231800318, 0.005167452618479729, 0.005128085613250732, 0.005080862902104855, 0.1072872132062912, 0.005034685134887695, 0.00499955378472805, 0.11604301631450653, 0.004971364978700876, 0.004970855079591274, 0.004947956185787916, 0.12194722890853882, 0.11020445078611374, 0.004998174961656332, 0.0050141396932303905, 0.00503925746306777, 0.005032217595726252, 0.005021651741117239, 0.005025519523769617, 0.004979978781193495, 0.004947970621287823, 0.004920783452689648, 0.004876378457993269, 0.004815441090613604, 0.0047597638331353664, 0.004698391538113356, 0.004639699123799801, 0.004585189279168844, 0.11446025222539902, 0.00450096046552062, 0.00444693211466074, 0.004417600110173225, 0.004370414651930332, 0.004335398785769939, 0.12006848305463791, 0.004270380362868309, 0.004255451261997223, 0.004230532329529524, 0.004194227512925863, 0.004167323000729084, 0.004120626952499151, 0.004081239458173513, 0.004044981673359871, 0.004005683120340109, 0.003949321340769529, 0.003904391545802355, 0.003852150170132518, 0.12366694957017899, 0.003779921680688858, 0.0037498169112950563, 0.40669265389442444, 0.0039063384756445885, 0.0040674698539078236, 0.004202831070870161, 0.004334099590778351, 0.004412935581058264, 0.004508664831519127, 0.11602964252233505, 0.004665201064199209, 0.004740170203149319, 0.004813685547560453, 0.0048426114954054356, 0.004876713268458843, 0.004909896291792393, 0.004911709111183882, 0.004903832450509071, 0.10721833258867264, 0.004905808717012405, 0.11234227567911148, 0.0049691516906023026, 0.00498567521572113, 0.10827837884426117, 0.005042738746851683, 0.11674002557992935, 0.39118143916130066, 0.005402083974331617, 0.00563932815566659, 0.005861224140971899, 0.0060426220297813416, 0.006205653306096792, 0.0063392408192157745, 0.10677481442689896, 0.006571653299033642, 0.0066880714148283005, 0.006764265242964029, 0.006803242489695549, 0.0068168858997523785, 0.006861694157123566, 0.1171288937330246, 0.006866320967674255, 0.006868737284094095, 0.11447618901729584, 0.0068771312944591045, 0.11220056563615799, 0.006908340845257044, 0.006928691174834967, 0.11643751710653305, 0.006951488554477692, 0.006963544525206089, 0.006939051207154989, 0.006935845594853163, 0.11575160920619965, 0.006866057403385639, 0.006859568413347006, 0.006817140616476536, 0.006772151216864586, 0.0066924323327839375, 0.0066245426423847675, 0.0065606338903307915, 0.006487770471721888, 0.0063894642516970634, 0.00627516396343708, 0.00618832977488637, 0.006079457234591246, 0.005987972021102905, 0.005875664297491312, 0.005776232574135065, 0.005675461143255234, 0.005574806593358517, 0.005450786557048559, 0.005349511746317148, 0.005246836692094803, 0.005143420770764351, 0.005058819893747568, 0.00496244290843606, 0.004855550359934568, 0.004760037641972303, 0.004684864543378353, 0.00458898488432169, 0.004502634983509779, 0.004416736774146557, 0.004331424832344055, 0.0042467741295695305, 0.004167989827692509, 0.004086705856025219, 0.0040188441053032875, 0.003941109869629145, 0.0038705153856426477, 0.0037972729187458754, 0.0037298460956662893, 0.0036566921044141054, 0.003597421105951071, 0.003537361742928624, 0.003471952863037586, 0.003417369443923235, 0.0033615033607929945, 0.0032925643026828766, 0.0032356753945350647, 0.13418126106262207, 0.003163748187944293, 0.0031591388396918774, 0.1278621405363083, 0.0031438409350812435, 0.0031510908156633377, 0.0031494353897869587, 0.0031423172913491726, 0.003127014497295022, 0.0031114635057747364, 0.0030969353392720222, 0.0030807636212557554, 0.003068681340664625, 0.003043313277885318, 0.0030230919364839792, 0.11956728249788284, 0.002991918008774519, 0.12094346433877945, 0.003024893580004573, 0.0030514542013406754, 0.0030595494899898767, 0.12132816016674042, 0.0031072176061570644, 0.1288364678621292, 0.0031938166357576847, 0.003240828635171056, 0.0032887335401028395, 0.003305143676698208, 0.003329998580738902, 0.003342356299981475, 0.0033610425889492035, 0.12328518182039261, 0.003389951540157199, 0.0034139188937842846, 0.11284124106168747, 0.0034690103493630886, 0.003505782689899206, 0.0035276999697089195, 0.0035513790789991617, 0.003569768974557519, 0.003563628764823079, 0.0035672823432832956, 0.0035550594329833984, 0.003550728550180793, 0.003529294626787305, 0.003509576665237546, 0.00348115642555058, 0.12183540314435959, 0.0034739908296614885, 0.003467363538220525, 0.0034669048618525267, 0.003462086897343397, 0.003439676482230425, 0.003424293827265501, 0.0034030841197818518, 0.003373741405084729, 0.0033492569345980883, 0.00332022737711668, 0.0032908781431615353, 0.0032628760673105717, 0.0032340448815375566, 0.0031957051251083612, 0.0031738446559756994, 0.003124763024970889, 0.11571194231510162, 0.0030987716745585203, 0.0030891424976289272, 0.0030656883027404547, 0.0030585522763431072, 0.003045730059966445, 0.00301578757353127, 0.0029995960649102926, 0.0029725797940045595, 0.002956661395728588, 0.0029253840912133455, 0.002893703756853938, 0.0028635538183152676, 0.0028370674699544907, 0.12764008343219757, 0.0028073140420019627, 0.0028107448015362024, 0.0028015272691845894, 0.002798091620206833, 0.0027834156062453985, 0.002776994602754712, 0.002759021008387208, 0.002735306043177843, 0.0027207874227315187, 0.002704184502363205, 0.0026747628580778837, 0.002650092588737607, 0.002625797176733613, 0.002607782604172826, 0.0025903256610035896, 0.002553539117798209, 0.13310495018959045, 0.002540046814829111, 0.13496112823486328, 0.0025663194246590137, 0.00258943741209805, 0.0026091448962688446, 0.002622590633109212, 0.0026340188924223185, 0.002639451529830694, 0.1273767352104187, 0.12076365947723389, 0.0027170032262802124, 0.0027535175904631615, 0.002797574270516634, 0.0028231353498995304, 0.13436943292617798, 0.0029016700573265553, 0.002937414450570941, 0.11790749430656433, 0.003030388616025448, 0.0030783608090132475, 0.003109627403318882, 0.0031536424066871405, 0.0031670830212533474, 0.0031934285070747137, 0.0031985635869205, 0.0032055885531008244, 0.0032134028151631355, 0.0032119061797857285, 0.0032020623330026865, 0.003194879274815321, 0.0031727664172649384, 0.12422536313533783, 0.003172587836161256, 0.003184741595759988, 0.0031746409367769957, 0.12019740790128708, 0.0031994138844311237, 0.12167146056890488, 0.1333467960357666, 0.0033303687814623117, 0.0033910998608917, 0.0034468050580471754, 0.003482518484815955, 0.003516522003337741, 0.1199984923005104, 0.0035880731884390116, 0.1227153018116951, 0.003684163559228182, 0.003736403537914157, 0.0037831838708370924, 0.003822899190708995, 0.11575349420309067, 0.0038925933185964823, 0.003932607360184193, 0.003950092475861311, 0.00398522661998868, 0.003982421010732651, 0.003988100681453943, 0.003988321404904127, 0.003987825475633144, 0.4250625669956207, 0.004138495307415724, 0.004281455185264349, 0.00442064693197608, 0.004540501162409782, 0.004631350748240948, 0.004718541167676449, 0.004781900439411402, 0.004828086122870445, 0.11009645462036133, 0.004929112270474434, 0.0049811070784926414, 0.00501988036558032, 0.00504496693611145, 0.005053564440459013, 0.005047149024903774, 0.10608316957950592, 0.005058723967522383, 0.005067871883511543, 0.005068785976618528, 0.0050641694106161594, 0.005050981882959604, 0.11142661422491074, 0.005028107203543186, 0.005014082882553339, 0.004998135846108198, 0.004991107154637575, 0.004967324901372194, 0.004936003126204014, 0.11656245589256287, 0.004885462112724781, 0.004865703172981739, 0.004852112848311663, 0.004819836001843214, 0.004784494638442993, 0.11066991835832596, 0.4154664874076843, 0.004920353181660175, 0.11401646584272385, 0.005270154681056738, 0.0054395925253629684, 0.005579524207860231, 0.005690131336450577, 0.005768215749412775, 0.005843660794198513, 0.005908534396439791, 0.005947173573076725, 0.005964837968349457, 0.005967991426587105, 0.005974093917757273, 0.005971659906208515, 0.005935350898653269, 0.11067324876785278, 0.005891047418117523, 0.005871826317161322, 0.005870795343071222, 0.005818456877022982, 0.005774593446403742, 0.005731138866394758, 0.0056807538494467735, 0.10463712364435196, 0.0056058866903185844, 0.37629038095474243, 0.005751101765781641, 0.0059438603930175304, 0.00600777380168438, 0.0061970860697329044, 0.00622692983597517, 0.0063154734671115875, 0.00633398350328207, 0.00641689682379365, 0.006384652107954025, 0.10981005430221558, 0.0065401759929955006, 0.006427795626223087, 0.006417015567421913, 0.006344922352582216, 0.37157484889030457, 0.006527008023113012, 0.10280599445104599, 0.006961124949157238, 0.1394442915916443, 0.00720581877976656, 0.007405464071780443, 0.007393350824713707, 0.00749600026756525, 0.007472301367670298, 0.007537304889410734, 0.007497822865843773, 0.007570092100650072, 0.007492412347346544, 0.09361259639263153, 0.007545612286776304, 0.007407112512737513, 0.007407054305076599, 0.007418221328407526, 0.007288095075637102, 0.007243826519697905, 0.007163807284086943, 0.12881551682949066, 0.10109817236661911, 0.12658940255641937, 0.007029245607554913, 0.007046266458928585, 0.007049975451081991, 0.006940210238099098, 0.006881544832140207, 0.006908892188221216, 0.0068227751180529594, 0.00679062120616436, 0.006664123386144638, 0.006534934975206852, 0.00646792771294713, 0.0064303455874323845, 0.006390063092112541, 0.006204142700880766, 0.00611028540879488, 0.00602939585223794, 0.00604584626853466, 0.00588252954185009, 0.005866765510290861, 0.00572228105738759, 0.00568582396954298, 0.00546087184920907, 0.005418444983661175, 0.0053179278038442135, 0.4190583825111389, 0.11383620649576187, 0.00545481126755476, 0.005564600229263306, 0.005593200214207172, 0.09447561204433441, 0.005841767881065607, 0.09355370700359344, 0.005927657708525658, 0.005998301785439253, 0.006025512237101793, 0.09906496852636337, 0.09573107212781906, 0.006197699345648289, 0.006298890803009272, 0.10797615349292755, 0.006407460663467646, 0.13577133417129517, 0.08947862684726715, 0.006534683518111706, 0.1299772560596466, 0.0066780587658286095, 0.006799185182899237, 0.006831523962318897, 0.006799074821174145, 0.006824381649494171, 0.006903213914483786, 0.006798057816922665, 0.0068193706683814526, 0.006781097035855055, 0.006768493447452784, 0.006578516215085983, 0.006574578583240509, 0.006527990102767944, 0.09068405628204346, 0.006406217813491821, 0.0063336631283164024, 0.09476625919342041, 0.006191097665578127, 0.006214388180524111, 0.0062495507299900055, 0.1367536336183548, 0.006054444704204798, 0.006122189573943615, 0.0060775564052164555, 0.006001194939017296, 0.0059369406662881374, 0.00592610752210021, 0.005858186166733503, 0.005777443293482065, 0.00572712067514658, 0.08659032732248306, 0.12686339020729065, 0.005616104695945978, 0.005630533676594496, 0.13451936841011047, 0.005510957445949316, 0.005533320363610983, 0.005580310244113207, 0.00557842617854476, 0.0054606106132268906, 0.005495388992130756, 0.005356708541512489, 0.005379633512347937, 0.005280273500829935, 0.005220010876655579, 0.005221198778599501, 0.005199490115046501, 0.00515073211863637, 0.0050583332777023315, 0.004962556064128876, 0.11733053624629974, 0.004904736764729023]\n",
            "Val loss 0.021734563002977613\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d7ccfe17e8f46868abfca214b4810e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1630.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0258\n",
            "Train Losses : [0.004835971165448427, 0.004782406147569418, 0.004789310973137617, 0.004730808082967997, 0.13655579090118408, 0.12283016741275787, 0.004707635845988989, 0.004725195467472076, 0.004696821793913841, 0.004710495937615633, 0.004668664652854204, 0.0045967926271259785, 0.0045947665348649025, 0.12836018204689026, 0.004592555109411478, 0.004549729637801647, 0.0045144809409976006, 0.004506041761487722, 0.00448601832613349, 0.13528184592723846, 0.09038896858692169, 0.004489358514547348, 0.0045356773771345615, 0.00447720754891634, 0.004521818365901709, 0.004523738753050566, 0.004516259301453829, 0.004481770098209381, 0.004476533737033606, 0.11050307005643845, 0.004491476342082024, 0.004434752278029919, 0.004372788593173027, 0.004397611599415541, 0.004395291209220886, 0.004319667350500822, 0.00433408422395587, 0.004256546031683683, 0.004261694382876158, 0.3815159797668457, 0.40540650486946106, 0.004652189556509256, 0.004842598456889391, 0.005093061365187168, 0.1326952427625656, 0.005419790279120207, 0.005705184768885374, 0.005800778511911631, 0.00591206643730402, 0.006055010017007589, 0.006181988399475813, 0.006180102936923504, 0.006260561756789684, 0.0063157882541418076, 0.006346054840832949, 0.006356264930218458, 0.006360167171806097, 0.006339204031974077, 0.006288932636380196, 0.006264595314860344, 0.00630289688706398, 0.006210636347532272, 0.006170535925775766, 0.006108928006142378, 0.006007295101881027, 0.006013836711645126, 0.005906667094677687, 0.005803020671010017, 0.005743512883782387, 0.005752138793468475, 0.005622112657874823, 0.005584449041634798, 0.005524068139493465, 0.1156691461801529, 0.005391047801822424, 0.005379259120672941, 0.005313585978001356, 0.005270223133265972, 0.0051539805717766285, 0.0051255179569125175, 0.005109571386128664, 0.004957627505064011, 0.0049428255297243595, 0.004991312511265278, 0.1098376139998436, 0.004842681344598532, 0.004799168556928635, 0.0047641717828810215, 0.37400349974632263, 0.0048864190466701984, 0.0048760948702692986, 0.10325372219085693, 0.005140966270118952, 0.005153580568730831, 0.0051814233884215355, 0.00526855606585741, 0.1222090944647789, 0.005439363420009613, 0.0054216948337852955, 0.005513758398592472, 0.005461578257381916, 0.005574721843004227, 0.005443530157208443, 0.005495698656886816, 0.005419309251010418, 0.09704817831516266, 0.005562415346503258, 0.0054474822245538235, 0.005468631628900766, 0.005341225769370794, 0.005375482141971588, 0.0053858682513237, 0.09178649634122849, 0.005308739375323057, 0.005348160397261381, 0.005265232641249895, 0.005230247974395752, 0.005197326652705669, 0.10880416631698608, 0.005143788177520037, 0.005113065708428621, 0.130621075630188, 0.0051688458770513535, 0.0050544715486466885, 0.00511038675904274, 0.005084074102342129, 0.005094710271805525, 0.004985551815479994, 0.005079626105725765, 0.005025690421462059, 0.004968388471752405, 0.004879220854490995, 0.1010708138346672, 0.004766954109072685, 0.004810151644051075, 0.004752029664814472, 0.12510110437870026, 0.1159537136554718, 0.004745171405375004, 0.004721571691334248, 0.004776897374540567, 0.004721633158624172, 0.0047400714829564095, 0.004750766325742006, 0.12381468713283539, 0.1322174072265625, 0.09514525532722473, 0.004776554647833109, 0.004814449697732925, 0.004822554066777229, 0.004813133738934994, 0.004844521172344685, 0.004867520648986101, 0.004820633213967085, 0.004838869441300631, 0.004800871945917606, 0.0048289187252521515, 0.004826177377253771, 0.004721405915915966, 0.004827343393117189, 0.004684475716203451, 0.004646602086722851, 0.00461040111258626, 0.004518396686762571, 0.004468462895601988, 0.004507204983383417, 0.0044336458668112755, 0.004342782776802778, 0.00436632102355361, 0.004293003585189581, 0.004239453002810478, 0.004258646629750729, 0.004232483915984631, 0.004127605352550745, 0.004026399925351143, 0.004017620347440243, 0.00394594669342041, 0.0038948417641222477, 0.003851228626444936, 0.0038382818456739187, 0.0038558358792215586, 0.003782229032367468, 0.0037166704423725605, 0.0036301424261182547, 0.0037185901310294867, 0.0035956029314547777, 0.0035261125303804874, 0.0034758776891976595, 0.003458367194980383, 0.003417933126911521, 0.0034458076115697622, 0.003314556321129203, 0.003304628422483802, 0.0033078459091484547, 0.003266202984377742, 0.003197932615876198, 0.0031934657599776983, 0.0032064858824014664, 0.10706253349781036, 0.0030753095634281635, 0.003088843310251832, 0.003042049240320921, 0.003018508665263653, 0.003103356808423996, 0.0030090240761637688, 0.003004280384629965, 0.0029768336098641157, 0.002987547079101205, 0.0029592220671474934, 0.002904662163928151, 0.4653341472148895, 0.0029997406527400017, 0.0030734429601579905, 0.0031131880823522806, 0.12526047229766846, 0.0032567717134952545, 0.0033893671352416277, 0.0033881880808621645, 0.003430028213188052, 0.0034915143623948097, 0.0035144404973834753, 0.0035348411183804274, 0.0035477096680551767, 0.0036061310674995184, 0.003613398876041174, 0.003687063232064247, 0.003638862632215023, 0.003691461868584156, 0.003651366103440523, 0.0035671889781951904, 0.003553616814315319, 0.003594243200495839, 0.003539784112945199, 0.00360895530320704, 0.15762874484062195, 0.0034868731163442135, 0.0035292764659971, 0.0035366679076105356, 0.003550048219040036, 0.12134953588247299, 0.0035027028061449528, 0.003502507461234927, 0.0035096867941319942, 0.003498220816254616, 0.0035087899304926395, 0.003611517371609807, 0.003474449273198843, 0.003571599256247282, 0.0035086360294371843, 0.1342315673828125, 0.003515547839924693, 0.0034926061052829027, 0.003432723693549633, 0.13406938314437866, 0.0034730120096355677, 0.0035064751282334328, 0.003485926892608404, 0.003475754288956523, 0.0035109161399304867, 0.0034880449529737234, 0.14558494091033936, 0.0034506386145949364, 0.0035073936451226473, 0.003500827355310321, 0.0035048932768404484, 0.003500554244965315, 0.0035183574073016644, 0.0035032774321734905, 0.0035025847610086203, 0.1475098431110382, 0.0034362119622528553, 0.0034725635778158903, 0.0034448723308742046, 0.003442574990913272, 0.0034558591432869434, 0.0034420620650053024, 0.0034139372874051332, 0.12135083228349686, 0.0034674659837037325, 0.0034132550936192274, 0.003451327094808221, 0.003417009487748146, 0.0034109049011021852, 0.003402255941182375, 0.0033876062370836735, 0.0033696580212563276, 0.003351247403770685, 0.003340834518894553, 0.003344266675412655, 0.0033131155651062727, 0.00327918934635818, 0.003260168246924877, 0.0032544154673814774, 0.1367211490869522, 0.0032164771109819412, 0.0032205521129071712, 0.003209560178220272, 0.0031929791439324617, 0.0031988786067813635, 0.0031753298826515675, 0.0031744034495204687, 0.0031792628578841686, 0.0031247353181242943, 0.0031228442676365376, 0.0030828802846372128, 0.11802157014608383, 0.13215038180351257, 0.003111917292699218, 0.0031112772412598133, 0.0031283306889235973, 0.003147364128381014, 0.003120043082162738, 0.003119762521237135, 0.003115991596132517, 0.0031171271111816168, 0.0031173296738415956, 0.003098452230915427, 0.003084431868046522, 0.0030754401814192533, 0.14251826703548431, 0.129952535033226, 0.0030999521259218454, 0.0031155229080468416, 0.003130936063826084, 0.003146157134324312, 0.0031434649135917425, 0.003172278869897127, 0.003151851939037442, 0.0031473233830183744, 0.0031373249366879463, 0.0031331253703683615, 0.003140273503959179, 0.003119814209640026, 0.0031127145048230886, 0.003084836294874549, 0.0030828055460006, 0.003055713837966323, 0.003057217225432396, 0.11726846545934677, 0.1147313192486763, 0.0030365155544131994, 0.0030514542013406754, 0.003063939744606614, 0.003068056656047702, 0.003078070003539324, 0.003067996818572283, 0.0030715721659362316, 0.11857257038354874, 0.0030774399638175964, 0.0030809487216174603, 0.003098204266279936, 0.0030957635026425123, 0.0030877222307026386, 0.0030834011267870665, 0.0030882693827152252, 0.0030765030533075333, 0.0030657113529741764, 0.003068637102842331, 0.0030508353374898434, 0.0030333520844578743, 0.003014115383848548, 0.11664382368326187, 0.0030049190390855074, 0.0030155370477586985, 0.0029987578745931387, 0.003008068073540926, 0.002991796936839819, 0.12703555822372437, 0.0029835712630301714, 0.0029878404457122087, 0.0030058324337005615, 0.002993594389408827, 0.0030029050540179014, 0.003005863865837455, 0.0029956905636936426, 0.0029764724895358086, 0.14099828898906708, 0.002978457836434245, 0.0029778541065752506, 0.002984554274007678, 0.0029841908253729343, 0.002981251571327448, 0.0029772850684821606, 0.002972120651975274, 0.0029637885745614767, 0.0029476513154804707, 0.0029580569826066494, 0.0029373399447649717, 0.002923741936683655, 0.002899465849623084, 0.0028979862108826637, 0.002875886159017682, 0.002848682226613164, 0.0028345962055027485, 0.0028264825232326984, 0.0027954536490142345, 0.0027981766033917665, 0.00276747765019536, 0.12296001613140106, 0.002751490566879511, 0.0027481934521347284, 0.0027323379181325436, 0.002727351151406765, 0.12341705709695816, 0.0027349141892045736, 0.0027448732871562243, 0.002749370178207755, 0.0027506169863045216, 0.002750929445028305, 0.0027380597312003374, 0.0027444756124168634, 0.0027353065088391304, 0.0027269520796835423, 0.002718309871852398, 0.0027112355455756187, 0.002703338861465454, 0.0026837459299713373, 0.129214808344841, 0.0026723090559244156, 0.002682577585801482, 0.0026879736687988043, 0.0026706501375883818, 0.002668799366801977, 0.0026655085384845734, 0.0026627664919942617, 0.0026542129926383495, 0.002645438304170966, 0.0026295778807252645, 0.00262445118278265, 0.002610183088108897, 0.0025978973135352135, 0.002596049103885889, 0.002577791688963771, 0.0025537991896271706, 0.12072326242923737, 0.002541078720241785, 0.0025587978307157755, 0.0025520066265016794, 0.002544616349041462, 0.002540033543482423, 0.002530784346163273, 0.002524749841541052, 0.0025156214833259583, 0.002503086579963565, 0.0025083133950829506, 0.0024909363128244877, 0.002473715925589204, 0.002464188262820244, 0.0024551593232899904, 0.0024467764887958765, 0.002423518570140004, 0.002411765279248357, 0.0024018152616918087, 0.0023896791972219944, 0.0023871136363595724, 0.0023672948591411114, 0.00234942976385355, 0.0023346554953604937, 0.002313736593350768, 0.002305817324668169, 0.0022811968810856342, 0.13024289906024933, 0.0022685315925627947, 0.12494935840368271, 0.00228425906971097, 0.0022955716121941805, 0.0023121864069253206, 0.0023253820836544037, 0.14441612362861633, 0.002354476135224104, 0.0023638466373085976, 0.002374830190092325, 0.0023843583185225725, 0.0024116034619510174, 0.002401906531304121, 0.002407933585345745, 0.12265408039093018, 0.12620341777801514, 0.0024554934352636337, 0.002488207072019577, 0.0025038609746843576, 0.12497157603502274, 0.002562480978667736, 0.002580955857411027, 0.002612506505101919, 0.12624289095401764, 0.002672678092494607, 0.002686132211238146, 0.0027131095994263887, 0.002743443939834833, 0.0027426995802670717, 0.002762738848105073, 0.0027683062944561243, 0.0027834968641400337, 0.002775816945359111, 0.002785253571346402, 0.002784684067592025, 0.12368211895227432, 0.0028015756979584694, 0.0027919751591980457, 0.002811330370604992, 0.0028143443632870913, 0.0028198568616062403, 0.0028129960410296917, 0.0028017701115459204, 0.0027963570319116116, 0.00279854703694582, 0.002783914329484105, 0.002773443004116416, 0.0027677437756210566, 0.0027523322496563196, 0.002738668117672205, 0.002728835679590702, 0.0027144476771354675, 0.002705482766032219, 0.0026880570221692324, 0.0026688124053180218, 0.0026554460637271404, 0.1390792429447174, 0.0026582188438624144, 0.0026461901143193245, 0.0026419637724757195, 0.0026473156176507473, 0.13205403089523315, 0.131960928440094, 0.11652916669845581, 0.002708890475332737, 0.0027435659430921078, 0.002773045329377055, 0.0027934478130191565, 0.002829805249348283, 0.11689721047878265, 0.002865944057703018, 0.12449482828378677, 0.002932455390691757, 0.002956367563456297, 0.0029829079285264015, 0.0030148690566420555, 0.0030289439018815756, 0.003051826497539878, 0.12808653712272644, 0.0030730299185961485, 0.0030982568860054016, 0.0031190100125968456, 0.003120433771982789, 0.003143695881590247, 0.12166418880224228, 0.003158412640914321, 0.003195259254425764, 0.0031879195012152195, 0.0032196782995015383, 0.0032029796857386827, 0.003205124754458666, 0.0032110698521137238, 0.0031933318823575974, 0.0031983174849301577, 0.11746733635663986, 0.0031929973047226667, 0.003202548949047923, 0.0032022185623645782, 0.452571839094162, 0.003291150089353323, 0.003386180615052581, 0.0034578745253384113, 0.003530990332365036, 0.003582025645300746, 0.0036318053025752306, 0.003663234179839492, 0.0036985366605222225, 0.0037260691169649363, 0.00374682922847569, 0.003768657799810171, 0.0037803847808390856, 0.391297310590744, 0.00389969814568758, 0.003994653467088938, 0.1375066339969635, 0.12037581950426102, 0.004270363133400679, 0.004337542224675417, 0.004418783821165562, 0.0045042396523058414, 0.004529174882918596, 0.004573462065309286, 0.004642532207071781, 0.004639139864593744, 0.004661159124225378, 0.004687104374170303, 0.0046987696550786495, 0.0046857199631631374, 0.004715987481176853, 0.004678923171013594, 0.004669226706027985, 0.004679588600993156, 0.004654514603316784, 0.12281661480665207, 0.00462325569242239, 0.00462636724114418, 0.004619203973561525, 0.12471379339694977, 0.004602047149091959, 0.004605036228895187, 0.004614361096173525, 0.00458679860457778, 0.11983118951320648, 0.00459981057792902, 0.004578315652906895, 0.407947301864624, 0.11172126978635788, 0.00479337852448225, 0.004894155543297529, 0.004962410312145948, 0.005044793710112572, 0.0051047601737082005, 0.0051359692588448524, 0.4059818685054779, 0.005332794040441513, 0.005459482315927744, 0.11093080788850784, 0.005695860832929611, 0.005774769466370344, 0.005848713219165802, 0.005924800410866737, 0.005983450450003147, 0.0060395365580916405, 0.006059395615011454, 0.006095931865274906, 0.0061080907471477985, 0.0061185406520962715, 0.006118852645158768, 0.006121651269495487, 0.00611861702054739, 0.1010371521115303, 0.1151680275797844, 0.11495789140462875, 0.10903946310281754, 0.12082362174987793, 0.11600972712039948, 0.006272556725889444, 0.006349724717438221, 0.006358142010867596, 0.0063750469125807285, 0.006388573441654444, 0.006396500859409571, 0.006425440311431885, 0.006409833673387766, 0.006389062386006117, 0.006373392418026924, 0.006341264117509127, 0.006343825254589319, 0.0062759495340287685, 0.006256082560867071, 0.006222723051905632, 0.006194158922880888, 0.0061224112287163734, 0.006103138439357281, 0.006032041739672422, 0.006003106012940407, 0.00594375841319561, 0.0058957478031516075, 0.005882415920495987, 0.005790006835013628, 0.1050010621547699, 0.10407676547765732, 0.005758216138929129, 0.0056654601357877254, 0.005660936702042818, 0.005626492202281952, 0.10629307478666306, 0.11931522935628891, 0.005586765240877867, 0.005586704239249229, 0.0055873822420835495, 0.0055617219768464565, 0.005545934662222862, 0.11075145751237869, 0.005504100117832422, 0.11888979375362396, 0.005505895242094994, 0.005517186131328344, 0.005529702641069889, 0.005570556037127972, 0.10331107676029205, 0.0055144005455076694, 0.005480752792209387, 0.10985848307609558, 0.005523536819964647, 0.1262086033821106, 0.005511004012078047, 0.005503397900611162, 0.00550158740952611, 0.00550187798216939, 0.005509295500814915, 0.005502675659954548, 0.005464515183120966, 0.005487365182489157, 0.005440906621515751, 0.0054135811515152454, 0.005366422701627016, 0.36607638001441956, 0.005430236924439669, 0.005466580390930176, 0.0055466992780566216, 0.005567406304180622, 0.0056027937680482864, 0.11330395191907883, 0.12284703552722931, 0.005723721347749233, 0.005716829560697079, 0.005746009293943644, 0.005798993166536093, 0.0058022779412567616, 0.005823661107569933, 0.005819878540933132, 0.0057863048277795315, 0.005761334672570229, 0.005788224283605814, 0.005729648284614086, 0.0057179126888513565, 0.005686425603926182, 0.005647209472954273, 0.12202537059783936, 0.00563849089667201, 0.005587527994066477, 0.005611806642264128, 0.0055459653958678246, 0.005547977052628994, 0.005564881954342127, 0.10554120689630508, 0.0054644327610731125, 0.00547945499420166, 0.005403750576078892, 0.005402596201747656, 0.005379347130656242, 0.005345312878489494, 0.005301414057612419, 0.005271278787404299, 0.005253890063613653, 0.11664146184921265, 0.005197477992624044, 0.005162716843187809, 0.005155108403414488, 0.005173723679035902, 0.0051180776208639145, 0.0050680008716881275, 0.0050469352863729, 0.10835936665534973, 0.00502808066084981, 0.005023946054279804, 0.0049895914271473885, 0.10449539870023727, 0.00494518643245101, 0.004937233868986368, 0.004907618742436171, 0.004888563416898251, 0.00487841572612524, 0.00486148102208972, 0.00483360979706049, 0.004802267532795668, 0.0047913361340761185, 0.004758404102176428, 0.004729695152491331, 0.004695809446275234, 0.004679333884268999, 0.12642322480678558, 0.004654030781239271, 0.0046372548677027225, 0.004607858136296272, 0.004606001544743776, 0.004575951956212521, 0.004556552041321993, 0.004520501475781202, 0.004510088823735714, 0.004459049552679062, 0.004441175609827042, 0.004465402569621801, 0.004381117876619101, 0.10964259505271912, 0.004343959037214518, 0.00432908209040761, 0.1131879910826683, 0.004333323799073696, 0.004305883776396513, 0.004298171494156122, 0.004301520995795727, 0.004292132332921028, 0.004278417676687241, 0.004260574001818895, 0.004229545593261719, 0.004240595735609531, 0.004259368404746056, 0.13227763772010803, 0.004168540704995394, 0.00417757174000144, 0.004149699583649635, 0.12068475037813187, 0.0041433414444327354, 0.0041468399576842785, 0.004155186004936695, 0.004155972506850958, 0.004136275500059128, 0.004119642544537783, 0.004100704565644264, 0.00409326795488596, 0.004067886155098677, 0.11347323656082153, 0.0040603987872600555, 0.00406149635091424, 0.004047682974487543, 0.0040250192396342754, 0.004017610102891922, 0.004022166132926941, 0.12260601669549942, 0.004017472267150879, 0.004054990131407976, 0.004010631237179041, 0.003977550659328699, 0.003981594927608967, 0.003973585553467274, 0.003954292740672827, 0.1203416958451271, 0.003926792647689581, 0.003928837366402149, 0.003925300668925047, 0.003935905639082193, 0.003908270038664341, 0.0038926072884351015, 0.00394402863457799, 0.003891998901963234, 0.0038592966739088297, 0.003847021609544754, 0.003826458239927888, 0.003805388929322362, 0.003793437499552965, 0.11207785457372665, 0.00377279263921082, 0.0037543531507253647, 0.003763935063034296, 0.0037366803735494614, 0.0037463833577930927, 0.00371936964802444, 0.0037155041936784983, 0.00370406243018806, 0.0036851943004876375, 0.003660113550722599, 0.003649222431704402, 0.0036266418173909187, 0.0036286923568695784, 0.0036389254964888096, 0.0035717571154236794, 0.0035610778722912073, 0.003540854435414076, 0.0035263921599835157, 0.13270103931427002, 0.0034915448632091284, 0.45754680037498474, 0.13283488154411316, 0.0036358290817588568, 0.0037143125664442778, 0.0037320542614907026, 0.12463364750146866, 0.0038352038245648146, 0.0038968995213508606, 0.003946404904127121, 0.003962828312069178, 0.12447760254144669, 0.00405542878434062, 0.0040686666034162045, 0.004091102629899979, 0.004171351902186871, 0.004124546889215708, 0.004154964350163937, 0.004168102517724037, 0.12447040528059006, 0.004159803502261639, 0.11799737811088562, 0.004201607778668404, 0.004254162777215242, 0.004226671066135168, 0.004243495874106884, 0.004264360759407282, 0.11847490817308426, 0.12034125626087189, 0.004296720027923584, 0.004297211766242981, 0.004348477348685265, 0.004329536110162735, 0.004341071005910635, 0.004362752661108971, 0.004318788647651672, 0.004350467585027218, 0.004323356319218874, 0.004315593279898167, 0.11195046454668045, 0.004308396019041538, 0.12420310825109482, 0.004322101827710867, 0.00431907968595624, 0.40790364146232605, 0.11386945843696594, 0.004465762991458178, 0.004542824346572161, 0.004591138102114201, 0.004639646969735622, 0.004728725180029869, 0.004750417545437813, 0.0047568511217832565, 0.11638791114091873, 0.00482722045853734, 0.004855904262512922, 0.11494624614715576, 0.004887853749096394, 0.12594284117221832, 0.004951054695993662, 0.004957925528287888, 0.004984451457858086, 0.0049972087144851685, 0.004999727010726929, 0.0050165969878435135, 0.005043136887252331, 0.00500539829954505, 0.005017174407839775, 0.004997639916837215, 0.004988959990441799, 0.11927014589309692, 0.004966388922184706, 0.004964021034538746, 0.10596705973148346, 0.3979470133781433, 0.0050632837228477, 0.10885106772184372, 0.005170085933059454, 0.005243294406682253, 0.005315248854458332, 0.005346400197595358, 0.005400676280260086, 0.005395448300987482, 0.005409811157733202, 0.005443211179226637, 0.005446804221719503, 0.10631236433982849, 0.10884643346071243, 0.005512142553925514, 0.005488854367285967, 0.11232012510299683, 0.005541671998798847, 0.005563946906477213, 0.005549470894038677, 0.0055780489929020405, 0.00556032033637166, 0.005549021530896425, 0.005570690613240004, 0.0055295671336352825, 0.0055170306004583836, 0.005503423046320677, 0.005522150546312332, 0.005482505541294813, 0.005474555306136608, 0.00542976101860404, 0.11895725131034851, 0.005395818501710892, 0.10769044607877731, 0.005379911512136459, 0.11198075860738754, 0.005389936733990908, 0.00541626987978816, 0.005383317358791828, 0.0053766341879963875, 0.005364442244172096, 0.005362518131732941, 0.00532902404665947, 0.0053343819454312325, 0.005303661338984966, 0.005281582474708557, 0.11535663902759552, 0.11928503960371017, 0.005280006676912308, 0.005259036086499691, 0.005286232102662325, 0.005236609373241663, 0.005227833520621061, 0.11065436899662018, 0.005219800863415003, 0.005217486526817083, 0.00521589070558548, 0.005212415475398302, 0.005199918989092112, 0.005171217489987612, 0.12527434527873993, 0.005148900207132101, 0.005148421507328749, 0.00513451499864459, 0.11311854422092438, 0.00514774676412344, 0.005157027393579483, 0.1158558800816536, 0.005125975701957941, 0.1208481565117836, 0.005126516800373793, 0.11362820118665695, 0.005153137259185314, 0.11252205818891525, 0.005164449103176594, 0.005187596660107374, 0.0051865354180336, 0.005194106139242649, 0.0051970104686915874, 0.005198674276471138, 0.005202956963330507, 0.00521967513486743, 0.005179884843528271, 0.3863583505153656, 0.0052364282310009, 0.10602431744337082, 0.005315273068845272, 0.005346129648387432, 0.005388027522712946, 0.005415896885097027, 0.11499045789241791, 0.00550316646695137, 0.005485576577484608, 0.0055274502374231815, 0.11132673919200897, 0.0055602919310331345, 0.1068219318985939, 0.005634402856230736, 0.00558557128533721, 0.10731389373540878, 0.005633945111185312, 0.005660069175064564, 0.1165650263428688, 0.005668005906045437, 0.10820077359676361, 0.005740009248256683, 0.005721922032535076, 0.0057113016955554485, 0.005718388594686985, 0.005759800784289837, 0.005742441862821579, 0.005743206944316626, 0.005731208715587854, 0.005690923426300287, 0.005668922793120146, 0.005671128164976835, 0.005635301116853952, 0.005651100073009729, 0.10437151044607162, 0.114468052983284, 0.005596625152975321, 0.005585432983934879, 0.005600904580205679, 0.005579044111073017, 0.005586788523942232, 0.11438550055027008, 0.005590629763901234, 0.005547636188566685, 0.005529601126909256, 0.005514036864042282, 0.0055006686598062515, 0.005512765608727932, 0.005464588291943073, 0.005460361484438181, 0.005433863028883934, 0.005438179709017277, 0.0053944056853652, 0.005379538983106613, 0.005359316244721413, 0.005377152934670448, 0.0053210509940981865, 0.005313904024660587, 0.005252561531960964, 0.005249800160527229, 0.11591529846191406, 0.005226080305874348, 0.005179154686629772, 0.0051564378663897514, 0.10732260346412659, 0.11277632415294647, 0.005139804910868406, 0.005135545507073402, 0.12185335904359818, 0.005126630421727896, 0.005131905898451805, 0.00513682933524251, 0.12163367867469788, 0.005129430443048477, 0.005157192703336477, 0.005138359032571316, 0.0051534161902964115, 0.381661593914032, 0.005193416029214859, 0.00521473865956068, 0.005250854417681694, 0.005288636777549982, 0.005326219368726015, 0.11083691567182541, 0.0053438451141119, 0.005385260563343763, 0.005391851533204317, 0.005398700013756752, 0.005401510279625654, 0.005436076316982508, 0.005430364049971104, 0.005417464766651392, 0.005401760805398226, 0.005405883304774761, 0.005386218428611755, 0.005376802757382393, 0.005389227531850338, 0.005347338039427996, 0.005328687373548746, 0.005333229899406433, 0.005312611814588308, 0.11034887284040451, 0.00528916297480464, 0.09793700277805328, 0.0052751535549759865, 0.0052638272754848, 0.005253829061985016, 0.005242576356977224, 0.005247048102319241, 0.005214223638176918, 0.005216623190790415, 0.005186946131289005, 0.1271761655807495, 0.005159648135304451, 0.005177275277674198, 0.0051404437981545925, 0.005128847435116768, 0.0051221237517893314, 0.005101423244923353, 0.0050839269533753395, 0.00506907282397151, 0.005070270039141178, 0.0050447373650968075, 0.11338088661432266, 0.12547007203102112, 0.005044871009886265, 0.00501648522913456, 0.10581812262535095, 0.005038944073021412, 0.005032127723097801, 0.005022316239774227, 0.005019036587327719, 0.0050103142857551575, 0.00500491913408041, 0.004978600423783064, 0.004993141163140535, 0.004967208951711655, 0.12809720635414124, 0.0049514006823301315, 0.004971604328602552, 0.12055028975009918, 0.004944049287587404, 0.004931584931910038, 0.004957025405019522, 0.004932610783725977, 0.004915654193609953, 0.11814074218273163, 0.11498438566923141, 0.11056651175022125, 0.004927561152726412, 0.004966937005519867, 0.00497460039332509, 0.004944284912198782, 0.00498536741361022, 0.10848169773817062, 0.0049477629363536835, 0.004971731454133987, 0.004949899856001139, 0.004987721797078848, 0.004953328054398298, 0.004961260594427586, 0.004941351246088743, 0.10879115760326385, 0.004927081521600485, 0.004942024126648903, 0.12359471619129181, 0.11765820533037186, 0.004950893111526966, 0.004941981751471758, 0.004937432240694761, 0.0049347965978085995, 0.004942289087921381, 0.00495813088491559, 0.00493808276951313, 0.004930705297738314, 0.004920981824398041, 0.0049132308922708035, 0.004929954651743174, 0.004896687343716621, 0.00490555539727211, 0.004863149486482143, 0.004853348713368177, 0.12172806262969971, 0.0048457863740623, 0.10621906816959381, 0.004845700692385435, 0.11835256963968277, 0.00485275499522686, 0.12924721837043762, 0.004857598338276148, 0.00484280101954937, 0.1081974059343338, 0.004869741387665272, 0.10379073768854141, 0.004875553771853447, 0.004895423538982868, 0.004918302409350872, 0.00491421390324831, 0.1113351583480835, 0.0048989783972501755, 0.12851935625076294, 0.11787945032119751, 0.12180501222610474, 0.004950092174112797, 0.004963976796716452, 0.004982254933565855, 0.0049960799515247345, 0.00506387697532773, 0.005046863108873367, 0.12226388603448868, 0.005037369206547737, 0.005031430162489414, 0.0050412812270224094, 0.005023582372814417, 0.12031888961791992, 0.1290075033903122, 0.005050854757428169, 0.10826537013053894, 0.005081367678940296, 0.005077304318547249, 0.005104528274387121, 0.005084356293082237, 0.0050849514082074165, 0.005075929686427116, 0.005110945086926222, 0.005111305043101311, 0.005069757346063852, 0.005060955882072449, 0.005059896036982536, 0.005066401790827513, 0.0050502377562224865, 0.10861952602863312, 0.005019478499889374, 0.005032105837017298, 0.0050012776628136635, 0.005006344988942146, 0.004995834548026323, 0.004984764847904444, 0.004970025736838579, 0.11074961721897125, 0.004951578564941883, 0.004953085910528898, 0.004958899691700935, 0.004958290606737137, 0.004922091029584408, 0.004940403159707785, 0.0049149771220982075, 0.004894338082522154, 0.12279067933559418, 0.00487293628975749, 0.004870189353823662, 0.004863775335252285, 0.11230520904064178, 0.004846298601478338, 0.004867111332714558, 0.004843229427933693, 0.11118272691965103, 0.11244496703147888, 0.004854923579841852, 0.004846842959523201, 0.0048557668924331665, 0.0048413751646876335, 0.0048431167379021645, 0.004843631759285927, 0.004887065850198269, 0.004827961791306734, 0.0048310840502381325, 0.004811946768313646, 0.004803536459803581, 0.004808438941836357, 0.004805435426533222, 0.004793079569935799, 0.004779694136232138, 0.004763317294418812, 0.11177196353673935, 0.004762268625199795, 0.004743142984807491, 0.004745993297547102, 0.004719982855021954, 0.004707171581685543, 0.0046993764117360115, 0.11077629029750824, 0.004682648926973343, 0.004715550225228071, 0.11760525405406952, 0.004674527794122696, 0.004673565737903118, 0.12473086267709732, 0.004686661995947361, 0.004680406302213669, 0.004673381336033344, 0.00468938983976841, 0.004692014306783676, 0.00466471491381526, 0.004677601624280214, 0.004674765281379223, 0.004647658206522465, 0.0046389661729335785, 0.004646061919629574, 0.1194814071059227, 0.004626200068742037, 0.12393560260534286, 0.004618005361407995, 0.004612789023667574, 0.004623047541826963, 0.004631873220205307, 0.0046093356795609, 0.004600709304213524, 0.004595842678099871, 0.004606043454259634, 0.004590855445712805, 0.004605793859809637, 0.004577175248414278, 0.004576187580823898, 0.004551912657916546, 0.0045788041315972805, 0.004554277285933495, 0.004526553209871054, 0.00453102495521307, 0.004517762456089258, 0.004509239457547665, 0.1018855944275856, 0.004475620575249195, 0.004480810835957527, 0.004479072988033295, 0.004463233053684235, 0.004453870002180338, 0.0044563463889062405, 0.004446620121598244, 0.00443758862093091, 0.0044191270135343075, 0.004406591411679983, 0.004393026698380709, 0.004396623466163874, 0.11595550179481506, 0.004382793325930834, 0.004368327558040619, 0.00437915837392211, 0.0043544890359044075, 0.004358870908617973, 0.004359815735369921, 0.0043345363810658455, 0.00432696333155036, 0.004351978190243244, 0.004329727962613106, 0.004320911131799221, 0.004298764280974865, 0.004302745684981346, 0.004296654835343361, 0.004290643148124218, 0.1253395527601242, 0.004266822710633278, 0.00426380755379796, 0.004252163227647543, 0.004246297292411327, 0.004233089275658131, 0.004226428456604481, 0.41719338297843933, 0.004241191782057285, 0.11764213442802429, 0.004308853764086962, 0.004298313055187464, 0.004315297119319439, 0.004341318737715483, 0.004330065101385117, 0.0043672481551766396, 0.11886143684387207, 0.004352089483290911, 0.004364973399788141, 0.0043634395115077496, 0.00438447343185544, 0.0043807728216052055, 0.004375389777123928, 0.0043829395435750484, 0.004384657833725214, 0.00437360443174839, 0.004385925363749266, 0.13181577622890472, 0.004399494733661413, 0.00441093510016799, 0.1176939457654953, 0.004371170420199633, 0.004371798597276211, 0.0043826038017869, 0.004393622279167175, 0.004373786970973015, 0.004373978357762098, 0.0043607973493635654, 0.1230347603559494, 0.004367459565401077, 0.004364599473774433, 0.004381128586828709, 0.004373847506940365, 0.004388496279716492, 0.0043531619012355804, 0.004367232788354158, 0.00434186402708292, 0.11817473918199539, 0.004363213665783405, 0.12366503477096558, 0.10920020192861557, 0.0043488722294569016, 0.12227090448141098, 0.004382005892693996, 0.00436348607763648, 0.0043775965459644794, 0.004371387418359518, 0.004403774160891771, 0.0043895854614675045, 0.004380248486995697, 0.004375058691948652, 0.11741658300161362, 0.004384428728371859, 0.1237320676445961, 0.11449239403009415, 0.004379076883196831, 0.004389955662190914, 0.00441223056986928, 0.004392407834529877, 0.004424654878675938, 0.004391989670693874, 0.0044110422022640705, 0.004387876950204372, 0.004420944023877382, 0.00438702292740345, 0.004390173591673374, 0.00437924824655056, 0.004374521318823099, 0.004371753893792629, 0.004367567133158445, 0.3874535858631134, 0.004378290381282568, 0.0044239843264222145, 0.004405347164720297, 0.004415077157318592, 0.1293877214193344, 0.1079241931438446, 0.004453151486814022, 0.00447196327149868, 0.0044538285583257675, 0.004464955069124699, 0.004465012811124325, 0.004473445937037468, 0.004482295364141464, 0.004486545920372009, 0.12263591587543488, 0.0044864267110824585, 0.11794662475585938, 0.004485205747187138, 0.004484792705625296, 0.004484786652028561, 0.12752889096736908, 0.00449003092944622, 0.004507360048592091, 0.004502484574913979, 0.004498900845646858, 0.004526479635387659, 0.004493904300034046, 0.11465051770210266, 0.004511901177465916, 0.004523774608969688, 0.004530147183686495, 0.004497770220041275, 0.004531008657068014, 0.004510610830038786, 0.10798557847738266, 0.004490867722779512, 0.004499117843806744, 0.004490520339459181, 0.004524097312241793, 0.13261376321315765, 0.10677234083414078, 0.00450609065592289, 0.10226336121559143, 0.004508193116635084, 0.004501041956245899, 0.00451641995459795, 0.004502339754253626, 0.004512025974690914, 0.004512953571975231, 0.004514793865382671, 0.004503639414906502, 0.004497128538787365, 0.0045153675600886345, 0.004529077094048262, 0.0044995700009167194, 0.004515684209764004, 0.004485112149268389, 0.004487856291234493, 0.004490855149924755, 0.10583378374576569, 0.004470428917557001, 0.1224515363574028, 0.10963790118694305, 0.004506681580096483, 0.004487592726945877, 0.004485794808715582, 0.004488833714276552, 0.004486009478569031, 0.004491431172937155, 0.004484275355935097, 0.0044820536859333515, 0.10590320080518723, 0.004475244786590338, 0.004469720646739006, 0.004485399927943945, 0.004474440589547157, 0.0044822171330451965, 0.1061609536409378, 0.004489209037274122, 0.004490164574235678, 0.004469121340662241, 0.41413670778274536, 0.004495969973504543, 0.004488801117986441, 0.004507511388510466, 0.10646378993988037, 0.004506704397499561, 0.004514787811785936, 0.004521285183727741, 0.0045206886716187, 0.0045174891129136086, 0.004525857511907816, 0.004520351067185402, 0.10985661298036575, 0.004521522205322981, 0.004524885211139917, 0.004533493425697088, 0.00454033724963665, 0.004542720504105091, 0.004538249224424362, 0.004560688976198435, 0.004536706954240799, 0.37368881702423096, 0.0045476085506379604, 0.4047949016094208, 0.00458513991907239, 0.004592841491103172, 0.0045908233150839806, 0.004591317847371101, 0.00460543017834425, 0.00460096076130867, 0.10771507024765015, 0.11690347641706467, 0.004619057755917311, 0.004646769724786282, 0.004625377710908651, 0.0046265930868685246, 0.0046470328234136105, 0.10769331455230713, 0.004656820558011532, 0.13065552711486816, 0.004639219958335161, 0.004649010021239519, 0.004657615441828966, 0.004656529985368252, 0.004681000020354986, 0.11350096762180328, 0.004659772384911776, 0.004662505351006985, 0.004690592642873526, 0.004677453078329563, 0.004650471732020378, 0.10811475664377213, 0.00465537142008543, 0.004653476178646088, 0.40470197796821594, 0.004657933488488197, 0.004660591017454863, 0.004711176734417677, 0.10543325543403625, 0.004677090793848038, 0.004683577921241522, 0.004688703455030918, 0.004684233572334051, 0.004703768063336611, 0.00468377023935318, 0.004693396855145693, 0.004682461731135845, 0.004693063907325268, 0.004683615639805794, 0.0046859011054039, 0.11262630671262741, 0.0046834200620651245, 0.004697719123214483, 0.004694696050137281, 0.004702040925621986, 0.00468116020783782, 0.00470729498192668, 0.004699722398072481, 0.125179722905159, 0.00468394486233592, 0.00469314306974411, 0.0047270930372178555, 0.12028658390045166, 0.004704322665929794, 0.004704456310719252, 0.40215742588043213, 0.004700036719441414, 0.00469264155253768, 0.0046951486729085445, 0.004704546649008989, 0.004693780094385147, 0.00470025185495615, 0.004693929105997086, 0.004689314402639866, 0.004713284317404032, 0.004721732344478369, 0.004696309100836515, 0.004689314402639866, 0.12032168358564377, 0.004714417736977339, 0.004691715817898512, 0.10583838820457458, 0.004709957167506218, 0.004704596474766731, 0.004693552386015654, 0.10517672449350357]\n",
            "Val loss 0.021583948311914364\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a25213-d5b0-4f74-c930-834529a6710c"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}