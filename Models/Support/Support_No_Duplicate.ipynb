{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_No_Duplication.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92af442a9a9440e495d41b50e3f81aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19aea715cd944063b616902312237920",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1b4295d9256c43738b09ad6042d8c6aa",
              "IPY_MODEL_9b85714d924642549be63319c492f473"
            ]
          }
        },
        "19aea715cd944063b616902312237920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b4295d9256c43738b09ad6042d8c6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63070593d0d2481f80bf5f339e6de851",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66a1f850c2e74f8eb41b30faf7cb5778"
          }
        },
        "9b85714d924642549be63319c492f473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03dd8056156e4639a453ab0fb08eb12e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:10&lt;00:00, 22.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_366c46189c824f69ab35973136de385e"
          }
        },
        "63070593d0d2481f80bf5f339e6de851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66a1f850c2e74f8eb41b30faf7cb5778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03dd8056156e4639a453ab0fb08eb12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "366c46189c824f69ab35973136de385e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "178411f599a842959ee18008bf34223f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c3fbf8bf9fdb428487e3b4c5f5eeadf0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21c5632d70dd4385b605df5d2d1ad0bd",
              "IPY_MODEL_82a9dcba95364cb8affbc81131fadf9e"
            ]
          }
        },
        "c3fbf8bf9fdb428487e3b4c5f5eeadf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21c5632d70dd4385b605df5d2d1ad0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f37086854ab426696766ae42eeb2512",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57589aca359640aaad7e737d9b8d0aa5"
          }
        },
        "82a9dcba95364cb8affbc81131fadf9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16dc5a345e2a4386bc82cd86a21ea3eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [1:29:48&lt;00:00,  3.38s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2351a095aabc406ba1caa558fb2fc1d6"
          }
        },
        "4f37086854ab426696766ae42eeb2512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57589aca359640aaad7e737d9b8d0aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16dc5a345e2a4386bc82cd86a21ea3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2351a095aabc406ba1caa558fb2fc1d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f14eb553e0024b908fb64b5bbdb2d4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d73672d7f3974e4ea87dd696548c19e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_11fc4a9c417541d0bd235911bc9cee00",
              "IPY_MODEL_b8c8d6975d8f476a80b400a69268b719"
            ]
          }
        },
        "d73672d7f3974e4ea87dd696548c19e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11fc4a9c417541d0bd235911bc9cee00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cc59ad3126a54f3791a122cb2041c25b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a14a865e4ca5478caefd920ffbd3aca1"
          }
        },
        "b8c8d6975d8f476a80b400a69268b719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a61926425c141588d34a4c7476eed19",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [29:25&lt;00:00,  1.11s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_327d14b66d3045d98cdb0463a112e0ed"
          }
        },
        "cc59ad3126a54f3791a122cb2041c25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a14a865e4ca5478caefd920ffbd3aca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a61926425c141588d34a4c7476eed19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "327d14b66d3045d98cdb0463a112e0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0488b86ede0410b8f011796b0526a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_71ec678467b2474bbcc16e6c9f4226ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7bc0d27483d24a6c9766b48281d5dc9d",
              "IPY_MODEL_7c3aae6d777541dda3be58e38d54bcc6"
            ]
          }
        },
        "71ec678467b2474bbcc16e6c9f4226ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7bc0d27483d24a6c9766b48281d5dc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6002f1d6b9448b7b4753c197f152bed",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3fbabd400a5d44389165c4be0c4fdea2"
          }
        },
        "7c3aae6d777541dda3be58e38d54bcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5b67c4246c145dab16a64b3399da32c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [29:27&lt;00:00,  1.11s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f56d7d9895014d7fbb88748ab5a98bcd"
          }
        },
        "a6002f1d6b9448b7b4753c197f152bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3fbabd400a5d44389165c4be0c4fdea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5b67c4246c145dab16a64b3399da32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f56d7d9895014d7fbb88748ab5a98bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "02f559e9-9976-4189-c3da-978f418a24c3"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 32.65 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a7a5edd-3b13-489c-bcc9-467200dbeaf3"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07edbc26-a96f-4514-f5bd-6751b208b82c"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "354c53e4-36ed-4be3-b892-5ccefb21f93d"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3c8f61f2-e1e8-4af3-942b-793b067eb6a7"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "b58b2645-f2c3-47d4-ebcd-15aad634c484"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "68994a93-ec78-4102-d1ba-723c425da2a1"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45c9ec7c-9478-4393-9140-ac2b6e33ccb5"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "07f4f895-e4e8-4ac3-d12a-8edbd5c6060b"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "153c7902-c04a-48bc-c553-d503a6701df1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Sweety Ahuja is one of the best service provid...\n",
              "1    Military jet mechanic. Civilian firefighter. A...\n",
              "2    S.N.L.: Kate McKinnon, Aidy Bryant, and Awkwaf...\n",
              "3    This Google project visualizes the global effe...\n",
              "4    #MeToo: Ajit Thakur ‘Head Of Reliance Content ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7fd5418-6fc8-4aec-b411-16bc395f5b27"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 47802250.45B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e0ac6ea-7483-41bb-f7b5-9747aa0d7182"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1771120.29B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "599afd89-b86e-4a40-efa7-b24cee2994f8"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85b3d879-ead5-42d8-abce-f2045dd24469"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "            random.shuffle(text)\n",
        "            text4 = ' '.join(text)\n",
        "            df4['text'][i]=text4\n",
        "            random.shuffle(text)\n",
        "            text5 = ' '.join(text)\n",
        "            df5['text'][i]=text5\n",
        "        #self.data = self.data.append(df2, ignore_index=True)\n",
        "        #self.data = self.data.append(df3, ignore_index=True)\n",
        "        #self.data = self.data.append(df4, ignore_index=True)\n",
        "        #self.data = self.data.append(df5, ignore_index=True)\n",
        "        #self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48978ab7-e07e-4d8b-a91a-2dc4b5c2ad58"
      },
      "source": [
        "col_name = \"Support\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "92af442a9a9440e495d41b50e3f81aad",
            "19aea715cd944063b616902312237920",
            "1b4295d9256c43738b09ad6042d8c6aa",
            "9b85714d924642549be63319c492f473",
            "63070593d0d2481f80bf5f339e6de851",
            "66a1f850c2e74f8eb41b30faf7cb5778",
            "03dd8056156e4639a453ab0fb08eb12e",
            "366c46189c824f69ab35973136de385e",
            "178411f599a842959ee18008bf34223f",
            "c3fbf8bf9fdb428487e3b4c5f5eeadf0",
            "21c5632d70dd4385b605df5d2d1ad0bd",
            "82a9dcba95364cb8affbc81131fadf9e",
            "4f37086854ab426696766ae42eeb2512",
            "57589aca359640aaad7e737d9b8d0aa5",
            "16dc5a345e2a4386bc82cd86a21ea3eb",
            "2351a095aabc406ba1caa558fb2fc1d6",
            "f14eb553e0024b908fb64b5bbdb2d4f6",
            "d73672d7f3974e4ea87dd696548c19e8",
            "11fc4a9c417541d0bd235911bc9cee00",
            "b8c8d6975d8f476a80b400a69268b719",
            "cc59ad3126a54f3791a122cb2041c25b",
            "a14a865e4ca5478caefd920ffbd3aca1",
            "3a61926425c141588d34a4c7476eed19",
            "327d14b66d3045d98cdb0463a112e0ed",
            "c0488b86ede0410b8f011796b0526a3d",
            "71ec678467b2474bbcc16e6c9f4226ca",
            "7bc0d27483d24a6c9766b48281d5dc9d",
            "7c3aae6d777541dda3be58e38d54bcc6",
            "a6002f1d6b9448b7b4753c197f152bed",
            "3fbabd400a5d44389165c4be0c4fdea2",
            "d5b67c4246c145dab16a64b3399da32c",
            "f56d7d9895014d7fbb88748ab5a98bcd"
          ]
        },
        "outputId": "6543eecd-725e-40e9-cb6a-67b5bcdf0894"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 6382\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92af442a9a9440e495d41b50e3f81aad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "178411f599a842959ee18008bf34223f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1628\n",
            "Train Losses : [0.1274285465478897, 0.7271353006362915, 0.02680499106645584, 3.789123058319092, 0.0005822840612381697, 1.6883264780044556, 0.18120317161083221, 1.5909011363983154, 0.5269749164581299, 0.7636534571647644, 0.1503244787454605, 0.004156442359089851, 0.6371952295303345, 1.694962978363037, 0.7330986857414246, 0.964945912361145, 0.04647958651185036, 0.6910262107849121, 0.16702185571193695, 0.3884396255016327, 0.15249429643154144, 0.11941590160131454, 0.5266179442405701, 0.09047038108110428, 0.15856006741523743, 0.17851224541664124, 0.3208348751068115, 0.14899161458015442, 0.46881380677223206, 0.3383176326751709, 0.1311291605234146, 0.1132497563958168, 0.3099706768989563, 0.21450750529766083, 0.18581649661064148, 0.3307875096797943, 0.5574460625648499, 0.0913693830370903, 0.10920023918151855, 0.017016636207699776, 0.06392326951026917, 0.16418801248073578, 0.5331941246986389, 0.5111889839172363, 0.0867604985833168, 0.11999340355396271, 0.040264010429382324, 0.04024098441004753, 0.1546638458967209, 0.5280131101608276, 0.09838008880615234, 0.04803900048136711, 0.16613517701625824, 0.17088744044303894, 0.4658545255661011, 0.17598681151866913, 0.8129237294197083, 0.05553903803229332, 0.5066879987716675, 0.3261122405529022, 0.1365434229373932, 0.1704201102256775, 0.18971899151802063, 0.17600466310977936, 0.3005914092063904, 0.05095195025205612, 0.19404199719429016, 0.5159631967544556, 0.17085565626621246, 0.14695420861244202, 0.31243425607681274, 0.07296451181173325, 0.08796224743127823, 0.25688961148262024, 0.15880045294761658, 0.10323803126811981, 0.30347293615341187, 0.07835346460342407, 0.11665917187929153, 0.16296494007110596, 0.29697278141975403, 0.07372668385505676, 0.042328573763370514, 0.04131210967898369, 0.11353564262390137, 0.06363112479448318, 0.03491780161857605, 0.03645772486925125, 0.6372270584106445, 0.17738662660121918, 0.14088091254234314, 0.3718283176422119, 0.045774977654218674, 0.0631042942404747, 0.13634541630744934, 0.24418121576309204, 0.21789395809173584, 0.1742628663778305, 0.30363401770591736, 0.058809224516153336, 0.15503071248531342, 0.33704036474227905, 0.20665685832500458, 0.3043130338191986, 0.08495216816663742, 0.2320924699306488, 0.3871288299560547, 0.07604456692934036, 0.08636809885501862, 0.30018749833106995, 0.15864992141723633, 0.29455801844596863, 0.14846588671207428, 0.4485405683517456, 0.12437088787555695, 0.35452377796173096, 0.12778395414352417, 0.2411772906780243, 0.06104655563831329, 0.14210021495819092, 0.2880810797214508, 0.2747117280960083, 0.08802440762519836, 0.05795940384268761, 0.05172518640756607, 0.3036542534828186, 0.08453633636236191, 0.10077658295631409, 0.08482294529676437, 0.16146795451641083, 0.23429499566555023, 0.201398104429245, 0.4399758577346802, 0.06103374809026718, 0.06334740668535233, 0.11229745298624039, 0.08812388777732849, 0.031206542626023293, 0.11265716701745987, 0.4116227626800537, 0.04772505536675453, 0.052859753370285034, 0.055991921573877335, 0.19554221630096436, 0.1125829741358757, 0.17381039261817932, 0.044379230588674545, 0.4570539891719818, 0.4782808721065521, 0.11438819766044617, 0.08926068991422653, 0.6013587117195129, 0.1759832203388214, 0.09036257863044739, 0.45201700925827026, 0.06252818554639816, 0.1487921178340912, 0.13541565835475922, 0.3252377212047577, 0.034926872700452805, 0.09034981578588486, 0.08136037737131119, 0.18391764163970947, 0.2783634662628174, 0.28790366649627686, 0.1946115791797638, 0.0812273770570755, 0.08928574621677399, 0.18667323887348175, 0.08948284387588501, 0.1481187790632248, 0.0769825428724289, 0.1761586219072342, 0.07481781393289566, 0.238589346408844, 0.15249066054821014, 0.16836503148078918, 0.10453833639621735, 0.24651019275188446, 0.2276010811328888, 0.19967970252037048, 0.08212669938802719, 0.06987462937831879, 0.13834834098815918, 0.12506859004497528, 0.1385105699300766, 0.2123207300901413, 0.059761855751276016, 0.15190207958221436, 0.1912308931350708, 0.3662819266319275, 0.15552811324596405, 0.06751520186662674, 0.06277372688055038, 0.15013225376605988, 0.2830979824066162, 0.060453642159700394, 0.25712233781814575, 0.44390860199928284, 0.1175803691148758, 0.06742367893457413, 0.08228399604558945, 0.2345067262649536, 0.07775231450796127, 0.2031160295009613, 0.5139867067337036, 0.25813472270965576, 0.2150602489709854, 0.06962179392576218, 0.1906920075416565, 0.09642647206783295, 0.33343976736068726, 0.14750835299491882, 0.09400303661823273, 0.04922662302851677, 0.24407941102981567, 0.08147798478603363, 0.17151324450969696, 0.13243591785430908, 0.13539178669452667, 0.20431925356388092, 0.20440331101417542, 0.08442830294370651, 0.22871707379817963, 0.15301454067230225, 0.21979717910289764, 0.211014062166214, 0.3476191461086273, 0.12090080231428146, 0.0687069296836853, 0.26112520694732666, 0.1838988959789276, 0.191667839884758, 0.11140484362840652, 0.2093891203403473, 0.12840183079242706, 0.17626330256462097, 0.12775149941444397, 0.1456126868724823, 0.10350874811410904, 0.1317152976989746, 0.18408066034317017, 0.2918625771999359, 0.16675284504890442, 0.06141924113035202, 0.21251869201660156, 0.16256441175937653, 0.19166167080402374, 0.09657341241836548, 0.07724758982658386, 0.10603155195713043, 0.22936119139194489, 0.24414053559303284, 0.1306181252002716, 0.10615149140357971, 0.3395915925502777, 0.14466902613639832, 0.1879241019487381, 0.09615855664014816, 0.0965612605214119, 0.3166735768318176, 0.10825215280056, 0.09139566868543625, 0.15113870799541473, 0.2583838403224945, 0.12489030510187149, 0.08050194382667542, 0.16650652885437012, 0.1929255574941635, 0.1089325100183487, 0.07852373272180557, 0.0596037432551384, 0.057482656091451645, 0.1226164922118187, 0.15648485720157623, 0.1303955465555191, 0.3138684034347534, 0.12761527299880981, 0.18979042768478394, 0.1561693400144577, 0.04309670627117157, 0.11540072411298752, 0.040021464228630066, 0.4222554564476013, 0.12169618159532547, 0.03816801309585571, 0.23269608616828918, 0.24193868041038513, 0.10130173712968826, 0.10988137125968933, 0.045102737843990326, 0.04085594788193703, 0.1702461540699005, 0.10587923228740692, 0.316802978515625, 0.04630196467041969, 0.1582408994436264, 0.19502153992652893, 0.21490181982517242, 0.08831498771905899, 0.058147892355918884, 0.12226834893226624, 0.14506375789642334, 0.15560296177864075, 0.3170902132987976, 0.15223759412765503, 0.25716710090637207, 0.0905529260635376, 0.11847173422574997, 0.19078278541564941, 0.04672589153051376, 0.26736584305763245, 0.08539828658103943, 0.051927223801612854, 0.14408111572265625, 0.09453573077917099, 0.16415657103061676, 0.0946771427989006, 0.3726033568382263, 0.04863584414124489, 0.25471991300582886, 0.08741719275712967, 0.3981392979621887, 0.05092988163232803, 0.1369055062532425, 0.080306775867939, 0.08697299659252167, 0.07056267559528351, 0.04307885095477104, 0.03912224620580673, 0.038760844618082047, 0.20590195059776306, 0.1866627335548401, 0.15130072832107544, 0.36615505814552307, 0.03600182756781578, 0.11662723124027252, 0.11917898803949356, 0.17979899048805237, 0.3657526671886444, 0.08837901055812836, 0.28259533643722534, 0.2739579677581787, 0.14117379486560822, 0.14817555248737335, 0.09504204988479614, 0.11276642233133316, 0.10640198737382889, 0.05600868910551071, 0.15084747970104218, 0.44492241740226746, 0.10799556970596313, 0.06265181303024292, 0.06568113714456558, 0.06830966472625732, 0.20467114448547363, 0.1768324226140976, 0.06766112148761749, 0.0702030211687088, 0.06618346273899078, 0.06323865801095963, 0.06087014824151993, 0.11986511945724487, 0.14091306924819946, 0.08859094232320786, 0.2530820667743683, 0.047774337232112885, 0.3945075273513794, 0.16191887855529785, 0.12132520973682404, 0.21534299850463867, 0.09543803334236145, 0.1867385357618332, 0.22708143293857574, 0.047824714332818985, 0.050432998687028885, 0.3174389898777008, 0.1310928463935852, 0.052488360553979874, 0.08909378200769424, 0.09194579720497131, 0.2618776261806488, 0.054909802973270416, 0.15687541663646698, 0.22349491715431213, 0.08073720335960388, 0.3082347512245178, 0.1358705759048462, 0.05978674441576004, 0.12257040292024612, 0.06340087205171585, 0.2401280254125595, 0.06844881176948547, 0.3859880864620209, 0.1648334562778473, 0.07007307559251785, 0.12313595414161682, 0.12165762484073639, 0.0641932487487793, 0.10740184783935547, 0.05892070755362511, 0.08499563485383987, 0.060802068561315536, 0.189422607421875, 0.2626326382160187, 0.17355704307556152, 0.2764902412891388, 0.11159737408161163, 0.05929907038807869, 0.19893522560596466, 0.2860260009765625, 0.05251744017004967, 0.09377109259366989, 0.12631189823150635, 0.15209642052650452, 0.12701468169689178, 0.18591852486133575, 0.2099335938692093, 0.05897268280386925, 0.05857020989060402, 0.1379363238811493, 0.2685876488685608, 0.12230151146650314, 0.27339497208595276, 0.13675810396671295, 0.0685405507683754, 0.2678802013397217, 0.29069027304649353, 0.0702812448143959, 0.16092635691165924, 0.2146489918231964, 0.29886043071746826, 0.13162685930728912, 0.19159165024757385, 0.18689419329166412, 0.1823783665895462, 0.16884292662143707, 0.10198049247264862, 0.18470652401447296, 0.12936675548553467, 0.157073974609375, 0.06367991119623184, 0.05599375069141388, 0.2632747292518616, 0.39980241656303406, 0.1814211905002594, 0.09797213971614838, 0.056158799678087234, 0.4369465410709381, 0.3095051348209381, 0.17022694647312164, 0.06532251089811325, 0.20530417561531067, 0.3292420208454132, 0.07267838716506958, 0.07632075995206833, 0.07674278318881989, 0.07882048934698105, 0.26447048783302307, 0.07659485191106796, 0.2716927230358124, 0.14098723232746124, 0.13268424570560455, 0.2325177788734436, 0.11593782156705856, 0.19561752676963806, 0.0788574367761612, 0.14543673396110535, 0.30683931708335876, 0.203088641166687, 0.11574916541576385, 0.34936854243278503, 0.08420198410749435, 0.2660597562789917, 0.24518731236457825, 0.1566711962223053, 0.11659252643585205, 0.0948820561170578, 0.09489168226718903, 0.13519753515720367, 0.0966486781835556, 0.2045520842075348, 0.14536543190479279, 0.09623438119888306, 0.14415115118026733, 0.08664707839488983, 0.17219838500022888, 0.18736806511878967, 0.10216199606657028, 0.11414624750614166, 0.24830955266952515, 0.08390657603740692, 0.07696709781885147, 0.2194967418909073, 0.07380086928606033, 0.2560948133468628, 0.3037932217121124, 0.08034113794565201, 0.15523725748062134, 0.09044721722602844, 0.3566742241382599, 0.07072178274393082, 0.0766318067908287, 0.0745263397693634, 0.21838322281837463, 0.07067173719406128, 0.19360452890396118, 0.26516175270080566, 0.07054594904184341, 0.0960906594991684, 0.06779052317142487, 0.06609418243169785, 0.06291627138853073, 0.23401348292827606, 0.06119878217577934, 0.05903680622577667, 0.3417893350124359, 0.19035857915878296, 0.11964832246303558, 0.21962812542915344, 0.05467906594276428, 0.3691798746585846, 0.08113402873277664, 0.05511917918920517, 0.12408079206943512, 0.1014544665813446, 0.15273483097553253, 0.10047367960214615, 0.12598441541194916, 0.11953495442867279, 0.1747002899646759, 0.16640155017375946, 0.11691500246524811, 0.11270863562822342, 0.04815833643078804, 0.17443101108074188, 0.11112858355045319, 0.17761865258216858, 0.04299696907401085, 0.0749383494257927, 0.04132836312055588, 0.4176345467567444, 0.13280272483825684, 0.33589494228363037, 0.08619341254234314, 0.13940170407295227, 0.19358839094638824, 0.07225237041711807, 0.36059045791625977, 0.1628301739692688, 0.12231171131134033, 0.20229336619377136, 0.19841992855072021, 0.11629839241504669, 0.0814954936504364, 0.05720928683876991, 0.09321275353431702, 0.3812903165817261, 0.09998534619808197, 0.06467358022928238, 0.0951698049902916, 0.326564759016037, 0.06234782189130783, 0.06468865275382996, 0.1291271448135376, 0.24311114847660065, 0.26195937395095825, 0.16359050571918488, 0.23939025402069092, 0.19121035933494568, 0.13251376152038574, 0.19979877769947052, 0.18515287339687347, 0.22679580748081207, 0.19151675701141357, 0.17315876483917236, 0.07800232619047165, 0.08004461973905563, 0.10647311806678772, 0.1362164467573166, 0.17264021933078766, 0.16922099888324738, 0.26450103521347046, 0.15562282502651215, 0.23780003190040588, 0.14941777288913727, 0.13240323960781097, 0.07287478446960449, 0.0830678716301918, 0.11807093024253845, 0.2158089429140091, 0.17840345203876495, 0.12574420869350433, 0.0737622007727623, 0.12535230815410614, 0.06968885660171509, 0.10010793060064316, 0.10579002648591995, 0.16860489547252655, 0.06420022249221802, 0.06393187493085861, 0.21007712185382843, 0.18063169717788696, 0.1629297435283661, 0.24212241172790527, 0.05683877691626549, 0.07619910687208176, 0.10332070291042328, 0.1824422925710678, 0.12652328610420227, 0.05161774903535843, 0.15034636855125427, 0.15743480622768402, 0.04911324754357338, 0.16875028610229492, 0.15006963908672333, 0.08715017139911652, 0.29406145215034485, 0.10913554579019547, 0.17791977524757385, 0.04979714751243591, 0.19517600536346436, 0.19770990312099457, 0.14683808386325836, 0.0506368950009346, 0.22182950377464294, 0.2857133448123932, 0.2544224262237549, 0.14408357441425323, 0.1778244823217392, 0.12353180348873138, 0.06227125599980354, 0.13765834271907806, 0.06404144316911697, 0.3839184045791626, 0.33775386214256287, 0.0653855949640274, 0.06562158465385437, 0.16850891709327698, 0.1947963684797287, 0.1578635573387146, 0.16999715566635132, 0.10480351001024246, 0.07028631120920181, 0.25553715229034424, 0.07116624712944031, 0.13063928484916687, 0.13160689175128937, 0.13573984801769257, 0.12715281546115875, 0.21330411732196808, 0.10175245255231857, 0.28089767694473267, 0.22271397709846497, 0.19735349714756012, 0.11871038377285004, 0.09563086926937103, 0.15263858437538147, 0.2112169712781906, 0.0995810478925705, 0.11204631626605988, 0.18823891878128052, 0.07232726365327835, 0.07112079858779907, 0.06665833294391632, 0.10675142705440521, 0.06642285734415054, 0.12396953999996185, 0.17266757786273956, 0.12076602876186371, 0.1372101604938507, 0.05331772565841675, 0.05190938711166382, 0.32733815908432007, 0.049782734364271164, 0.356890469789505, 0.12441661208868027, 0.3321312963962555, 0.05026967078447342, 0.22758284211158752, 0.174089714884758, 0.05330030992627144, 0.11101025342941284, 0.2536502182483673, 0.3264302909374237, 0.2588241696357727, 0.15930311381816864, 0.0640929564833641, 0.27729520201683044, 0.09329230338335037, 0.14465390145778656, 0.10305473208427429, 0.07127682864665985, 0.15295666456222534, 0.1901872158050537, 0.22822609543800354, 0.10350542515516281, 0.25096458196640015, 0.1332813948392868, 0.20797976851463318, 0.1915835440158844, 0.10467753559350967, 0.11826147884130478, 0.13933700323104858, 0.20720995962619781, 0.23464849591255188, 0.12705165147781372, 0.25429707765579224, 0.22343257069587708, 0.14019013941287994, 0.18048767745494843, 0.08337051421403885, 0.1385774165391922, 0.08256319910287857, 0.08027295768260956, 0.10718300193548203, 0.12675607204437256, 0.322916716337204, 0.14062848687171936, 0.09681658446788788, 0.23914143443107605, 0.15972761809825897, 0.07066573202610016, 0.08211010694503784, 0.38942310214042664, 0.19372723996639252, 0.07138240337371826, 0.1328200250864029, 0.26439228653907776, 0.17454683780670166, 0.14116789400577545, 0.2622905373573303, 0.0991099551320076, 0.23510926961898804, 0.21855595707893372, 0.15195322036743164, 0.07479873299598694, 0.07500376552343369, 0.07513274252414703, 0.23726701736450195, 0.1884557157754898, 0.1094202771782875, 0.07074848562479019, 0.10858823359012604, 0.07000498473644257, 0.11934877932071686, 0.13797052204608917, 0.1526295393705368, 0.060705289244651794, 0.05893167853355408, 0.11491032689809799, 0.11307605355978012, 0.10003051906824112, 0.27728840708732605, 0.04817580059170723, 0.22277885675430298, 0.12082228064537048, 0.3853955566883087, 0.22283683717250824, 0.13652777671813965, 0.12611420452594757, 0.20979690551757812, 0.3066958785057068, 0.04855402559041977, 0.09436345100402832, 0.08658160269260406, 0.2910664975643158, 0.10940242558717728, 0.08834214508533478, 0.05333944037556648, 0.2451581358909607, 0.21571969985961914, 0.19560594856739044, 0.05439988523721695, 0.0555206760764122, 0.09912969172000885, 0.09574900567531586, 0.11493951082229614, 0.1053372323513031, 0.05305808410048485, 0.28149646520614624, 0.3929979205131531, 0.05371076986193657, 0.19812385737895966, 0.056088391691446304, 0.196136936545372, 0.13005487620830536, 0.12475745379924774, 0.09929560869932175, 0.05782925337553024, 0.3587338924407959, 0.058673713356256485, 0.2602936625480652, 0.0606534518301487, 0.06046682223677635, 0.06239786744117737, 0.12978854775428772, 0.15549379587173462, 0.17396385967731476, 0.21978186070919037, 0.13023775815963745, 0.34291723370552063, 0.09476181119680405, 0.15747849643230438, 0.1643839329481125, 0.061998892575502396, 0.1258569359779358, 0.10511644929647446, 0.06154835224151611, 0.12891769409179688, 0.3114863634109497, 0.20093083381652832, 0.060005854815244675, 0.09770689904689789, 0.16592782735824585, 0.1865166872739792, 0.0604575015604496, 0.06056567281484604, 0.12343553453683853, 0.21875876188278198, 0.2217479646205902, 0.058500949293375015, 0.18920299410820007, 0.05964329093694687, 0.057618189603090286, 0.05836351960897446, 0.0866338312625885, 0.1115264892578125, 0.05331716686487198, 0.10297342389822006, 0.09561657905578613, 0.22016365826129913, 0.20664292573928833, 0.16561515629291534, 0.09705071151256561, 0.17170771956443787, 0.11874492466449738, 0.04483027756214142, 0.10452431440353394, 0.04425099864602089, 0.1755862683057785, 0.042217496782541275, 0.10802406817674637, 0.0405978225171566, 0.24830538034439087, 0.41808995604515076, 0.04014244303107262, 0.3619333803653717, 0.040837258100509644, 0.27531328797340393, 0.34940478205680847, 0.2708912193775177, 0.10442421585321426, 0.11557929217815399, 0.22322653234004974, 0.18659336864948273, 0.142108753323555, 0.1516842097043991, 0.05908311903476715, 0.20363180339336395, 0.12254490703344345, 0.06230975687503815, 0.12208617478609085, 0.12300491333007812, 0.13925334811210632, 0.2566708028316498, 0.12057877331972122, 0.1644083708524704, 0.21787230670452118, 0.3279534578323364, 0.06350979954004288, 0.2162371724843979, 0.13099487125873566, 0.11700967699289322, 0.06653931736946106, 0.11411967873573303, 0.06626377254724503, 0.1335011124610901, 0.06343663483858109, 0.3393740653991699, 0.2947615385055542, 0.11432355642318726, 0.12073200941085815, 0.13341428339481354, 0.13939930498600006, 0.2532180845737457, 0.15402135252952576, 0.30205416679382324, 0.12198211997747421, 0.1572221964597702, 0.18942046165466309, 0.06680939346551895, 0.11266645789146423, 0.15808850526809692, 0.1054019182920456, 0.06711295247077942, 0.26499757170677185, 0.1006818413734436, 0.3850896954536438, 0.19550275802612305, 0.07304883003234863, 0.0731653943657875, 0.07413884252309799, 0.2099088877439499, 0.19298118352890015, 0.10017137974500656, 0.10359300673007965, 0.11054977774620056, 0.19723938405513763, 0.07547613233327866, 0.1736518293619156, 0.07483826577663422, 0.1416182518005371, 0.13708597421646118, 0.1136430874466896, 0.13206204771995544, 0.14094260334968567, 0.06812912970781326, 0.06481614708900452, 0.1826031506061554, 0.2237153798341751, 0.1742132306098938, 0.15243269503116608, 0.05811866372823715, 0.1308099627494812, 0.10534472018480301, 0.11756724119186401, 0.052874140441417694, 0.05273094400763512, 0.20341157913208008, 0.20624612271785736, 0.10901690274477005, 0.04733516648411751, 0.14629431068897247, 0.1897917240858078, 0.14080287516117096, 0.19353683292865753, 0.11443385481834412, 0.12199760228395462, 0.2008073329925537, 0.08387116342782974, 0.1268189400434494, 0.5135424137115479, 0.21321874856948853, 0.13270866870880127, 0.048535775393247604, 0.0788562223315239, 0.05024951323866844, 0.3447251617908478, 0.05315576121211052, 0.21276827156543732, 0.17279060184955597, 0.12360318750143051, 0.20596008002758026, 0.17881463468074799, 0.059916842728853226, 0.12051864713430405, 0.20151787996292114, 0.06174565851688385, 0.06330252438783646, 0.1348249912261963, 0.3295334279537201, 0.08874059468507767, 0.20924556255340576, 0.12599773705005646, 0.06485289335250854, 0.06505072861909866, 0.19936494529247284, 0.11848904937505722, 0.06290532648563385, 0.1675538569688797, 0.22472533583641052, 0.18343020975589752, 0.060321081429719925, 0.25276610255241394, 0.23680712282657623, 0.23874755203723907, 0.09680595993995667, 0.1267411857843399, 0.13450917601585388, 0.10562179237604141, 0.20962432026863098, 0.13893485069274902, 0.14668019115924835, 0.1688797026872635, 0.06267216801643372, 0.22903400659561157, 0.22101745009422302, 0.22018156945705414, 0.14330828189849854, 0.4063699245452881, 0.28428488969802856, 0.17374931275844574, 0.18384288251399994, 0.2751794159412384, 0.17837269604206085, 0.1357334405183792, 0.11980482190847397, 0.13379333913326263, 0.08661841601133347, 0.10819794982671738, 0.1544368714094162, 0.13114316761493683, 0.22982656955718994, 0.1731475293636322, 0.16872520744800568, 0.20670294761657715, 0.1478710174560547, 0.18674364686012268, 0.09194497764110565, 0.18813514709472656, 0.09130017459392548, 0.2009318619966507, 0.19926689565181732, 0.18720997869968414, 0.15239430963993073, 0.13676506280899048, 0.18028487265110016, 0.21007615327835083, 0.135370135307312, 0.2135404497385025, 0.18116962909698486, 0.14232711493968964, 0.1821257472038269, 0.18435585498809814, 0.22107325494289398, 0.1264149695634842, 0.09205605834722519, 0.11766650527715683, 0.24591657519340515, 0.23965372145175934, 0.12875764071941376, 0.21130713820457458, 0.1385868936777115, 0.23733961582183838, 0.14169956743717194, 0.18282952904701233, 0.13294664025306702, 0.20662187039852142, 0.2237476259469986, 0.09984328597784042, 0.18626564741134644, 0.2231867015361786, 0.1407242864370346, 0.10105187445878983, 0.1642809510231018, 0.09895884245634079, 0.15339650213718414, 0.183226078748703, 0.14384067058563232, 0.1469399482011795, 0.13873180747032166, 0.18940570950508118, 0.14764881134033203, 0.1909491866827011, 0.12569105625152588, 0.24405883252620697, 0.11786873638629913, 0.13707099854946136, 0.1808127462863922, 0.14044973254203796, 0.12849512696266174, 0.12715953588485718, 0.13341109454631805, 0.254486620426178, 0.12788867950439453, 0.12214262038469315, 0.07909753173589706, 0.0779358521103859, 0.17665734887123108, 0.13369469344615936, 0.07420309633016586, 0.07202504575252533, 0.0693378672003746, 0.06672599166631699, 0.12095052003860474, 0.061684153974056244, 0.1908567100763321, 0.11170311272144318, 0.1823849081993103, 0.18334664404392242, 0.11466508358716965, 0.2039925456047058, 0.05157436430454254, 0.19426876306533813, 0.226441890001297, 0.05052601173520088, 0.21946746110916138, 0.09751777350902557, 0.11529746651649475, 0.1898624300956726, 0.04908636957406998, 0.29806917905807495, 0.11453144252300262, 0.050054505467414856, 0.22814884781837463, 0.13615058362483978, 0.1760295182466507, 0.05081166699528694, 0.12494450807571411, 0.05115145444869995, 0.05025142803788185, 0.10570240765810013, 0.11790596693754196, 0.4761209487915039, 0.15905795991420746, 0.0509045273065567, 0.050251033157110214, 0.25029826164245605, 0.050977662205696106, 0.2861754298210144, 0.05161404237151146, 0.0938938781619072, 0.13794535398483276, 0.21790502965450287, 0.21038047969341278, 0.21048155426979065, 0.2942696511745453, 0.0560547299683094, 0.20038525760173798, 0.32081377506256104, 0.13355891406536102, 0.39918380975723267, 0.0660279244184494, 0.06915625184774399, 0.14024803042411804, 0.12956194579601288, 0.07424314320087433, 0.32348760962486267, 0.11933840811252594, 0.1873175948858261, 0.07915506511926651, 0.2006223350763321, 0.08079245686531067, 0.12656672298908234, 0.2464752495288849, 0.20561791956424713, 0.2522803843021393, 0.1974390298128128, 0.17152343690395355, 0.11693990230560303, 0.08849555253982544, 0.24431174993515015, 0.14537112414836884, 0.09384747594594955, 0.13582773506641388, 0.23193764686584473, 0.2469814568758011, 0.16013075411319733, 0.14358726143836975, 0.12312538921833038, 0.1673326939344406, 0.2955741286277771, 0.15320786833763123, 0.13319627940654755, 0.1888735294342041, 0.21213671565055847, 0.12367035448551178, 0.10426755994558334, 0.10451184213161469, 0.14043128490447998, 0.15555298328399658, 0.12094205617904663, 0.2099682241678238, 0.21235398948192596, 0.2002808004617691, 0.10357138514518738, 0.1521310657262802, 0.2364974468946457, 0.15150117874145508, 0.15600734949111938, 0.14120949804782867, 0.20370200276374817, 0.10367083549499512, 0.10275647044181824, 0.10100996494293213, 0.1460569053888321, 0.21398967504501343, 0.1239500641822815, 0.14725901186466217, 0.12387590855360031, 0.08733290433883667, 0.12822100520133972, 0.1841103732585907, 0.08019629865884781, 0.2671307325363159, 0.14698927104473114, 0.07605434954166412, 0.21192236244678497, 0.13083404302597046, 0.07164548337459564, 0.06979648768901825, 0.06748839467763901, 0.12293549627065659, 0.12569689750671387, 0.20001910626888275, 0.11567863821983337, 0.1177654042840004, 0.05735745653510094, 0.0553559735417366, 0.053768567740917206, 0.20161114633083344, 0.11665621399879456, 0.0494568832218647, 0.047616612166166306, 0.1348721832036972, 0.04455452039837837, 0.2383311241865158, 0.1991487592458725, 0.10266130417585373, 0.23115849494934082, 0.1870950162410736, 0.12586671113967896, 0.11820847541093826, 0.10477637499570847, 0.04025625064969063, 0.1193099319934845, 0.10433855652809143, 0.03855980560183525, 0.2227172553539276, 0.10999000072479248, 0.12769146263599396, 0.0369870625436306, 0.036557842046022415, 0.12289309501647949, 0.035104040056467056, 0.11169250309467316, 0.03396112099289894, 0.09407926350831985, 0.03246227651834488, 0.24109748005867004, 0.12589599192142487, 0.030772807076573372, 0.12446223199367523, 0.22676198184490204, 0.10011889040470123, 0.11602206528186798, 0.11907504498958588, 0.028606321662664413, 0.028329113498330116, 0.24379827082157135, 0.23338846862316132, 0.0280285831540823, 0.24984601140022278, 0.11292187869548798, 0.10216795653104782, 0.10280412435531616, 0.2596855163574219, 0.029877163469791412, 0.09914443641901016, 0.10355473309755325, 0.22429032623767853, 0.030572175979614258, 0.40197324752807617, 0.08431953936815262, 0.03255000710487366, 0.2444153130054474, 0.370572954416275, 0.3452391028404236, 0.09744929522275925, 0.19944685697555542, 0.21061666309833527, 0.044674988836050034, 0.10299421101808548, 0.12564542889595032, 0.09327462315559387, 0.11080827564001083, 0.1224302351474762, 0.05348658934235573, 0.11711135506629944, 0.05297664552927017, 0.11167991161346436, 0.30656930804252625, 0.1289210170507431, 0.12356916069984436, 0.1222783699631691, 0.2913658916950226, 0.054867666214704514, 0.43308210372924805, 0.09800005704164505, 0.19194698333740234, 0.1289341002702713, 0.18803703784942627, 0.10250329971313477, 0.13132068514823914, 0.10551715642213821, 0.1125069409608841, 0.29608437418937683, 0.18458718061447144, 0.23744966089725494, 0.20837201178073883, 0.13693730533123016, 0.15938761830329895, 0.20237606763839722, 0.07756398618221283, 0.14968650043010712, 0.11125913262367249, 0.07907576113939285, 0.18473581969738007, 0.09976537525653839, 0.15772618353366852, 0.07767430692911148, 0.14262400567531586, 0.13013295829296112, 0.16682069003582, 0.11546304076910019, 0.14381203055381775, 0.12886890769004822, 0.11694056540727615, 0.27802151441574097, 0.06985475867986679, 0.16179907321929932, 0.10883869975805283, 0.14288677275180817, 0.2332576960325241, 0.06703328341245651, 0.14177720248699188, 0.06597127765417099, 0.15074121952056885, 0.18223918974399567, 0.1170300617814064, 0.148225337266922, 0.09331131726503372, 0.13159526884555817, 0.06036590412259102, 0.2443927526473999, 0.31507954001426697, 0.1984197199344635, 0.1389734447002411, 0.08017181605100632, 0.059916265308856964, 0.09157048165798187, 0.2230904996395111, 0.19810886681079865, 0.09781432151794434, 0.1614387482404709, 0.2017367035150528, 0.4128657281398773, 0.12185565382242203, 0.13688956201076508, 0.06479137390851974, 0.06530435383319855, 0.06668222695589066, 0.15247340500354767, 0.1285865604877472, 0.22015692293643951, 0.27243471145629883, 0.23090822994709015, 0.10687968134880066, 0.16951154172420502, 0.10897769778966904, 0.06797102093696594, 0.2838214635848999, 0.24409976601600647, 0.21412956714630127, 0.29194170236587524, 0.1977481245994568, 0.11403336375951767, 0.12691590189933777, 0.13443587720394135, 0.11572732776403427, 0.10925590246915817, 0.1324857920408249, 0.1481514871120453, 0.20742323994636536, 0.08435851335525513, 0.19185270369052887, 0.25019291043281555, 0.08839660882949829, 0.12314793467521667, 0.08720878511667252, 0.2196086049079895, 0.20781345665454865, 0.1370910257101059, 0.0875888466835022, 0.2337166965007782, 0.13553109765052795, 0.12589843571186066, 0.11627470701932907, 0.1556014120578766, 0.08458834886550903, 0.14571425318717957, 0.1280142366886139, 0.13052290678024292, 0.2573428750038147, 0.0773947536945343, 0.077165387570858, 0.07523511350154877, 0.13753893971443176, 0.15487021207809448, 0.13482336699962616, 0.06878791749477386, 0.20134691894054413, 0.12219733744859695, 0.15385684370994568, 0.06504885107278824, 0.1350587010383606, 0.09323786944150925, 0.06012360379099846, 0.12437130510807037, 0.18448269367218018, 0.11333858221769333, 0.24322378635406494, 0.23051013052463531, 0.13687720894813538, 0.051732756197452545, 0.20407330989837646, 0.22771164774894714, 0.13268132507801056, 0.09477325528860092, 0.050265271216630936, 0.2108253687620163, 0.2461281418800354, 0.12044825404882431, 0.09767552465200424, 0.2934235632419586, 0.2114771455526352, 0.11614455282688141, 0.05129591003060341, 0.20909275114536285, 0.12447023391723633, 0.08615776896476746, 0.11519695073366165, 0.09814641624689102, 0.07472913712263107, 0.17230893671512604, 0.17214885354042053, 0.1926983743906021, 0.052841074764728546, 0.16993850469589233, 0.1283321976661682, 0.3247852921485901, 0.09379694610834122, 0.11343668401241302, 0.14191332459449768, 0.2639167606830597, 0.12927491962909698, 0.10895389318466187, 0.057092878967523575, 0.2091538906097412, 0.21212470531463623, 0.22645312547683716, 0.15942813456058502, 0.11835060268640518, 0.12953434884548187, 0.23087462782859802, 0.11508369445800781, 0.11999442428350449, 0.13509023189544678, 0.106570303440094, 0.0602770633995533, 0.15694016218185425, 0.13285939395427704, 0.14732331037521362, 0.32227185368537903, 0.13239896297454834, 0.12448538094758987, 0.10957731306552887, 0.10746662318706512, 0.20397567749023438, 0.10615388303995132, 0.09497911483049393, 0.19700120389461517, 0.18136996030807495, 0.22265532612800598, 0.27650389075279236, 0.12691941857337952, 0.1252739131450653, 0.33088502287864685, 0.28389236330986023, 0.22242559492588043, 0.15070226788520813, 0.06645219773054123, 0.17378781735897064, 0.14883506298065186, 0.10647755861282349, 0.19231553375720978, 0.22320976853370667, 0.0725206509232521, 0.07525525987148285, 0.20934490859508514, 0.10916900634765625, 0.13698868453502655, 0.20362919569015503, 0.12775221467018127, 0.20060913264751434, 0.11865828931331635, 0.11978242546319962, 0.18520745635032654, 0.14776429533958435, 0.12512661516666412, 0.07151151448488235, 0.16957524418830872, 0.07219359278678894, 0.19892540574073792, 0.136323481798172, 0.2887759506702423, 0.06826681643724442, 0.1992851346731186, 0.17687778174877167, 0.06866396218538284, 0.06856335699558258, 0.12269079685211182, 0.06714022904634476, 0.14121273159980774, 0.17783188819885254, 0.26215213537216187, 0.19831690192222595, 0.06448095291852951, 0.12728901207447052, 0.06409763544797897, 0.12115874886512756, 0.06265179067850113, 0.12941060960292816, 0.060837067663669586, 0.2735482454299927, 0.21285584568977356, 0.3045833706855774, 0.12038923799991608, 0.27723386883735657, 0.06147581711411476, 0.21370550990104675, 0.09882403165102005, 0.15851806104183197, 0.17098814249038696, 0.196221724152565, 0.18481341004371643, 0.15591596066951752, 0.06912557035684586, 0.1795087605714798, 0.11287330836057663, 0.1258881390094757, 0.2354133576154709, 0.0728909894824028, 0.11501237750053406, 0.07461873441934586, 0.17062890529632568, 0.13213680684566498, 0.12421423941850662, 0.07277785986661911, 0.20796607434749603, 0.21093347668647766, 0.21147222816944122, 0.07409290224313736, 0.11433711647987366, 0.07524779438972473, 0.13821130990982056, 0.07500948011875153, 0.14198781549930573, 0.3668704032897949, 0.10091476142406464, 0.10554509609937668, 0.07493122667074203, 0.07440558820962906, 0.13934043049812317, 0.07441847026348114, 0.20651277899742126, 0.08786242455244064, 0.11617444455623627, 0.07075677812099457, 0.07004333287477493, 0.10818853229284286, 0.06768178194761276, 0.19292236864566803, 0.2711593806743622, 0.18783323466777802, 0.3217131793498993, 0.23778748512268066, 0.18778173625469208, 0.12783755362033844, 0.17630504071712494, 0.2734914720058441, 0.16893966495990753, 0.12240355461835861, 0.07104828208684921, 0.07246556878089905, 0.09136167913675308, 0.06904283165931702, 0.0706813856959343, 0.09370682388544083, 0.14584891498088837, 0.1563505381345749, 0.15863576531410217, 0.25715959072113037, 0.10695277154445648, 0.15935160219669342, 0.060425687581300735, 0.0950097069144249, 0.11205436289310455, 0.24301575124263763, 0.2601722776889801, 0.05846362188458443, 0.22834746539592743, 0.10455836355686188, 0.15789756178855896]\n",
            "Val loss 0.14314396341417665\n",
            "Val auc roc 0.4949711433242669\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f14eb553e0024b908fb64b5bbdb2d4f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1477\n",
            "Train Losses : [0.0829041451215744, 0.1256793886423111, 0.08659681677818298, 0.2137039601802826, 0.10893946141004562, 0.1906449794769287, 0.24070867896080017, 0.13370868563652039, 0.17735648155212402, 0.06049630790948868, 0.06163778156042099, 0.1714341789484024, 0.20480157434940338, 0.0861898809671402, 0.28596147894859314, 0.11779619008302689, 0.20191609859466553, 0.1328509896993637, 0.11756891012191772, 0.17152716219425201, 0.1272570788860321, 0.09568313509225845, 0.18683233857154846, 0.14233927428722382, 0.2372753769159317, 0.14196324348449707, 0.10397826880216599, 0.11030848324298859, 0.06582167744636536, 0.12012329697608948, 0.32230013608932495, 0.06639008224010468, 0.06654787808656693, 0.0645308643579483, 0.0628138929605484, 0.17445717751979828, 0.16915374994277954, 0.30826178193092346, 0.10667583346366882, 0.09727221727371216, 0.06025199219584465, 0.10094151645898819, 0.12789170444011688, 0.20675912499427795, 0.11731931567192078, 0.0573071651160717, 0.16333846747875214, 0.20399563014507294, 0.35262957215309143, 0.11536160111427307, 0.18235401809215546, 0.10253824293613434, 0.055396758019924164, 0.23407712578773499, 0.2809804081916809, 0.17565372586250305, 0.16562779247760773, 0.11725907772779465, 0.058137085288763046, 0.09605301916599274, 0.17762239277362823, 0.09707977622747421, 0.05647062137722969, 0.24445541203022003, 0.12150216847658157, 0.05658040568232536, 0.1309245228767395, 0.19801001250743866, 0.08604589849710464, 0.05487120524048805, 0.14186115562915802, 0.05365727096796036, 0.05213117226958275, 0.22064606845378876, 0.19735760986804962, 0.17102620005607605, 0.2923111617565155, 0.20731355249881744, 0.16756680607795715, 0.08713176846504211, 0.12865765392780304, 0.22290320694446564, 0.11620502173900604, 0.18699148297309875, 0.24299867451190948, 0.13478244841098785, 0.13851840794086456, 0.27814990282058716, 0.18734963238239288, 0.12860670685768127, 0.1475716084241867, 0.06584654748439789, 0.10323752462863922, 0.06708361953496933, 0.067253977060318, 0.23985712230205536, 0.1340486705303192, 0.12601004540920258, 0.11944422870874405, 0.3475401699542999, 0.12617695331573486, 0.22629280388355255, 0.10528217256069183, 0.1538884937763214, 0.20922979712486267, 0.06847785413265228, 0.14529044926166534, 0.21876561641693115, 0.06879443675279617, 0.10269381105899811, 0.06791657954454422, 0.06698682904243469, 0.11618699878454208, 0.06525629013776779, 0.09695088863372803, 0.11366409063339233, 0.06316226720809937, 0.2360142171382904, 0.05846475064754486, 0.12085463106632233, 0.13018546998500824, 0.2667781710624695, 0.05621800944209099, 0.2896077334880829, 0.13729327917099, 0.09888052940368652, 0.053915999829769135, 0.05316279083490372, 0.3075730502605438, 0.24161522090435028, 0.05316375941038132, 0.19612444937229156, 0.25059637427330017, 0.05450300872325897, 0.3382239043712616, 0.22224543988704681, 0.22851887345314026, 0.11368124932050705, 0.14941999316215515, 0.05943983048200607, 0.13360682129859924, 0.10448065400123596, 0.11610361188650131, 0.25175145268440247, 0.1269453465938568, 0.1396925151348114, 0.09455099701881409, 0.1982915997505188, 0.22242072224617004, 0.31923896074295044, 0.2851799726486206, 0.17365692555904388, 0.0675339475274086, 0.11096108704805374, 0.14296670258045197, 0.07021579891443253, 0.0693403035402298, 0.15764804184436798, 0.15839071571826935, 0.11016526073217392, 0.12314574420452118, 0.07000232487916946, 0.08828029036521912, 0.0786249190568924, 0.2183176875114441, 0.06846550852060318, 0.11041400581598282, 0.2945515513420105, 0.06787886470556259, 0.3239244520664215, 0.0667768344283104, 0.2489016354084015, 0.2660326659679413, 0.06896549463272095, 0.16451850533485413, 0.20433266460895538, 0.07117192447185516, 0.13710865378379822, 0.07171180099248886, 0.14056336879730225, 0.20029465854167938, 0.12934039533138275, 0.21121960878372192, 0.10602652281522751, 0.09802954643964767, 0.1581665575504303, 0.14727237820625305, 0.11621749401092529, 0.12239521741867065, 0.19792495667934418, 0.242971271276474, 0.21304452419281006, 0.15697145462036133, 0.22319920361042023, 0.20886246860027313, 0.06813669949769974, 0.10172805190086365, 0.1419508010149002, 0.20895931124687195, 0.13947586715221405, 0.186894029378891, 0.14058537781238556, 0.12837685644626617, 0.2556195557117462, 0.06651950627565384, 0.09348879754543304, 0.20538945496082306, 0.17332513630390167, 0.22922354936599731, 0.08911368995904922, 0.17254053056240082, 0.17598870396614075, 0.20875515043735504, 0.21735325455665588, 0.13983121514320374, 0.14758436381816864, 0.06825216114521027, 0.0679130032658577, 0.17756512761116028, 0.06827127188444138, 0.1274200826883316, 0.06726931780576706, 0.10921734571456909, 0.1178879514336586, 0.1312938630580902, 0.10851836204528809, 0.11125309765338898, 0.10103749483823776, 0.09865336120128632, 0.05878782644867897, 0.22340047359466553, 0.0894494578242302, 0.19948728382587433, 0.054177410900592804, 0.16698585450649261, 0.137680321931839, 0.24455513060092926, 0.19611553847789764, 0.1288711130619049, 0.09482891112565994, 0.2397954761981964, 0.12051195651292801, 0.13648901879787445, 0.1893278807401657, 0.052179280668497086, 0.21464158594608307, 0.12950031459331512, 0.11686881631612778, 0.21225154399871826, 0.10367337614297867, 0.11297836899757385, 0.2678241729736328, 0.22212201356887817, 0.23057341575622559, 0.2951304316520691, 0.12198608368635178, 0.2836945354938507, 0.1261121928691864, 0.10718438774347305, 0.05807800218462944, 0.18814855813980103, 0.12300271540880203, 0.11422965675592422, 0.17939722537994385, 0.11179668456315994, 0.28795161843299866, 0.13233976066112518, 0.12154577672481537, 0.12988458573818207, 0.18225376307964325, 0.2262742817401886, 0.06633132696151733, 0.09835240989923477, 0.18777352571487427, 0.29396548867225647, 0.18745458126068115, 0.11746939271688461, 0.06841801106929779, 0.16545292735099792, 0.07012400776147842, 0.06929260492324829, 0.18973290920257568, 0.10979078710079193, 0.0694640651345253, 0.08561422675848007, 0.06833375245332718, 0.06743290275335312, 0.09889815002679825, 0.19686102867126465, 0.1751600205898285, 0.2842950224876404, 0.06439336389303207, 0.1268903762102127, 0.14022216200828552, 0.13144731521606445, 0.28905758261680603, 0.06382875889539719, 0.062345508486032486, 0.10733509808778763, 0.06199537590146065, 0.16693292558193207, 0.2224271297454834, 0.3354175388813019, 0.05969627946615219, 0.059866711497306824, 0.059281304478645325, 0.12057770043611526, 0.42109575867652893, 0.30105680227279663, 0.12071137875318527, 0.15155170857906342, 0.06210586801171303, 0.10040973871946335, 0.06413907557725906, 0.12206362932920456, 0.12246189266443253, 0.12125292420387268, 0.28692150115966797, 0.18246474862098694, 0.1179303228855133, 0.1591540277004242, 0.18925262987613678, 0.12421368062496185, 0.27672627568244934, 0.12690849602222443, 0.21952906250953674, 0.19558995962142944, 0.06962764263153076, 0.21689271926879883, 0.10542746633291245, 0.1420736163854599, 0.20675028860569, 0.2842753827571869, 0.10391277819871902, 0.0741569772362709, 0.13788026571273804, 0.11778189986944199, 0.07521145790815353, 0.11487869918346405, 0.18035471439361572, 0.14188769459724426, 0.19464462995529175, 0.27099302411079407, 0.1420815885066986, 0.1241752952337265, 0.17185980081558228, 0.07315754890441895, 0.07407279312610626, 0.07362329959869385, 0.07145559787750244, 0.21117790043354034, 0.070048026740551, 0.20113861560821533, 0.14240577816963196, 0.12351825088262558, 0.0661495178937912, 0.2064242959022522, 0.19550864398479462, 0.194932758808136, 0.17996226251125336, 0.3129715323448181, 0.1951557844877243, 0.11432190239429474, 0.2439431995153427, 0.12989866733551025, 0.3813972771167755, 0.10995670408010483, 0.14097826182842255, 0.07224947959184647, 0.07332386076450348, 0.07386256009340286, 0.11815539002418518, 0.15080280601978302, 0.11776692420244217, 0.07261886447668076, 0.2699993848800659, 0.10500001162290573, 0.1476234346628189, 0.1293797791004181, 0.18465283513069153, 0.07190699130296707, 0.20499764382839203, 0.13557469844818115, 0.11561965942382812, 0.06968727707862854, 0.17886406183242798, 0.1341770738363266, 0.12909674644470215, 0.2936537265777588, 0.06619489192962646, 0.11048654466867447, 0.1191582977771759, 0.10803438723087311, 0.06479606032371521, 0.09206520020961761, 0.06291467696428299, 0.1574128270149231, 0.18589071929454803, 0.326766699552536, 0.18320445716381073, 0.06136181205511093, 0.10602851212024689, 0.06113499030470848, 0.13844546675682068, 0.06045512109994888, 0.12680567800998688, 0.29049497842788696, 0.17067928612232208, 0.4145146608352661, 0.26631712913513184, 0.21200190484523773, 0.25292831659317017, 0.09420592337846756, 0.12070025503635406, 0.17142310738563538, 0.07179206609725952, 0.18670016527175903, 0.20775827765464783, 0.23536153137683868, 0.13222871720790863, 0.07899437099695206, 0.1142842024564743, 0.14155153930187225, 0.11045810580253601, 0.14193953573703766, 0.07887358963489532, 0.07823912054300308, 0.1936066746711731, 0.11122584342956543, 0.0762208104133606, 0.12089872360229492, 0.07447695732116699, 0.10960741341114044, 0.18256951868534088, 0.2091817408800125, 0.13548053801059723, 0.10205993056297302, 0.06915920227766037, 0.1361437290906906, 0.12837707996368408, 0.0648484006524086, 0.18207387626171112, 0.06277486681938171, 0.061967793852090836, 0.197774276137352, 0.29796308279037476, 0.05956515669822693, 0.11010868102312088, 0.16773857176303864, 0.19221609830856323, 0.05810998007655144, 0.057440198957920074, 0.1325385868549347, 0.17628026008605957, 0.12096137553453445, 0.18705493211746216, 0.05509283021092415, 0.18506750464439392, 0.054299600422382355, 0.2755916118621826, 0.13610078394412994, 0.05411703139543533, 0.2817389965057373, 0.10463160276412964, 0.19360318779945374, 0.05562598630785942, 0.23708896338939667, 0.11828381568193436, 0.056632962077856064, 0.05717562884092331, 0.05599069595336914, 0.12741747498512268, 0.10064300894737244, 0.17133723199367523, 0.09333280473947525, 0.20965726673603058, 0.2022240161895752, 0.33678138256073, 0.3021770715713501, 0.13922517001628876, 0.05593753978610039, 0.05693228170275688, 0.10719158500432968, 0.2104717195034027, 0.11897607892751694, 0.11649181693792343, 0.11512070149183273, 0.10535983741283417, 0.2889519929885864, 0.1336061805486679, 0.18324428796768188, 0.059441324323415756, 0.059326477348804474, 0.13674263656139374, 0.059378720819950104, 0.14801611006259918, 0.3595292866230011, 0.21267598867416382, 0.2189861536026001, 0.10630889981985092, 0.12219065427780151, 0.14231252670288086, 0.06042223796248436, 0.22880244255065918, 0.13303185999393463, 0.12930887937545776, 0.10555139183998108, 0.1484297811985016, 0.18654681742191315, 0.11230245977640152, 0.13450591266155243, 0.11849058419466019, 0.26911547780036926, 0.19250591099262238, 0.05790126323699951, 0.11626821011304855, 0.10542582720518112, 0.11468325555324554, 0.14699451625347137, 0.05788253992795944, 0.12166870385408401, 0.0562230609357357, 0.12900479137897491, 0.1927739828824997, 0.05385887250304222, 0.16944293677806854, 0.19000858068466187, 0.13791623711585999, 0.05230383202433586, 0.3477446734905243, 0.20929887890815735, 0.12947410345077515, 0.19297955930233002, 0.12865906953811646, 0.16089867055416107, 0.17786121368408203, 0.09511947631835938, 0.1008387953042984, 0.05524870753288269, 0.05515578016638756, 0.09205760061740875, 0.054345689713954926, 0.1154046282172203, 0.22958239912986755, 0.10961899906396866, 0.18125373125076294, 0.1770647019147873, 0.11285444349050522, 0.20156608521938324, 0.21131174266338348, 0.29603642225265503, 0.09822443872690201, 0.12364412844181061, 0.15067577362060547, 0.21622520685195923, 0.05628767982125282, 0.09476642310619354, 0.056539230048656464, 0.19765207171440125, 0.0564495287835598, 0.05611639842391014, 0.10178709775209427, 0.1474488377571106, 0.1122271940112114, 0.1945279836654663, 0.12973229587078094, 0.2964370846748352, 0.09906719624996185, 0.05452096089720726, 0.15660274028778076, 0.053569138050079346, 0.20883974432945251, 0.25281020998954773, 0.23379851877689362, 0.20093496143817902, 0.16039536893367767, 0.1140606626868248, 0.1774670034646988, 0.05592894181609154, 0.1852240264415741, 0.23666737973690033, 0.19760730862617493, 0.11089823395013809, 0.11501123756170273, 0.11001227051019669, 0.10355304181575775, 0.18490587174892426, 0.11979421973228455, 0.20312030613422394, 0.1149756982922554, 0.21225085854530334, 0.11598917841911316, 0.059260331094264984, 0.10329712927341461, 0.1350426822900772, 0.20970547199249268, 0.16852790117263794, 0.10141139477491379, 0.062359947711229324, 0.1784249097108841, 0.22763748466968536, 0.26270878314971924, 0.06039535254240036, 0.15765438973903656, 0.09016990661621094, 0.12110383063554764, 0.061719734221696854, 0.061922959983348846, 0.31584835052490234, 0.41949036717414856, 0.09164027124643326, 0.3436580300331116, 0.2690407633781433, 0.14254945516586304, 0.19302287697792053, 0.2666330337524414, 0.07793483883142471, 0.18072357773780823, 0.1686735451221466, 0.15480808913707733, 0.1753469556570053, 0.07790356129407883, 0.09661087393760681, 0.1337711066007614, 0.12571169435977936, 0.08011945337057114, 0.07902444154024124, 0.08884512633085251, 0.11013377457857132, 0.22673535346984863, 0.07809203118085861, 0.26778295636177063, 0.24763962626457214, 0.0776716098189354, 0.10926611721515656, 0.16980202496051788, 0.16857001185417175, 0.29621827602386475, 0.1877998411655426, 0.07681480795145035, 0.0744776651263237, 0.10876744985580444, 0.3119221329689026, 0.21490256488323212, 0.22352610528469086, 0.1347966343164444, 0.1456928551197052, 0.09599585086107254, 0.1231047660112381, 0.16220539808273315, 0.07506287097930908, 0.07524152100086212, 0.0737919807434082, 0.14628025889396667, 0.15568843483924866, 0.07130131870508194, 0.11002007126808167, 0.06892840564250946, 0.09492272138595581, 0.12565216422080994, 0.08664005249738693, 0.18208745121955872, 0.2535882890224457, 0.11897660046815872, 0.06307607889175415, 0.11515331268310547, 0.1924864798784256, 0.2928970456123352, 0.32269513607025146, 0.11816473305225372, 0.2732889950275421, 0.17690929770469666, 0.13748858869075775, 0.06459589302539825, 0.20339369773864746, 0.06575983017683029, 0.3326122462749481, 0.06683669984340668, 0.10762514173984528, 0.1467701494693756, 0.16760554909706116, 0.1628153920173645, 0.10946868360042572, 0.12116172164678574, 0.1958749145269394, 0.17345568537712097, 0.08536980301141739, 0.06989369541406631, 0.07073380053043365, 0.15274621546268463, 0.18592604994773865, 0.1657775640487671, 0.13562677800655365, 0.09838081896305084, 0.10635470598936081, 0.3081153631210327, 0.1145174577832222, 0.2341754138469696, 0.06937538832426071, 0.10037043690681458, 0.09372899681329727, 0.10362610965967178, 0.19388189911842346, 0.20057839155197144, 0.2660984694957733, 0.06994694471359253, 0.11365421116352081, 0.06900924444198608, 0.14526568353176117, 0.09409984946250916, 0.08913633972406387, 0.06586983054876328, 0.10189568251371384, 0.30455994606018066, 0.11774180829524994, 0.06465575098991394, 0.06388307362794876, 0.11453346163034439, 0.06300105899572372, 0.27549925446510315, 0.1909610778093338, 0.10980618000030518, 0.05933516472578049, 0.11298909038305283, 0.17981654405593872, 0.15457502007484436, 0.056662801653146744, 0.1502421349287033, 0.05700793117284775, 0.08815614134073257, 0.1614363044500351, 0.3201068043708801, 0.28728023171424866, 0.053923241794109344, 0.13225945830345154, 0.13997864723205566, 0.11851120740175247, 0.2554493248462677, 0.05512924864888191, 0.27349022030830383, 0.20213863253593445, 0.08585673570632935, 0.19709551334381104, 0.05530519410967827, 0.18169915676116943, 0.19480544328689575, 0.3284304141998291, 0.16033712029457092, 0.05801142379641533, 0.3361286222934723, 0.2110336571931839, 0.06058882176876068, 0.06248122453689575, 0.1777111440896988, 0.18217839300632477, 0.0924927294254303, 0.19507719576358795, 0.15253204107284546, 0.25572240352630615, 0.0672789216041565, 0.22669216990470886, 0.25001171231269836, 0.0673195868730545, 0.19116729497909546, 0.10581564158201218, 0.30939963459968567, 0.13192512094974518, 0.24854418635368347, 0.16309359669685364, 0.176662415266037, 0.12699489295482635, 0.28973567485809326, 0.07396842539310455, 0.12129078805446625, 0.09034713357686996, 0.208870530128479, 0.07698734849691391, 0.14600957930088043, 0.13417573273181915, 0.16125516593456268, 0.19637137651443481, 0.1581728458404541, 0.07733147591352463, 0.07778584957122803, 0.11095263063907623, 0.18927490711212158, 0.1639893501996994, 0.19399426877498627, 0.0757257342338562, 0.17676721513271332, 0.32712000608444214, 0.13502976298332214, 0.07443159818649292, 0.07392760366201401, 0.07336540520191193, 0.15514767169952393, 0.2690143585205078, 0.18359017372131348, 0.07138065993785858, 0.10417723655700684, 0.11094662547111511, 0.15898844599723816, 0.18232198059558868, 0.07019562274217606, 0.07091014832258224, 0.06965342909097672, 0.199651300907135, 0.18891209363937378, 0.06862921267747879, 0.1294587105512619, 0.20035585761070251, 0.22227329015731812, 0.12394512444734573, 0.2670324146747589, 0.09635511040687561, 0.10661269724369049, 0.06649305671453476, 0.06573814898729324, 0.18802186846733093, 0.10413748770952225, 0.10395148396492004, 0.0641583576798439, 0.12421010434627533, 0.20163828134536743, 0.11008784919977188, 0.3209037184715271, 0.19056661427021027, 0.06202924624085426, 0.06263096630573273, 0.1145729050040245, 0.12038395553827286, 0.15719440579414368, 0.3325686454772949, 0.22988729178905487, 0.21087482571601868, 0.22924500703811646, 0.17043697834014893, 0.4041251540184021, 0.17821118235588074, 0.12134359031915665, 0.1716361790895462, 0.12546363472938538, 0.26987332105636597, 0.07042445987462997, 0.18783052265644073, 0.2052147537469864, 0.18434755504131317, 0.13509432971477509, 0.1260259598493576, 0.09393306076526642, 0.11518796533346176, 0.16266083717346191, 0.1327754259109497, 0.13167332112789154, 0.11965468525886536, 0.1456807553768158, 0.07767736166715622, 0.17354895174503326, 0.0999443382024765, 0.14627514779567719, 0.20292863249778748, 0.16024231910705566, 0.0738079845905304, 0.07361943274736404, 0.2521721124649048, 0.13753198087215424, 0.12436749786138535, 0.07168307155370712, 0.13027288019657135, 0.19258302450180054, 0.0710906833410263, 0.10780218988656998, 0.11977764219045639, 0.1897079199552536, 0.12787026166915894, 0.13473281264305115, 0.13594409823417664, 0.11952313035726547, 0.0640125721693039, 0.06307338178157806, 0.06363467127084732, 0.06105603277683258, 0.23778894543647766, 0.27773162722587585, 0.1053779274225235, 0.05868404731154442, 0.10799746215343475, 0.09876842051744461, 0.05595710501074791, 0.10769019275903702, 0.17625747621059418, 0.19824440777301788, 0.13603071868419647, 0.09834057837724686, 0.10136745125055313, 0.18582725524902344, 0.11638081818819046, 0.1993391066789627, 0.20046088099479675, 0.10645628720521927, 0.3273567259311676, 0.11000292748212814, 0.0528886541724205, 0.19882020354270935, 0.24303977191448212, 0.11082541197538376, 0.05432307720184326, 0.1365191489458084, 0.11015423387289047, 0.1750352382659912, 0.0957687571644783, 0.15330637991428375, 0.054053544998168945, 0.12081719934940338, 0.05446378514170647, 0.0537307932972908, 0.05292461812496185, 0.053352635353803635, 0.21466778218746185, 0.09106628596782684, 0.24001839756965637, 0.10443875938653946, 0.16980543732643127, 0.12250667810440063, 0.23099152743816376, 0.11032384634017944, 0.15497566759586334, 0.20237797498703003, 0.1196894720196724, 0.18990299105644226, 0.049749236553907394, 0.10796644538640976, 0.3481716513633728, 0.05032667517662048, 0.20014308393001556, 0.13253095746040344, 0.285474956035614, 0.24055208265781403, 0.17591053247451782, 0.11859302222728729, 0.21127809584140778, 0.08769775927066803, 0.10295616835355759, 0.1377640962600708, 0.05600673332810402, 0.09581655263900757, 0.12106440961360931, 0.12197442352771759, 0.055676817893981934, 0.16088081896305084, 0.23151172697544098, 0.14374344050884247, 0.22913919389247894, 0.054996855556964874, 0.05429074913263321, 0.21486234664916992, 0.21648594737052917, 0.14850519597530365, 0.053724512457847595, 0.2292599231004715, 0.05298115685582161, 0.05286220833659172, 0.05278691649436951, 0.16213096678256989, 0.11939078569412231, 0.23053796589374542, 0.13680778443813324, 0.08726513385772705, 0.10338682681322098, 0.3583213686943054, 0.12370945513248444, 0.1297903060913086, 0.04898667335510254, 0.09618277847766876, 0.1434081494808197, 0.09650933742523193, 0.047850366681814194, 0.047392699867486954, 0.20028366148471832, 0.10060672461986542, 0.11626695096492767, 0.21173590421676636, 0.22427575290203094, 0.09429479390382767, 0.2291208803653717, 0.33771494030952454, 0.10608333349227905, 0.10577797889709473, 0.30221065878868103, 0.20660293102264404, 0.11601912975311279, 0.09133242815732956, 0.050895966589450836, 0.1439213901758194, 0.19769519567489624, 0.3012276887893677, 0.10229256749153137, 0.053836897015571594, 0.05367433652281761, 0.22270554304122925, 0.1311851292848587, 0.11754930764436722, 0.11363715678453445, 0.2893247902393341, 0.2955436706542969, 0.1861252337694168, 0.13372863829135895, 0.057960767298936844, 0.05841120332479477, 0.124045230448246, 0.059002164751291275, 0.1302860826253891, 0.05870755389332771, 0.05891311541199684, 0.1087065190076828, 0.20620860159397125, 0.05692162737250328, 0.1976541131734848, 0.20518700778484344, 0.11926091462373734, 0.19914977252483368, 0.20059306919574738, 0.206106036901474, 0.12827636301517487, 0.1423831433057785, 0.05692625790834427, 0.1169290542602539, 0.05727969855070114, 0.11688061058521271, 0.05545911565423012, 0.20678384602069855, 0.28551024198532104, 0.11939072608947754, 0.19966734945774078, 0.19401314854621887, 0.055284079164266586, 0.19208677113056183, 0.05553056672215462, 0.117686428129673, 0.11804473400115967, 0.10412182658910751, 0.17886117100715637, 0.11593671888113022, 0.20028387010097504, 0.13232532143592834, 0.11255589127540588, 0.11290881037712097, 0.054681822657585144, 0.2972283363342285, 0.054659344255924225, 0.20968568325042725, 0.11289840936660767, 0.12969155609607697, 0.1390911191701889, 0.3354306221008301, 0.05464750528335571, 0.09681382775306702, 0.11503129452466965, 0.3277513086795807, 0.13483315706253052, 0.27085328102111816, 0.1137852743268013, 0.13267411291599274, 0.05727342516183853, 0.17895705997943878, 0.10041417181491852, 0.30778563022613525, 0.12909722328186035, 0.11365091055631638, 0.2867894768714905, 0.14647923409938812, 0.40438365936279297, 0.10971694439649582, 0.189264714717865, 0.11640048027038574, 0.20836330950260162, 0.20497465133666992, 0.2756790220737457, 0.11550712585449219, 0.1042773425579071, 0.13118796050548553, 0.26157069206237793, 0.2645925283432007, 0.20294882357120514, 0.0783262625336647, 0.19297003746032715, 0.08104398846626282, 0.20237717032432556, 0.1949089914560318, 0.1317177265882492, 0.0839444026350975, 0.08408130705356598, 0.08414044231176376, 0.1621496081352234, 0.08436671644449234, 0.12185575067996979, 0.15471914410591125, 0.17767919600009918, 0.13286270201206207, 0.08109776675701141, 0.13530418276786804, 0.080441914498806, 0.11259911954402924, 0.20902304351329803, 0.19106383621692657, 0.07634090632200241, 0.18147064745426178, 0.1830102652311325, 0.1886838972568512, 0.13812212646007538, 0.20957833528518677, 0.07348136603832245, 0.1161905825138092, 0.13591155409812927, 0.07329515367746353, 0.22271457314491272, 0.071280337870121, 0.1664702147245407, 0.1697283834218979, 0.15104295313358307, 0.17028221487998962, 0.06882333755493164, 0.15665192902088165, 0.19259734451770782, 0.0673106238245964, 0.3059577941894531, 0.15286238491535187, 0.12601828575134277, 0.24229542911052704, 0.1310916543006897, 0.1253790557384491, 0.2797897756099701, 0.06689245998859406, 0.09058115631341934, 0.06710749119520187, 0.0671139806509018, 0.38368701934814453, 0.06748373806476593, 0.2907322645187378, 0.13807548582553864, 0.17401257157325745, 0.10747361928224564, 0.21575602889060974, 0.2473740428686142, 0.09998786449432373, 0.2734580636024475, 0.12334045022726059, 0.09584435820579529, 0.14420229196548462, 0.20750956237316132, 0.20841658115386963, 0.18636122345924377, 0.07694761455059052, 0.34729328751564026, 0.13985423743724823, 0.0797121524810791, 0.18495866656303406, 0.1148986741900444, 0.08056249469518661, 0.08156193792819977, 0.2123592495918274, 0.20582422614097595, 0.12035121023654938, 0.13168898224830627, 0.08154977113008499, 0.11045246571302414, 0.12441447377204895, 0.11510073393583298, 0.18809860944747925, 0.0775337964296341, 0.14219748973846436, 0.12645334005355835, 0.09830569475889206, 0.18932507932186127, 0.11698931455612183, 0.24542458355426788, 0.07363515347242355, 0.07354795187711716, 0.13716822862625122, 0.07299049198627472, 0.10657012462615967, 0.12628087401390076, 0.2672320604324341, 0.10264305025339127, 0.11228826642036438, 0.13826584815979004, 0.22417810559272766, 0.21003055572509766, 0.2851484417915344, 0.133148655295372, 0.19616930186748505, 0.1264561116695404, 0.20382051169872284, 0.1277187317609787, 0.10796293616294861, 0.2023731768131256, 0.27955153584480286, 0.11671741306781769, 0.06909680366516113, 0.18058402836322784, 0.27883753180503845, 0.14124758541584015, 0.177810400724411, 0.16342347860336304, 0.1635848730802536, 0.1229783445596695, 0.10539142042398453, 0.18298344314098358, 0.24299298226833344, 0.1065559834241867, 0.16404353082180023, 0.0777633860707283, 0.22339537739753723, 0.18682008981704712, 0.07840864360332489, 0.15073618292808533, 0.1028650626540184, 0.20337192714214325, 0.07739080488681793, 0.24644280970096588, 0.1483556032180786, 0.10894910246133804, 0.07827269285917282, 0.3113190829753876, 0.17363229393959045, 0.1896314173936844, 0.13549594581127167, 0.09427420794963837, 0.07888883352279663, 0.22776825726032257, 0.12897063791751862, 0.12676334381103516, 0.13786707818508148, 0.07916339486837387, 0.12933960556983948, 0.07827238738536835, 0.15161588788032532, 0.23623639345169067, 0.07498548179864883, 0.07442908734083176, 0.21312035620212555, 0.10382470488548279, 0.11365346610546112, 0.07126849889755249, 0.2713777720928192, 0.24262452125549316, 0.17577557265758514, 0.13537141680717468, 0.21853147447109222, 0.06967336684465408, 0.17662717401981354, 0.0697670429944992, 0.1164739727973938, 0.23300203680992126, 0.2058398425579071, 0.06888467073440552, 0.11077141016721725, 0.1941841095685959, 0.15552091598510742, 0.06815169751644135, 0.09568338096141815, 0.11074745655059814, 0.10829824954271317, 0.06902745366096497, 0.06737940013408661, 0.13091622292995453, 0.09828431159257889, 0.06406477093696594, 0.3121945858001709, 0.10122255235910416, 0.11469237506389618, 0.10214093327522278, 0.20401228964328766, 0.11007967591285706, 0.06146100535988808, 0.16093647480010986, 0.13943351805210114, 0.11745328456163406, 0.28085026144981384, 0.059729307889938354, 0.059378497302532196, 0.33820274472236633, 0.22826628386974335, 0.1115485280752182, 0.1058168113231659, 0.09773286432027817, 0.21908776462078094, 0.11234498023986816, 0.19076159596443176, 0.1284978836774826, 0.23515912890434265, 0.11416225880384445, 0.2162850797176361, 0.06009677052497864, 0.12060564011335373, 0.2012333869934082, 0.19609719514846802, 0.14208245277404785, 0.12428674101829529, 0.1993395835161209, 0.22569048404693604, 0.11697877943515778, 0.06089906394481659, 0.3191320300102234, 0.16151729226112366, 0.13423612713813782, 0.12675780057907104, 0.183652862906456, 0.13184158504009247, 0.09920338541269302, 0.1213417500257492, 0.10697914659976959, 0.12382704764604568, 0.11440509557723999, 0.20351792871952057, 0.062056947499513626, 0.061675239354372025, 0.20718641579151154, 0.11947327852249146, 0.06099115312099457, 0.14787285029888153, 0.35638007521629333, 0.11243929713964462, 0.3145516812801361, 0.10713221877813339, 0.06004839763045311, 0.11181589961051941, 0.11797652393579483, 0.2159411758184433, 0.05975658446550369, 0.09363120794296265, 0.19729241728782654, 0.05989781767129898, 0.19541360437870026, 0.13683316111564636, 0.3093642294406891, 0.21211647987365723, 0.11514836549758911, 0.2989773154258728, 0.19169491529464722, 0.19104348123073578, 0.061903201043605804, 0.1807597577571869, 0.12373855710029602, 0.2975331246852875, 0.20781993865966797, 0.10103975236415863, 0.06564291566610336, 0.16268084943294525, 0.16862300038337708, 0.3352198600769043, 0.15826371312141418, 0.06927364319562912, 0.06927118450403214, 0.1566004455089569, 0.16437524557113647, 0.1201288253068924, 0.11579930037260056, 0.12301787734031677, 0.12174267321825027, 0.07070496678352356, 0.24385453760623932, 0.07061774283647537, 0.13253836333751678, 0.07016737014055252, 0.18406236171722412, 0.2640843391418457, 0.1974250227212906, 0.1307051181793213, 0.18788516521453857, 0.11470295488834381, 0.06965361535549164, 0.15340976417064667, 0.07009586691856384, 0.06942391395568848, 0.12759116291999817, 0.1254682093858719, 0.12327954918146133, 0.12871453166007996, 0.2043714076280594, 0.06608723104000092, 0.17087611556053162, 0.22177529335021973, 0.06388498842716217, 0.12329468876123428, 0.20446324348449707, 0.06345537304878235, 0.1138087809085846, 0.06217391788959503, 0.32866135239601135, 0.27163517475128174, 0.20997583866119385, 0.13974781334400177, 0.13827955722808838, 0.12118405103683472, 0.30261915922164917, 0.32947495579719543, 0.10946012288331985, 0.28755244612693787, 0.06466780602931976, 0.13723713159561157, 0.20765186846256256, 0.1905757188796997, 0.17099356651306152, 0.25448617339134216, 0.19858495891094208, 0.30016782879829407, 0.266562283039093, 0.07194247841835022, 0.07270015776157379, 0.20940446853637695, 0.074458546936512, 0.1198161393404007, 0.1300448328256607, 0.07606419920921326, 0.15743374824523926, 0.07554365694522858, 0.12385671585798264, 0.19470912218093872, 0.20227310061454773, 0.07516023516654968, 0.07489948719739914, 0.11869235336780548, 0.16574299335479736, 0.15803486108779907, 0.1660626381635666, 0.2062259167432785, 0.07313850522041321, 0.07263845950365067, 0.11424101889133453, 0.12873342633247375, 0.11703168600797653, 0.07104290276765823, 0.18874739110469818, 0.07063312828540802, 0.15979120135307312, 0.11326564103364944, 0.06786826997995377, 0.06772160530090332, 0.11140722781419754, 0.11296391487121582, 0.19488468766212463, 0.12146836519241333, 0.12965328991413116, 0.1233573630452156, 0.1404370218515396, 0.3315674066543579, 0.12001381069421768, 0.09499895572662354, 0.10643883794546127, 0.09453614056110382, 0.21022826433181763, 0.1831529289484024, 0.1870303601026535, 0.2900032699108124, 0.12910185754299164, 0.061139874160289764, 0.1570335477590561, 0.2058427780866623, 0.31235793232917786, 0.3204675614833832, 0.26628434658050537, 0.19383050501346588, 0.13489143550395966, 0.1290285438299179, 0.11360549181699753, 0.06681568920612335, 0.06762595474720001, 0.12880589067935944, 0.06757263094186783, 0.12729325890541077, 0.13913477957248688, 0.11948707699775696, 0.11741439998149872, 0.13371828198432922, 0.128801628947258, 0.19312173128128052, 0.10696050524711609, 0.09358210861682892, 0.06494759023189545, 0.08982164412736893, 0.398568332195282, 0.16450050473213196, 0.20218950510025024, 0.1407465785741806, 0.08482349663972855, 0.16212645173072815, 0.0657261461019516, 0.302080899477005, 0.4025077223777771, 0.129710391163826, 0.27170687913894653, 0.13001227378845215, 0.1607683151960373, 0.1538502275943756, 0.06901166588068008, 0.1236034482717514, 0.13265331089496613, 0.14240038394927979, 0.10859183222055435, 0.11543979495763779, 0.38999995589256287, 0.13250701129436493, 0.12027835845947266, 0.14363545179367065, 0.06723620742559433, 0.16440215706825256, 0.1634068489074707, 0.10810071229934692, 0.2928585112094879, 0.06637360155582428, 0.2320212572813034, 0.10247091203927994, 0.10281018912792206, 0.12385914474725723, 0.08512254059314728, 0.06570969521999359, 0.14538584649562836, 0.2187173068523407, 0.06500199437141418, 0.06519336998462677, 0.09809620678424835, 0.13327990472316742, 0.06527313590049744, 0.2654222548007965, 0.21404492855072021, 0.06251050531864166, 0.15690946578979492, 0.1069069355726242, 0.14652763307094574, 0.12572862207889557, 0.08648169785737991, 0.058023106306791306, 0.11142019927501678, 0.10839972645044327, 0.16342858970165253, 0.18840144574642181, 0.17995791137218475, 0.19367346167564392, 0.3354622721672058, 0.12194101512432098, 0.0574062243103981, 0.2858266234397888, 0.17240993678569794, 0.11381331086158752, 0.20660269260406494, 0.05785348638892174, 0.11862865835428238, 0.168937087059021, 0.05870708078145981, 0.05862855538725853, 0.12169484794139862, 0.13592292368412018, 0.31570664048194885, 0.10391595959663391, 0.05878280848264694, 0.21124419569969177, 0.27786874771118164, 0.20924651622772217, 0.3411591649055481, 0.10866901278495789, 0.20771819353103638, 0.27300694584846497, 0.15062254667282104, 0.17756174504756927, 0.06525370478630066, 0.0649135559797287, 0.06580855697393417, 0.0978376492857933, 0.11859903484582901, 0.06621553003787994, 0.23188847303390503, 0.19516150653362274, 0.12078554928302765, 0.2195959836244583, 0.10452623665332794, 0.13923567533493042, 0.24771130084991455, 0.24973337352275848, 0.17171470820903778, 0.17370963096618652, 0.06813663989305496, 0.06795511394739151, 0.1948995590209961, 0.0682423859834671, 0.12184184044599533, 0.10081873089075089, 0.06894219666719437, 0.17923559248447418, 0.13150633871555328, 0.0977732315659523, 0.10046696662902832, 0.16290241479873657, 0.14791066944599152, 0.16603747010231018]\n",
            "Val loss 0.14267948260507488\n",
            "Val auc roc 0.5055047094613235\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0488b86ede0410b8f011796b0526a3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1465\n",
            "Train Losses : [0.14917951822280884, 0.13087375462055206, 0.11771140247583389, 0.11811015009880066, 0.16581615805625916, 0.11476032435894012, 0.13721075654029846, 0.2767305076122284, 0.08981616050004959, 0.14567622542381287, 0.0631042867898941, 0.18147122859954834, 0.0629005953669548, 0.12672263383865356, 0.11980622261762619, 0.12067767977714539, 0.23535944521427155, 0.12643058598041534, 0.061117276549339294, 0.20703564584255219, 0.23234015703201294, 0.24901290237903595, 0.22224082052707672, 0.1116839125752449, 0.21481771767139435, 0.06005994975566864, 0.18255138397216797, 0.22801154851913452, 0.12650339305400848, 0.19208022952079773, 0.14969468116760254, 0.12818127870559692, 0.05961841344833374, 0.28616973757743835, 0.16989676654338837, 0.26298362016677856, 0.11189503967761993, 0.1344260424375534, 0.08937595784664154, 0.1889594942331314, 0.12730908393859863, 0.14282508194446564, 0.4153750538825989, 0.06035515293478966, 0.3159710466861725, 0.062011074274778366, 0.2617894411087036, 0.12527428567409515, 0.13888779282569885, 0.16063088178634644, 0.06493030488491058, 0.06521379202604294, 0.25793546438217163, 0.16797782480716705, 0.2980494201183319, 0.1115444153547287, 0.09908188879489899, 0.06806960701942444, 0.13037171959877014, 0.06802897900342941, 0.204792782664299, 0.13827791810035706, 0.14596344530582428, 0.07060164958238602, 0.06925183534622192, 0.1756286770105362, 0.24542590975761414, 0.11219588667154312, 0.11883731931447983, 0.12678712606430054, 0.06650795042514801, 0.06643059849739075, 0.06587179750204086, 0.06545286625623703, 0.06424224376678467, 0.06478859484195709, 0.23557938635349274, 0.11670549213886261, 0.06190736964344978, 0.16401103138923645, 0.26894864439964294, 0.16391946375370026, 0.22871972620487213, 0.17458926141262054, 0.306850790977478, 0.12218385189771652, 0.06038307398557663, 0.06046653538942337, 0.0978340283036232, 0.05958232656121254, 0.058984484523534775, 0.05996374785900116, 0.12670372426509857, 0.05747823789715767, 0.2354305386543274, 0.09330718964338303, 0.22369949519634247, 0.09669866412878036, 0.17552000284194946, 0.19207286834716797, 0.0558711402118206, 0.12961870431900024, 0.05533566325902939, 0.1699083149433136, 0.055061280727386475, 0.17837664484977722, 0.13399451971054077, 0.18513771891593933, 0.21107178926467896, 0.1757410764694214, 0.053838592022657394, 0.12582968175411224, 0.14398665726184845, 0.12584812939167023, 0.2002609223127365, 0.1656217873096466, 0.2071927785873413, 0.11620722711086273, 0.1862175166606903, 0.3229251503944397, 0.10440701246261597, 0.13036569952964783, 0.16863419115543365, 0.22134150564670563, 0.14269891381263733, 0.05520770698785782, 0.11603303253650665, 0.1857401728630066, 0.19783248007297516, 0.15744565427303314, 0.2774839997291565, 0.19620218873023987, 0.1186811774969101, 0.05789932608604431, 0.058075081557035446, 0.11595449596643448, 0.1481754034757614, 0.058638956397771835, 0.05831478536128998, 0.11038780957460403, 0.05811433494091034, 0.2375880628824234, 0.08647859841585159, 0.10960856825113297, 0.2721709907054901, 0.24796612560749054, 0.0580282099545002, 0.05837387964129448, 0.10929837077856064, 0.10152735561132431, 0.1114577203989029, 0.11311956495046616, 0.22008484601974487, 0.12169726192951202, 0.13089080154895782, 0.05708947777748108, 0.13173718750476837, 0.24536040425300598, 0.09910298138856888, 0.16910895705223083, 0.13882943987846375, 0.18928194046020508, 0.23265250027179718, 0.1795881986618042, 0.09565611928701401, 0.10418355464935303, 0.12656627595424652, 0.05593704804778099, 0.056567903608083725, 0.29333552718162537, 0.056450024247169495, 0.2596917748451233, 0.0705476850271225, 0.13407979905605316, 0.16930031776428223, 0.05717915669083595, 0.17026524245738983, 0.24208639562129974, 0.09934709221124649, 0.058205943554639816, 0.057853955775499344, 0.10606987774372101, 0.05737290531396866, 0.089722640812397, 0.11326505988836288, 0.14070133864879608, 0.2047688215970993, 0.11292409896850586, 0.11264681816101074, 0.12602262198925018, 0.25145429372787476, 0.14019371569156647, 0.12378436326980591, 0.09876282513141632, 0.11312440037727356, 0.05447430908679962, 0.12974032759666443, 0.1013750433921814, 0.11987562477588654, 0.2815709710121155, 0.09708239138126373, 0.18527871370315552, 0.16463138163089752, 0.05249395594000816, 0.11413663625717163, 0.052056584507226944, 0.13304905593395233, 0.12030749022960663, 0.19889895617961884, 0.2340540587902069, 0.10751768201589584, 0.10389131307601929, 0.2699655294418335, 0.21799173951148987, 0.11990667879581451, 0.12427555024623871, 0.12046264857053757, 0.1736120879650116, 0.1213090643286705, 0.051154766231775284, 0.11920669674873352, 0.26263245940208435, 0.10025392472743988, 0.10862108319997787, 0.13937342166900635, 0.10306807607412338, 0.16348505020141602, 0.2902989387512207, 0.27187156677246094, 0.33711886405944824, 0.25929930806159973, 0.0933593362569809, 0.13311168551445007, 0.1945432871580124, 0.10856825113296509, 0.05453920364379883, 0.101314015686512, 0.11071619391441345, 0.19289423525333405, 0.055041566491127014, 0.055327508598566055, 0.05554988980293274, 0.1291462779045105, 0.2998141348361969, 0.05557287111878395, 0.15432193875312805, 0.3209846615791321, 0.16322453320026398, 0.131052166223526, 0.0965099036693573, 0.10516079515218735, 0.14042185246944427, 0.1822432577610016, 0.11026745289564133, 0.1154702827334404, 0.16030777990818024, 0.05926502123475075, 0.2768268585205078, 0.19077864289283752, 0.33743607997894287, 0.18761762976646423, 0.12509283423423767, 0.059915486723184586, 0.06147721782326698, 0.3179144859313965, 0.06146639212965965, 0.2085958570241928, 0.062242541462183, 0.06340380012989044, 0.06269311159849167, 0.1246916875243187, 0.2211436927318573, 0.11770331114530563, 0.13852182030677795, 0.18899238109588623, 0.13527899980545044, 0.1277809888124466, 0.1322319358587265, 0.143265962600708, 0.20247258245944977, 0.3114803433418274, 0.1309865564107895, 0.1420450508594513, 0.10475510358810425, 0.22433437407016754, 0.1066102609038353, 0.13705503940582275, 0.2120053470134735, 0.12964771687984467, 0.09089607000350952, 0.1000845804810524, 0.06289620697498322, 0.11441317945718765, 0.060709398239851, 0.11981838196516037, 0.19695444405078888, 0.05979103595018387, 0.13759015500545502, 0.11694683134555817, 0.058107659220695496, 0.23720236122608185, 0.05725279450416565, 0.1441044956445694, 0.05632972717285156, 0.15799297392368317, 0.10933096706867218, 0.05541064962744713, 0.2846262454986572, 0.13890378177165985, 0.054642271250486374, 0.1492598056793213, 0.11874323338270187, 0.2235630452632904, 0.16307923197746277, 0.10947809368371964, 0.05426066741347313, 0.13355319201946259, 0.09646636247634888, 0.14940990507602692, 0.17385102808475494, 0.22557391226291656, 0.09421546012163162, 0.05321821570396423, 0.10413392633199692, 0.1730005294084549, 0.052473969757556915, 0.33783408999443054, 0.19781263172626495, 0.1597367823123932, 0.16745440661907196, 0.0921526551246643, 0.14631782472133636, 0.05422816425561905, 0.12330818176269531, 0.29310211539268494, 0.05514637380838394, 0.11137233674526215, 0.12451861798763275, 0.09504338353872299, 0.24885372817516327, 0.05508590489625931, 0.23402947187423706, 0.1320086270570755, 0.12827271223068237, 0.20500409603118896, 0.23983663320541382, 0.12116771191358566, 0.23942221701145172, 0.12748967111110687, 0.13375045359134674, 0.10195756703615189, 0.05509386211633682, 0.21172606945037842, 0.2291761189699173, 0.12646134197711945, 0.21568694710731506, 0.21800634264945984, 0.05571450665593147, 0.054839521646499634, 0.18349547684192657, 0.22285617887973785, 0.12439273297786713, 0.20121631026268005, 0.12229220569133759, 0.11862440407276154, 0.05641675740480423, 0.1898324191570282, 0.12366437166929245, 0.21695318818092346, 0.05608757212758064, 0.1733780950307846, 0.14381839334964752, 0.4336399435997009, 0.18223658204078674, 0.05626167729496956, 0.24310371279716492, 0.11480700969696045, 0.09777940064668655, 0.12339171022176743, 0.18584658205509186, 0.05807170644402504, 0.11460988968610764, 0.2219112664461136, 0.06003674492239952, 0.06002301350235939, 0.15424764156341553, 0.1798449456691742, 0.20409724116325378, 0.13096265494823456, 0.10163526237010956, 0.14658494293689728, 0.2952280044555664, 0.23073996603488922, 0.19881832599639893, 0.10820475965738297, 0.11157681047916412, 0.058643888682127, 0.1539711207151413, 0.11056225001811981, 0.1437685638666153, 0.21241804957389832, 0.1349574327468872, 0.11241987347602844, 0.10409332811832428, 0.17543165385723114, 0.18534855544567108, 0.21568450331687927, 0.1042775958776474, 0.11482164263725281, 0.20667263865470886, 0.3047523498535156, 0.2045588195323944, 0.19545666873455048, 0.09877287596464157, 0.05971807986497879, 0.11626096814870834, 0.17228537797927856, 0.1991746574640274, 0.2504645586013794, 0.18404924869537354, 0.11209249496459961, 0.117496058344841, 0.33037397265434265, 0.06213594600558281, 0.06275246292352676, 0.13766595721244812, 0.12480110675096512, 0.09620458632707596, 0.11285291612148285, 0.21950839459896088, 0.1941320300102234, 0.3262842893600464, 0.13010334968566895, 0.21602173149585724, 0.10511474311351776, 0.20514465868473053, 0.13076546788215637, 0.10863526910543442, 0.12069578468799591, 0.19176794588565826, 0.17105485498905182, 0.136264830827713, 0.16532039642333984, 0.3916027545928955, 0.12245017290115356, 0.2001306563615799, 0.19818280637264252, 0.1768054962158203, 0.1317034512758255, 0.11520428955554962, 0.11848229914903641, 0.06858605891466141, 0.2069036215543747, 0.13605912029743195, 0.12103743851184845, 0.27927881479263306, 0.2074371725320816, 0.06997314095497131, 0.06974608451128006, 0.11151684075593948, 0.10642323642969131, 0.24470026791095734, 0.07071057707071304, 0.11952484399080276, 0.20176097750663757, 0.11521115154027939, 0.20226284861564636, 0.06913436204195023, 0.1346987783908844, 0.2139020711183548, 0.17362414300441742, 0.2054499089717865, 0.18493130803108215, 0.10484328866004944, 0.1911294162273407, 0.1200142353773117, 0.25519800186157227, 0.12948916852474213, 0.22877943515777588, 0.11758533120155334, 0.11396196484565735, 0.2153748869895935, 0.07053497433662415, 0.09571926295757294, 0.12844985723495483, 0.1982962042093277, 0.1775977611541748, 0.10150010883808136, 0.07027594745159149, 0.12264183908700943, 0.13967996835708618, 0.07196642458438873, 0.1529230773448944, 0.10512150079011917, 0.120465949177742, 0.1694006323814392, 0.17048828303813934, 0.25434204936027527, 0.12120465189218521, 0.1634712517261505, 0.06807941198348999, 0.06792157888412476, 0.30250996351242065, 0.2068002074956894, 0.11697404831647873, 0.09401407092809677, 0.11622010171413422, 0.12771335244178772, 0.14220628142356873, 0.13203741610050201, 0.1246662363409996, 0.0664113238453865, 0.12492670118808746, 0.1378834843635559, 0.21665945649147034, 0.12390922009944916, 0.19797198474407196, 0.11234324425458908, 0.12546169757843018, 0.19071424007415771, 0.12389515340328217, 0.10736262053251266, 0.10373694449663162, 0.06373148411512375, 0.06348799169063568, 0.20170962810516357, 0.062383897602558136, 0.061278652399778366, 0.061908118426799774, 0.20179376006126404, 0.06018022447824478, 0.179917573928833, 0.132030189037323, 0.16006746888160706, 0.12591005861759186, 0.10388192534446716, 0.11255054175853729, 0.17069154977798462, 0.1385505348443985, 0.11624263972043991, 0.11944027990102768, 0.10609197616577148, 0.056437015533447266, 0.21667832136154175, 0.11634858697652817, 0.054646607488393784, 0.1425648182630539, 0.22119711339473724, 0.10572796314954758, 0.13646025955677032, 0.12008259445428848, 0.053709253668785095, 0.1038941815495491, 0.12953101098537445, 0.31202754378318787, 0.05298798158764839, 0.3588363826274872, 0.14859984815120697, 0.12972217798233032, 0.21931487321853638, 0.05218365043401718, 0.05198917165398598, 0.31499236822128296, 0.09518548846244812, 0.18177354335784912, 0.3094158470630646, 0.05319720134139061, 0.11415436118841171, 0.12267813831567764, 0.05361417680978775, 0.11269296705722809, 0.3177790343761444, 0.05393107607960701, 0.1259569227695465, 0.11048845946788788, 0.11831233650445938, 0.19465507566928864, 0.22310252487659454, 0.05434929579496384, 0.054336316883563995, 0.1090788021683693, 0.054334644228219986, 0.14298538863658905, 0.11587969958782196, 0.05340602621436119, 0.12738633155822754, 0.052850108593702316, 0.13960899412631989, 0.448604941368103, 0.13244111835956573, 0.20358936488628387, 0.05236687883734703, 0.1920214593410492, 0.10340269654989243, 0.17626769840717316, 0.13190343976020813, 0.12990106642246246, 0.12710849940776825, 0.20653003454208374, 0.05355103686451912, 0.20091773569583893, 0.18728391826152802, 0.1173943355679512, 0.1784256249666214, 0.10748536884784698, 0.20536492764949799, 0.10697650164365768, 0.22891901433467865, 0.13032642006874084, 0.3432244062423706, 0.13757407665252686, 0.09505785256624222, 0.11780322343111038, 0.2935999035835266, 0.19068682193756104, 0.1920352578163147, 0.05670830234885216, 0.12627723813056946, 0.1312635987997055, 0.12381117045879364, 0.11388638615608215, 0.19328294694423676, 0.10276956856250763, 0.056922927498817444, 0.05682851001620293, 0.19502370059490204, 0.12578533589839935, 0.22679176926612854, 0.11093296110630035, 0.1125926673412323, 0.05647411197423935, 0.10962290316820145, 0.05607738345861435, 0.3432244062423706, 0.1408768743276596, 0.21644259989261627, 0.2877247631549835, 0.05647594481706619, 0.13382254540920258, 0.056657321751117706, 0.05664996802806854, 0.19976092875003815, 0.213692769408226, 0.12401887774467468, 0.11143787205219269, 0.05651836469769478, 0.22202572226524353, 0.056231167167425156, 0.05633869022130966, 0.12801706790924072, 0.11292612552642822, 0.3199285864830017, 0.05546281114220619, 0.19984115660190582, 0.21215137839317322, 0.19495157897472382, 0.056099824607372284, 0.12652122974395752, 0.05569463595747948, 0.05588553473353386, 0.21193815767765045, 0.10920663177967072, 0.20166653394699097, 0.09981754422187805, 0.29181307554244995, 0.05520547181367874, 0.10870368033647537, 0.13074925541877747, 0.05522778257727623, 0.30185624957084656, 0.11037915199995041, 0.31583085656166077, 0.05555510148406029, 0.05582389980554581, 0.10883281379938126, 0.12569041550159454, 0.11394193768501282, 0.20459307730197906, 0.12259206175804138, 0.20885562896728516, 0.11085709184408188, 0.16835418343544006, 0.20497408509254456, 0.3105897903442383, 0.10957204550504684, 0.1209956705570221, 0.1143612265586853, 0.05702078714966774, 0.05726552754640579, 0.18656286597251892, 0.21581771969795227, 0.12059961259365082, 0.05720718950033188, 0.11483051627874374, 0.10398987680673599, 0.13858754932880402, 0.2143934667110443, 0.05689543858170509, 0.20136581361293793, 0.22055408358573914, 0.20643362402915955, 0.1110771968960762, 0.20633943378925323, 0.187792107462883, 0.05711549147963524, 0.1027798280119896, 0.11665621399879456, 0.056704387068748474, 0.31297287344932556, 0.056845005601644516, 0.05700796842575073, 0.20020009577274323, 0.05679706111550331, 0.21091711521148682, 0.11743676662445068, 0.11777931451797485, 0.10431109368801117, 0.3176606595516205, 0.056764520704746246, 0.34791111946105957, 0.17819325625896454, 0.42283087968826294, 0.20910607278347015, 0.20533235371112823, 0.1072123572230339, 0.05955077335238457, 0.1057485044002533, 0.12305077165365219, 0.06053990498185158, 0.11898977309465408, 0.18398402631282806, 0.19304779171943665, 0.12025092542171478, 0.06109737977385521, 0.18194212019443512, 0.18983511626720428, 0.11949274688959122, 0.27723655104637146, 0.0619635209441185, 0.12904934585094452, 0.29652243852615356, 0.1826527714729309, 0.20718541741371155, 0.1243559718132019, 0.06298857182264328, 0.19142726063728333, 0.1893671303987503, 0.2827337384223938, 0.10126957297325134, 0.06463086605072021, 0.1259128451347351, 0.11077649891376495, 0.1064770370721817, 0.1097056046128273, 0.2006501704454422, 0.12365466356277466, 0.14237435162067413, 0.17976635694503784, 0.06482954323291779, 0.1408640742301941, 0.06468454748392105, 0.39156749844551086, 0.11845237016677856, 0.0648483857512474, 0.1813143938779831, 0.1099010705947876, 0.18458513915538788, 0.12921179831027985, 0.19950005412101746, 0.06516832113265991, 0.16441956162452698, 0.18805749714374542, 0.12211614847183228, 0.06514538824558258, 0.21399615705013275, 0.12679484486579895, 0.1946665644645691, 0.20024904608726501, 0.191365584731102, 0.06509289145469666, 0.06533483415842056, 0.06536442041397095, 0.2013595700263977, 0.12800081074237823, 0.30327919125556946, 0.06502784043550491, 0.2689428925514221, 0.12799224257469177, 0.10646878182888031, 0.11245477199554443, 0.29182520508766174, 0.11433158814907074, 0.1818709671497345, 0.19367800652980804, 0.06587795913219452, 0.29198941588401794, 0.06631878018379211, 0.1245761290192604, 0.06659360975027084, 0.06690680980682373, 0.28651559352874756, 0.12586377561092377, 0.27485784888267517, 0.1716311126947403, 0.27170345187187195, 0.2108483612537384, 0.19864027202129364, 0.19555605947971344, 0.2111375629901886, 0.12335440516471863, 0.2315303087234497, 0.27638882398605347, 0.19761958718299866, 0.1975288987159729, 0.20009587705135345, 0.07202836871147156, 0.07256251573562622, 0.13879138231277466, 0.35940754413604736, 0.1126261055469513, 0.12923456728458405, 0.12436693906784058, 0.10888087004423141, 0.12251152098178864, 0.07471512258052826, 0.19704818725585938, 0.18206308782100677, 0.1820051372051239, 0.16547006368637085, 0.07493308931589127, 0.20277738571166992, 0.17877714335918427, 0.29295212030410767, 0.27578017115592957, 0.07564585655927658, 0.18971124291419983, 0.18264150619506836, 0.2231295257806778, 0.07721731811761856, 0.13367775082588196, 0.1390066146850586, 0.19905661046504974, 0.2022087574005127, 0.07775887101888657, 0.271518737077713, 0.0777164101600647, 0.07815291732549667, 0.13351713120937347, 0.18399125337600708, 0.1749202013015747, 0.07756020128726959, 0.2690615653991699, 0.07770402729511261, 0.07786650955677032, 0.2698761522769928, 0.12421572208404541, 0.13305097818374634, 0.20163452625274658, 0.07775849103927612, 0.1316397488117218, 0.1409440040588379, 0.07762965559959412, 0.11355752497911453, 0.18355515599250793, 0.2692616581916809, 0.20976629853248596, 0.11647039651870728, 0.07657644897699356, 0.07648798823356628, 0.12382853776216507, 0.12193308025598526, 0.2526610195636749, 0.11044661700725555, 0.14500653743743896, 0.12482692301273346, 0.07540226727724075, 0.1877048909664154, 0.18209752440452576, 0.13270659744739532, 0.19781672954559326, 0.20799846947193146, 0.07447286695241928, 0.12174098193645477, 0.13173137605190277, 0.07431778311729431, 0.12401790171861649, 0.12756095826625824, 0.29179683327674866, 0.25801751017570496, 0.16343705356121063, 0.18652190268039703, 0.07371198385953903, 0.07339291274547577, 0.12287216633558273, 0.13147510588169098, 0.07332682609558105, 0.11647097766399384, 0.07289502024650574, 0.11972071975469589, 0.1756201535463333, 0.07232982665300369, 0.2789337933063507, 0.11817514896392822, 0.1881665140390396, 0.1229230985045433, 0.12567372620105743, 0.17746064066886902, 0.1840234249830246, 0.181322380900383, 0.11895977705717087, 0.18452158570289612, 0.11254516988992691, 0.12475927174091339, 0.1978304386138916, 0.36548134684562683, 0.26963287591934204, 0.11673606187105179, 0.11765962839126587, 0.1311415731906891, 0.07288599014282227, 0.19658836722373962, 0.20508970320224762, 0.18831294775009155, 0.12633594870567322, 0.07339299470186234, 0.11366456001996994, 0.12416835874319077, 0.2882460355758667, 0.19579167664051056, 0.07408927381038666, 0.20884516835212708, 0.11344145983457565, 0.07364357262849808, 0.1168399527668953, 0.07378090173006058, 0.12825164198875427, 0.07311158627271652, 0.20190994441509247, 0.20465275645256042, 0.2918974757194519, 0.10777127742767334, 0.07247263938188553, 0.11657483875751495, 0.07248303294181824, 0.12129033356904984, 0.1983398199081421, 0.16419914364814758, 0.17731580138206482, 0.12936539947986603, 0.16818326711654663, 0.11859728395938873, 0.26057833433151245, 0.11049157381057739, 0.07180046290159225, 0.11274480819702148, 0.2614094614982605, 0.10830225795507431, 0.12788517773151398, 0.07182831317186356, 0.20507590472698212, 0.13842590153217316, 0.18668216466903687, 0.0716019719839096, 0.11775729060173035, 0.19700750708580017, 0.13180780410766602, 0.19511312246322632, 0.1254143863916397, 0.16848015785217285, 0.1925695240497589, 0.07122693210840225, 0.1259205937385559, 0.12914712727069855, 0.12504178285598755, 0.07076125591993332, 0.36990851163864136, 0.11526231467723846, 0.11490599811077118, 0.2739678621292114, 0.19596916437149048, 0.2678747773170471, 0.11500442028045654, 0.2470189779996872, 0.13215942680835724, 0.14151285588741302, 0.0722428485751152, 0.07221897691488266, 0.07233462482690811, 0.0721830204129219, 0.2025706171989441, 0.17883415520191193, 0.27940404415130615, 0.18504078686237335, 0.1327579766511917, 0.07252157479524612, 0.12738290429115295, 0.07265029847621918, 0.07254250347614288, 0.26390692591667175, 0.2142568677663803, 0.129924476146698, 0.10183090716600418, 0.07211091369390488, 0.13624343276023865, 0.21362054347991943, 0.1168326884508133, 0.18181249499320984, 0.26424625515937805, 0.07180427014827728, 0.27273210883140564, 0.19300253689289093, 0.07224053889513016, 0.13453848659992218, 0.1217069923877716, 0.07188812643289566, 0.1984611451625824, 0.19318100810050964, 0.11737734079360962, 0.0717482939362526, 0.12218853086233139, 0.0715709999203682, 0.18026742339134216, 0.07113581150770187, 0.11875961720943451, 0.12520655989646912, 0.3002778887748718, 0.13247652351856232, 0.11551051586866379, 0.07044587284326553, 0.2627001702785492, 0.21287107467651367, 0.21156421303749084, 0.07054010033607483, 0.11124390363693237, 0.11426356434822083, 0.13005276024341583, 0.11373214423656464, 0.19389954209327698, 0.13206468522548676, 0.19252042472362518, 0.12027180939912796, 0.11157006025314331, 0.19125820696353912, 0.3007139563560486, 0.06961766630411148, 0.12774711847305298, 0.12244009226560593, 0.06989093869924545, 0.10674510151147842, 0.06945227831602097, 0.11560281366109848, 0.12544476985931396, 0.06885529309511185, 0.37455132603645325, 0.06845733523368835, 0.2040187269449234, 0.12372070550918579, 0.20652459561824799, 0.17060060799121857, 0.12752406299114227, 0.26453742384910583, 0.1272420585155487, 0.11611095815896988, 0.139114111661911, 0.18203742802143097, 0.06880836933851242, 0.18813692033290863, 0.06915140151977539, 0.11795563995838165, 0.11650986224412918, 0.2044428586959839, 0.06862510740756989, 0.17227637767791748, 0.12345275282859802, 0.18377874791622162, 0.13096725940704346, 0.137104794383049, 0.11995650827884674, 0.11003778874874115, 0.20448416471481323, 0.06805907934904099, 0.21092207729816437, 0.19048534333705902, 0.22052520513534546, 0.1979934275150299, 0.1322968304157257, 0.13503916561603546, 0.20352715253829956, 0.19087034463882446, 0.12164672464132309, 0.12054841965436935, 0.06765852123498917, 0.2836253046989441, 0.11281608045101166, 0.11041195690631866, 0.06766655296087265, 0.2859879434108734, 0.27957066893577576, 0.12561850249767303, 0.12007459998130798, 0.0680379867553711, 0.12304623425006866, 0.19340930879116058, 0.06831526011228561, 0.10742558538913727, 0.11302996426820755, 0.12878991663455963, 0.1165168508887291, 0.0675017386674881, 0.20210744440555573, 0.17745326459407806, 0.06707095354795456, 0.17918908596038818, 0.1306159347295761, 0.11981778591871262, 0.12452926486730576, 0.2948744595050812, 0.12623527646064758, 0.0665045976638794, 0.1133643314242363, 0.12357837706804276, 0.09988193213939667, 0.1211671382188797, 0.11425773054361343, 0.13450737297534943, 0.11706981062889099, 0.13577647507190704, 0.19464273750782013, 0.12251846492290497, 0.12166235595941544, 0.18785108625888824, 0.184297576546669, 0.12473861128091812, 0.12023542076349258, 0.14326566457748413, 0.06458740681409836, 0.2688398063182831, 0.19887199997901917, 0.11175629496574402, 0.12902450561523438, 0.18236464262008667, 0.1321631819009781, 0.11806958168745041, 0.12694858014583588, 0.10934241861104965, 0.12122225016355515, 0.19377891719341278, 0.12681402266025543, 0.19657249748706818, 0.06400048732757568, 0.11399383842945099, 0.2009548395872116, 0.20935091376304626, 0.10622841119766235, 0.21579279005527496, 0.1321709007024765, 0.12386442720890045, 0.11484271287918091, 0.11750414222478867, 0.27174609899520874, 0.12897507846355438, 0.18127897381782532, 0.18759405612945557, 0.11844633519649506, 0.13195890188217163, 0.18188825249671936, 0.14052125811576843, 0.11234598606824875, 0.2837512493133545, 0.1148800402879715, 0.10886652022600174, 0.10699766129255295, 0.198293074965477, 0.11405744403600693, 0.1873902827501297, 0.06386983394622803, 0.18125173449516296, 0.12063917517662048, 0.29323339462280273, 0.17379672825336456, 0.0639328733086586, 0.12132740765810013, 0.1149669736623764, 0.140774205327034, 0.19954073429107666, 0.13648228347301483, 0.06403674185276031, 0.2719038724899292, 0.06409362703561783, 0.12169243395328522, 0.13259808719158173, 0.1849890649318695, 0.2021191567182541, 0.18398529291152954, 0.28782209753990173, 0.06431317329406738, 0.11327622085809708, 0.17251352965831757, 0.19341517984867096, 0.2074098289012909, 0.12630324065685272, 0.38992270827293396, 0.06486926227807999, 0.14095334708690643, 0.12090151011943817, 0.19719910621643066, 0.181893453001976, 0.06546298414468765, 0.18656325340270996, 0.18142364919185638, 0.12032255530357361, 0.12096187472343445, 0.13456225395202637, 0.065852090716362, 0.06586462259292603, 0.1143401712179184, 0.2703738212585449, 0.174927219748497, 0.06571230292320251, 0.20539645850658417, 0.11630290001630783, 0.21223631501197815, 0.06591884791851044, 0.18842267990112305, 0.2578218877315521, 0.18976867198944092, 0.26482611894607544, 0.06631576269865036, 0.06643076241016388, 0.11903247982263565, 0.11536628752946854, 0.17531225085258484, 0.12323243916034698, 0.19223769009113312, 0.12148494273424149, 0.12108708918094635, 0.12082479894161224, 0.3826875686645508, 0.06682202965021133, 0.10718708485364914, 0.21814042329788208, 0.22836709022521973, 0.12627416849136353, 0.13496562838554382, 0.2928290069103241, 0.06729297339916229, 0.19486168026924133, 0.11845061182975769, 0.12885485589504242, 0.21028825640678406, 0.06734546273946762, 0.1329953819513321, 0.12527374923229218, 0.2839566469192505, 0.21046285331249237, 0.06732558459043503, 0.1843242049217224, 0.20567229390144348, 0.06749597936868668, 0.19684013724327087, 0.1831243634223938, 0.20603686571121216, 0.2692747414112091, 0.11560073494911194, 0.0679435208439827, 0.06805596500635147, 0.06791230291128159, 0.09922384470701218, 0.21097925305366516, 0.06813381612300873, 0.1200314611196518, 0.28115516901016235, 0.22274230420589447, 0.06831865012645721, 0.06781375408172607, 0.11909298598766327, 0.1910773664712906, 0.37737491726875305, 0.06792430579662323, 0.1299929916858673, 0.06818468868732452, 0.13105840981006622, 0.18977805972099304, 0.06824374198913574, 0.11827411502599716, 0.12561556696891785, 0.06806264072656631, 0.11553461849689484, 0.2177177220582962, 0.276649534702301, 0.13349023461341858, 0.06762873381376266, 0.06786739081144333, 0.06759925931692123, 0.11907817423343658, 0.19293847680091858, 0.06737570464611053, 0.18859858810901642, 0.2161705493927002, 0.06709402054548264, 0.1245533674955368, 0.06685802340507507, 0.2886936068534851, 0.06682317703962326, 0.13738907873630524, 0.0667981207370758, 0.18620748817920685, 0.18740347027778625, 0.18957413733005524, 0.13586384057998657, 0.20899488031864166, 0.10680768638849258, 0.11656668782234192, 0.06648845225572586, 0.12019633501768112, 0.0663180723786354, 0.06605223566293716, 0.20974352955818176, 0.12227748334407806, 0.11406401544809341, 0.18187682330608368, 0.0656503215432167, 0.11612901091575623, 0.10507974028587341, 0.1343359351158142, 0.19008830189704895, 0.10900209099054337, 0.28359097242355347, 0.2128649652004242, 0.20936262607574463, 0.13188937306404114, 0.1966181993484497, 0.13790805637836456, 0.18817965686321259, 0.12252439558506012, 0.17865319550037384, 0.19427475333213806, 0.11930471658706665, 0.11332789808511734, 0.12455359846353531, 0.20780111849308014, 0.20778056979179382, 0.10860302299261093, 0.12898623943328857, 0.06543009728193283, 0.12343404442071915, 0.20050837099552155, 0.2698832154273987, 0.19343878328800201, 0.06550821661949158, 0.12481951713562012, 0.06554877758026123, 0.13108476996421814, 0.27778053283691406, 0.13871616125106812, 0.06539016216993332, 0.2585064768791199, 0.20319144427776337, 0.20818351209163666, 0.11681544035673141, 0.12732160091400146, 0.11416377127170563, 0.06582149863243103, 0.1964312046766281, 0.17982430756092072, 0.06547525525093079, 0.3865911662578583, 0.18524177372455597, 0.27463647723197937, 0.20878708362579346, 0.1971510499715805, 0.13200095295906067, 0.11618197709321976, 0.12564626336097717, 0.18821020424365997, 0.24825981259346008, 0.11911216378211975, 0.11911126971244812, 0.11990625411272049, 0.12154631316661835, 0.11612895876169205, 0.12815998494625092, 0.29805976152420044, 0.06710102409124374, 0.12615667283535004, 0.12701788544654846, 0.12011617422103882, 0.1863074004650116, 0.12031565606594086, 0.06691517680883408, 0.1922801434993744, 0.17820343375205994, 0.18489038944244385, 0.06684410572052002, 0.11719828844070435, 0.06677165627479553, 0.12342090159654617, 0.06739179790019989, 0.06669899821281433, 0.12401440739631653, 0.06641248613595963, 0.13078510761260986, 0.28520363569259644, 0.12947431206703186, 0.1950894147157669, 0.13292230665683746, 0.21551398932933807, 0.1233757734298706, 0.18362373113632202, 0.1294858157634735, 0.1251160353422165, 0.06647105515003204, 0.06618056446313858, 0.06611824780702591, 0.11337918788194656, 0.10675712674856186, 0.20379677414894104, 0.18037575483322144, 0.11106753349304199, 0.13407307863235474, 0.1734001785516739, 0.13026997447013855, 0.11705852299928665, 0.06541885435581207, 0.13470935821533203, 0.12169671803712845, 0.21401354670524597, 0.11693400889635086, 0.0651167556643486, 0.17441242933273315, 0.11740640550851822, 0.06537340581417084, 0.10678407549858093, 0.06486815959215164, 0.06483464688062668, 0.19480401277542114, 0.06459442526102066, 0.13132867217063904, 0.11800255626440048, 0.06449270993471146, 0.1844557374715805, 0.17309126257896423, 0.3912493884563446, 0.13366389274597168, 0.2818978428840637, 0.12371911108493805, 0.13590076565742493, 0.06455838680267334, 0.1149417832493782, 0.19910022616386414, 0.2099587768316269, 0.11694168299436569, 0.2022591531276703, 0.1306641697883606, 0.1302422732114792, 0.11793828010559082, 0.10685709118843079, 0.06454605609178543, 0.06494268029928207, 0.20458391308784485, 0.21110154688358307, 0.11996407061815262, 0.13025645911693573, 0.06425630301237106, 0.06415536999702454, 0.06440206617116928, 0.1742255836725235, 0.11811158806085587, 0.1984916627407074, 0.06411661952733994, 0.21357804536819458, 0.11171431839466095, 0.18410471081733704, 0.06382232904434204, 0.06409887224435806, 0.1924266219139099, 0.1275283247232437, 0.06380467861890793, 0.06384319812059402, 0.11061292886734009, 0.1127517819404602, 0.2827225923538208, 0.12252701818943024, 0.1232852190732956, 0.13443271815776825, 0.06366574764251709, 0.06347249448299408, 0.11144543439149857, 0.12413027137517929, 0.06340997666120529, 0.0635061115026474, 0.11682842671871185, 0.06328045576810837, 0.2693827152252197, 0.1405145227909088, 0.2905009984970093, 0.11609791964292526, 0.11884669214487076, 0.1930597871541977, 0.1286643147468567, 0.1908484697341919, 0.11719902604818344, 0.12181124091148376, 0.06296791136264801, 0.28661438822746277, 0.12437634915113449, 0.211421936750412, 0.06311114132404327, 0.19087080657482147, 0.12112054228782654, 0.13265547156333923, 0.06295797228813171, 0.1346086859703064, 0.17927595973014832, 0.2013660967350006, 0.3033175766468048, 0.12185315042734146, 0.17967166006565094, 0.29303863644599915, 0.2906333804130554, 0.1229044497013092, 0.11866360902786255, 0.1201244369149208, 0.19456163048744202, 0.20117472112178802, 0.1270948201417923, 0.16152505576610565, 0.10490334779024124, 0.22640079259872437, 0.13011637330055237, 0.1928716003894806, 0.06335093080997467, 0.17186275124549866, 0.202583447098732, 0.2107597440481186, 0.19446749985218048, 0.12149852514266968, 0.20867253839969635, 0.0633622482419014, 0.0634499341249466, 0.12251416593790054, 0.06353536248207092, 0.27503278851509094, 0.18576404452323914, 0.06349902600049973, 0.2002873420715332, 0.1250123530626297, 0.12371520698070526, 0.13213124871253967, 0.11464928090572357, 0.12396960705518723, 0.18288643658161163, 0.11140551418066025, 0.11672802269458771, 0.1218644306063652, 0.06329629570245743, 0.12249011546373367, 0.1941499412059784, 0.18534375727176666, 0.19977352023124695, 0.11597896367311478, 0.12007582187652588, 0.12312637269496918, 0.06345435231924057, 0.2904983162879944, 0.06336008012294769, 0.13071273267269135, 0.12467788904905319, 0.06366561353206635, 0.19241639971733093, 0.1978442221879959, 0.12635858356952667, 0.1134817823767662, 0.27910149097442627, 0.12840227782726288, 0.21498021483421326, 0.12338566780090332, 0.13178297877311707, 0.27054518461227417, 0.06321311742067337, 0.06344782561063766, 0.1104808822274208, 0.16874317824840546, 0.21330779790878296, 0.18518899381160736, 0.11902544647455215, 0.18167108297348022, 0.29421523213386536, 0.06344673037528992, 0.06339070945978165, 0.19872933626174927, 0.06335229426622391, 0.11301717907190323, 0.1958829164505005, 0.18363161385059357]\n",
            "Val loss 0.14272948886666978\n",
            "Val auc roc 0.49709396518683235\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}