{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a106c8c106c24f399c4c004d8aa66cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d00ba4ef8d1401db646624e5a98d966",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df379d7a873e490e88757dd75b40073d",
              "IPY_MODEL_887da3a3e2b744b4b3bce279a92cfa64"
            ]
          }
        },
        "9d00ba4ef8d1401db646624e5a98d966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df379d7a873e490e88757dd75b40073d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0d96c5bfd804745b1c5aacc2342ba03",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2596,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2596,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d950e438e2794572867e5e4c901e2c6f"
          }
        },
        "887da3a3e2b744b4b3bce279a92cfa64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_241e70664adf4f09b02c9f563fef14cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2596/2596 [48:24&lt;00:00,  1.12s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_14511beb2c4d46f79bf1b1678e01d2c5"
          }
        },
        "a0d96c5bfd804745b1c5aacc2342ba03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d950e438e2794572867e5e4c901e2c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "241e70664adf4f09b02c9f563fef14cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "14511beb2c4d46f79bf1b1678e01d2c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4732f740e7ba4a2e869ff3b0b7dca03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65fcda1898b4494ba1a65b5ec3c20db4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a5920ae5c62b4ced919188855a005e78",
              "IPY_MODEL_998edd7b04074b8eb315c30e14e7c4bf"
            ]
          }
        },
        "65fcda1898b4494ba1a65b5ec3c20db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5920ae5c62b4ced919188855a005e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_493b7895a44d46398959df5fe726b2bd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2596,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2596,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_840e128848fd4fb19a2ab62445ea5a3a"
          }
        },
        "998edd7b04074b8eb315c30e14e7c4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3967c215fb2404690a66de52245cd2d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2596/2596 [47:02&lt;00:00,  1.09s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a622ba6a73d407b9af36540fa448394"
          }
        },
        "493b7895a44d46398959df5fe726b2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "840e128848fd4fb19a2ab62445ea5a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3967c215fb2404690a66de52245cd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a622ba6a73d407b9af36540fa448394": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c51e6851c3e499aab5e64cf1f11730c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1bdcc19ec3149269c3adecdc54556d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afb7011b11ad4bb8a90cb0086747239e",
              "IPY_MODEL_7beb56e7668841a1ad3d053ea53c6546"
            ]
          }
        },
        "c1bdcc19ec3149269c3adecdc54556d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afb7011b11ad4bb8a90cb0086747239e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d35c3fdecaa4df6912455cf02ce8fc0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2596,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2596,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91fad6ec16704bd68c2082a308e281c6"
          }
        },
        "7beb56e7668841a1ad3d053ea53c6546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6fc665db8ff405b9afb70f5ffd33be3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2596/2596 [47:10&lt;00:00,  1.09s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d53439ee5b3543cb950b6d78c70358ad"
          }
        },
        "9d35c3fdecaa4df6912455cf02ce8fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91fad6ec16704bd68c2082a308e281c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6fc665db8ff405b9afb70f5ffd33be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d53439ee5b3543cb950b6d78c70358ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d73a70e5-d204-453d-f031-35193fd7ca67"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 33.08 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae51da11-a23b-4f9d-bf54-bf8f1b5f5525"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e319c61-c9a1-48f2-e7a2-70777c064522"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "22a1296a-55e8-42c3-f731-9aa4e4e9a346"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "45c1bceb-f83f-40fb-ed4e-0ec3a80eb7d5"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "bc62f671-b6ee-4a90-eefa-eb62b05a766d"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b49fdee9-fb19-4d98-9c88-66b87a7dfbab"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23bd18b3-313b-4ce6-b255-9573612c5cfd"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "951befa0-7ccc-4662-d22e-f46bfc5a2e39"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "21ba399a-6669-4421-c22f-71638c0e98df"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    At last all of us respect law, So lets file th...\n",
              "1    @TheRestlessQuil Got this from another ex-empl...\n",
              "2    Tanushree Dutta, Who Triggered India's #MeToo,...\n",
              "3    Nothing better than an inspiring, intellectual...\n",
              "4                                        ption - no:2 \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee2cc59a-6581-4dd5-d8bb-1c8bdcdb1575"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 49574815.88B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcee686c-a67f-4faf-cf12-26c53ceff838"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1902820.34B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9d105edb-de04-408a-d81f-8e6a224adacf"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf1c32f-29b7-4ff3-f1f7-f44c4d7c08cc"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54c0a848-d8b1-4054-a4da-6c794e09f960"
      },
      "source": [
        "col_name = \"Support\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762,
          "referenced_widgets": [
            "a106c8c106c24f399c4c004d8aa66cfb",
            "9d00ba4ef8d1401db646624e5a98d966",
            "df379d7a873e490e88757dd75b40073d",
            "887da3a3e2b744b4b3bce279a92cfa64",
            "a0d96c5bfd804745b1c5aacc2342ba03",
            "d950e438e2794572867e5e4c901e2c6f",
            "241e70664adf4f09b02c9f563fef14cb",
            "14511beb2c4d46f79bf1b1678e01d2c5",
            "4732f740e7ba4a2e869ff3b0b7dca03f",
            "65fcda1898b4494ba1a65b5ec3c20db4",
            "a5920ae5c62b4ced919188855a005e78",
            "998edd7b04074b8eb315c30e14e7c4bf",
            "493b7895a44d46398959df5fe726b2bd",
            "840e128848fd4fb19a2ab62445ea5a3a",
            "a3967c215fb2404690a66de52245cd2d",
            "8a622ba6a73d407b9af36540fa448394",
            "1c51e6851c3e499aab5e64cf1f11730c",
            "c1bdcc19ec3149269c3adecdc54556d1",
            "afb7011b11ad4bb8a90cb0086747239e",
            "7beb56e7668841a1ad3d053ea53c6546",
            "9d35c3fdecaa4df6912455cf02ce8fc0",
            "91fad6ec16704bd68c2082a308e281c6",
            "d6fc665db8ff405b9afb70f5ffd33be3",
            "d53439ee5b3543cb950b6d78c70358ad"
          ]
        },
        "outputId": "35da0a7a-c97e-4b1a-b805-b518818436d9"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 10384\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 2596\n",
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a106c8c106c24f399c4c004d8aa66cfb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2596.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1708\n",
            "Train Losses : [0.3875095248222351, 0.6262785792350769, 0.09476656466722488, 0.45682841539382935, 0.23827490210533142, 0.24485938251018524, 0.1433563530445099, 0.22041282057762146, 0.19185695052146912, 0.05782255157828331, 0.1680353581905365, 0.47192054986953735, 0.1083461120724678, 0.5548770427703857, 0.009913020767271519, 0.008575794287025928, 0.1400693953037262, 0.17645268142223358, 0.4141014814376831, 0.8317123651504517, 0.7820740342140198, 0.18009649217128754, 0.15388882160186768, 0.06908165663480759, 0.0917649120092392, 0.2385261207818985, 0.2416849434375763, 0.15309767425060272, 0.18987402319908142, 0.29015129804611206, 0.08125925064086914, 0.2907200753688812, 0.13158820569515228, 0.1295080929994583, 0.28068631887435913, 0.10768212378025055, 0.27521243691444397, 0.08955068141222, 0.19492849707603455, 0.19902878999710083, 0.18809491395950317, 0.44966447353363037, 0.28068602085113525, 0.1191113293170929, 0.17545603215694427, 0.184406116604805, 0.11309836804866791, 0.11465710401535034, 0.1575549989938736, 0.3307154178619385, 0.16332407295703888, 0.20589801669120789, 0.34297701716423035, 0.21273921430110931, 0.1689607948064804, 0.233618825674057, 0.25502681732177734, 0.10421210527420044, 0.21021023392677307, 0.18215106427669525, 0.1515284925699234, 0.20701748132705688, 0.14001286029815674, 0.15739789605140686, 0.16123701632022858, 0.14828208088874817, 0.14978152513504028, 0.21162791550159454, 0.1760147511959076, 0.20275291800498962, 0.20173944532871246, 0.14572910964488983, 0.18484976887702942, 0.17940598726272583, 0.1643674075603485, 0.20860789716243744, 0.1833111196756363, 0.18145014345645905, 0.18804596364498138, 0.18763519823551178, 0.169748917222023, 0.23325863480567932, 0.20690570771694183, 0.15746742486953735, 0.12850357592105865, 0.2053726762533188, 0.1761334240436554, 0.19041799008846283, 0.13297320902347565, 0.17167693376541138, 0.169582799077034, 0.19093914330005646, 0.22807577252388, 0.2415582239627838, 0.14427420496940613, 0.16961176693439484, 0.13723914325237274, 0.1583538055419922, 0.145106703042984, 0.17543204128742218, 0.19211071729660034, 0.11670926958322525, 0.12109065055847168, 0.1843903362751007, 0.18693912029266357, 0.22271482646465302, 0.13148388266563416, 0.1316874772310257, 0.15298829972743988, 0.08967921882867813, 0.13173915445804596, 0.13007129728794098, 0.16580958664417267, 0.26614582538604736, 0.15961214900016785, 0.11783266067504883, 0.3042481243610382, 0.24953129887580872, 0.10427618026733398, 0.21068844199180603, 0.09195062518119812, 0.26224833726882935, 0.10936543345451355, 0.1568017452955246, 0.1339816153049469, 0.17859624326229095, 0.13506890833377838, 0.22454945743083954, 0.12350596487522125, 0.21956531703472137, 0.11873741447925568, 0.11489710956811905, 0.32429829239845276, 0.11350977420806885, 0.26589587330818176, 0.11520058661699295, 0.24346676468849182, 0.2698882520198822, 0.2567288279533386, 0.12218021601438522, 0.23382721841335297, 0.16184446215629578, 0.17053358256816864, 0.14204353094100952, 0.15573473274707794, 0.19913385808467865, 0.22067996859550476, 0.16854743659496307, 0.1298624873161316, 0.22922071814537048, 0.15410706400871277, 0.17932440340518951, 0.19115564227104187, 0.1363779902458191, 0.13608744740486145, 0.13393671810626984, 0.20173269510269165, 0.17749659717082977, 0.1454821527004242, 0.20445530116558075, 0.16329489648342133, 0.21746966242790222, 0.11498535424470901, 0.15394876897335052, 0.18201954662799835, 0.13020038604736328, 0.19733937084674835, 0.1408834010362625, 0.1317186951637268, 0.09735045582056046, 0.09305212646722794, 0.0880315825343132, 0.1671372950077057, 0.1244179829955101, 0.10780031234025955, 0.11213882267475128, 0.26073548197746277, 0.21099090576171875, 0.3028712868690491, 0.19876526296138763, 0.11796977370977402, 0.11141469329595566, 0.282487690448761, 0.11571715027093887, 0.1505877822637558, 0.07301002740859985, 0.28784993290901184, 0.1915549784898758, 0.19281500577926636, 0.12348935753107071, 0.14647634327411652, 0.2558410167694092, 0.1723419576883316, 0.188263937830925, 0.18582268059253693, 0.11964075267314911, 0.16080623865127563, 0.10614464432001114, 0.15374964475631714, 0.1071908250451088, 0.1684388965368271, 0.15081624686717987, 0.17653563618659973, 0.1406489461660385, 0.15633103251457214, 0.14053340256214142, 0.17791405320167542, 0.08757736533880234, 0.19418208301067352, 0.24117764830589294, 0.2898271679878235, 0.08272385597229004, 0.08295504003763199, 0.23559267818927765, 0.3303802013397217, 0.21364162862300873, 0.2427457869052887, 0.2830616533756256, 0.15404711663722992, 0.1632290929555893, 0.13758164644241333, 0.1564313769340515, 0.1268644481897354, 0.19111032783985138, 0.13044291734695435, 0.18179064989089966, 0.18580150604248047, 0.16675284504890442, 0.20651604235172272, 0.16285336017608643, 0.14367711544036865, 0.16178922355175018, 0.20629027485847473, 0.14906267821788788, 0.17440013587474823, 0.1809026002883911, 0.17422465980052948, 0.16964499652385712, 0.15298694372177124, 0.1826344132423401, 0.1707920879125595, 0.21156702935695648, 0.17425566911697388, 0.15762947499752045, 0.18491677939891815, 0.16556759178638458, 0.15415167808532715, 0.1711902618408203, 0.15588177740573883, 0.18070288002490997, 0.1804428994655609, 0.1502217799425125, 0.14856530725955963, 0.18783262372016907, 0.15054951608181, 0.16651701927185059, 0.13601520657539368, 0.15475334227085114, 0.1813037246465683, 0.1537688970565796, 0.24152764678001404, 0.19936148822307587, 0.22046229243278503, 0.12174592167139053, 0.22336767613887787, 0.1219579353928566, 0.12199853360652924, 0.1448971927165985, 0.17723923921585083, 0.11893363296985626, 0.1744619607925415, 0.17527322471141815, 0.10996861755847931, 0.10724702477455139, 0.10372620820999146, 0.16454312205314636, 0.19285482168197632, 0.19874514639377594, 0.09137702733278275, 0.21049998700618744, 0.26654568314552307, 0.1816665381193161, 0.19025345146656036, 0.3196927607059479, 0.18081516027450562, 0.2578369677066803, 0.14356586337089539, 0.11579872667789459, 0.2823781967163086, 0.17446960508823395, 0.14485594630241394, 0.15588758885860443, 0.11630499362945557, 0.1735670417547226, 0.2214161604642868, 0.15005385875701904, 0.14000317454338074, 0.13760097324848175, 0.16438446938991547, 0.17552804946899414, 0.1517442762851715, 0.13549001514911652, 0.16252164542675018, 0.13657692074775696, 0.16222712397575378, 0.22293230891227722, 0.14976754784584045, 0.16965056955814362, 0.19877992570400238, 0.14973729848861694, 0.17993633449077606, 0.24766482412815094, 0.16702663898468018, 0.21006979048252106, 0.16484534740447998, 0.1860668659210205, 0.1436908096075058, 0.14203625917434692, 0.14571155607700348, 0.18283340334892273, 0.20003674924373627, 0.15718011558055878, 0.17306572198867798, 0.14600048959255219, 0.17468276619911194, 0.13383419811725616, 0.21284471452236176, 0.17221421003341675, 0.2332865297794342, 0.164437934756279, 0.2128504067659378, 0.15725740790367126, 0.21926285326480865, 0.18638627231121063, 0.16890956461429596, 0.14355453848838806, 0.14234422147274017, 0.16313762962818146, 0.2041378766298294, 0.1507631093263626, 0.1888405978679657, 0.17766155302524567, 0.17372530698776245, 0.15359963476657867, 0.17105601727962494, 0.13838784396648407, 0.19346371293067932, 0.16961105167865753, 0.13906808197498322, 0.1633550077676773, 0.1616407185792923, 0.1262959986925125, 0.20977535843849182, 0.13912449777126312, 0.19645875692367554, 0.11608054488897324, 0.11310172080993652, 0.1754775047302246, 0.14741681516170502, 0.18356671929359436, 0.14513979852199554, 0.1425342857837677, 0.12904556095600128, 0.24701561033725739, 0.25753509998321533, 0.21837162971496582, 0.18742839992046356, 0.21166250109672546, 0.09387326240539551, 0.18840599060058594, 0.13628262281417847, 0.17420710623264313, 0.22998274862766266, 0.29241514205932617, 0.13687418401241302, 0.21335068345069885, 0.105253204703331, 0.2378421127796173, 0.13252581655979156, 0.15151438117027283, 0.15029354393482208, 0.14524208009243011, 0.17380012571811676, 0.21923701465129852, 0.13809263706207275, 0.11718463897705078, 0.17377620935440063, 0.15125489234924316, 0.16747699677944183, 0.11519637703895569, 0.11390705406665802, 0.11157809942960739, 0.13030433654785156, 0.21848052740097046, 0.18478822708129883, 0.18882066011428833, 0.14344778656959534, 0.14791172742843628, 0.09880857169628143, 0.13039027154445648, 0.24098201096057892, 0.14468669891357422, 0.23985493183135986, 0.13706661760807037, 0.17250098288059235, 0.19721245765686035, 0.19482241570949554, 0.23106610774993896, 0.235608771443367, 0.13600988686084747, 0.18198835849761963, 0.22850145399570465, 0.17996983230113983, 0.13567298650741577, 0.17843568325042725, 0.14648135006427765, 0.17141668498516083, 0.17141593992710114, 0.1362769454717636, 0.19139310717582703, 0.11085709184408188, 0.14670734107494354, 0.13452152907848358, 0.18168306350708008, 0.21196229755878448, 0.1448526531457901, 0.18506774306297302, 0.1904974728822708, 0.17833168804645538, 0.19590678811073303, 0.13746877014636993, 0.10998056828975677, 0.1416550725698471, 0.21856918931007385, 0.10783050954341888, 0.13639041781425476, 0.13589289784431458, 0.2324046492576599, 0.22428655624389648, 0.14763124287128448, 0.18623700737953186, 0.10393498837947845, 0.13812963664531708, 0.17936651408672333, 0.182721346616745, 0.14091579616069794, 0.1314297318458557, 0.17223872244358063, 0.13765893876552582, 0.12827365100383759, 0.18141227960586548, 0.09472954273223877, 0.13579536974430084, 0.24610865116119385, 0.16964896023273468, 0.13251905143260956, 0.1395309716463089, 0.18058450520038605, 0.17859973013401031, 0.123838409781456, 0.12398901581764221, 0.12671233713626862, 0.1864546686410904, 0.19076330959796906, 0.12634998559951782, 0.265377014875412, 0.18207015097141266, 0.1257036328315735, 0.1894056797027588, 0.13138659298419952, 0.13557182252407074, 0.3235103487968445, 0.13387402892112732, 0.18697309494018555, 0.17374561727046967, 0.17552965879440308, 0.24307258427143097, 0.18012270331382751, 0.22926750779151917, 0.23631983995437622, 0.17631325125694275, 0.22623927891254425, 0.14571142196655273, 0.20613771677017212, 0.24683989584445953, 0.14974789321422577, 0.12749360501766205, 0.1760711669921875, 0.13301204144954681, 0.1771608144044876, 0.14453203976154327, 0.1800907552242279, 0.1913166344165802, 0.1701561063528061, 0.18127623200416565, 0.15456195175647736, 0.1593053638935089, 0.1808847188949585, 0.15273208916187286, 0.14808538556098938, 0.1862843781709671, 0.15609686076641083, 0.1633852869272232, 0.21197451651096344, 0.20855197310447693, 0.18899424374103546, 0.16111399233341217, 0.1559922695159912, 0.16302907466888428, 0.14537487924098969, 0.16212144494056702, 0.20722177624702454, 0.16291198134422302, 0.14536431431770325, 0.17100884020328522, 0.14368245005607605, 0.15492379665374756, 0.19948147237300873, 0.16501036286354065, 0.19188594818115234, 0.16994445025920868, 0.13543206453323364, 0.1387876719236374, 0.17890429496765137, 0.19280843436717987, 0.1694149523973465, 0.18633660674095154, 0.14586511254310608, 0.19694784283638, 0.15463513135910034, 0.1855112463235855, 0.18830344080924988, 0.14984539151191711, 0.14972281455993652, 0.20038644969463348, 0.14773064851760864, 0.14116784930229187, 0.1719544380903244, 0.1825104057788849, 0.16543975472450256, 0.20902569591999054, 0.21848826110363007, 0.15858450531959534, 0.20721779763698578, 0.16012950241565704, 0.1572367548942566, 0.13404254615306854, 0.17607508599758148, 0.17387913167476654, 0.19907104969024658, 0.1642894744873047, 0.18844972550868988, 0.13567501306533813, 0.2194153517484665, 0.1498546600341797, 0.18926715850830078, 0.17956721782684326, 0.21709634363651276, 0.18222905695438385, 0.16572266817092896, 0.18874205648899078, 0.13315041363239288, 0.1324576735496521, 0.13133960962295532, 0.14620310068130493, 0.15841947495937347, 0.16306382417678833, 0.16139660775661469, 0.1478751301765442, 0.17177912592887878, 0.15575259923934937, 0.21855857968330383, 0.213462695479393, 0.11162038892507553, 0.13682124018669128, 0.21142101287841797, 0.1402270495891571, 0.1405262053012848, 0.10673325508832932, 0.18879422545433044, 0.1841709315776825, 0.18614253401756287, 0.10201399773359299, 0.14231623709201813, 0.1787141114473343, 0.1326049268245697, 0.09631059318780899, 0.12771564722061157, 0.2342918962240219, 0.22422757744789124, 0.11913624405860901, 0.2476775348186493, 0.09117795526981354, 0.30470043420791626, 0.14052340388298035, 0.23423363268375397, 0.23255044221878052, 0.14391492307186127, 0.12877191603183746, 0.1397525668144226, 0.1439785361289978, 0.14595931768417358, 0.2256876826286316, 0.16011610627174377, 0.2324192374944687, 0.13929884135723114, 0.1840963512659073, 0.21974438428878784, 0.21326947212219238, 0.14339864253997803, 0.1762799471616745, 0.1426558792591095, 0.1358606517314911, 0.11678405106067657, 0.14334364235401154, 0.1516409069299698, 0.14388485252857208, 0.14588913321495056, 0.14617684483528137, 0.21811725199222565, 0.18040254712104797, 0.17352811992168427, 0.18103338778018951, 0.20748278498649597, 0.1744505763053894, 0.21461106836795807, 0.15117371082305908, 0.17176760733127594, 0.17336730659008026, 0.19091437757015228, 0.11574340611696243, 0.22706806659698486, 0.1168881356716156, 0.17010730504989624, 0.18377846479415894, 0.15275362133979797, 0.17285101115703583, 0.18144631385803223, 0.14179734885692596, 0.16996492445468903, 0.20883752405643463, 0.14943237602710724, 0.14864526689052582, 0.14995315670967102, 0.11598564684391022, 0.11484059691429138, 0.16544607281684875, 0.2214261144399643, 0.18836696445941925, 0.1726302057504654, 0.18592633306980133, 0.22282856702804565, 0.1751350611448288, 0.1730639636516571, 0.22219297289848328, 0.14054174721240997, 0.14212515950202942, 0.1704353541135788, 0.11482957005500793, 0.14225275814533234, 0.18974751234054565, 0.17195849120616913, 0.17100778222084045, 0.2556969225406647, 0.1665423959493637, 0.18047493696212769, 0.1805708259344101, 0.18372592329978943, 0.11924317479133606, 0.170473113656044, 0.12012458592653275, 0.2442992478609085, 0.14983133971691132, 0.17821258306503296, 0.12177184969186783, 0.2160530388355255, 0.15494221448898315, 0.18357731401920319, 0.24025966227054596, 0.14109311997890472, 0.16868558526039124, 0.18204063177108765, 0.12578187882900238, 0.16080722212791443, 0.16565391421318054, 0.15265938639640808, 0.15028774738311768, 0.17766360938549042, 0.12157319486141205, 0.13745003938674927, 0.11834216117858887, 0.13572902977466583, 0.14892329275608063, 0.18468844890594482, 0.14231231808662415, 0.1470634639263153, 0.1427106112241745, 0.24317805469036102, 0.2281547337770462, 0.18345220386981964, 0.14287348091602325, 0.23325996100902557, 0.13374440371990204, 0.14122965931892395, 0.13505536317825317, 0.17564000189304352, 0.18928605318069458, 0.18297389149665833, 0.23119384050369263, 0.22996656596660614, 0.1725948303937912, 0.18065950274467468, 0.2769753634929657, 0.14337970316410065, 0.17371048033237457, 0.17919938266277313, 0.1110682412981987, 0.22047638893127441, 0.1478663980960846, 0.22206880152225494, 0.15283142030239105, 0.18357490003108978, 0.11858733743429184, 0.15634854137897491, 0.20475883781909943, 0.21676427125930786, 0.17323018610477448, 0.15086236596107483, 0.23969784379005432, 0.12400789558887482, 0.14992371201515198, 0.14758242666721344, 0.20733363926410675, 0.14298520982265472, 0.15546220541000366, 0.18006159365177155, 0.17438510060310364, 0.16454972326755524, 0.12625566124916077, 0.15039142966270447, 0.17659975588321686, 0.15028324723243713, 0.146841898560524, 0.1795009970664978, 0.17854692041873932, 0.12046493589878082, 0.1512301117181778, 0.1710735261440277, 0.11625642329454422, 0.17304782569408417, 0.2164267748594284, 0.17398184537887573, 0.17719602584838867, 0.16942186653614044, 0.20883060991764069, 0.13521143794059753, 0.1860053837299347, 0.2155456393957138, 0.19051752984523773, 0.18462371826171875, 0.16967956721782684, 0.1152193695306778, 0.179536834359169, 0.13978834450244904, 0.14946717023849487, 0.21077466011047363, 0.16733670234680176, 0.14067547023296356, 0.21787454187870026, 0.14089149236679077, 0.18277516961097717, 0.17990964651107788, 0.1441495269536972, 0.14551781117916107, 0.14551161229610443, 0.21417352557182312, 0.14471176266670227, 0.1800488382577896, 0.14463330805301666, 0.25210708379745483, 0.1535317450761795, 0.18778692185878754, 0.1803431212902069, 0.21251244843006134, 0.145107239484787, 0.14876355230808258, 0.20187774300575256, 0.14548750221729279, 0.14234685897827148, 0.12039642035961151, 0.1701408177614212, 0.2447347640991211, 0.14577172696590424, 0.17071466147899628, 0.1693275421857834, 0.14560140669345856, 0.17344816029071808, 0.1479189246892929, 0.19791212677955627, 0.19675873219966888, 0.2134002447128296, 0.21158024668693542, 0.18232209980487823, 0.20062805712223053, 0.17862951755523682, 0.20048224925994873, 0.16640719771385193, 0.14953480660915375, 0.20215903222560883, 0.15481342375278473, 0.16285882890224457, 0.1395406723022461, 0.17034472525119781, 0.16989408433437347, 0.1553458571434021, 0.14948025345802307, 0.1687597632408142, 0.1557866334915161, 0.16694137454032898, 0.13413110375404358, 0.20387008786201477, 0.16528132557868958, 0.15788830816745758, 0.1554032266139984, 0.17042067646980286, 0.23001599311828613, 0.17128774523735046, 0.18143421411514282, 0.15183942019939423, 0.1712683141231537, 0.16260632872581482, 0.12864549458026886, 0.16837775707244873, 0.15122267603874207, 0.17520856857299805, 0.15497590601444244, 0.17187801003456116, 0.19009654223918915, 0.12223818898200989, 0.16043883562088013, 0.14463578164577484, 0.18405434489250183, 0.18714018166065216, 0.18324390053749084, 0.14676032960414886, 0.20383940637111664, 0.13922078907489777, 0.22428418695926666, 0.18803463876247406, 0.17529545724391937, 0.183125302195549, 0.2124367505311966, 0.14972859621047974, 0.2539648413658142, 0.17440934479236603, 0.18278662860393524, 0.15561217069625854, 0.14739319682121277, 0.18207399547100067, 0.12071842700242996, 0.1780877560377121, 0.17744556069374084, 0.2096480280160904, 0.14000216126441956, 0.1728992611169815, 0.1804090440273285, 0.20412577688694, 0.20388276875019073, 0.1743386685848236, 0.1466560661792755, 0.12632271647453308, 0.2088155299425125, 0.14975230395793915, 0.1983889639377594, 0.17765362560749054, 0.18638253211975098, 0.1916540116071701, 0.13049998879432678, 0.18205803632736206, 0.15342207252979279, 0.1809278428554535, 0.1800130009651184, 0.18248866498470306, 0.15320004522800446, 0.15415231883525848, 0.1526588648557663, 0.1769578605890274, 0.1300457864999771, 0.15449543297290802, 0.15058988332748413, 0.17202547192573547, 0.1473214328289032, 0.17914573848247528, 0.21043746173381805, 0.17800739407539368, 0.1467275470495224, 0.2085370123386383, 0.21334399282932281, 0.14827096462249756, 0.2070680856704712, 0.18232737481594086, 0.17376817762851715, 0.20790447294712067, 0.15278910100460052, 0.1727810800075531, 0.1468907743692398, 0.17826929688453674, 0.1774921715259552, 0.17766129970550537, 0.15136757493019104, 0.20179133117198944, 0.15039117634296417, 0.14999011158943176, 0.14964419603347778, 0.2052670270204544, 0.20015795528888702, 0.17360392212867737, 0.17595933377742767, 0.20298971235752106, 0.17460398375988007, 0.17594003677368164, 0.1793169230222702, 0.1543901562690735, 0.13370844721794128, 0.1563977599143982, 0.17292188107967377, 0.17257890105247498, 0.15589474141597748, 0.15160833299160004, 0.2251691073179245, 0.17704331874847412, 0.17537568509578705, 0.17747072875499725, 0.19685067236423492, 0.16810093820095062, 0.15460647642612457, 0.17868468165397644, 0.15415722131729126, 0.2201763540506363, 0.2188306301832199, 0.13722839951515198, 0.21505101025104523, 0.1573234498500824, 0.15750724077224731, 0.20988406240940094, 0.17682750523090363, 0.171102836728096, 0.16913045942783356, 0.17501971125602722, 0.16157227754592896, 0.16207484900951385, 0.14970515668392181, 0.165098175406456, 0.1566370576620102, 0.17065194249153137, 0.19207990169525146, 0.16271628439426422, 0.17355100810527802, 0.1620294153690338, 0.16712763905525208, 0.19130225479602814, 0.18572492897510529, 0.19043253362178802, 0.1589265763759613, 0.1455705463886261, 0.1850247085094452, 0.14528106153011322, 0.18990889191627502, 0.16890792548656464, 0.14457936584949493, 0.1927146017551422, 0.17182448506355286, 0.1800847202539444, 0.15944738686084747, 0.18911698460578918, 0.1714387834072113, 0.1737448275089264, 0.19680461287498474, 0.1441022753715515, 0.17881451547145844, 0.15722285211086273, 0.1648218184709549, 0.14293795824050903, 0.18475697934627533, 0.17329305410385132, 0.1582326889038086, 0.1985776275396347, 0.1983192265033722, 0.13974179327487946, 0.18776941299438477, 0.16349443793296814, 0.15068677067756653, 0.17464511096477509, 0.13803324103355408, 0.179269477725029, 0.15591832995414734, 0.19495633244514465, 0.1993022859096527, 0.17198745906352997, 0.19738909602165222, 0.15600766241550446, 0.15472735464572906, 0.1580875813961029, 0.15436230599880219, 0.1553163230419159, 0.22251108288764954, 0.19837115705013275, 0.20009595155715942, 0.15505868196487427, 0.13496074080467224, 0.19468636810779572, 0.1746479868888855, 0.17176441848278046, 0.19954583048820496, 0.17281346023082733, 0.199440136551857, 0.17329734563827515, 0.1566891074180603, 0.15620870888233185, 0.14019106328487396, 0.16024787724018097, 0.16382738947868347, 0.19855093955993652, 0.21515622735023499, 0.15619415044784546, 0.1387888640165329, 0.1746917963027954, 0.17570555210113525, 0.15745334327220917, 0.157123863697052, 0.15827274322509766, 0.1514660120010376, 0.17324087023735046, 0.17506280541419983, 0.1526680886745453, 0.15409225225448608, 0.1304393708705902, 0.15030018985271454, 0.20331025123596191, 0.17730624973773956, 0.1490650326013565, 0.17300434410572052, 0.1463644653558731, 0.14946231245994568, 0.1759897768497467, 0.17756426334381104, 0.17641793191432953, 0.11786350607872009, 0.14666488766670227, 0.14495518803596497, 0.17373573780059814, 0.14247795939445496, 0.14294372498989105, 0.22015967965126038, 0.17464806139469147, 0.1793907880783081, 0.18110336363315582, 0.13878731429576874, 0.17925776541233063, 0.18049907684326172, 0.18109364807605743, 0.2240331768989563, 0.21756011247634888, 0.14404189586639404, 0.17817705869674683, 0.10913323611021042, 0.2181372046470642, 0.1427101194858551, 0.17954134941101074, 0.14456972479820251, 0.17712828516960144, 0.11056774854660034, 0.14248040318489075, 0.14097310602664948, 0.1419292390346527, 0.17551535367965698, 0.17914070188999176, 0.17919199168682098, 0.17954878509044647, 0.1420275866985321, 0.1838575154542923, 0.18321508169174194, 0.17909546196460724, 0.22410675883293152, 0.222572460770607, 0.1762746274471283, 0.21901260316371918, 0.217259481549263, 0.17877404391765594, 0.14139331877231598, 0.1441105455160141, 0.17588435113430023, 0.2519150674343109, 0.17341910302639008, 0.21250659227371216, 0.17385652661323547, 0.1490822285413742, 0.1240396797657013, 0.1741456240415573, 0.17257747054100037, 0.15098348259925842, 0.203059583902359, 0.17327164113521576, 0.17472955584526062, 0.19948895275592804, 0.15173979103565216, 0.13072407245635986, 0.1548057496547699, 0.1306680589914322, 0.2039663940668106, 0.17382346093654633, 0.15111447870731354, 0.1518874317407608, 0.14883765578269958, 0.12789423763751984, 0.20212163031101227, 0.17520786821842194, 0.20780937373638153, 0.15160305798053741, 0.14626771211624146, 0.17307615280151367, 0.12405022978782654, 0.17812293767929077, 0.21145857870578766, 0.14754509925842285, 0.12171532958745956, 0.18256472051143646, 0.17644326388835907, 0.14695049822330475, 0.17367243766784668, 0.2152116298675537, 0.14800894260406494, 0.18009747564792633, 0.149817556142807, 0.17571361362934113, 0.2496100515127182, 0.17612317204475403, 0.21060512959957123, 0.11880844086408615, 0.17574623227119446, 0.2089117169380188, 0.14667551219463348, 0.14778302609920502, 0.14817719161510468, 0.1764085590839386, 0.1481756567955017, 0.17622575163841248, 0.2418600171804428, 0.1461630016565323, 0.1497548371553421, 0.14774906635284424, 0.17858365178108215, 0.17753483355045319, 0.17561574280261993, 0.17772361636161804, 0.17787182331085205, 0.12249186635017395, 0.17704947292804718, 0.1486554890871048, 0.17863847315311432, 0.17692646384239197, 0.14634260535240173, 0.17674368619918823, 0.14730705320835114, 0.2105361819267273, 0.17617347836494446, 0.17717128992080688, 0.17649488151073456, 0.1455540508031845, 0.14603948593139648, 0.17691011726856232, 0.1764639914035797, 0.14583326876163483, 0.11853543668985367, 0.17541658878326416, 0.14474667608737946, 0.14445993304252625, 0.2138596773147583, 0.1761341542005539, 0.21194039285182953, 0.17993076145648956, 0.17877425253391266, 0.1751466989517212, 0.11543743312358856, 0.17534424364566803, 0.11516597121953964, 0.11457894742488861, 0.17588132619857788, 0.14616426825523376, 0.14453250169754028, 0.17458295822143555, 0.1767304688692093, 0.13881990313529968, 0.13915783166885376, 0.2163325399160385, 0.13840024173259735, 0.1800006777048111, 0.1438855230808258, 0.14225919544696808, 0.13726292550563812, 0.1365811824798584, 0.1811802238225937, 0.10065983235836029, 0.17564834654331207, 0.13278208673000336, 0.2297997623682022, 0.24511359632015228, 0.17814289033412933, 0.13531726598739624, 0.1857454925775528, 0.1851670742034912, 0.13624157011508942, 0.09753263741731644, 0.14087341725826263, 0.09640585631132126, 0.1344720870256424, 0.1313399374485016, 0.24202726781368256, 0.23860089480876923, 0.18890859186649323, 0.13178184628486633, 0.13298283517360687, 0.18222640454769135, 0.18695604801177979, 0.17743907868862152, 0.1817522644996643, 0.1322922706604004, 0.13890133798122406, 0.18544794619083405, 0.18227042257785797, 0.1318102926015854, 0.13302728533744812, 0.18481266498565674, 0.09227703511714935, 0.1787841022014618, 0.13376624882221222, 0.305355429649353, 0.09126845002174377, 0.09138324111700058, 0.1851235330104828, 0.133713498711586, 0.18292567133903503, 0.1314927637577057, 0.24153700470924377, 0.1812559962272644, 0.1847817301750183, 0.13155187666416168, 0.1841806024312973, 0.13183437287807465, 0.09161045402288437, 0.23883217573165894, 0.30315688252449036, 0.09289678931236267, 0.18098761141300201, 0.18062825500965118, 0.13726304471492767, 0.179892897605896, 0.18229320645332336, 0.09736944735050201, 0.09752511978149414, 0.09721559286117554, 0.1349841207265854, 0.1320689618587494, 0.23639056086540222, 0.23235809803009033, 0.13445249199867249, 0.18463528156280518, 0.17856954038143158, 0.1299545019865036, 0.236632838845253, 0.13290809094905853, 0.18780794739723206, 0.17867374420166016, 0.23395207524299622, 0.1413469910621643, 0.17523764073848724, 0.23020321130752563, 0.22665691375732422, 0.18504443764686584, 0.18674424290657043, 0.10489694029092789, 0.1057819202542305, 0.22640171647071838, 0.18582481145858765, 0.10770053416490555, 0.17988285422325134, 0.17738167941570282, 0.14255283772945404, 0.14158302545547485, 0.21840819716453552, 0.14240112900733948, 0.17542339861392975, 0.17924152314662933, 0.13964733481407166, 0.18048977851867676, 0.17854076623916626, 0.10975994169712067, 0.10942152142524719, 0.17855677008628845, 0.21887296438217163, 0.1424718052148819, 0.17774903774261475, 0.1777956187725067, 0.2174736112356186, 0.1803407520055771, 0.14065028727054596, 0.17662280797958374, 0.17538702487945557, 0.2201935052871704, 0.21621595323085785, 0.14150302112102509, 0.1440044790506363, 0.17960107326507568, 0.14324788749217987, 0.1775919646024704, 0.17888684570789337, 0.21525122225284576, 0.21171461045742035, 0.21416814625263214, 0.14737984538078308, 0.17724370956420898, 0.17820462584495544, 0.20556950569152832, 0.12199901789426804, 0.12272277474403381, 0.1508287936449051, 0.20737743377685547, 0.17568176984786987, 0.2046300619840622, 0.15031404793262482, 0.20714634656906128, 0.12547697126865387, 0.17643120884895325, 0.17572984099388123, 0.17589986324310303, 0.15192200243473053, 0.17611397802829742, 0.17674067616462708, 0.14993980526924133, 0.1733863651752472, 0.17726051807403564, 0.1486179083585739, 0.20308060944080353, 0.14688214659690857, 0.17374804615974426, 0.12670402228832245, 0.23301254212856293, 0.1265622228384018, 0.17415212094783783, 0.17831698060035706, 0.20706456899642944, 0.14829391241073608, 0.12669897079467773, 0.17522302269935608, 0.2037041038274765, 0.1261056661605835, 0.17731867730617523, 0.1736341118812561, 0.2021956890821457, 0.17478826642036438, 0.1492755264043808, 0.20333726704120636, 0.15115822851657867, 0.17388682067394257, 0.1727343201637268, 0.20499210059642792, 0.1271635890007019, 0.17721866071224213, 0.15260562300682068, 0.1775156557559967, 0.12701043486595154, 0.1524224579334259, 0.1480996161699295, 0.12475381046533585, 0.1753128319978714, 0.23941771686077118, 0.17749544978141785, 0.17686867713928223, 0.1479489505290985, 0.15108606219291687, 0.20867721736431122, 0.14916831254959106, 0.1486242264509201, 0.1786801964044571, 0.14675495028495789, 0.20481809973716736, 0.2103564590215683, 0.21065887808799744, 0.2080863118171692, 0.15037719905376434, 0.17938043177127838, 0.1232745423913002, 0.17799551784992218, 0.20722036063671112, 0.17463482916355133, 0.17980526387691498, 0.16924411058425903, 0.17253147065639496, 0.16412502527236938, 0.14535200595855713, 0.16883519291877747, 0.23178914189338684, 0.15942932665348053, 0.12858358025550842, 0.16119693219661713, 0.15079811215400696, 0.1531667560338974, 0.14762404561042786, 0.144600048661232, 0.16785942018032074, 0.19677715003490448, 0.1424816995859146, 0.16366638243198395, 0.1685151606798172, 0.18066349625587463, 0.16572347283363342, 0.20989631116390228, 0.2096318155527115, 0.12481538206338882, 0.22092033922672272, 0.1251092106103897, 0.15190061926841736, 0.1919475793838501, 0.19025330245494843, 0.161182701587677, 0.1710381954908371, 0.12328550219535828, 0.14953398704528809, 0.22002100944519043, 0.16710783541202545, 0.16881760954856873, 0.12065163999795914, 0.188745379447937, 0.16736769676208496, 0.14426173269748688, 0.16436930000782013, 0.11796771734952927, 0.1405642181634903, 0.1416226476430893, 0.11492572724819183, 0.21399903297424316, 0.16831205785274506, 0.14209507405757904, 0.2605619430541992, 0.18886525928974152, 0.11126329749822617, 0.15482620894908905, 0.16551606357097626, 0.1556827276945114, 0.13848918676376343, 0.13470745086669922, 0.14656692743301392, 0.1517881155014038, 0.271092027425766, 0.17006680369377136, 0.17239946126937866, 0.13686984777450562, 0.10560322552919388, 0.17161402106285095, 0.16884221136569977, 0.20831362903118134, 0.17812827229499817, 0.13162697851657867, 0.17125076055526733, 0.2307790368795395, 0.10568325966596603, 0.17159850895404816, 0.23122599720954895, 0.1560935080051422, 0.16718356311321259, 0.21804961562156677, 0.16484865546226501, 0.20641577243804932, 0.13086961209774017, 0.1686318963766098, 0.1679629534482956, 0.1356721818447113, 0.1388612538576126, 0.11364951729774475, 0.11344469338655472, 0.16426217555999756, 0.12727749347686768, 0.1476469188928604, 0.2297896146774292, 0.14499856531620026, 0.151182159781456, 0.13188359141349792, 0.15698547661304474, 0.20125143229961395, 0.2011280357837677, 0.20397429168224335, 0.13095438480377197, 0.19082003831863403, 0.23279112577438354, 0.13762293756008148, 0.10474991798400879, 0.1942613124847412, 0.23521368205547333, 0.10379590094089508, 0.103548064827919, 0.1313522905111313, 0.15239830315113068, 0.16604654490947723, 0.1693035364151001, 0.14529603719711304, 0.13800309598445892, 0.1290472000837326, 0.19377127289772034, 0.19763301312923431, 0.13417929410934448, 0.2962128520011902, 0.1255144327878952, 0.19698435068130493, 0.17066335678100586, 0.2341143637895584, 0.2356608659029007, 0.19177696108818054, 0.09901659935712814, 0.1322704255580902, 0.09999306499958038, 0.23831549286842346, 0.1844542771577835, 0.22042316198349, 0.17335791885852814, 0.23732461035251617, 0.13501499593257904, 0.23459935188293457, 0.14151552319526672, 0.17203693091869354, 0.14271336793899536, 0.21821406483650208, 0.21018409729003906, 0.14072531461715698, 0.1421002596616745, 0.12973734736442566, 0.15356101095676422, 0.1482306271791458, 0.13439880311489105, 0.22261185944080353, 0.20411314070224762, 0.2046598196029663, 0.13547390699386597, 0.1411479264497757, 0.14434021711349487, 0.17875860631465912, 0.20850089192390442, 0.1613549292087555, 0.16972970962524414, 0.18682734668254852, 0.16054277122020721, 0.19463589787483215, 0.1201840490102768, 0.1670396476984024, 0.13953140377998352, 0.18997876346111298, 0.14776606857776642, 0.1720181107521057, 0.16562959551811218, 0.1198853924870491, 0.21793194115161896, 0.21380874514579773, 0.15948227047920227, 0.1791788786649704, 0.14700379967689514, 0.1900607794523239, 0.17155256867408752, 0.13715900480747223, 0.16503123939037323, 0.16311369836330414, 0.1868148148059845, 0.14075984060764313, 0.1222192719578743, 0.1818918138742447, 0.15303482115268707, 0.1369606852531433, 0.24506284296512604, 0.1953388750553131, 0.19003739953041077, 0.12060829997062683, 0.12068597227334976, 0.14353817701339722, 0.17986996471881866, 0.18560665845870972, 0.15577879548072815, 0.16736139357089996, 0.21626843512058258, 0.1954105943441391, 0.16388890147209167, 0.11905767023563385, 0.22890318930149078, 0.17357611656188965, 0.16503500938415527, 0.1481722742319107, 0.14324241876602173, 0.14879003167152405, 0.13415947556495667, 0.1655031442642212, 0.14836731553077698, 0.24941863119602203, 0.19595059752464294, 0.14350177347660065, 0.16538740694522858, 0.1845684051513672, 0.13826213777065277, 0.17099611461162567, 0.17413589358329773, 0.16946277022361755, 0.23364560306072235, 0.2276863157749176, 0.18738079071044922, 0.21454493701457977, 0.11833616346120834, 0.2118549346923828, 0.11959749460220337, 0.21916787326335907, 0.1652381271123886, 0.16833271086215973, 0.21092471480369568, 0.18197570741176605, 0.15036652982234955, 0.14344429969787598, 0.14652922749519348, 0.15161846578121185, 0.2055985927581787, 0.23542752861976624, 0.23369812965393066, 0.16080240905284882, 0.17505979537963867, 0.12967121601104736, 0.1762528419494629, 0.17919887602329254, 0.197105273604393, 0.1522957682609558, 0.19804199039936066, 0.17181161046028137, 0.13415533304214478, 0.1727389395236969, 0.1978362649679184, 0.15266603231430054, 0.17605750262737274, 0.19456340372562408, 0.17742866277694702, 0.17312179505825043, 0.17577719688415527, 0.13792841136455536, 0.13796252012252808, 0.15739558637142181, 0.1542627364397049, 0.17564955353736877, 0.1539963036775589, 0.19697768986225128, 0.1743633896112442, 0.17554886639118195, 0.1736682951450348, 0.15172527730464935, 0.17285844683647156, 0.15146368741989136, 0.17779766023159027, 0.1781178116798401, 0.17170418798923492, 0.20311956107616425, 0.22424398362636566, 0.13262927532196045, 0.15513695776462555, 0.17752009630203247, 0.15471166372299194, 0.19660602509975433, 0.17883248627185822, 0.13323566317558289, 0.1749623864889145, 0.15020877122879028, 0.1499481201171875, 0.1523943394422531, 0.17409129440784454, 0.1519964635372162, 0.22728990018367767, 0.15206028521060944, 0.17444351315498352, 0.14877738058567047, 0.20018619298934937, 0.1509581208229065, 0.12902824580669403, 0.17913345992565155, 0.17714622616767883, 0.12753252685070038, 0.15085917711257935, 0.17907632887363434, 0.1503252238035202, 0.17580336332321167, 0.15039435029029846, 0.14558452367782593, 0.1502934992313385, 0.1771058589220047, 0.20980145037174225, 0.20744648575782776, 0.24546246230602264, 0.17930781841278076, 0.14824512600898743, 0.17832642793655396, 0.14922475814819336, 0.2093294858932495, 0.12169988453388214, 0.2067101150751114, 0.14637388288974762, 0.20987862348556519, 0.17340095341205597, 0.20527510344982147, 0.14893986284732819, 0.17581714689731598, 0.14611300826072693, 0.125364288687706, 0.17532703280448914, 0.2036140263080597, 0.15101714432239532, 0.17819230258464813, 0.12536846101284027, 0.17957115173339844, 0.15030048787593842, 0.1506951004266739, 0.12366142123937607, 0.17694006860256195, 0.12187622487545013, 0.14542560279369354, 0.17970211803913116, 0.20630903542041779, 0.17532403767108917, 0.14661738276481628, 0.17951510846614838, 0.17903803288936615, 0.21146491169929504, 0.25058722496032715, 0.17843830585479736, 0.14853577315807343, 0.14535380899906158, 0.14711351692676544, 0.11822401732206345, 0.17526274919509888, 0.14705806970596313, 0.1460796445608139, 0.17570161819458008, 0.1781126707792282, 0.14352934062480927, 0.11498600244522095, 0.21240197122097015, 0.145598366856575, 0.14372572302818298, 0.11235614120960236, 0.17521995306015015, 0.17774009704589844, 0.1806388795375824, 0.14225484430789948, 0.1406044065952301, 0.14124217629432678, 0.13998958468437195, 0.17583470046520233, 0.1364637166261673, 0.13564738631248474, 0.10264575481414795, 0.18140694499015808, 0.13897205889225006, 0.1335635632276535, 0.184737429022789, 0.18348312377929688, 0.2340877801179886, 0.23422780632972717, 0.176480233669281, 0.180161252617836, 0.1830814629793167, 0.09692249447107315, 0.18096809089183807, 0.23047210276126862, 0.09762232005596161, 0.13713133335113525, 0.13608673214912415, 0.2338055968284607, 0.13355641067028046, 0.2334972769021988, 0.13865596055984497, 0.23309226334095, 0.136045441031456, 0.1340342015028, 0.13732457160949707, 0.17969022691249847, 0.1798001378774643, 0.1361687034368515, 0.17765939235687256, 0.1819177269935608, 0.17860376834869385, 0.13612954318523407, 0.13669893145561218, 0.1002504751086235, 0.22940020263195038, 0.09964393079280853, 0.13913807272911072, 0.098641537129879, 0.18048526346683502, 0.23290280997753143, 0.13489024341106415, 0.23185589909553528, 0.1796475052833557, 0.09690597653388977, 0.1803445965051651, 0.23561783134937286, 0.18040136992931366, 0.1346147358417511, 0.22892701625823975, 0.1360844373703003, 0.09893248975276947, 0.13644205033779144, 0.13836558163166046, 0.23149289190769196, 0.17849142849445343, 0.1352783739566803, 0.1355094462633133, 0.180191308259964, 0.13477635383605957, 0.227931410074234, 0.23302847146987915, 0.18199272453784943, 0.23347657918930054, 0.22459347546100616, 0.180739626288414, 0.13784921169281006, 0.1827167421579361, 0.2705274224281311, 0.14413732290267944, 0.17794016003608704, 0.17885586619377136, 0.14366096258163452, 0.14392109215259552, 0.145163431763649, 0.17829975485801697, 0.21304845809936523, 0.14576570689678192, 0.14366835355758667, 0.17839857935905457, 0.14718139171600342, 0.1760225147008896, 0.17575979232788086, 0.1441570669412613, 0.21197764575481415, 0.14801660180091858, 0.1422346979379654, 0.11601399630308151, 0.21263474225997925, 0.14535251259803772, 0.17726832628250122, 0.14395640790462494, 0.17383405566215515, 0.2141391932964325, 0.213874951004982, 0.1470608115196228, 0.21114368736743927, 0.14334283769130707, 0.17806276679039001, 0.14664015173912048, 0.1785755604505539, 0.14625118672847748, 0.17397540807724, 0.14556936919689178, 0.1453157514333725, 0.11647318303585052, 0.21065855026245117, 0.14258691668510437, 0.17855970561504364, 0.11477378755807877, 0.2160443663597107, 0.11385884881019592, 0.14390413463115692, 0.14382708072662354, 0.17587777972221375, 0.1747700273990631, 0.11042413860559464, 0.1095343753695488, 0.14223627746105194, 0.17594033479690552, 0.1806757003068924, 0.17881987988948822, 0.14010745286941528, 0.2748199999332428, 0.22224625945091248, 0.22642043232917786, 0.1814180314540863, 0.27005085349082947, 0.18090268969535828, 0.18049314618110657, 0.1759963184595108, 0.1793886125087738, 0.14403405785560608, 0.14203259348869324, 0.17425759136676788, 0.1419534832239151, 0.17449486255645752, 0.1160234659910202, 0.14224761724472046, 0.21091297268867493, 0.17881722748279572, 0.17589347064495087, 0.14220063388347626, 0.21076825261116028, 0.17453613877296448, 0.17565426230430603, 0.17464427649974823, 0.11823257803916931, 0.17324355244636536, 0.2068725824356079, 0.11871319264173508, 0.14807605743408203, 0.17924809455871582, 0.2065471112728119, 0.14294494688510895, 0.18116720020771027, 0.21364432573318481, 0.1421663761138916, 0.1435941457748413, 0.1825159639120102, 0.17149412631988525, 0.14919251203536987, 0.17536112666130066, 0.17489886283874512, 0.1807071417570114, 0.17966952919960022, 0.17960263788700104, 0.1792588233947754, 0.2065807580947876, 0.12025292962789536, 0.17204727232456207, 0.12045081704854965, 0.17227758467197418, 0.14651034772396088, 0.1494879275560379, 0.17380212247371674, 0.14772990345954895, 0.14672404527664185, 0.1410873532295227, 0.20955634117126465, 0.14529962837696075, 0.14031478762626648, 0.14803148806095123, 0.17450599372386932, 0.1395457684993744, 0.1723065972328186, 0.11232484877109528, 0.14468830823898315, 0.13767923414707184, 0.10925357788801193, 0.181808739900589, 0.1797591745853424, 0.10577869415283203, 0.22094908356666565, 0.1406777948141098, 0.13378332555294037, 0.1765991747379303, 0.22489425539970398, 0.22712136805057526, 0.17905353009700775, 0.22477206587791443, 0.185799703001976, 0.10283559560775757, 0.140090212225914, 0.13992062211036682, 0.13760241866111755, 0.14031264185905457, 0.1021670252084732, 0.17380712926387787, 0.10075979679822922, 0.13771282136440277, 0.13288027048110962, 0.1751491278409958, 0.13027825951576233, 0.18461424112319946, 0.13648374378681183, 0.18730762600898743, 0.1849709451198578, 0.2993752062320709, 0.13703188300132751, 0.17965593934059143, 0.13303309679031372, 0.18457502126693726, 0.17933396995067596, 0.13019579648971558, 0.13712219893932343, 0.13077931106090546, 0.09420192986726761, 0.13554747402668, 0.13187167048454285, 0.13553202152252197, 0.126760333776474, 0.1357281357049942, 0.2396668940782547, 0.13454273343086243, 0.1274779736995697, 0.18625390529632568, 0.2501790523529053, 0.12476391345262527, 0.1824653297662735, 0.1899803876876831, 0.0869520977139473, 0.19280846416950226, 0.1274036318063736, 0.12999454140663147, 0.13419874012470245, 0.18536141514778137, 0.2456101030111313, 0.24898043274879456, 0.1326909065246582, 0.19204330444335938, 0.3153655529022217, 0.237169548869133, 0.23684221506118774, 0.13650016486644745, 0.17415590584278107, 0.1374577283859253, 0.18330596387386322, 0.23263901472091675, 0.1727966070175171, 0.13607847690582275, 0.1725720763206482, 0.13958010077476501, 0.22820214927196503, 0.17202739417552948, 0.17349812388420105, 0.17124652862548828, 0.1865062117576599, 0.21458126604557037, 0.11024974286556244, 0.2254120409488678, 0.18441522121429443, 0.21049918234348297, 0.21389949321746826, 0.20880384743213654, 0.1756071299314499, 0.18439288437366486, 0.17643465101718903, 0.1402791440486908, 0.14645294845104218, 0.14496998488903046, 0.20472745597362518, 0.1744733452796936, 0.15402477979660034, 0.23364539444446564, 0.20445549488067627, 0.20212973654270172, 0.17200550436973572, 0.19492396712303162, 0.17206786572933197, 0.14867395162582397, 0.13438954949378967, 0.14858518540859222, 0.17866137623786926, 0.15185469388961792, 0.135640949010849, 0.17128106951713562, 0.14765039086341858, 0.18352046608924866, 0.22039894759655, 0.18337701261043549, 0.1818895936012268, 0.21923792362213135, 0.17610257863998413, 0.16904059052467346, 0.19332030415534973, 0.1498926728963852, 0.19049254059791565, 0.17207062244415283, 0.18767568469047546, 0.20069116353988647, 0.18343141674995422, 0.15557122230529785, 0.20714452862739563, 0.16493108868598938, 0.1736634522676468, 0.1532624512910843, 0.15279878675937653, 0.1474638283252716, 0.16486306488513947, 0.1470896452665329, 0.1567644327878952, 0.2048550397157669, 0.1518719643354416, 0.16309645771980286, 0.17378444969654083, 0.15135221183300018, 0.20739205181598663, 0.20741957426071167, 0.16318488121032715, 0.1445118486881256, 0.1603693664073944, 0.20708778500556946, 0.17842578887939453, 0.1764649897813797, 0.18274402618408203, 0.16288086771965027, 0.1448611319065094, 0.16071753203868866, 0.15832720696926117, 0.1546531617641449, 0.18826799094676971, 0.14233523607254028, 0.1812860071659088, 0.16145938634872437, 0.1907983273267746, 0.17483381927013397, 0.2132244110107422, 0.1397036463022232, 0.18877623975276947, 0.15957506000995636, 0.15018853545188904, 0.1531207412481308, 0.15975867211818695, 0.17118902504444122, 0.17442686855793, 0.1832403987646103, 0.17860673367977142, 0.19419708847999573, 0.20337636768817902, 0.19130092859268188, 0.15160153806209564, 0.16965588927268982, 0.17058396339416504, 0.1795424371957779, 0.17613743245601654, 0.20058731734752655, 0.13939158618450165, 0.18849648535251617, 0.17683108150959015, 0.19187672436237335, 0.18126513063907623, 0.15596230328083038, 0.16027264297008514, 0.17742635309696198, 0.18058620393276215, 0.17067110538482666, 0.17943793535232544, 0.20990857481956482, 0.17231476306915283, 0.18326158821582794, 0.1975531131029129, 0.1442307084798813, 0.17269061505794525, 0.193013995885849, 0.16145604848861694, 0.17091432213783264, 0.1685078740119934, 0.15528200566768646, 0.1675204038619995, 0.18576039373874664, 0.1771448701620102, 0.1786465048789978, 0.15468984842300415, 0.17429174482822418, 0.172634556889534, 0.1791674941778183, 0.1806086003780365, 0.15920722484588623, 0.20270918309688568, 0.16065186262130737, 0.1856478452682495, 0.17680001258850098, 0.17018988728523254, 0.17874570190906525, 0.1877664476633072, 0.14979544281959534, 0.1729019582271576, 0.1914566457271576, 0.16456817090511322, 0.18827907741069794, 0.18404518067836761, 0.1633300632238388, 0.17749238014221191, 0.16600099205970764, 0.15192753076553345, 0.1701989471912384, 0.17619028687477112, 0.17206819355487823, 0.1639484167098999, 0.17084595561027527, 0.1632770299911499, 0.1623004823923111, 0.17446660995483398, 0.16002574563026428, 0.14846119284629822, 0.17016297578811646, 0.19198454916477203, 0.15755653381347656, 0.15929025411605835, 0.17357000708580017, 0.1724216341972351, 0.15541860461235046, 0.15841253101825714, 0.17707279324531555, 0.1728333979845047, 0.1954883635044098, 0.15883135795593262, 0.15780897438526154, 0.15802784264087677, 0.19978146255016327, 0.15295331180095673, 0.16932225227355957, 0.15197758376598358, 0.19810651242733002, 0.13751104474067688, 0.19355610013008118, 0.1555667668581009, 0.16957105696201324, 0.19426767528057098, 0.13596923649311066, 0.1508009284734726, 0.17859219014644623, 0.17456063628196716, 0.17250892519950867, 0.17010562121868134, 0.17821528017520905, 0.14882220327854156, 0.20335952937602997, 0.13327348232269287, 0.22270020842552185, 0.1800379753112793, 0.15311118960380554, 0.1529117077589035, 0.15461388230323792, 0.13279180228710175, 0.175529345870018, 0.17230115830898285, 0.20457980036735535, 0.1972275823354721, 0.15575174987316132, 0.14793241024017334, 0.1971893459558487, 0.1544370800256729, 0.14841610193252563, 0.19542750716209412, 0.1989498883485794, 0.17737926542758942, 0.1713121235370636, 0.17308717966079712, 0.18148598074913025, 0.17601899802684784, 0.1738000512123108, 0.18058528006076813, 0.15353576838970184, 0.1765175610780716, 0.14808808267116547, 0.19591781497001648, 0.13348659873008728, 0.1333441138267517, 0.17845085263252258, 0.22329936921596527, 0.19334299862384796, 0.1330426037311554, 0.20321395993232727, 0.15798500180244446, 0.19655342400074005, 0.17883563041687012, 0.1340368688106537, 0.19285932183265686, 0.14914384484291077, 0.1542789787054062, 0.19822266697883606, 0.17910897731781006, 0.16839231550693512, 0.13448014855384827, 0.1768520623445511, 0.20200517773628235, 0.17051315307617188, 0.17891959846019745, 0.20149515569210052, 0.14970654249191284, 0.1521531343460083, 0.17523162066936493, 0.1927688866853714, 0.13543395698070526, 0.15366272628307343, 0.1512020230293274, 0.16790319979190826, 0.15663935244083405, 0.1339825540781021, 0.19140329957008362, 0.15381796658039093, 0.1513325572013855, 0.15515126287937164, 0.15490829944610596, 0.15149249136447906, 0.17233388125896454, 0.20358313620090485, 0.17940104007720947, 0.17848481237888336, 0.15213416516780853, 0.1557418406009674, 0.1461561918258667, 0.1752939522266388, 0.2347152829170227, 0.17796377837657928, 0.12531444430351257, 0.17855462431907654, 0.15034924447536469, 0.14737293124198914, 0.14732398092746735, 0.176707461476326, 0.12304629385471344, 0.17623426020145416, 0.20510564744472504, 0.17457301914691925, 0.17330002784729004, 0.14686793088912964, 0.1783597469329834, 0.17645776271820068, 0.21306829154491425, 0.14980916678905487, 0.12043285369873047, 0.17817261815071106, 0.14439940452575684, 0.21474076807498932, 0.1739029586315155, 0.1492835432291031, 0.1761341691017151, 0.18224547803401947, 0.21137315034866333, 0.20355060696601868, 0.18047982454299927, 0.20926238596439362, 0.1482149362564087, 0.20777775347232819, 0.14574679732322693, 0.12278931587934494, 0.18089593946933746, 0.12308338284492493, 0.14723193645477295, 0.23906369507312775, 0.1801772117614746, 0.17338651418685913, 0.15305382013320923, 0.14667324721813202, 0.17501235008239746, 0.17499573528766632, 0.17535580694675446, 0.12340311706066132, 0.2073991894721985, 0.17811964452266693, 0.12321967631578445, 0.15040329098701477, 0.17899958789348602, 0.17645004391670227, 0.1220146119594574, 0.146668940782547, 0.17321668565273285, 0.1766572743654251, 0.11983270198106766, 0.14643293619155884, 0.17537283897399902, 0.14945361018180847, 0.14541617035865784, 0.21118326485157013, 0.14467166364192963, 0.17701537907123566, 0.21203595399856567, 0.14241577684879303, 0.14682964980602264, 0.11357589811086655, 0.14471781253814697, 0.21674653887748718, 0.17757977545261383, 0.18097123503684998, 0.14114685356616974, 0.26007938385009766, 0.14117488265037537, 0.1418367624282837, 0.18024680018424988, 0.14302532374858856, 0.21465648710727692, 0.14252831041812897, 0.2207554131746292, 0.17760689556598663, 0.1455855667591095, 0.1123131662607193, 0.17503783106803894, 0.18129539489746094, 0.17301061749458313, 0.14117592573165894, 0.21515566110610962, 0.2156008630990982, 0.14154498279094696, 0.11287069320678711, 0.14420346915721893, 0.17762690782546997, 0.17883822321891785, 0.1778748482465744, 0.17324243485927582, 0.17680701613426208, 0.17738953232765198, 0.14375311136245728, 0.11260613054037094, 0.17450487613677979, 0.1119501143693924, 0.17944757640361786, 0.1740279495716095, 0.11062780767679214, 0.14378629624843597, 0.14471372961997986, 0.21857687830924988, 0.1402527540922165, 0.17550858855247498, 0.14048190414905548, 0.10653235018253326, 0.17505040764808655, 0.141629159450531, 0.1822027862071991, 0.21976341307163239, 0.17960189282894135, 0.13974612951278687, 0.1375526338815689, 0.17892871797084808, 0.1383766233921051, 0.23005719482898712, 0.18002726137638092, 0.13620984554290771, 0.18222416937351227, 0.2291559875011444, 0.10254810750484467, 0.18284012377262115, 0.1858253926038742, 0.1743062138557434, 0.14047646522521973, 0.18047764897346497, 0.22899220883846283, 0.13511551916599274, 0.13747301697731018, 0.17204585671424866, 0.17632102966308594, 0.22344756126403809, 0.14063672721385956, 0.178240567445755, 0.14233334362506866, 0.18063423037528992, 0.2206909954547882, 0.10619581490755081, 0.21715204417705536, 0.14147934317588806, 0.17852619290351868, 0.22048360109329224, 0.10799289494752884, 0.1788841038942337, 0.13866177201271057, 0.17565372586250305, 0.1779187023639679, 0.10883351415395737, 0.14349372684955597, 0.13503094017505646, 0.13846485316753387, 0.13606801629066467, 0.18192090094089508, 0.10623875260353088, 0.1820017397403717, 0.13857512176036835, 0.10410654544830322, 0.22448880970478058, 0.2236238420009613, 0.1800851672887802, 0.13616682589054108, 0.17842695116996765, 0.13912802934646606, 0.22509565949440002, 0.18367484211921692, 0.13895586133003235, 0.10231631249189377, 0.14151324331760406, 0.13898302614688873, 0.1378740817308426, 0.18569932878017426, 0.13755877315998077, 0.1809612214565277, 0.18713541328907013, 0.1365947425365448, 0.235182523727417, 0.2273094356060028, 0.09831120073795319, 0.1805211752653122, 0.23000887036323547, 0.2282344251871109, 0.18504026532173157, 0.17808832228183746, 0.1370849609375, 0.10168968886137009, 0.22722655534744263, 0.22417612373828888, 0.14055627584457397, 0.1345716416835785, 0.22407177090644836, 0.13711556792259216, 0.27166420221328735, 0.14083117246627808, 0.13922391831874847, 0.10765915364027023, 0.17627449333667755, 0.228382870554924, 0.14471174776554108, 0.17777065932750702, 0.14085836708545685, 0.14019590616226196, 0.17668655514717102, 0.1823890656232834, 0.1395888775587082, 0.10955620557069778, 0.1404728889465332, 0.17727383971214294, 0.14047913253307343, 0.1392393559217453, 0.26737529039382935, 0.18159453570842743, 0.14296914637088776, 0.2197316288948059, 0.18730416893959045, 0.2271611988544464, 0.1409354954957962, 0.13716185092926025, 0.2139589935541153, 0.21976444125175476, 0.11077700555324554, 0.1809176802635193, 0.21902869641780853, 0.13919438421726227, 0.1442819982767105, 0.1436358094215393, 0.1789328157901764, 0.1439915895462036, 0.17377249896526337, 0.17859399318695068, 0.11323030292987823, 0.18348442018032074, 0.14403222501277924, 0.14458923041820526, 0.11207561939954758, 0.14231173694133759, 0.11058016866445541, 0.14392995834350586, 0.21970351040363312, 0.1380702257156372, 0.26762303709983826, 0.22521604597568512, 0.17969520390033722, 0.17995938658714294, 0.18028700351715088, 0.1751665472984314, 0.22000077366828918, 0.10961432754993439, 0.21591472625732422, 0.14127510786056519, 0.17848864197731018, 0.17661894857883453, 0.1400793045759201, 0.21547290682792664, 0.17864401638507843, 0.14360712468624115, 0.11321531981229782, 0.1784753054380417, 0.21863235533237457, 0.14661292731761932, 0.1782301515340805, 0.214013010263443, 0.21185684204101562, 0.17496617138385773, 0.21552589535713196, 0.2135762721300125, 0.17750608921051025, 0.17902278900146484, 0.11959124356508255, 0.21195849776268005, 0.17551885545253754, 0.24055972695350647, 0.14879363775253296, 0.123746357858181, 0.20594975352287292, 0.17640697956085205, 0.17499348521232605, 0.12631674110889435, 0.15255407989025116, 0.17289626598358154, 0.17534084618091583, 0.14691683650016785, 0.20180709660053253, 0.14675912261009216, 0.17324596643447876, 0.12734884023666382, 0.17902444303035736, 0.17543643712997437, 0.17320971190929413, 0.1465923935174942, 0.15124398469924927, 0.12625014781951904, 0.23395118117332458, 0.17423392832279205, 0.20323042571544647, 0.14984500408172607, 0.17642416059970856, 0.2053385227918625, 0.17228390276432037]\n",
            "Val loss 0.16839491124788675\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4732f740e7ba4a2e869ff3b0b7dca03f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2596.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1687\n",
            "Train Losses : [0.2014278918504715, 0.18028314411640167, 0.14784707129001617, 0.19885902106761932, 0.17195408046245575, 0.17233994603157043, 0.15633298456668854, 0.14792031049728394, 0.17364268004894257, 0.17620602250099182, 0.17185428738594055, 0.156385600566864, 0.1542401760816574, 0.13028043508529663, 0.15338990092277527, 0.1787067949771881, 0.17135633528232574, 0.12847423553466797, 0.14994163811206818, 0.15417078137397766, 0.17578479647636414, 0.19978450238704681, 0.17177897691726685, 0.20479974150657654, 0.20645923912525177, 0.20162716507911682, 0.15088367462158203, 0.1259063184261322, 0.150229349732399, 0.12564684450626373, 0.12516584992408752, 0.1514165848493576, 0.15091924369335175, 0.21187274158000946, 0.1755043864250183, 0.1732105016708374, 0.21031351387500763, 0.14742253720760345, 0.14401686191558838, 0.17988371849060059, 0.14397476613521576, 0.14285001158714294, 0.14368687570095062, 0.14784178137779236, 0.11849972605705261, 0.17455197870731354, 0.1764395833015442, 0.14653220772743225, 0.1725548356771469, 0.1780717819929123, 0.21202482283115387, 0.18017582595348358, 0.11411208659410477, 0.21253462135791779, 0.11365656554698944, 0.2109254151582718, 0.1776701956987381, 0.14546208083629608, 0.21223650872707367, 0.14090359210968018, 0.17707398533821106, 0.17373579740524292, 0.17409206926822662, 0.18127161264419556, 0.18257084488868713, 0.1779830902814865, 0.21836699545383453, 0.14731934666633606, 0.11505571752786636, 0.17404793202877045, 0.17800889909267426, 0.13958099484443665, 0.17288696765899658, 0.11501051485538483, 0.18241195380687714, 0.14114417135715485, 0.1406175196170807, 0.2549850046634674, 0.2112983614206314, 0.18077319860458374, 0.13910308480262756, 0.2180328518152237, 0.14989306032657623, 0.11558938026428223, 0.1488327533006668, 0.14290465414524078, 0.17943142354488373, 0.14253869652748108, 0.1435546576976776, 0.13927672803401947, 0.21599368751049042, 0.21170012652873993, 0.21422307193279266, 0.1712406873703003, 0.21015530824661255, 0.14223480224609375, 0.14682187139987946, 0.17246533930301666, 0.13984546065330505, 0.208254873752594, 0.20745912194252014, 0.1814640760421753, 0.1805095672607422, 0.17814500629901886, 0.11833032965660095, 0.1710975617170334, 0.18034672737121582, 0.15216676890850067, 0.17185336351394653, 0.1799011528491974, 0.148744136095047, 0.14920859038829803, 0.11909675598144531, 0.21162815392017365, 0.21512413024902344, 0.14825086295604706, 0.18049749732017517, 0.11866426467895508, 0.14410211145877838, 0.1806238889694214, 0.20914220809936523, 0.21304811537265778, 0.18144181370735168, 0.14875006675720215, 0.2091156542301178, 0.17685234546661377, 0.1190493032336235, 0.1515800505876541, 0.14938347041606903, 0.20840467512607574, 0.17996838688850403, 0.1761874258518219, 0.14832419157028198, 0.21534262597560883, 0.24500620365142822, 0.17953509092330933, 0.12049980461597443, 0.17282600700855255, 0.1793515533208847, 0.12148018181324005, 0.17282453179359436, 0.1787392646074295, 0.12161985784769058, 0.20373021066188812, 0.14424297213554382, 0.1450657993555069, 0.17963159084320068, 0.1782943159341812, 0.14500588178634644, 0.12063906341791153, 0.21055200695991516, 0.14407867193222046, 0.21183359622955322, 0.20683173835277557, 0.11982325464487076, 0.17928564548492432, 0.17267197370529175, 0.17862968146800995, 0.17956365644931793, 0.20813752710819244, 0.20463573932647705, 0.12074434012174606, 0.14380379021167755, 0.12100251764059067, 0.17963626980781555, 0.1487567275762558, 0.14903053641319275, 0.11984079331159592, 0.20759131014347076, 0.14883728325366974, 0.24629652500152588, 0.17350532114505768, 0.11860599368810654, 0.17319302260875702, 0.14868436753749847, 0.20853771269321442, 0.14173035323619843, 0.11803033947944641, 0.14813101291656494, 0.1476874202489853, 0.20809021592140198, 0.14651456475257874, 0.1802680790424347, 0.1419103890657425, 0.18010298907756805, 0.1435311734676361, 0.1752721518278122, 0.14165331423282623, 0.18201017379760742, 0.17764617502689362, 0.14329302310943604, 0.18124613165855408, 0.1743568778038025, 0.1414494812488556, 0.1810709834098816, 0.18174061179161072, 0.14500656723976135, 0.1385914385318756, 0.2604128122329712, 0.17371824383735657, 0.18121971189975739, 0.1445831060409546, 0.1115211546421051, 0.1788872331380844, 0.22229543328285217, 0.1802549511194229, 0.1757812798023224, 0.17468763887882233, 0.14133727550506592, 0.21328066289424896, 0.21251562237739563, 0.1787511706352234, 0.14117497205734253, 0.1770509034395218, 0.14578914642333984, 0.2102668583393097, 0.21678610146045685, 0.11586114764213562, 0.14088355004787445, 0.21650764346122742, 0.21337614953517914, 0.21282707154750824, 0.14305119216442108, 0.11886362731456757, 0.17641763389110565, 0.205283060669899, 0.11991038173437119, 0.21339143812656403, 0.1491076946258545, 0.17291489243507385, 0.17349740862846375, 0.12119998782873154, 0.1459263563156128, 0.1736355423927307, 0.12082327902317047, 0.12042063474655151, 0.17925108969211578, 0.14948832988739014, 0.20691709220409393, 0.14682289958000183, 0.1472521871328354, 0.17848822474479675, 0.18212509155273438, 0.17905765771865845, 0.14533241093158722, 0.18097372353076935, 0.14739766716957092, 0.17738205194473267, 0.11491803079843521, 0.1797448843717575, 0.11387329548597336, 0.17332611978054047, 0.2121419459581375, 0.18038775026798248, 0.18029390275478363, 0.17930811643600464, 0.18140694499015808, 0.17827154695987701, 0.21455784142017365, 0.17336945235729218, 0.14343108236789703, 0.17670838534832, 0.14033541083335876, 0.17970064282417297, 0.21420526504516602, 0.14536045491695404, 0.14423736929893494, 0.21244800090789795, 0.14060743153095245, 0.14478836953639984, 0.1457761526107788, 0.17823365330696106, 0.21206948161125183, 0.14115333557128906, 0.17900243401527405, 0.21262973546981812, 0.17287753522396088, 0.2149931937456131, 0.17349588871002197, 0.21460820734500885, 0.14695808291435242, 0.14850500226020813, 0.11761512607336044, 0.24754977226257324, 0.14808179438114166, 0.1765664666891098, 0.20871801674365997, 0.1477338820695877, 0.20606635510921478, 0.14934813976287842, 0.12078894674777985, 0.2060682773590088, 0.14406238496303558, 0.14727966487407684, 0.14847080409526825, 0.20572438836097717, 0.17936143279075623, 0.20492486655712128, 0.1786285936832428, 0.14940699934959412, 0.23954594135284424, 0.15093962848186493, 0.17683210968971252, 0.12377141416072845, 0.12387090921401978, 0.1499384343624115, 0.17728452384471893, 0.12315252423286438, 0.14444763958454132, 0.1756988912820816, 0.1735680252313614, 0.17865115404129028, 0.1763245165348053, 0.14383967220783234, 0.20454846322536469, 0.17785321176052094, 0.17424795031547546, 0.1459476798772812, 0.17838099598884583, 0.1740744560956955, 0.11992153525352478, 0.2060164213180542, 0.11958294361829758, 0.17245399951934814, 0.11904481053352356, 0.17293186485767365, 0.14345185458660126, 0.21052204072475433, 0.1464652568101883, 0.17990177869796753, 0.14798292517662048, 0.21398703753948212, 0.2076009213924408, 0.17374113202095032, 0.17496833205223083, 0.20766399800777435, 0.14684359729290009, 0.14267851412296295, 0.14539580047130585, 0.1826097071170807, 0.1775057464838028, 0.2467803806066513, 0.11866994947195053, 0.17928782105445862, 0.13928358256816864, 0.15189887583255768, 0.17966602742671967, 0.1492740511894226, 0.1780974268913269, 0.18036814033985138, 0.21297556161880493, 0.14096733927726746, 0.11900939047336578, 0.11878049373626709, 0.2147430181503296, 0.1470692753791809, 0.18136553466320038, 0.17358480393886566, 0.2109164446592331, 0.14484040439128876, 0.1428873986005783, 0.17189088463783264, 0.1728864163160324, 0.17733296751976013, 0.1737164705991745, 0.14216604828834534, 0.1442870944738388, 0.14668740332126617, 0.11643414944410324, 0.21703819930553436, 0.14763714373111725, 0.17266756296157837, 0.14652514457702637, 0.14428724348545074, 0.13783155381679535, 0.11339821666479111, 0.13873858749866486, 0.11171445995569229, 0.2158518135547638, 0.21652339398860931, 0.10966114699840546, 0.14090275764465332, 0.10843069106340408, 0.18129262328147888, 0.2191779762506485, 0.17773322761058807, 0.1794619858264923, 0.2267143577337265, 0.17448511719703674, 0.21741795539855957, 0.1430625021457672, 0.1780443638563156, 0.14017336070537567, 0.17173391580581665, 0.14063997566699982, 0.22564877569675446, 0.22106695175170898, 0.1401151418685913, 0.2639678716659546, 0.1400338113307953, 0.14092372357845306, 0.13477547466754913, 0.14700298011302948, 0.13818000257015228, 0.14335305988788605, 0.17493382096290588, 0.21080282330513, 0.14219793677330017, 0.1458500474691391, 0.13931620121002197, 0.11094854027032852, 0.13681460916996002, 0.10989031195640564, 0.1706591248512268, 0.14530043303966522, 0.18015232682228088, 0.17542797327041626, 0.14487966895103455, 0.17786063253879547, 0.17938467860221863, 0.14235907793045044, 0.21881267428398132, 0.22671076655387878, 0.22442783415317535, 0.17645904421806335, 0.23071102797985077, 0.14341717958450317, 0.18006479740142822, 0.16991189122200012, 0.18377681076526642, 0.21432693302631378, 0.26308301091194153, 0.17059463262557983, 0.18399852514266968, 0.2212606817483902, 0.17830057442188263, 0.17925553023815155, 0.11632291972637177, 0.14518794417381287, 0.14631366729736328, 0.14381961524486542, 0.17711420357227325, 0.17936596274375916, 0.14445042610168457, 0.1516289860010147, 0.14936363697052002, 0.21579237282276154, 0.20662328600883484, 0.1705070436000824, 0.14583304524421692, 0.17893680930137634, 0.16915655136108398, 0.17428463697433472, 0.11939432471990585, 0.11932450532913208, 0.18065190315246582, 0.2085886001586914, 0.21177636086940765, 0.20972521603107452, 0.14687125384807587, 0.14898726344108582, 0.20786476135253906, 0.24330323934555054, 0.14865624904632568, 0.17286808788776398, 0.1485384702682495, 0.14641427993774414, 0.20868879556655884, 0.2384921908378601, 0.12375018000602722, 0.18049342930316925, 0.18331091105937958, 0.17327435314655304, 0.15145088732242584, 0.17720694839954376, 0.17782048881053925, 0.15024149417877197, 0.20439942181110382, 0.12663966417312622, 0.1486857533454895, 0.19852356612682343, 0.2069576233625412, 0.1508628875017166, 0.15356741845607758, 0.17835749685764313, 0.17911185324192047, 0.152467742562294, 0.14855177700519562, 0.17913024127483368, 0.1811743974685669, 0.14721757173538208, 0.17715443670749664, 0.17416656017303467, 0.23318445682525635, 0.17596369981765747, 0.1267443597316742, 0.12679384648799896, 0.14889860153198242, 0.2034170776605606, 0.172214075922966, 0.1509792059659958, 0.14612534642219543, 0.20356355607509613, 0.14341668784618378, 0.14703775942325592, 0.17379659414291382, 0.15308281779289246, 0.1679416447877884, 0.15143118798732758, 0.20795021951198578, 0.14868444204330444, 0.20133012533187866, 0.16765788197517395, 0.2109031230211258, 0.23697631061077118, 0.1734861582517624, 0.17453481256961823, 0.17278343439102173, 0.15136989951133728, 0.1557481586933136, 0.20904722809791565, 0.20642970502376556, 0.1899622082710266, 0.17940664291381836, 0.16892948746681213, 0.19879868626594543, 0.1964382827281952, 0.17409157752990723, 0.1583770215511322, 0.19350673258304596, 0.16263604164123535, 0.1964087188243866, 0.1748104840517044, 0.14949458837509155, 0.15580281615257263, 0.16372071206569672, 0.1673928201198578, 0.18133224546909332, 0.19065123796463013, 0.17287304997444153, 0.1714211404323578, 0.13701389729976654, 0.14763480424880981, 0.17174950242042542, 0.19743755459785461, 0.15602584183216095, 0.1992681920528412, 0.13732469081878662, 0.20093505084514618, 0.17364253103733063, 0.19774895906448364, 0.17269085347652435, 0.193485826253891, 0.17089976370334625, 0.1754986196756363, 0.1876613050699234, 0.15581250190734863, 0.15441586077213287, 0.1764151155948639, 0.19631463289260864, 0.17576564848423004, 0.17824260890483856, 0.15407289564609528, 0.1899641752243042, 0.2101178765296936, 0.15196573734283447, 0.15653450787067413, 0.1739869862794876, 0.1547074168920517, 0.1915442943572998, 0.1751054972410202, 0.1781771332025528, 0.16191373765468597, 0.16368725895881653, 0.1437327265739441, 0.19352605938911438, 0.176993265748024, 0.1432395577430725, 0.14424654841423035, 0.1431531012058258, 0.16321229934692383, 0.14232827723026276, 0.16341707110404968, 0.15944761037826538, 0.14027291536331177, 0.1673397272825241, 0.1698327660560608, 0.17054720222949982, 0.18880370259284973, 0.1503126174211502, 0.15356530249118805, 0.21981815993785858, 0.13513612747192383, 0.1890253871679306, 0.14991138875484467, 0.2071288377046585, 0.15872429311275482, 0.19887207448482513, 0.16720780730247498, 0.18480417132377625, 0.19326144456863403, 0.1336991786956787, 0.1792755275964737, 0.14601558446884155, 0.13324113190174103, 0.18097589910030365, 0.1490621268749237, 0.17115339636802673, 0.17282898724079132, 0.1503273844718933, 0.1649484932422638, 0.1516721546649933, 0.12986095249652863, 0.1519157737493515, 0.144872784614563, 0.15209244191646576, 0.1260574907064438, 0.16564783453941345, 0.17944617569446564, 0.15008176863193512, 0.19970743358135223, 0.19298504292964935, 0.12135288864374161, 0.1765577346086502, 0.18271881341934204, 0.11966554820537567, 0.17139673233032227, 0.18605110049247742, 0.17901456356048584, 0.17741312086582184, 0.18250636756420135, 0.1892555058002472, 0.1763615608215332, 0.1165163666009903, 0.15134434401988983, 0.2172662317752838, 0.1414380520582199, 0.1430760622024536, 0.11484259366989136, 0.14250211417675018, 0.1745682954788208, 0.1447158306837082, 0.14027297496795654, 0.17369046807289124, 0.11096518486738205, 0.20712213218212128, 0.26264166831970215, 0.14093442261219025, 0.14507722854614258, 0.22009049355983734, 0.2154342085123062, 0.14253313839435577, 0.17155461013317108, 0.1907513439655304, 0.21264314651489258, 0.11137711256742477, 0.14329057931900024, 0.14137397706508636, 0.11149664223194122, 0.11114807426929474, 0.1805616319179535, 0.14021709561347961, 0.2200823724269867, 0.10933655500411987, 0.17740708589553833, 0.22015129029750824, 0.17697715759277344, 0.145480215549469, 0.10830825567245483, 0.17577992379665375, 0.13807980716228485, 0.14089281857013702, 0.17230281233787537, 0.17966274917125702, 0.26990991830825806, 0.2263481318950653, 0.18447700142860413, 0.18317291140556335, 0.2660936117172241, 0.13687193393707275, 0.17601171135902405, 0.11041774600744247, 0.14317001402378082, 0.25983747839927673, 0.14305254817008972, 0.21000325679779053, 0.17557024955749512, 0.25493451952934265, 0.17371734976768494, 0.1771947145462036, 0.17124532163143158, 0.11752793937921524, 0.2469121217727661, 0.17361003160476685, 0.17060214281082153, 0.18477940559387207, 0.21172773838043213, 0.20500478148460388, 0.14773337543010712, 0.15354837477207184, 0.12468893826007843, 0.14690260589122772, 0.1442764848470688, 0.17162349820137024, 0.21170778572559357, 0.16984038054943085, 0.1256374567747116, 0.1459851711988449, 0.14939221739768982, 0.14973032474517822, 0.21195615828037262, 0.1962072104215622, 0.21152925491333008, 0.17672894895076752, 0.1828700602054596, 0.1497199535369873, 0.1259569227695465, 0.20399755239486694, 0.23352061212062836, 0.14693893492221832, 0.1525426059961319, 0.14820441603660583, 0.17568674683570862, 0.1505800187587738, 0.17836715281009674, 0.20082826912403107, 0.1270432472229004, 0.14855672419071198, 0.1468106359243393, 0.15279941260814667, 0.12607313692569733, 0.12548121809959412, 0.1720239371061325, 0.15065009891986847, 0.17514608800411224, 0.12273303419351578, 0.14513984322547913, 0.17972014844417572, 0.17662473022937775, 0.2089262157678604, 0.14663825929164886, 0.1517259180545807, 0.17625026404857635, 0.14291973412036896, 0.21249063313007355, 0.11739285290241241, 0.1416875272989273, 0.18055611848831177, 0.20830407738685608, 0.1808260679244995, 0.14645163714885712, 0.18016886711120605, 0.2108875960111618, 0.2106897234916687, 0.1424942910671234, 0.17418423295021057, 0.17464405298233032, 0.1796010136604309, 0.1736496388912201, 0.1812610626220703, 0.14573214948177338, 0.14573530852794647, 0.21190296113491058, 0.17943304777145386, 0.14787469804286957, 0.17409643530845642, 0.1468755155801773, 0.1763458549976349, 0.21014733612537384, 0.21447458863258362, 0.14921745657920837, 0.1470799297094345, 0.11867718398571014, 0.17267870903015137, 0.14013271033763885, 0.18587848544120789, 0.17245294153690338, 0.203389972448349, 0.1722564697265625, 0.14590807259082794, 0.17968343198299408, 0.16829673945903778, 0.1526414006948471, 0.11843568831682205, 0.1421176642179489, 0.1398775577545166, 0.15369343757629395, 0.19027851521968842, 0.16878046095371246, 0.15061692893505096, 0.11561043560504913, 0.17424122989177704, 0.182663694024086, 0.18225662410259247, 0.20905762910842896, 0.11356471478939056, 0.18194136023521423, 0.2150043100118637, 0.2563598155975342, 0.1500200480222702, 0.11370334029197693, 0.14592398703098297, 0.25500035285949707, 0.2173122763633728, 0.18048107624053955, 0.21811416745185852, 0.1809292733669281, 0.11648939549922943, 0.11686532944440842, 0.21711695194244385, 0.14879950881004333, 0.17773373425006866, 0.21569763123989105, 0.17818401753902435, 0.18062134087085724, 0.21360184252262115, 0.1761886030435562, 0.14660461246967316, 0.14554360508918762, 0.14806705713272095, 0.17687273025512695, 0.17751283943653107, 0.14717324078083038, 0.12066814303398132, 0.1471828818321228, 0.24320490658283234, 0.1763124018907547, 0.12033717334270477, 0.14712460339069366, 0.14618730545043945, 0.1468074768781662, 0.17643842101097107, 0.20965297520160675, 0.21010473370552063, 0.17698223888874054, 0.21030183136463165, 0.20911744236946106, 0.12003418803215027, 0.12026454508304596, 0.12024035304784775, 0.1465524584054947, 0.14674893021583557, 0.17648111283779144, 0.11887338757514954, 0.1453387439250946, 0.14526481926441193, 0.14481642842292786, 0.21097467839717865, 0.11593159288167953, 0.17636792361736298, 0.14419710636138916, 0.1438545286655426, 0.11358607560396194, 0.2155231535434723, 0.17682155966758728, 0.14085419476032257, 0.17578452825546265, 0.13950107991695404, 0.26083749532699585, 0.14250396192073822, 0.17827793955802917, 0.17682965099811554, 0.14472617208957672, 0.22093287110328674, 0.181405708193779, 0.22172150015830994, 0.11109299212694168, 0.11119847744703293, 0.1395520269870758, 0.13990655541419983, 0.21536649763584137, 0.11047417670488358, 0.1826510727405548, 0.13712307810783386, 0.26255276799201965, 0.14229173958301544, 0.1759522408246994, 0.2167842537164688, 0.1774011105298996, 0.26064619421958923, 0.21924161911010742, 0.1807878166437149, 0.21164870262145996, 0.17691044509410858, 0.17388209700584412, 0.11589077860116959, 0.14776386320590973, 0.20814351737499237, 0.17798547446727753, 0.15035848319530487, 0.14281293749809265, 0.17359250783920288, 0.17479214072227478, 0.14828906953334808, 0.1700553148984909, 0.1456686407327652, 0.14243389666080475, 0.15209661424160004, 0.16990894079208374, 0.17199374735355377, 0.1757124662399292, 0.1745375394821167, 0.20602039992809296, 0.17556391656398773, 0.21749520301818848, 0.17670468986034393, 0.21720796823501587, 0.1412588208913803, 0.15166425704956055, 0.17779900133609772, 0.2134997695684433, 0.1812238246202469, 0.12174370139837265, 0.17422223091125488, 0.20497234165668488, 0.17298027873039246, 0.14721155166625977, 0.1987554430961609, 0.1537327915430069, 0.2121724635362625, 0.21064206957817078, 0.12432211637496948, 0.16932208836078644, 0.12483090907335281, 0.21070300042629242, 0.1693757027387619, 0.20209357142448425, 0.17703840136528015, 0.1716492623090744, 0.12627145648002625, 0.15183337032794952, 0.23293988406658173, 0.2058829665184021, 0.15175491571426392, 0.17038649320602417, 0.15559352934360504, 0.18116270005702972, 0.182623952627182, 0.20816706120967865, 0.15203864872455597, 0.12882749736309052, 0.17986322939395905, 0.17283685505390167, 0.15355829894542694, 0.17064101994037628, 0.15197595953941345, 0.15167933702468872, 0.22994160652160645, 0.1476830095052719, 0.15349943935871124, 0.14680349826812744, 0.1526452749967575, 0.1512354016304016, 0.18208718299865723, 0.2078370451927185, 0.17353738844394684, 0.15326225757598877, 0.17222048342227936, 0.12651361525058746, 0.17960700392723083, 0.20020119845867157, 0.17830505967140198, 0.12579980492591858, 0.1255442351102829, 0.1478727012872696, 0.18018940091133118, 0.20339441299438477, 0.12394551932811737, 0.15118350088596344, 0.14424803853034973, 0.20798903703689575, 0.17441529035568237, 0.17662188410758972, 0.14470793306827545, 0.14473260939121246, 0.17148356139659882, 0.14893744885921478, 0.17562565207481384, 0.24297264218330383, 0.14982378482818604, 0.14844202995300293, 0.20885227620601654, 0.17354774475097656, 0.14929339289665222, 0.20569227635860443, 0.17995989322662354, 0.14900845289230347, 0.1501609832048416, 0.17134125530719757, 0.17529501020908356, 0.17062042653560638, 0.14716985821723938, 0.17487932741641998, 0.17962002754211426, 0.1753516048192978, 0.1439407914876938, 0.17489153146743774, 0.1486801952123642, 0.12052714079618454, 0.14895501732826233, 0.17902156710624695, 0.2445105016231537, 0.20873652398586273, 0.14886054396629333, 0.20911581814289093, 0.14280754327774048, 0.1489240974187851, 0.14547774195671082, 0.17508642375469208, 0.20625163614749908, 0.20610886812210083, 0.1516960710287094, 0.20473667979240417, 0.18081001937389374, 0.2056390792131424, 0.14755463600158691, 0.20552721619606018, 0.12306369096040726, 0.14804993569850922, 0.15114012360572815, 0.1501418650150299, 0.1802300214767456, 0.14428740739822388, 0.14982664585113525, 0.12274415791034698, 0.17341428995132446, 0.149624764919281, 0.1497727483510971, 0.18021740019321442, 0.14834406971931458, 0.14713449776172638, 0.20745845139026642, 0.18049055337905884, 0.2045128345489502, 0.17845948040485382, 0.1441362202167511, 0.11875947564840317, 0.14413070678710938, 0.18058107793331146, 0.20796477794647217, 0.14872722327709198, 0.20723475515842438, 0.11751027405261993, 0.11731691658496857, 0.21285825967788696, 0.14586910605430603, 0.11653399467468262, 0.14057491719722748, 0.146493598818779, 0.11503393948078156, 0.14016233384609222, 0.17875897884368896, 0.21456360816955566, 0.14713770151138306, 0.14261426031589508, 0.11157652735710144, 0.1810886561870575, 0.1727471798658371, 0.10985292494297028, 0.14041629433631897, 0.14482565224170685, 0.10777126252651215, 0.21760748326778412, 0.14307624101638794, 0.2201918214559555, 0.17433182895183563, 0.13965077698230743, 0.22748513519763947, 0.1377989947795868, 0.10462700575590134, 0.10424873977899551, 0.14238901436328888, 0.1766413152217865, 0.17386779189109802, 0.17770658433437347, 0.13978753983974457, 0.18215635418891907, 0.17567794024944305, 0.22538599371910095, 0.22451794147491455, 0.22788748145103455, 0.22142282128334045, 0.23259660601615906, 0.1351337432861328, 0.10425027459859848, 0.23008310794830322, 0.17768210172653198, 0.10584765672683716, 0.14014402031898499, 0.10633540898561478, 0.13829611241817474, 0.10610651224851608, 0.14041827619075775, 0.14203980565071106, 0.13671480119228363, 0.22971734404563904, 0.14262208342552185, 0.22281119227409363, 0.13303831219673157, 0.1747099608182907, 0.10329271852970123, 0.14040300250053406, 0.1801047921180725, 0.13299138844013214, 0.10191071778535843, 0.17705141007900238, 0.1740146279335022, 0.18556073307991028, 0.13349612057209015, 0.17611974477767944, 0.17679880559444427, 0.09946294128894806, 0.17705510556697845, 0.22338227927684784, 0.13756093382835388, 0.1771649718284607, 0.22804516553878784, 0.12977294623851776, 0.2854062020778656, 0.13791458308696747, 0.18993979692459106, 0.10037415474653244, 0.17150819301605225, 0.1767614334821701, 0.13428689539432526, 0.17593948543071747, 0.18155869841575623, 0.13969166576862335, 0.1395399272441864, 0.18272282183170319, 0.178522989153862, 0.10143385082483292, 0.18127897381782532, 0.1318751871585846, 0.14011570811271667, 0.1897454559803009, 0.1330047845840454, 0.1864737570285797, 0.09995241463184357, 0.2347596436738968, 0.09945636987686157, 0.13574999570846558, 0.17569002509117126, 0.2865128517150879, 0.231929212808609, 0.17494827508926392, 0.14045223593711853, 0.1852831244468689, 0.18622180819511414, 0.2816193103790283, 0.14129221439361572, 0.13210850954055786, 0.13184919953346252, 0.13990086317062378, 0.17983295023441315, 0.17093819379806519, 0.22346171736717224, 0.17280906438827515, 0.22010891139507294, 0.10489189624786377, 0.18168187141418457, 0.13406643271446228, 0.177648663520813, 0.13478876650333405, 0.1814698874950409, 0.14462810754776, 0.10631193965673447, 0.13271962106227875, 0.10585256665945053, 0.22920463979244232, 0.17052143812179565, 0.17338864505290985, 0.17831335961818695, 0.18496331572532654, 0.22097247838974, 0.18789353966712952, 0.1694270670413971, 0.14512032270431519, 0.18736673891544342, 0.10622909665107727, 0.14282874763011932, 0.22258099913597107, 0.17215600609779358, 0.14009444415569305, 0.1887090802192688, 0.1063612625002861, 0.14371943473815918, 0.10590708255767822, 0.13245047628879547, 0.17388410866260529, 0.14596977829933167, 0.23186653852462769, 0.18010137975215912, 0.173787459731102, 0.1349029242992401, 0.14143222570419312, 0.23470374941825867, 0.10333169996738434, 0.23331153392791748, 0.13957659900188446, 0.1778419017791748, 0.14073483645915985, 0.140791654586792, 0.14756682515144348, 0.22478584945201874, 0.21674098074436188, 0.14159423112869263, 0.22861647605895996, 0.10364926606416702, 0.1240895465016365, 0.10353921353816986, 0.10326620191335678, 0.13173915445804596, 0.16445711255073547, 0.21328105032444, 0.17161118984222412, 0.18714487552642822, 0.10268963873386383, 0.2606496810913086, 0.20761163532733917, 0.21022461354732513, 0.12031765282154083, 0.1593097448348999, 0.23795185983181, 0.20590542256832123, 0.17719435691833496, 0.1296941190958023, 0.10390020906925201, 0.1635507345199585, 0.17717325687408447, 0.1556619256734848, 0.1317874789237976, 0.10285982489585876, 0.16486909985542297, 0.18666917085647583, 0.19749414920806885, 0.24564673006534576, 0.12453705072402954, 0.23500674962997437, 0.14204834401607513, 0.10183003544807434, 0.22877953946590424, 0.1551695466041565, 0.23403513431549072, 0.13826502859592438, 0.13215598464012146, 0.2790504992008209, 0.18912869691848755, 0.14290988445281982, 0.1865924894809723, 0.2329663336277008, 0.1456478089094162, 0.1979980170726776, 0.22326263785362244, 0.17986595630645752, 0.13798123598098755, 0.1840517669916153, 0.21801181137561798, 0.13873745501041412, 0.22000496089458466, 0.2201542854309082, 0.10916625708341599, 0.13652588427066803, 0.22756613790988922, 0.13702881336212158, 0.15001198649406433, 0.22481422126293182, 0.13877981901168823, 0.17567069828510284, 0.22706608474254608, 0.17819178104400635, 0.16948093473911285, 0.1691136658191681, 0.1841161698102951, 0.1469721645116806, 0.1755320280790329, 0.1421673446893692, 0.1148771420121193, 0.2133714109659195, 0.14688695967197418, 0.1599680632352829, 0.18960487842559814, 0.18483050167560577, 0.21129921078681946, 0.22078028321266174, 0.1467100977897644, 0.13360758125782013, 0.16164450347423553, 0.19147200882434845, 0.173525869846344, 0.20523962378501892, 0.18838639557361603, 0.16314926743507385, 0.20630225539207458, 0.17953352630138397, 0.14622940123081207, 0.21168039739131927, 0.18417121469974518, 0.1182757169008255, 0.18189100921154022, 0.17635276913642883, 0.14409057796001434, 0.1448306143283844, 0.14533017575740814, 0.14059984683990479, 0.16937100887298584, 0.24677340686321259, 0.19870735704898834, 0.11909108608961105, 0.1327323615550995, 0.24533981084823608, 0.24467112123966217, 0.1393888294696808, 0.1557817906141281, 0.13426713645458221, 0.16356739401817322, 0.14204169809818268, 0.12150728702545166, 0.1754668951034546, 0.16986820101737976, 0.16526781022548676, 0.14419783651828766, 0.1702646017074585, 0.18182049691677094, 0.14556479454040527, 0.15508949756622314, 0.21385550498962402, 0.22038330137729645, 0.22202321887016296, 0.17507340013980865, 0.14696712791919708, 0.1936500519514084, 0.22577513754367828, 0.15911681950092316, 0.19185961782932281, 0.17035295069217682, 0.17374511063098907, 0.1696784943342209, 0.24056823551654816, 0.20753538608551025, 0.19518600404262543, 0.14318016171455383, 0.17728903889656067, 0.14157992601394653, 0.16315005719661713, 0.16048279404640198, 0.12611614167690277, 0.1514514535665512, 0.15228772163391113, 0.14613007009029388, 0.16393990814685822, 0.12559399008750916, 0.1751565784215927, 0.1443815380334854, 0.15672074258327484, 0.1438351273536682, 0.21564321219921112, 0.14138053357601166, 0.1467786580324173, 0.17612546682357788, 0.15633167326450348, 0.20244096219539642, 0.1437089890241623, 0.19428586959838867, 0.17416581511497498, 0.1486821472644806, 0.21753650903701782, 0.15397220849990845, 0.11991981416940689, 0.1807490885257721, 0.21007579565048218, 0.11919554322957993, 0.19870951771736145, 0.17474588751792908, 0.20613738894462585, 0.11891794204711914, 0.13917945325374603, 0.17843322455883026, 0.11860537528991699, 0.18999727070331573, 0.18152719736099243, 0.14315667748451233, 0.18683260679244995, 0.14763712882995605, 0.18397106230258942, 0.1744367480278015, 0.1915004700422287, 0.14188703894615173, 0.16708531975746155, 0.11631876230239868, 0.1680779606103897, 0.1416187435388565, 0.18133606016635895, 0.15306225419044495, 0.22605425119400024, 0.1146295964717865, 0.18026690185070038, 0.17594090104103088, 0.15218862891197205, 0.18412914872169495, 0.25552594661712646, 0.1135576143860817, 0.14698131382465363, 0.14868979156017303, 0.13827767968177795, 0.11289916932582855, 0.14288391172885895, 0.17861472070217133, 0.14096902310848236, 0.14225833117961884, 0.13985124230384827, 0.10992641001939774, 0.17987513542175293, 0.14293231070041656, 0.10787743330001831, 0.21991205215454102, 0.13786660134792328, 0.18179744482040405, 0.14700788259506226, 0.18185222148895264, 0.20828740298748016, 0.23034696280956268, 0.1836797446012497, 0.18392673134803772, 0.1429363638162613, 0.10493539273738861, 0.13647110760211945, 0.2734912931919098, 0.18549247086048126, 0.22400054335594177, 0.18614943325519562, 0.17748494446277618, 0.1489715576171875, 0.18122956156730652, 0.1379403918981552, 0.1874755173921585, 0.1397446244955063, 0.17488093674182892, 0.18119634687900543, 0.13951563835144043, 0.13958099484443665, 0.10748567432165146, 0.22414502501487732, 0.20841844379901886, 0.13797618448734283, 0.14341098070144653, 0.1705610603094101, 0.15022706985473633, 0.18454289436340332, 0.14545108377933502, 0.17927001416683197, 0.1299823373556137, 0.22322052717208862, 0.22108642756938934, 0.10711560398340225, 0.14069417119026184, 0.1422480046749115, 0.16865013539791107, 0.1383950412273407, 0.14078395068645477, 0.17858301103115082, 0.1366855949163437, 0.17071522772312164, 0.18005570769309998, 0.1697443425655365, 0.14409081637859344, 0.1846432089805603, 0.17466142773628235, 0.1808372437953949, 0.18167899549007416, 0.18021978437900543, 0.10525904595851898, 0.22198081016540527, 0.22547781467437744, 0.27141088247299194, 0.16585080325603485, 0.10672901570796967, 0.1742209494113922, 0.1811562329530716, 0.22676485776901245, 0.17107811570167542, 0.16895166039466858, 0.17937147617340088, 0.2271725833415985, 0.16884513199329376, 0.1433638036251068, 0.1115775853395462, 0.1430439054965973, 0.13959500193595886, 0.1457112729549408, 0.2585385739803314, 0.18671298027038574, 0.14455990493297577, 0.1126357838511467, 0.13263069093227386, 0.17609624564647675, 0.16595958173274994, 0.21142059564590454, 0.11244695633649826, 0.22079727053642273, 0.13774490356445312, 0.17524652183055878, 0.14631764590740204, 0.2172418087720871, 0.14062127470970154, 0.17845967411994934, 0.11294905096292496, 0.11267926543951035, 0.2246067076921463, 0.1687755286693573, 0.17196539044380188, 0.1887485235929489, 0.12733659148216248, 0.14608438313007355, 0.15430614352226257, 0.13160091638565063, 0.2596721947193146, 0.1486821472644806, 0.14034762978553772, 0.1877647042274475, 0.19910481572151184, 0.21307328343391418, 0.23176929354667664, 0.20591799914836884, 0.1739613562822342, 0.15282846987247467, 0.22194431722164154, 0.14213603734970093, 0.1398717164993286, 0.18709184229373932, 0.1376487910747528, 0.20369523763656616, 0.16143402457237244, 0.17660899460315704, 0.15152771770954132, 0.15273305773735046, 0.18833479285240173, 0.24915112555027008, 0.1923595815896988, 0.15645457804203033, 0.15164650976657867, 0.20054979622364044, 0.19819381833076477, 0.11888647824525833, 0.18293814361095428, 0.1837955117225647, 0.21722835302352905, 0.20167537033557892, 0.1659926027059555, 0.1919327974319458, 0.13993510603904724, 0.2042239010334015, 0.1213519349694252, 0.21700891852378845, 0.15701301395893097, 0.16028982400894165, 0.2400851547718048, 0.17026524245738983, 0.17548242211341858, 0.12344258278608322, 0.18403446674346924, 0.17871595919132233, 0.14384640753269196, 0.21028843522071838, 0.20628488063812256, 0.1425466537475586, 0.1776585429906845, 0.1594279259443283, 0.17194105684757233, 0.125737264752388, 0.21228677034378052, 0.14438192546367645, 0.2145550698041916, 0.1656903177499771, 0.17907029390335083, 0.126256063580513, 0.17406374216079712, 0.1409318596124649, 0.18301694095134735, 0.17966963350772858, 0.17322203516960144, 0.19433672726154327, 0.21264448761940002, 0.1727742701768875, 0.14040474593639374, 0.20145076513290405, 0.16536477208137512, 0.16890403628349304, 0.1574888974428177, 0.13475202023983002, 0.12788134813308716, 0.15503665804862976, 0.14449121057987213, 0.12748797237873077, 0.17930524051189423, 0.20333969593048096, 0.16268189251422882, 0.15356527268886566, 0.13527068495750427, 0.16367869079113007, 0.15727025270462036, 0.17437711358070374, 0.20427504181861877, 0.12500780820846558, 0.1598542481660843, 0.16949059069156647, 0.1718185991048813, 0.19512465596199036, 0.22081004083156586, 0.1354101449251175, 0.1467340737581253, 0.16608519852161407, 0.12475921213626862, 0.16379722952842712, 0.12405172735452652, 0.16601431369781494, 0.16648808121681213, 0.1816125065088272, 0.19684232771396637, 0.23929163813591003, 0.18876098096370697, 0.16531875729560852, 0.20724834501743317, 0.21099470555782318, 0.15767395496368408, 0.1487344205379486, 0.17734768986701965, 0.2350975126028061, 0.19019480049610138, 0.16583576798439026, 0.14256037771701813, 0.16678954660892487, 0.165728360414505, 0.15133166313171387, 0.17529575526714325, 0.15544678270816803, 0.1666228026151657, 0.1895984709262848, 0.15224961936473846, 0.18599209189414978, 0.13589917123317719, 0.1849108189344406, 0.12853997945785522, 0.1593569964170456, 0.21171417832374573, 0.12842720746994019, 0.2304181456565857, 0.18996097147464752, 0.1370525360107422, 0.17450591921806335, 0.1453036516904831, 0.20434924960136414, 0.19465777277946472, 0.16098901629447937, 0.21606843173503876, 0.14544963836669922, 0.19501681625843048, 0.1591210663318634, 0.13641691207885742, 0.17799638211727142, 0.150689497590065, 0.16478857398033142, 0.13053542375564575, 0.14221526682376862, 0.14455699920654297, 0.228469118475914, 0.14234934747219086, 0.18126419186592102, 0.16892579197883606, 0.1770888715982437, 0.1975724995136261, 0.1294482797384262, 0.13049615919589996, 0.1903063952922821, 0.15500402450561523, 0.1809598058462143, 0.21241328120231628, 0.19595967233181, 0.17829850316047668, 0.16607220470905304, 0.2033953070640564, 0.23121598362922668, 0.1530066579580307, 0.12943409383296967, 0.20487304031848907, 0.16791783273220062, 0.1594364494085312, 0.14446111023426056, 0.13176701962947845, 0.19396443665027618, 0.1724567711353302, 0.18522605299949646, 0.15517280995845795, 0.13037171959877014, 0.14788317680358887, 0.17327527701854706, 0.13060671091079712, 0.15632280707359314, 0.15413427352905273, 0.198161318898201, 0.16515323519706726, 0.17069047689437866, 0.20064599812030792, 0.17137417197227478, 0.20451751351356506, 0.16582372784614563, 0.23114123940467834, 0.14772336184978485, 0.1388493925333023, 0.23004266619682312, 0.1759241372346878, 0.17612223327159882, 0.2295403629541397, 0.16095341742038727, 0.17024005949497223, 0.19889004528522491, 0.1453089863061905, 0.17714746296405792, 0.16957910358905792, 0.13200291991233826, 0.15562403202056885, 0.1891034096479416, 0.200803741812706, 0.1528158038854599, 0.13311679661273956, 0.2122585028409958, 0.22316068410873413, 0.1604730784893036, 0.16052737832069397, 0.16108043491840363, 0.13321168720722198, 0.19343754649162292, 0.19205814599990845, 0.16572163999080658, 0.13749927282333374, 0.17111720144748688, 0.1473986804485321, 0.16495463252067566, 0.22474119067192078, 0.18450911343097687, 0.16069090366363525, 0.16207702457904816, 0.1717243790626526, 0.16564340889453888, 0.15642356872558594, 0.21979430317878723, 0.14922183752059937, 0.19679898023605347, 0.19104832410812378, 0.17443829774856567, 0.18876761198043823, 0.20522938668727875, 0.16374973952770233, 0.16381773352622986, 0.157688170671463, 0.15181951224803925, 0.17029139399528503, 0.13170452415943146, 0.22505562007427216, 0.21091927587985992, 0.20454829931259155, 0.15623261034488678, 0.1741960346698761, 0.13639549911022186, 0.22316163778305054, 0.2033206671476364, 0.13388794660568237, 0.19380249083042145, 0.1450003981590271, 0.199173703789711, 0.1348610520362854, 0.18444320559501648, 0.14934688806533813, 0.19952692091464996, 0.1625157594680786, 0.14694732427597046, 0.1544714719057083, 0.18514057993888855, 0.19351595640182495, 0.13518525660037994, 0.13508917391300201, 0.16074852645397186, 0.1691136360168457, 0.17941543459892273, 0.15589362382888794, 0.2227640450000763, 0.17687371373176575, 0.15250767767429352, 0.17179931700229645, 0.1817965805530548, 0.18031354248523712, 0.162836953997612, 0.14412406086921692, 0.16311092674732208, 0.1504545509815216, 0.17782001197338104, 0.21100224554538727, 0.16566695272922516, 0.1437443196773529, 0.1961895227432251, 0.17716194689273834, 0.20482458174228668, 0.1323864459991455, 0.17616719007492065, 0.182478666305542, 0.21239912509918213, 0.17389923334121704, 0.13245362043380737, 0.1947631686925888, 0.19891571998596191, 0.22347894310951233, 0.1869364082813263, 0.18955165147781372, 0.13382697105407715, 0.1541665643453598, 0.19702167809009552, 0.1667310893535614, 0.16817951202392578, 0.1456623673439026, 0.1956673562526703, 0.15097595751285553, 0.15616096556186676, 0.13583247363567352, 0.17114326357841492, 0.1530812680721283, 0.1893412321805954, 0.1815112829208374, 0.13509292900562286, 0.14910459518432617, 0.13448184728622437, 0.1609210968017578, 0.17347468435764313, 0.1669491082429886, 0.16340601444244385, 0.137501522898674, 0.15791729092597961, 0.16247564554214478, 0.14796318113803864, 0.16559109091758728, 0.13011226058006287, 0.2209678292274475, 0.19131578505039215, 0.17015686631202698, 0.16972364485263824, 0.1415816992521286, 0.15040449798107147, 0.15570080280303955, 0.1424889713525772, 0.1516752541065216, 0.18818296492099762, 0.16501973569393158, 0.16182813048362732, 0.17695875465869904, 0.1606803685426712, 0.12627847492694855, 0.1281103789806366, 0.18739119172096252, 0.14636275172233582, 0.19926941394805908, 0.1705857813358307, 0.14872822165489197, 0.1987503170967102, 0.18969401717185974, 0.15352468192577362, 0.12281137704849243, 0.163059264421463, 0.20048949122428894, 0.1434752643108368, 0.15554994344711304, 0.14040450751781464, 0.1687559336423874, 0.1489890068769455, 0.1710415780544281, 0.1408272236585617, 0.1753774881362915, 0.21296347677707672, 0.1603873372077942, 0.11750181019306183, 0.16779732704162598, 0.17082300782203674, 0.16583788394927979, 0.11679725348949432, 0.15006530284881592, 0.24192924797534943, 0.2003401517868042, 0.15468965470790863, 0.1482325941324234, 0.11587963998317719, 0.23690281808376312, 0.15092334151268005, 0.18299810588359833, 0.14796064794063568, 0.129754900932312, 0.16886276006698608, 0.18282687664031982, 0.18419210612773895, 0.19195249676704407, 0.18132798373699188, 0.16854161024093628, 0.18196064233779907, 0.1590050607919693, 0.14980864524841309, 0.16683126986026764, 0.14259229600429535, 0.22789910435676575, 0.1348690688610077, 0.11398347467184067, 0.13879713416099548, 0.20507574081420898, 0.1847957819700241, 0.11162231862545013, 0.15340283513069153, 0.17256513237953186, 0.2635943293571472, 0.21072670817375183, 0.16809989511966705, 0.1887092888355255, 0.11063123494386673, 0.1777641773223877, 0.17764997482299805, 0.2629404664039612, 0.11056259274482727, 0.26057666540145874, 0.18264620006084442, 0.1494150459766388, 0.17766985297203064, 0.13406871259212494, 0.14446069300174713, 0.20194299519062042, 0.18607617914676666, 0.22035424411296844, 0.17550140619277954, 0.1812114417552948, 0.2515422999858856, 0.1549787074327469, 0.11725682020187378, 0.13981692492961884, 0.186884805560112, 0.11797380447387695, 0.17252026498317719, 0.11762634664773941, 0.15194512903690338, 0.13968190550804138, 0.11735787242650986, 0.11670041084289551, 0.12743620574474335, 0.1570950597524643, 0.1916387528181076, 0.12776561081409454, 0.14334501326084137, 0.11264295876026154, 0.20151077210903168, 0.19510532915592194, 0.1772921234369278, 0.14124450087547302, 0.17955389618873596, 0.22644799947738647, 0.18050548434257507, 0.19620363414287567, 0.16810141503810883, 0.21195602416992188, 0.15224164724349976, 0.20194795727729797, 0.16255433857440948, 0.11079711467027664, 0.21138659119606018, 0.16388539969921112, 0.18375061452388763, 0.15037590265274048, 0.20539459586143494, 0.13435278832912445, 0.15222646296024323, 0.21234223246574402, 0.13104259967803955, 0.199622243642807, 0.15155604481697083, 0.21958042681217194, 0.17121073603630066, 0.16215987503528595, 0.141507089138031, 0.1265944093465805, 0.18698982894420624, 0.20967724919319153, 0.11492417007684708, 0.11489910632371902, 0.17174294590950012, 0.19902127981185913, 0.1929779201745987, 0.19553214311599731, 0.1149679571390152, 0.1551283746957779, 0.19187481701374054, 0.14670033752918243, 0.18355289101600647, 0.1982405036687851, 0.11466685682535172, 0.1141948252916336, 0.1536940336227417, 0.11355576664209366, 0.18089382350444794, 0.11300179362297058, 0.17851603031158447, 0.13984985649585724, 0.26021072268486023, 0.13906782865524292, 0.14262166619300842, 0.15639275312423706, 0.18898451328277588, 0.21455800533294678, 0.21112285554409027, 0.12471450865268707, 0.18819428980350494, 0.20264066755771637, 0.22336246073246002, 0.15278884768486023, 0.1399582028388977, 0.23599207401275635, 0.16736873984336853, 0.17388497292995453, 0.19272467494010925, 0.23924700915813446, 0.143265962600708, 0.18689629435539246, 0.13443273305892944, 0.11410977691411972, 0.1743013858795166, 0.1709117889404297, 0.13213634490966797, 0.18848451972007751, 0.11417089402675629, 0.22526885569095612, 0.16052088141441345, 0.18912000954151154, 0.11389029771089554, 0.14192667603492737, 0.19448362290859222, 0.14283959567546844, 0.19198136031627655, 0.19802121818065643, 0.2204219549894333, 0.18155720829963684, 0.2137271761894226, 0.21422256529331207, 0.11427862197160721, 0.16897614300251007, 0.1446523517370224, 0.14026157557964325, 0.2154734879732132, 0.15616074204444885, 0.1785832792520523, 0.1467571258544922, 0.142479807138443, 0.1660941243171692, 0.25204724073410034, 0.14748254418373108, 0.14201606810092926, 0.18295921385288239, 0.1310700923204422, 0.1484374850988388, 0.16162510216236115, 0.17344337701797485, 0.1755363941192627, 0.17059364914894104, 0.17951485514640808, 0.11572127789258957, 0.17913027107715607, 0.1870492398738861, 0.18347929418087006, 0.1866968423128128, 0.15428423881530762, 0.13465583324432373, 0.14404769241809845, 0.179214209318161, 0.21988481283187866, 0.20691512525081635, 0.19900256395339966, 0.1855432540178299, 0.16937381029129028, 0.14630752801895142, 0.18998701870441437, 0.13656283915042877, 0.11597634851932526, 0.2109064906835556, 0.2321741133928299, 0.15918578207492828, 0.20585407316684723, 0.19055595993995667, 0.14606893062591553, 0.1679735630750656, 0.14413467049598694, 0.14409370720386505, 0.17176561057567596, 0.19507300853729248, 0.16656357049942017, 0.14503145217895508, 0.11806367337703705, 0.20155206322669983, 0.15721532702445984, 0.21685798466205597, 0.1802188754081726, 0.1847788542509079, 0.1506144404411316, 0.1825459599494934, 0.17354810237884521, 0.13265858590602875, 0.14142613112926483, 0.17514610290527344, 0.16662052273750305, 0.1398385614156723, 0.13684815168380737, 0.15585896372795105, 0.149396151304245, 0.20813213288784027, 0.1900196224451065, 0.15250013768672943, 0.11708519607782364, 0.18476827442646027, 0.1726515293121338, 0.21038153767585754, 0.15137770771980286, 0.13580240309238434, 0.14162002503871918, 0.14554300904273987, 0.15795476734638214, 0.17436739802360535, 0.1386944204568863, 0.1549900770187378, 0.15398621559143066, 0.1624210625886917, 0.17333970963954926, 0.2554629147052765, 0.13813742995262146, 0.1826171576976776, 0.1299552172422409, 0.11421452462673187, 0.167860746383667, 0.18727044761180878, 0.18302510678768158, 0.191380113363266, 0.22139102220535278, 0.22421889007091522, 0.13354523479938507, 0.1690959930419922, 0.13871563971042633, 0.13867537677288055, 0.17925934493541718, 0.18752987682819366, 0.2007206827402115, 0.14363883435726166, 0.11514244973659515, 0.18106597661972046, 0.20984047651290894, 0.16479219496250153, 0.14568717777729034, 0.14306138455867767, 0.14119815826416016, 0.2536233365535736, 0.13986152410507202, 0.14042015373706818, 0.18308812379837036, 0.17650072276592255, 0.17358240485191345, 0.2147204726934433, 0.16927678883075714, 0.18315055966377258, 0.20387905836105347, 0.1671476811170578, 0.16744297742843628, 0.15893788635730743, 0.11749844998121262, 0.2083486020565033, 0.15705814957618713, 0.1758580058813095, 0.15894322097301483, 0.1926422268152237, 0.24904155731201172, 0.13830877840518951, 0.18623940646648407, 0.20552381873130798, 0.17328451573848724, 0.19780835509300232, 0.199935644865036, 0.17394515872001648, 0.173405721783638, 0.13201802968978882, 0.15447622537612915, 0.145478293299675, 0.19096331298351288, 0.12145683169364929, 0.20354029536247253, 0.1867358684539795, 0.20958077907562256, 0.21706947684288025, 0.15522536635398865, 0.2402525544166565, 0.2186780869960785, 0.21315450966358185, 0.1837931126356125, 0.17302674055099487, 0.17597098648548126, 0.16824476420879364, 0.15637706220149994, 0.1811257302761078, 0.2128974348306656, 0.15402667224407196, 0.12735067307949066, 0.16129763424396515, 0.1279175579547882, 0.12756018340587616, 0.15452073514461517, 0.17665241658687592, 0.1518627107143402, 0.18093861639499664, 0.1982242614030838, 0.16684067249298096, 0.14342685043811798, 0.14849264919757843, 0.19328700006008148, 0.12587840855121613, 0.21207697689533234, 0.19782127439975739, 0.1657448261976242, 0.209580659866333, 0.1494826078414917, 0.1793009489774704, 0.1850043088197708, 0.15267252922058105, 0.13144633173942566, 0.12647318840026855, 0.18525952100753784, 0.15208126604557037, 0.14559507369995117, 0.17464424669742584, 0.14656706154346466, 0.18452271819114685, 0.14260809123516083, 0.16054008901119232, 0.1779107302427292, 0.20625604689121246, 0.15135234594345093, 0.1652556210756302, 0.1234484612941742, 0.18789955973625183, 0.14489026367664337, 0.1698603332042694, 0.14307869970798492, 0.14425592124462128, 0.14868126809597015, 0.1420305222272873, 0.12095450609922409, 0.2179557830095291, 0.21693620085716248, 0.1542464643716812, 0.16915859282016754, 0.18042713403701782, 0.20066052675247192, 0.170277938246727, 0.13475042581558228, 0.19210074841976166, 0.14253181219100952, 0.11941318958997726, 0.14580677449703217, 0.2088267058134079, 0.14424683153629303, 0.11845109611749649, 0.14727820456027985, 0.2092522829771042, 0.16769324243068695, 0.11758876591920853, 0.1679903119802475, 0.14941301941871643, 0.15171551704406738, 0.17853836715221405, 0.1677461564540863, 0.11563290655612946, 0.1853918582201004, 0.15170826017856598, 0.20581665635108948, 0.14374971389770508, 0.22092309594154358, 0.1583670973777771, 0.1144506111741066, 0.16235923767089844, 0.13913321495056152, 0.15196307003498077, 0.17279058694839478, 0.17352744936943054, 0.11314502358436584, 0.21007701754570007, 0.15925434231758118, 0.2139141857624054, 0.13426385819911957, 0.1124623715877533, 0.17585858702659607, 0.11239197850227356, 0.1896732896566391, 0.1356538087129593, 0.13867749273777008, 0.13750994205474854, 0.1629277914762497, 0.2349460870027542, 0.18394841253757477, 0.20560207962989807, 0.14042726159095764, 0.11089687049388885, 0.22624866664409637, 0.14218048751354218, 0.1103675588965416, 0.15330404043197632, 0.14074604213237762, 0.1735476702451706, 0.1990506798028946, 0.15882040560245514, 0.10926776379346848, 0.1501646637916565, 0.22846151888370514, 0.1663171648979187, 0.22666281461715698, 0.13435734808444977, 0.19063447415828705, 0.20298628509044647, 0.16342735290527344, 0.10932616144418716, 0.1894402951002121, 0.21930991113185883, 0.20929405093193054, 0.11027852445840836, 0.19141024351119995, 0.13224945962429047, 0.17709149420261383, 0.19235511124134064, 0.13557477295398712, 0.1768404245376587, 0.1655859798192978, 0.14351069927215576, 0.12612253427505493, 0.22317160665988922, 0.18902012705802917, 0.17472957074642181, 0.15303108096122742, 0.15611489117145538, 0.18225471675395966, 0.18434026837348938, 0.15680260956287384, 0.2090815156698227, 0.1624656319618225, 0.15072761476039886, 0.14635595679283142, 0.18563997745513916, 0.1118643581867218, 0.2589339315891266, 0.12590043246746063, 0.13114938139915466, 0.13933220505714417, 0.2245911806821823, 0.17875897884368896, 0.13720384240150452, 0.1653408259153366, 0.16015002131462097, 0.14575399458408356, 0.19547729194164276, 0.17587406933307648, 0.11248292028903961, 0.2215600460767746, 0.17273381352424622, 0.21251758933067322, 0.11223116517066956, 0.14242464303970337, 0.15907414257526398, 0.11165536195039749, 0.1116863563656807, 0.1905982494354248, 0.15226192772388458, 0.22628015279769897, 0.14113542437553406, 0.15298573672771454, 0.18638470768928528, 0.20953556895256042, 0.19189506769180298, 0.16965368390083313, 0.14091962575912476, 0.20718711614608765, 0.14130134880542755, 0.21917971968650818, 0.1406257599592209, 0.10972878336906433, 0.2266089767217636, 0.10945507138967514, 0.14595837891101837, 0.1641550213098526, 0.26463425159454346, 0.1423131227493286, 0.13784953951835632, 0.2641357183456421, 0.26314061880111694, 0.14562895894050598, 0.1511656492948532, 0.17523230612277985, 0.18544672429561615, 0.13405217230319977, 0.13362617790699005, 0.11217363178730011, 0.16341614723205566, 0.14788047969341278, 0.2586328983306885, 0.15139871835708618, 0.1801193654537201, 0.144779235124588, 0.224339097738266, 0.13653691112995148, 0.22056931257247925, 0.11301456391811371, 0.1951497346162796, 0.15964171290397644, 0.25661787390708923, 0.14479805529117584, 0.19297514855861664, 0.19574715197086334, 0.19987717270851135, 0.17058831453323364, 0.23506733775138855, 0.13688303530216217, 0.17688153684139252, 0.16832588613033295, 0.22247318923473358, 0.19684042036533356, 0.11749515682458878, 0.20293079316616058, 0.17938049137592316, 0.22271592915058136, 0.17964205145835876, 0.11930961906909943, 0.16655442118644714, 0.20337486267089844, 0.16202308237552643, 0.13678722083568573, 0.12064508348703384, 0.14419125020503998, 0.19735416769981384, 0.2017284631729126, 0.19788967072963715, 0.17272034287452698, 0.2176545411348343, 0.13768190145492554, 0.18976640701293945, 0.22333282232284546, 0.19061648845672607, 0.15166547894477844, 0.16344983875751495, 0.18845440447330475, 0.16993997991085052, 0.14852723479270935, 0.23904132843017578, 0.17531847953796387, 0.14563214778900146, 0.15241223573684692, 0.1720249056816101, 0.12524650990962982, 0.17201383411884308, 0.19507963955402374, 0.1531527191400528, 0.14170409739017487, 0.1537996381521225, 0.1817350834608078, 0.17686767876148224, 0.12343146651983261, 0.14410294592380524, 0.2128598541021347, 0.17757922410964966, 0.16726307570934296, 0.18260443210601807, 0.19274801015853882, 0.15391628444194794, 0.12215208262205124, 0.20667597651481628, 0.14154230058193207, 0.19085364043712616, 0.18095356225967407, 0.14851340651512146, 0.20053333044052124, 0.21377262473106384, 0.1660224348306656, 0.18822403252124786, 0.2395387887954712, 0.1429111659526825, 0.12337206304073334, 0.18536505103111267, 0.16875135898590088, 0.1692093312740326, 0.18508654832839966, 0.1613028645515442, 0.1422039270401001, 0.12424960732460022, 0.16903896629810333, 0.12381073087453842, 0.15593063831329346, 0.15681412816047668, 0.15391860902309418, 0.1225070133805275, 0.24015873670578003, 0.15311624109745026, 0.16439825296401978, 0.14882418513298035, 0.24161429703235626, 0.16792769730091095, 0.14237287640571594, 0.16533295810222626, 0.1365714818239212, 0.14894628524780273, 0.1403616964817047, 0.18878625333309174, 0.15509194135665894, 0.17647384107112885, 0.17698878049850464, 0.11971614509820938, 0.170486181974411, 0.17556925117969513, 0.14325813949108124, 0.21106451749801636, 0.16014902293682098, 0.16288790106773376, 0.11833908408880234, 0.14671078324317932, 0.21338190138339996, 0.175955131649971, 0.13937854766845703, 0.17550133168697357, 0.19679661095142365, 0.22083094716072083, 0.21379153430461884, 0.158951535820961, 0.17616824805736542, 0.21317441761493683, 0.216200053691864, 0.14878913760185242, 0.16980089247226715, 0.1455952525138855, 0.16716235876083374, 0.11953724920749664, 0.1769866943359375, 0.1807846575975418, 0.18659819662570953, 0.14478808641433716, 0.11974364519119263, 0.1547618806362152, 0.14029648900032043, 0.1485472470521927, 0.16366487741470337, 0.1977802813053131, 0.16095279157161713, 0.16964861750602722, 0.1828731745481491, 0.14858022332191467, 0.24722853302955627, 0.15613523125648499, 0.1705666035413742, 0.14643394947052002, 0.19038797914981842, 0.21691612899303436, 0.19596128165721893, 0.1325460523366928, 0.1454806625843048, 0.14341221749782562, 0.22082290053367615, 0.17777487635612488, 0.20747831463813782, 0.17012055218219757, 0.17790432274341583, 0.1503608375787735, 0.15123149752616882, 0.2151133269071579, 0.1355842500925064, 0.15959814190864563, 0.18914726376533508, 0.14518749713897705, 0.16255664825439453, 0.172283336520195, 0.16098953783512115, 0.17636291682720184, 0.15321820974349976, 0.1574614942073822, 0.1346784383058548, 0.1343722641468048, 0.2099183052778244, 0.18626625835895538, 0.14780856668949127, 0.202455073595047, 0.17416632175445557, 0.14896951615810394, 0.15080241858959198, 0.14729344844818115, 0.1497296839952469, 0.1872239112854004, 0.18353591859340668, 0.1721782237291336, 0.17593267560005188]\n",
            "Val loss 0.16862031893204835\n",
            "Val auc roc 0.4936861313868613\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c51e6851c3e499aab5e64cf1f11730c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2596.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1689\n",
            "Train Losses : [0.15143024921417236, 0.12004895508289337, 0.18173848092556, 0.21061448752880096, 0.15010686218738556, 0.14455056190490723, 0.228705033659935, 0.2008291780948639, 0.17926324903964996, 0.1865123212337494, 0.1549241840839386, 0.13139452040195465, 0.14798370003700256, 0.15951067209243774, 0.11984383314847946, 0.15778848528862, 0.15741004049777985, 0.18945081532001495, 0.20818860828876495, 0.15391132235527039, 0.17613688111305237, 0.11981892585754395, 0.14120177924633026, 0.1632343828678131, 0.14726021885871887, 0.1457187831401825, 0.1526719182729721, 0.1746283769607544, 0.1995469182729721, 0.19261527061462402, 0.17765502631664276, 0.19194790720939636, 0.11930613964796066, 0.15160740911960602, 0.1197907105088234, 0.119456447660923, 0.17401228845119476, 0.1957319676876068, 0.15778622031211853, 0.18699756264686584, 0.18851077556610107, 0.152541846036911, 0.16095300018787384, 0.17793066799640656, 0.19130508601665497, 0.11892718076705933, 0.13653726875782013, 0.11911426484584808, 0.19107288122177124, 0.20173753798007965, 0.14464227855205536, 0.21709243953227997, 0.19838222861289978, 0.15212811529636383, 0.15647344291210175, 0.11891946196556091, 0.1942429542541504, 0.18378789722919464, 0.15190933644771576, 0.19346633553504944, 0.17176978290081024, 0.18383808434009552, 0.16403673589229584, 0.1555052399635315, 0.17340488731861115, 0.17306570708751678, 0.21552135050296783, 0.11914383620023727, 0.18157055974006653, 0.11872737109661102, 0.13464586436748505, 0.1955985724925995, 0.19420896470546722, 0.14551104605197906, 0.14308960735797882, 0.13442540168762207, 0.20313455164432526, 0.21292144060134888, 0.19347970187664032, 0.1726136952638626, 0.16958239674568176, 0.21404235064983368, 0.13599206507205963, 0.14093457162380219, 0.17469018697738647, 0.2048560231924057, 0.16921526193618774, 0.19837871193885803, 0.18600691854953766, 0.16444925963878632, 0.19020451605319977, 0.15439966320991516, 0.18511231243610382, 0.18098518252372742, 0.1597353219985962, 0.1810436099767685, 0.11926409602165222, 0.1530851572751999, 0.1627463698387146, 0.14652575552463531, 0.135131374001503, 0.11882297694683075, 0.1522199511528015, 0.17793211340904236, 0.13529029488563538, 0.21534298360347748, 0.17762082815170288, 0.1965918391942978, 0.1917358785867691, 0.17162859439849854, 0.17596891522407532, 0.22687210142612457, 0.16741326451301575, 0.16166862845420837, 0.18164600431919098, 0.18168438971042633, 0.16854389011859894, 0.11874730885028839, 0.2208578884601593, 0.1685834378004074, 0.11862844228744507, 0.14978660643100739, 0.11873891949653625, 0.18105657398700714, 0.21947132050991058, 0.14678889513015747, 0.22373735904693604, 0.13761498034000397, 0.16595904529094696, 0.17899931967258453, 0.15304365754127502, 0.14801710844039917, 0.20658734440803528, 0.2135920226573944, 0.17982064187526703, 0.1403127908706665, 0.11875366419553757, 0.14480379223823547, 0.1607871800661087, 0.19393464922904968, 0.13959641754627228, 0.13802793622016907, 0.11840126663446426, 0.1411619633436203, 0.16412091255187988, 0.22296778857707977, 0.22631306946277618, 0.14093472063541412, 0.15674687922000885, 0.2179809808731079, 0.22268147766590118, 0.1852443814277649, 0.1431325525045395, 0.1723547726869583, 0.1990712434053421, 0.1392749696969986, 0.15856648981571198, 0.1557370275259018, 0.18701492249965668, 0.16770724952220917, 0.17064635455608368, 0.11835170537233353, 0.1184956356883049, 0.17274655401706696, 0.11821967363357544, 0.17979148030281067, 0.17759586870670319, 0.1441628634929657, 0.21158133447170258, 0.1181582435965538, 0.20442402362823486, 0.14417673647403717, 0.11824773997068405, 0.11820178478956223, 0.16363944113254547, 0.22813361883163452, 0.1695793718099594, 0.18350908160209656, 0.1765120029449463, 0.24735809862613678, 0.1541142761707306, 0.12773726880550385, 0.11797142028808594, 0.19415082037448883, 0.18375305831432343, 0.16868658363819122, 0.2105778604745865, 0.21744664013385773, 0.21327877044677734, 0.19104017317295074, 0.18083679676055908, 0.17198893427848816, 0.15980173647403717, 0.22303736209869385, 0.153474360704422, 0.18233372271060944, 0.24741467833518982, 0.15162613987922668, 0.21165543794631958, 0.15642882883548737, 0.17847776412963867, 0.14762040972709656, 0.17354224622249603, 0.17701727151870728, 0.11834115535020828, 0.1904834508895874, 0.14969466626644135, 0.15906596183776855, 0.14500774443149567, 0.21458660066127777, 0.15952248871326447, 0.16863404214382172, 0.21903759241104126, 0.1457424759864807, 0.11865830421447754, 0.1856539100408554, 0.1565680354833603, 0.11847267299890518, 0.2059670388698578, 0.11831090599298477, 0.13838890194892883, 0.2473469376564026, 0.17730110883712769, 0.20095375180244446, 0.1755661517381668, 0.15781064331531525, 0.2133309245109558, 0.15156665444374084, 0.22138270735740662, 0.1495768427848816, 0.22450529038906097, 0.20906832814216614, 0.15663114190101624, 0.18089249730110168, 0.18733316659927368, 0.2464756965637207, 0.207864910364151, 0.2463212013244629, 0.1186460554599762, 0.16846944391727448, 0.15792742371559143, 0.13938114047050476, 0.17582184076309204, 0.14453734457492828, 0.11883661895990372, 0.18850165605545044, 0.11899809539318085, 0.14439451694488525, 0.14184419810771942, 0.14759071171283722, 0.1187441423535347, 0.1187216192483902, 0.20505449175834656, 0.1547503024339676, 0.20673182606697083, 0.19836634397506714, 0.20091284811496735, 0.24620217084884644, 0.14837606251239777, 0.13817107677459717, 0.1778186559677124, 0.11874441057443619, 0.2021106332540512, 0.1867244392633438, 0.16181999444961548, 0.2164917290210724, 0.13748496770858765, 0.14155824482440948, 0.1449604481458664, 0.21639183163642883, 0.18000178039073944, 0.14007459580898285, 0.171611487865448, 0.1683432161808014, 0.146410271525383, 0.11893855780363083, 0.16327369213104248, 0.14187240600585938, 0.19291424751281738, 0.11876334249973297, 0.16146919131278992, 0.15346084535121918, 0.2006729692220688, 0.19493140280246735, 0.18544569611549377, 0.16688476502895355, 0.15272679924964905, 0.22376951575279236, 0.1890655905008316, 0.11860797554254532, 0.15362243354320526, 0.21892854571342468, 0.1306592971086502, 0.2049727588891983, 0.14606119692325592, 0.2466655820608139, 0.14311830699443817, 0.14607839286327362, 0.20958347618579865, 0.11866872012615204, 0.17692238092422485, 0.20647603273391724, 0.15599875152111053, 0.15783755481243134, 0.22937802970409393, 0.16731448471546173, 0.14706626534461975, 0.1404222846031189, 0.22162632644176483, 0.11872227489948273, 0.11875515431165695, 0.17978836596012115, 0.2462174892425537, 0.17609243094921112, 0.21419081091880798, 0.13173650205135345, 0.163965106010437, 0.19190555810928345, 0.15444520115852356, 0.16102172434329987, 0.11868884414434433, 0.1749482899904251, 0.206568643450737, 0.1589893102645874, 0.22282233834266663, 0.11897601932287216, 0.14526350796222687, 0.14990603923797607, 0.21026915311813354, 0.15991438925266266, 0.195567786693573, 0.13901910185813904, 0.11889538168907166, 0.15096670389175415, 0.16340003907680511, 0.17299585044384003, 0.1525288224220276, 0.1757878214120865, 0.11868388950824738, 0.17190176248550415, 0.16968373954296112, 0.1537829041481018, 0.11901790648698807, 0.1589709371328354, 0.18577656149864197, 0.17102424800395966, 0.19683939218521118, 0.24630393087863922, 0.20799076557159424, 0.1771279275417328, 0.1852238029241562, 0.1497156172990799, 0.15756933391094208, 0.1387428194284439, 0.11867697536945343, 0.15540890395641327, 0.2116670161485672, 0.195188969373703, 0.11885574460029602, 0.19348357617855072, 0.18215090036392212, 0.17208433151245117, 0.22014108300209045, 0.11883120238780975, 0.14120233058929443, 0.24624712765216827, 0.19235560297966003, 0.11855453252792358, 0.14824266731739044, 0.2462340146303177, 0.18095563352108002, 0.18026579916477203, 0.13187450170516968, 0.21063272655010223, 0.2460879385471344, 0.1805909425020218, 0.11920787394046783, 0.14397002756595612, 0.11885488033294678, 0.2060168981552124, 0.21396000683307648, 0.1333194077014923, 0.18605472147464752, 0.11929824203252792, 0.17983292043209076, 0.2294139415025711, 0.19614970684051514, 0.17846174538135529, 0.13813842833042145, 0.1716497540473938, 0.21577277779579163, 0.20216205716133118, 0.17369866371154785, 0.15222036838531494, 0.11905131489038467, 0.21979719400405884, 0.11924673616886139, 0.17729048430919647, 0.16190700232982635, 0.2077217996120453, 0.2082970142364502, 0.18936371803283691, 0.1832992434501648, 0.1575915366411209, 0.18627585470676422, 0.17211800813674927, 0.15979814529418945, 0.1738322228193283, 0.1465456336736679, 0.15533408522605896, 0.21092146635055542, 0.1498100608587265, 0.14556430280208588, 0.20232553780078888, 0.11921171098947525, 0.14536958932876587, 0.119547039270401, 0.11939207464456558, 0.11910571157932281, 0.1571170836687088, 0.15532368421554565, 0.1759243607521057, 0.13987793028354645, 0.1709907501935959, 0.1841367483139038, 0.11893273890018463, 0.11895415931940079, 0.21652932465076447, 0.16448430716991425, 0.14984074234962463, 0.1185651570558548, 0.17079171538352966, 0.15726955235004425, 0.11870059370994568, 0.15690144896507263, 0.18370571732521057, 0.16005542874336243, 0.13993437588214874, 0.2148454785346985, 0.199534609913826, 0.15450824797153473, 0.15956304967403412, 0.11824412643909454, 0.18531534075737, 0.17351453006267548, 0.11837263405323029, 0.21645043790340424, 0.15893135964870453, 0.1566222757101059, 0.183920755982399, 0.14054641127586365, 0.1807616651058197, 0.11800028383731842, 0.15917831659317017, 0.16079945862293243, 0.19430826604366302, 0.11796381324529648, 0.2175215184688568, 0.13607032597064972, 0.13936705887317657, 0.14939042925834656, 0.16818253695964813, 0.1841668039560318, 0.16622203588485718, 0.16259360313415527, 0.1325424164533615, 0.18616507947444916, 0.16484494507312775, 0.14678727090358734, 0.19026006758213043, 0.24788948893547058, 0.14543505012989044, 0.16513283550739288, 0.13974763453006744, 0.1373959183692932, 0.1425248235464096, 0.14863009750843048, 0.1178509071469307, 0.15819789469242096, 0.1393730193376541, 0.14696738123893738, 0.18203380703926086, 0.16998516023159027, 0.11765725910663605, 0.21518497169017792, 0.2185225933790207, 0.1858605444431305, 0.18079426884651184, 0.13410907983779907, 0.1710905134677887, 0.11759109795093536, 0.16857071220874786, 0.20977512001991272, 0.24824689328670502, 0.1631828397512436, 0.1738848239183426, 0.1708492636680603, 0.1751806139945984, 0.21769025921821594, 0.1687471568584442, 0.117843396961689, 0.14349451661109924, 0.18352705240249634, 0.2085219919681549, 0.1954338401556015, 0.22076664865016937, 0.18648919463157654, 0.1635885387659073, 0.1985360085964203, 0.14136990904808044, 0.18060825765132904, 0.1855861246585846, 0.1781441867351532, 0.1580401510000229, 0.2145780771970749, 0.11798684298992157, 0.21076150238513947, 0.17303824424743652, 0.17899367213249207, 0.16796156764030457, 0.16015230119228363, 0.18584081530570984, 0.17379356920719147, 0.1457015424966812, 0.21353742480278015, 0.16095119714736938, 0.14651750028133392, 0.1319330781698227, 0.14347071945667267, 0.20559540390968323, 0.11834784597158432, 0.18090008199214935, 0.17050571739673615, 0.1924924999475479, 0.16165857017040253, 0.11830060184001923, 0.11800327152013779, 0.16640202701091766, 0.14277957379817963, 0.11808211356401443, 0.17982004582881927, 0.1396074742078781, 0.16790221631526947, 0.11824996769428253, 0.16786913573741913, 0.17463909089565277, 0.13819439709186554, 0.22063320875167847, 0.18187303841114044, 0.172926664352417, 0.11789988726377487, 0.11787229031324387, 0.18267005681991577, 0.2035251408815384, 0.1691959947347641, 0.19717690348625183, 0.15545298159122467, 0.19019894301891327, 0.18216374516487122, 0.14896078407764435, 0.2475898414850235, 0.19119861721992493, 0.13866059482097626, 0.15656888484954834, 0.19308525323867798, 0.22227339446544647, 0.14731045067310333, 0.23264555633068085, 0.18793858587741852, 0.14226219058036804, 0.16091439127922058, 0.14185015857219696, 0.1743309497833252, 0.1701890528202057, 0.1182018518447876, 0.18192625045776367, 0.15177854895591736, 0.18451300263404846, 0.1399247795343399, 0.15070918202400208, 0.14949657022953033, 0.17077364027500153, 0.13523241877555847, 0.14906446635723114, 0.1883879005908966, 0.1683262288570404, 0.16183063387870789, 0.13815367221832275, 0.20594264566898346, 0.1946863830089569, 0.2480742335319519, 0.2040347456932068, 0.17005187273025513, 0.1695578396320343, 0.11806429922580719, 0.14971131086349487, 0.13490374386310577, 0.18676218390464783, 0.13251781463623047, 0.14104364812374115, 0.1772819608449936, 0.19193094968795776, 0.17506536841392517, 0.129937544465065, 0.22669854760169983, 0.15840768814086914, 0.15491195023059845, 0.15012028813362122, 0.1668267697095871, 0.20698298513889313, 0.1176777258515358, 0.11756963282823563, 0.2006341964006424, 0.1176714226603508, 0.1176886111497879, 0.1272750347852707, 0.19584383070468903, 0.18906748294830322, 0.14204950630664825, 0.17328070104122162, 0.22723011672496796, 0.1840936541557312, 0.16749152541160583, 0.17751763761043549, 0.1643269956111908, 0.17310409247875214, 0.1487327516078949, 0.15521308779716492, 0.13131608068943024, 0.1792878657579422, 0.16643628478050232, 0.14008058607578278, 0.1425388902425766, 0.1655721813440323, 0.13482576608657837, 0.22449858486652374, 0.11763014644384384, 0.15831826627254486, 0.16795988380908966, 0.16275443136692047, 0.24883440136909485, 0.18127407133579254, 0.11733334511518478, 0.1863279491662979, 0.18206268548965454, 0.20387528836727142, 0.11729446798563004, 0.11736377328634262, 0.14716728031635284, 0.1603979766368866, 0.15648113191127777, 0.16807565093040466, 0.14074711501598358, 0.15327157080173492, 0.1800251603126526, 0.17471639811992645, 0.21847189962863922, 0.20817364752292633, 0.12970218062400818, 0.15219178795814514, 0.1713285744190216, 0.18327981233596802, 0.1890547126531601, 0.14062103629112244, 0.1364973783493042, 0.1172473132610321, 0.19554714858531952, 0.17576603591442108, 0.18454615771770477, 0.18007630109786987, 0.11709199100732803, 0.20228339731693268, 0.18683035671710968, 0.18524394929409027, 0.13876231014728546, 0.2317613661289215, 0.19655553996562958, 0.14939919114112854, 0.15441325306892395, 0.2020721584558487, 0.14649033546447754, 0.14811758697032928, 0.1891278624534607, 0.14275868237018585, 0.11711964011192322, 0.13811279833316803, 0.19032062590122223, 0.1508200466632843, 0.16999369859695435, 0.11705959588289261, 0.14154139161109924, 0.14041660726070404, 0.1622033715248108, 0.17722076177597046, 0.1430891752243042, 0.13779930770397186, 0.2190476357936859, 0.1593628078699112, 0.15706902742385864, 0.18533040583133698, 0.1565193086862564, 0.1167769506573677, 0.18125216662883759, 0.11666835844516754, 0.14234741032123566, 0.16879689693450928, 0.11654386669397354, 0.14271818101406097, 0.174147829413414, 0.1961771547794342, 0.11654043942689896, 0.14279437065124512, 0.13241828978061676, 0.1861410140991211, 0.13861018419265747, 0.18341073393821716, 0.16737496852874756, 0.1520705670118332, 0.16951967775821686, 0.11624316871166229, 0.17552290856838226, 0.13881397247314453, 0.14842569828033447, 0.17915990948677063, 0.21279418468475342, 0.15059976279735565, 0.13330119848251343, 0.22148646414279938, 0.17329716682434082, 0.19441188871860504, 0.183579221367836, 0.1762676239013672, 0.11582369357347488, 0.14965751767158508, 0.15096814930438995, 0.15417371690273285, 0.17986725270748138, 0.11611802130937576, 0.17885100841522217, 0.14425334334373474, 0.1782396137714386, 0.1736821085214615, 0.11573166400194168, 0.2021719366312027, 0.14941205084323883, 0.21905747056007385, 0.2515355050563812, 0.16734196245670319, 0.15860366821289062, 0.21495461463928223, 0.1722508817911148, 0.18353091180324554, 0.2086750864982605, 0.15834732353687286, 0.19915832579135895, 0.16098570823669434, 0.17219781875610352, 0.25167563557624817, 0.2112913876771927, 0.14460672438144684, 0.25091618299484253, 0.1644732505083084, 0.1501714140176773, 0.20505116879940033, 0.14988134801387787, 0.20911549031734467, 0.1623009741306305, 0.14110581576824188, 0.11609409749507904, 0.1463027149438858, 0.17041191458702087, 0.14615288376808167, 0.11666970700025558, 0.20892322063446045, 0.13884557783603668, 0.16061879694461823, 0.17447134852409363, 0.1499558687210083, 0.25063997507095337, 0.19737710058689117, 0.154722660779953, 0.11624453216791153, 0.17759044468402863, 0.14508236944675446, 0.14834243059158325, 0.1597585380077362, 0.2054278701543808, 0.21527862548828125, 0.18728411197662354, 0.1458604782819748, 0.21325187385082245, 0.18397948145866394, 0.16357196867465973, 0.1358741968870163, 0.1483043134212494, 0.17060516774654388, 0.11622241139411926, 0.2505364716053009, 0.15936751663684845, 0.14163261651992798, 0.25042811036109924, 0.2014184296131134, 0.21879029273986816, 0.2501756250858307, 0.14233766496181488, 0.2195701003074646, 0.1369270384311676, 0.17798812687397003, 0.11640038341283798, 0.11652439087629318, 0.17958198487758636, 0.13809531927108765, 0.21954898536205292, 0.16419416666030884, 0.19045643508434296, 0.1386733502149582, 0.14496764540672302, 0.1683548390865326, 0.1962667852640152, 0.15151239931583405, 0.16023974120616913, 0.212156280875206, 0.13699425756931305, 0.1564992070198059, 0.15360033512115479, 0.1439821571111679, 0.11654843389987946, 0.11655662953853607, 0.16663895547389984, 0.13330388069152832, 0.2505648732185364, 0.15696439146995544, 0.15097640454769135, 0.16548652946949005, 0.14275261759757996, 0.14639492332935333, 0.2505965828895569, 0.14601874351501465, 0.11662024259567261, 0.14458397030830383, 0.17483338713645935, 0.14549607038497925, 0.1978246122598648, 0.11649422347545624, 0.13869675993919373, 0.17393597960472107, 0.17119431495666504, 0.18501459062099457, 0.21165993809700012, 0.16394372284412384, 0.17136996984481812, 0.21342433989048004, 0.18591926991939545, 0.22520698606967926, 0.1338377445936203, 0.14319299161434174, 0.1892603039741516, 0.16261844336986542, 0.1483994424343109, 0.19694116711616516, 0.16392287611961365, 0.1683860421180725, 0.1515740007162094, 0.17322564125061035, 0.14387252926826477, 0.18250706791877747, 0.2124509960412979, 0.17444853484630585, 0.1812855452299118, 0.16093555092811584, 0.22001692652702332, 0.17639674246311188, 0.1579132378101349, 0.18375590443611145, 0.1669280230998993, 0.18603621423244476, 0.12941238284111023, 0.1835792511701584, 0.16684438288211823, 0.18139295279979706, 0.11674243211746216, 0.18253016471862793, 0.21185016632080078, 0.1444149762392044, 0.11653362959623337, 0.1389128565788269, 0.2500811517238617, 0.2173965573310852, 0.15790022909641266, 0.1352684497833252, 0.17856711149215698, 0.2082325965166092, 0.11658008396625519, 0.19114457070827484, 0.16972783207893372, 0.21935395896434784, 0.11683856695890427, 0.19680525362491608, 0.17973311245441437, 0.11699172854423523, 0.11666767299175262, 0.18122714757919312, 0.2199290543794632, 0.16996441781520844, 0.1428755670785904, 0.1596078872680664, 0.16190285980701447, 0.24956464767456055, 0.1421934813261032, 0.1640503704547882, 0.1811738908290863, 0.21242089569568634, 0.1822260320186615, 0.21010088920593262, 0.18828457593917847, 0.11713118106126785, 0.15003658831119537, 0.19471319019794464, 0.24986328184604645, 0.1169971376657486, 0.11710655689239502, 0.22317057847976685, 0.15428780019283295, 0.15729007124900818, 0.1830296367406845, 0.14885887503623962, 0.14304526150226593, 0.16764546930789948, 0.14923380315303802, 0.1291617602109909, 0.1813054233789444, 0.1701534390449524, 0.19327640533447266, 0.15746067464351654, 0.14154121279716492, 0.13519318401813507, 0.14522819221019745, 0.24982528388500214, 0.2015412598848343, 0.1496107280254364, 0.15150298178195953, 0.13311679661273956, 0.11683179438114166, 0.1484421342611313, 0.19603057205677032, 0.14975394308567047, 0.24992607533931732, 0.19461672008037567, 0.18156039714813232, 0.17609566450119019, 0.1759026050567627, 0.21651729941368103, 0.1729673594236374, 0.17665664851665497, 0.15165740251541138, 0.24937233328819275, 0.13874715566635132, 0.1334294080734253, 0.1897912621498108, 0.17774401605129242, 0.16981378197669983, 0.19565054774284363, 0.14500977098941803, 0.1692182719707489, 0.15243205428123474, 0.14502960443496704, 0.16801384091377258, 0.1398237943649292, 0.2493203580379486, 0.1590656191110611, 0.1431017518043518, 0.2201698124408722, 0.2043449729681015, 0.14926688373088837, 0.17611584067344666, 0.17661245167255402, 0.15382687747478485, 0.11724033206701279, 0.14325833320617676, 0.2190854400396347, 0.17765076458454132, 0.17339521646499634, 0.11713797599077225, 0.11703057587146759, 0.11699840426445007, 0.21499310433864594, 0.1761830896139145, 0.23033951222896576, 0.16371746361255646, 0.14327000081539154, 0.19063709676265717, 0.13943718373775482, 0.1643490493297577, 0.17932356894016266, 0.24956172704696655, 0.19112294912338257, 0.11700554937124252, 0.17717039585113525, 0.185259148478508, 0.15482932329177856, 0.15260560810565948, 0.20218954980373383, 0.24899937212467194, 0.2126944661140442, 0.2492624670267105, 0.18280792236328125, 0.20080317556858063, 0.19143955409526825, 0.17422553896903992, 0.21553316712379456, 0.11721716821193695, 0.17378517985343933, 0.17187634110450745, 0.18895699083805084, 0.202397882938385, 0.13343986868858337, 0.16851815581321716, 0.24834072589874268, 0.1697729080915451, 0.15952596068382263, 0.17291325330734253, 0.24824906885623932, 0.11760298162698746, 0.11755430698394775, 0.19637498259544373, 0.13563719391822815, 0.2234901785850525, 0.15979018807411194, 0.20719516277313232, 0.15080590546131134, 0.1703503429889679, 0.21404951810836792, 0.18543007969856262, 0.17984992265701294, 0.15528880059719086, 0.18362653255462646, 0.1376255750656128, 0.14180909097194672, 0.21184219419956207, 0.13828982412815094, 0.11793947964906693, 0.1799590289592743, 0.13854408264160156, 0.13937000930309296, 0.1772223711013794, 0.1441539078950882, 0.14163845777511597, 0.182021364569664, 0.19204479455947876, 0.13763022422790527, 0.19876621663570404, 0.2478426843881607, 0.1178712472319603, 0.2224854677915573, 0.18429410457611084, 0.14626692235469818, 0.17984022200107574, 0.1790499985218048, 0.17978186905384064, 0.1633235365152359, 0.19474992156028748, 0.1485079675912857, 0.2149728238582611, 0.11819122731685638, 0.17056962847709656, 0.221281960606575, 0.11788799613714218, 0.17782041430473328, 0.1953587681055069, 0.11791682988405228, 0.15154772996902466, 0.22296656668186188, 0.1665068119764328, 0.16221535205841064, 0.24793894588947296, 0.17318014800548553, 0.13500599563121796, 0.18642491102218628, 0.1501830518245697, 0.18306683003902435, 0.14047805964946747, 0.19199399650096893, 0.14177905023097992, 0.18896062672138214, 0.16462571918964386, 0.14886116981506348, 0.1868746280670166, 0.13839666545391083, 0.14376863837242126, 0.1526491343975067, 0.172162726521492, 0.13105933368206024, 0.1512002795934677, 0.18757472932338715, 0.1316782832145691, 0.198683500289917, 0.1393771767616272, 0.1998838633298874, 0.24801352620124817, 0.1176738515496254, 0.17022167146205902, 0.12841521203517914, 0.2253592610359192, 0.18299910426139832, 0.1176910400390625, 0.2113083153963089, 0.18926723301410675, 0.11781236529350281, 0.1532483696937561, 0.168657585978508, 0.11781243234872818, 0.15234367549419403, 0.17955218255519867, 0.16722224652767181, 0.2314460724592209, 0.14017172157764435, 0.13369008898735046, 0.16371941566467285, 0.20575709640979767, 0.1370592564344406, 0.22346049547195435, 0.14358578622341156, 0.14539191126823425, 0.1781059056520462, 0.17516976594924927, 0.1767723709344864, 0.1402483880519867, 0.17761586606502533, 0.18299444019794464, 0.20730866491794586, 0.1364438831806183, 0.14072096347808838, 0.14877308905124664, 0.17741325497627258, 0.11764489114284515, 0.1683092713356018, 0.1836957037448883, 0.13351352512836456, 0.19144681096076965, 0.1177164688706398, 0.11753900349140167, 0.18063412606716156, 0.18139663338661194, 0.1397479921579361, 0.207029327750206, 0.14248181879520416, 0.1428034007549286, 0.20974430441856384, 0.188125878572464, 0.1438172161579132, 0.21612152457237244, 0.13665851950645447, 0.20463988184928894, 0.13277877867221832, 0.20399200916290283, 0.22355780005455017, 0.16306687891483307, 0.22092309594154358, 0.13801902532577515, 0.14797329902648926, 0.1740872859954834, 0.15114928781986237, 0.18399615585803986, 0.1442873477935791, 0.18011754751205444, 0.19437937438488007, 0.13946105539798737, 0.1408720165491104, 0.15151041746139526, 0.18064747750759125, 0.22607733309268951, 0.14770475029945374, 0.1705557256937027, 0.1178312823176384, 0.21058301627635956, 0.215546652674675, 0.22388100624084473, 0.14252248406410217, 0.14018507301807404, 0.13813605904579163, 0.1955122947692871, 0.13973544538021088, 0.14600953459739685, 0.21541908383369446, 0.17783008515834808, 0.17031033337116241, 0.22954557836055756, 0.11763542890548706, 0.18033534288406372, 0.2226981222629547, 0.18786779046058655, 0.14514102041721344, 0.15084794163703918, 0.1709774285554886, 0.16998089849948883, 0.18989911675453186, 0.11812350898981094, 0.11772377789020538, 0.15359462797641754, 0.24792064726352692, 0.11779850721359253, 0.13614042103290558, 0.17939604818820953, 0.20017465949058533, 0.16626191139221191, 0.17182962596416473, 0.18190240859985352, 0.11764736473560333, 0.11769428104162216, 0.18121613562107086, 0.17487147450447083, 0.13222983479499817, 0.18289503455162048, 0.14261914789676666, 0.11773630976676941, 0.11750444024801254, 0.20504175126552582, 0.17902834713459015, 0.13567104935646057, 0.15279518067836761, 0.14737394452095032, 0.24844259023666382, 0.2487596720457077, 0.16833661496639252, 0.18500389158725739, 0.1661711037158966, 0.1796257495880127, 0.20197468996047974, 0.16165919601917267, 0.18361195921897888, 0.17160183191299438, 0.18322400748729706, 0.11754392087459564, 0.18691614270210266, 0.13831683993339539, 0.1399550586938858, 0.17346607148647308, 0.22182171046733856, 0.1836150735616684, 0.1174926832318306, 0.1572529375553131, 0.14405688643455505, 0.18588878214359283, 0.14269860088825226, 0.16692355275154114, 0.13513749837875366, 0.14469070732593536, 0.13091185688972473, 0.11746726930141449, 0.13924193382263184, 0.1707039177417755, 0.20749828219413757, 0.17607657611370087, 0.1789274662733078, 0.19019809365272522, 0.14050817489624023, 0.17365311086177826, 0.17197005450725555, 0.1297760307788849, 0.14742277562618256, 0.18864886462688446, 0.1441950649023056, 0.14789526164531708, 0.17025548219680786, 0.19517257809638977, 0.1613791584968567, 0.16067373752593994, 0.2022777944803238, 0.1679726094007492, 0.1397169530391693, 0.21905294060707092, 0.19985473155975342, 0.13894817233085632, 0.15602530539035797, 0.14365112781524658, 0.2035384178161621, 0.2486097663640976, 0.14556269347667694, 0.1360948383808136, 0.13790084421634674, 0.17779041826725006, 0.19930292665958405, 0.11732885241508484, 0.11739485710859299, 0.17224828898906708, 0.15908659994602203, 0.17217028141021729, 0.14804533123970032, 0.15329194068908691, 0.23529601097106934, 0.2054467350244522, 0.16952042281627655, 0.11730485409498215, 0.14538350701332092, 0.13638395071029663, 0.18813438713550568, 0.18734978139400482, 0.14726823568344116, 0.14552652835845947, 0.16035981476306915, 0.11740753799676895, 0.2172917276620865, 0.15095236897468567, 0.22333519160747528, 0.1285589188337326, 0.14892303943634033, 0.14393603801727295, 0.18316034972667694, 0.1174527183175087, 0.17818757891654968, 0.11721810698509216, 0.16879533231258392, 0.18842965364456177, 0.1286706179380417, 0.18679644167423248, 0.18628419935703278, 0.14947614073753357, 0.24861280620098114, 0.19989369809627533, 0.15535661578178406, 0.11751274764537811, 0.20446345210075378, 0.149536594748497, 0.22120235860347748, 0.2140464335680008, 0.13664619624614716, 0.11717332154512405, 0.1778697818517685, 0.11743012815713882, 0.11727716773748398, 0.1753653585910797, 0.1424146443605423, 0.17100757360458374, 0.18541884422302246, 0.14563438296318054, 0.1454336792230606, 0.18035884201526642, 0.11710645258426666, 0.17346763610839844, 0.1751628816127777, 0.16673661768436432, 0.17914369702339172, 0.23102521896362305, 0.2492191046476364, 0.11709265410900116, 0.15117685496807098, 0.18878281116485596, 0.19461005926132202, 0.18121521174907684, 0.152207612991333, 0.21520449221134186, 0.11708333343267441, 0.18126307427883148, 0.22656609117984772, 0.17440344393253326, 0.14502602815628052, 0.18649965524673462, 0.11712922900915146, 0.16782042384147644, 0.21276359260082245, 0.14796724915504456, 0.14270778000354767, 0.16996441781520844, 0.1922629177570343, 0.18911385536193848, 0.1853935867547989, 0.1758689433336258, 0.20171813666820526, 0.18924583494663239, 0.17098385095596313, 0.1465681791305542, 0.24898527562618256, 0.14677119255065918, 0.16885873675346375, 0.17175884544849396, 0.13999150693416595, 0.220163956284523, 0.16984064877033234, 0.11754841357469559, 0.18190355598926544, 0.14283952116966248, 0.1535770148038864, 0.16354691982269287, 0.11721569299697876, 0.16407661139965057, 0.11741878092288971, 0.21066290140151978, 0.14921365678310394, 0.21853391826152802, 0.2121652215719223, 0.1783876270055771, 0.1425427496433258, 0.14163456857204437, 0.19009579718112946, 0.13429473340511322, 0.17182743549346924, 0.17769776284694672, 0.21598687767982483, 0.16907797753810883, 0.1970905363559723, 0.18698742985725403, 0.14117853343486786, 0.2033800184726715, 0.2149350941181183, 0.17646272480487823, 0.18588504195213318, 0.1445438265800476, 0.16338005661964417, 0.16115573048591614, 0.11725613474845886, 0.16090816259384155, 0.11722422391176224, 0.14029285311698914, 0.18021637201309204, 0.14639948308467865, 0.14330285787582397, 0.1474255919456482, 0.14363691210746765, 0.14430487155914307, 0.21605248749256134, 0.1509915590286255, 0.17594383656978607, 0.17685134708881378, 0.24886901676654816, 0.17872485518455505, 0.19818861782550812, 0.21556755900382996, 0.18361063301563263, 0.11712300777435303, 0.149837926030159, 0.1578933447599411, 0.14813201129436493, 0.1578661948442459, 0.11707291007041931, 0.20953220129013062, 0.22592957317829132, 0.17659814655780792, 0.19047148525714874, 0.11738012731075287, 0.11707966029644012, 0.15741360187530518, 0.14825648069381714, 0.16999182105064392, 0.248890221118927, 0.1710987538099289, 0.1575145721435547, 0.17054353654384613, 0.18720653653144836, 0.11708750575780869, 0.19967061281204224, 0.17488780617713928, 0.11708366125822067, 0.18509528040885925, 0.18437862396240234, 0.15376515686511993, 0.15551002323627472, 0.15609987080097198, 0.17797604203224182, 0.14070291817188263, 0.1789344847202301, 0.17582355439662933, 0.17622102797031403, 0.1565116047859192, 0.14303335547447205, 0.24899490177631378, 0.1333692967891693, 0.20831507444381714, 0.1380029022693634, 0.13806946575641632, 0.18696364760398865, 0.21306432783603668, 0.14321984350681305, 0.2491077184677124, 0.15883123874664307, 0.2213127464056015, 0.15414738655090332, 0.16894301772117615, 0.11702387779951096, 0.14730098843574524, 0.1630297154188156, 0.12876258790493011, 0.1171039342880249, 0.1712435930967331, 0.13956613838672638, 0.183147594332695, 0.20501570403575897, 0.143052875995636, 0.17237290740013123, 0.2491198480129242, 0.15355059504508972, 0.15311336517333984, 0.20293745398521423, 0.19663706421852112, 0.1828848123550415, 0.18466272950172424, 0.19248583912849426, 0.1815072000026703, 0.14807525277137756, 0.14077676832675934, 0.1543104648590088, 0.1809050291776657, 0.11741823703050613, 0.11707202345132828, 0.21332137286663055, 0.11747825890779495, 0.13991796970367432, 0.14702636003494263, 0.1784973293542862, 0.14305654168128967, 0.11725640296936035, 0.13773207366466522, 0.19009985029697418, 0.15074001252651215, 0.17884378135204315, 0.1597421020269394, 0.1519736647605896, 0.14311017096042633, 0.21931776404380798, 0.14161644876003265, 0.1870725005865097, 0.1578001230955124, 0.18356925249099731, 0.1807643175125122, 0.1617921143770218, 0.17561125755310059, 0.11698193848133087, 0.14988060295581818, 0.18150772154331207, 0.11671505123376846, 0.15208226442337036, 0.13766345381736755, 0.21356940269470215, 0.14394764602184296, 0.19101877510547638, 0.24982228875160217, 0.116618312895298, 0.13513778150081635, 0.1449882835149765, 0.14962705969810486, 0.15081198513507843, 0.1498623490333557, 0.16198812425136566, 0.14934110641479492, 0.21992476284503937, 0.20125705003738403, 0.13315105438232422, 0.176522359251976, 0.1501697301864624, 0.18327920138835907, 0.1824474036693573, 0.16346420347690582, 0.17526602745056152, 0.11671388149261475, 0.14896085858345032, 0.14575953781604767, 0.13651897013187408, 0.18312694132328033, 0.13295380771160126, 0.14629711210727692, 0.14647753536701202, 0.23722036182880402, 0.1590273231267929, 0.12902288138866425, 0.21882081031799316, 0.15196889638900757, 0.2155558317899704, 0.18410629034042358, 0.19251649081707, 0.139322891831398, 0.1325731873512268, 0.2033148854970932, 0.17290201783180237, 0.12771828472614288, 0.24994024634361267, 0.19961245357990265, 0.1661800593137741, 0.19639694690704346, 0.21965819597244263, 0.13584944605827332, 0.18398261070251465, 0.19967547059059143, 0.20562022924423218, 0.22701069712638855, 0.213724285364151, 0.1904457062482834, 0.14558525383472443, 0.18072456121444702, 0.1676219254732132, 0.11671999096870422, 0.11669693887233734, 0.20126862823963165, 0.1745183765888214, 0.15822461247444153, 0.15438418090343475, 0.2043352574110031, 0.1865093857049942, 0.1370837688446045, 0.17200306057929993, 0.14110076427459717, 0.1638331562280655, 0.11685497313737869, 0.19088859856128693, 0.19706004858016968, 0.17126737534999847, 0.14684629440307617, 0.18684032559394836, 0.19741006195545197, 0.11709898710250854, 0.11673251539468765, 0.2149246633052826, 0.14895111322402954, 0.19231745600700378, 0.17391400039196014, 0.13990247249603271, 0.14162737131118774, 0.11698794364929199, 0.2173333466053009, 0.18362995982170105, 0.11669386923313141, 0.173675999045372, 0.2290256917476654, 0.1899217814207077, 0.1429804563522339, 0.11674338579177856, 0.24992462992668152, 0.14468733966350555, 0.15772570669651031, 0.14960521459579468, 0.20133395493030548, 0.14508993923664093, 0.24979321658611298, 0.21587535738945007, 0.1970975548028946, 0.15582576394081116, 0.1631179004907608, 0.11710616201162338, 0.14696703851222992, 0.14352114498615265, 0.11678779125213623, 0.17557060718536377, 0.14874710142612457, 0.17082233726978302, 0.19287948310375214, 0.17600920796394348, 0.2242031991481781, 0.14426347613334656, 0.20123161375522614, 0.17182223498821259, 0.18894341588020325, 0.2501298189163208, 0.13637439906597137, 0.22698116302490234, 0.15026508271694183, 0.18298979103565216, 0.14558419585227966, 0.11667191982269287, 0.19863879680633545, 0.20783624053001404, 0.15639255940914154, 0.1703689992427826, 0.20568934082984924, 0.18897351622581482, 0.15215259790420532, 0.22338660061359406, 0.21022023260593414, 0.14949162304401398, 0.11690226942300797, 0.17590570449829102, 0.1948702335357666, 0.1799204796552658, 0.24920740723609924, 0.2074965387582779, 0.1407204270362854, 0.24906964600086212, 0.11689791828393936, 0.16359561681747437, 0.16581520438194275, 0.14397260546684265, 0.19113978743553162, 0.1944303810596466, 0.11735312640666962, 0.20245811343193054, 0.14681099355220795, 0.17944017052650452, 0.24928224086761475, 0.17561903595924377, 0.18999077379703522, 0.15775442123413086, 0.17804694175720215, 0.21971289813518524, 0.13833369314670563, 0.21628379821777344, 0.11724536120891571, 0.1349526047706604, 0.20657728612422943, 0.11728903651237488, 0.16851551830768585, 0.17117798328399658, 0.11733803153038025, 0.17775411903858185, 0.16775821149349213, 0.11704253405332565, 0.1655367910861969, 0.1648176610469818, 0.13883262872695923, 0.15352901816368103, 0.18607409298419952, 0.14311100542545319, 0.15510685741901398, 0.17033302783966064, 0.16181619465351105, 0.16959768533706665, 0.1659098118543625, 0.1471923142671585, 0.2104348987340927, 0.1538381725549698, 0.1927664875984192, 0.17842091619968414, 0.17937947809696198, 0.1734207272529602, 0.13503117859363556, 0.1379988044500351, 0.1812993735074997, 0.15030299127101898, 0.1969940960407257, 0.16195444762706757, 0.17319969832897186, 0.21593602001667023, 0.11706359684467316, 0.14252783358097076, 0.22811533510684967, 0.1543111503124237, 0.17839249968528748, 0.18639367818832397, 0.11727654933929443, 0.15436194837093353, 0.1983984112739563, 0.14468973875045776, 0.17345529794692993, 0.13474246859550476, 0.2259112447500229, 0.1342656910419464, 0.17484882473945618, 0.14714930951595306, 0.13848376274108887, 0.15303075313568115, 0.14931875467300415, 0.16663655638694763, 0.1719382405281067, 0.11686097830533981, 0.2248578518629074, 0.21029327809810638, 0.18012094497680664, 0.17095042765140533, 0.17030227184295654, 0.17042025923728943, 0.1771707534790039, 0.11701258271932602, 0.2221197783946991, 0.17248275876045227, 0.14217185974121094, 0.19840893149375916, 0.16912369430065155, 0.17786048352718353, 0.21625657379627228, 0.1817612200975418, 0.11696840077638626, 0.16122092306613922, 0.17825531959533691, 0.22071455419063568, 0.1829995959997177, 0.1416001319885254, 0.18954113125801086, 0.1737888753414154, 0.14087457954883575, 0.18332749605178833, 0.1495484858751297, 0.1876497119665146, 0.1716567873954773, 0.1589362919330597, 0.15308243036270142, 0.19998152554035187, 0.1747577041387558, 0.22208882868289948, 0.13675853610038757, 0.16216224431991577, 0.17020350694656372, 0.1772099882364273, 0.16855335235595703, 0.16361361742019653, 0.19338683784008026, 0.14656680822372437, 0.18903736770153046, 0.17315590381622314, 0.20608606934547424, 0.15895771980285645, 0.17426162958145142, 0.17896561324596405, 0.20873117446899414, 0.19623634219169617, 0.20842407643795013, 0.21458736062049866, 0.1541118621826172, 0.1775105893611908, 0.11702921241521835, 0.11746722459793091, 0.21232284605503082, 0.11736968904733658, 0.148176372051239, 0.11725486814975739, 0.14785996079444885, 0.2492053061723709, 0.16996540129184723, 0.16537164151668549, 0.14323259890079498, 0.189106747508049, 0.11700740456581116, 0.15328942239284515, 0.172897070646286, 0.19763222336769104, 0.1171458512544632, 0.165115088224411, 0.18550164997577667, 0.16187816858291626, 0.17085853219032288, 0.1779792308807373, 0.17752332985401154, 0.20712286233901978, 0.18509835004806519, 0.16429878771305084, 0.15294285118579865, 0.11694463342428207, 0.17591089010238647, 0.24929960072040558, 0.2037927359342575, 0.17652693390846252, 0.15863342583179474, 0.14895592629909515, 0.1426885724067688, 0.192660853266716, 0.11695286631584167, 0.20401877164840698, 0.1499413251876831, 0.16661255061626434, 0.11722086369991302, 0.15630561113357544, 0.15381544828414917, 0.15199145674705505, 0.18645569682121277, 0.17459304630756378, 0.19366492331027985, 0.17595559358596802, 0.16694176197052002, 0.15912678837776184, 0.17879627645015717, 0.21962152421474457, 0.14602023363113403, 0.13903464376926422, 0.1820574253797531, 0.20559555292129517, 0.1739410161972046, 0.18150536715984344, 0.1824405938386917, 0.11706443876028061, 0.14302439987659454, 0.13402782380580902, 0.17504222691059113, 0.17937925457954407, 0.11690100282430649, 0.23027652502059937, 0.15893611311912537, 0.1820557564496994, 0.21305400133132935, 0.13625018298625946, 0.20635682344436646, 0.17212465405464172, 0.16237857937812805, 0.15368340909481049, 0.18719899654388428, 0.17078819870948792, 0.16566695272922516, 0.11687570810317993, 0.1801435798406601, 0.1823572814464569, 0.15528959035873413, 0.16375401616096497, 0.2188085913658142, 0.20570676028728485, 0.15589991211891174, 0.24904555082321167, 0.11710288375616074, 0.1706847995519638, 0.16329264640808105, 0.21205222606658936, 0.2099602222442627, 0.1656181365251541, 0.21280987560749054, 0.1604911983013153, 0.20749852061271667, 0.2162325531244278, 0.17158883810043335, 0.1425708383321762, 0.17833168804645538, 0.15614107251167297, 0.17120248079299927, 0.18349547684192657, 0.1557535082101822, 0.2156471312046051, 0.15290150046348572, 0.14049234986305237, 0.1646767258644104, 0.1768682450056076, 0.11734418570995331, 0.24869310855865479, 0.21523985266685486, 0.21632304787635803, 0.17763780057430267, 0.179765522480011, 0.16994929313659668, 0.18668200075626373, 0.17935776710510254, 0.17486950755119324, 0.16576221585273743, 0.16214238107204437, 0.14359304308891296, 0.11722191423177719, 0.11715871840715408, 0.11720477044582367, 0.16919243335723877, 0.1600661426782608, 0.14582040905952454, 0.1412193328142166, 0.1666366159915924, 0.20772750675678253, 0.1805327832698822, 0.16244269907474518, 0.1689615696668625, 0.17205335199832916, 0.1638757437467575, 0.17047937214374542, 0.11722686886787415, 0.13979119062423706, 0.2263445407152176, 0.13970179855823517, 0.17806857824325562, 0.24865426123142242, 0.1500396728515625, 0.15011514723300934, 0.13573797047138214, 0.21122756600379944, 0.18237559497356415, 0.21776562929153442, 0.21181721985340118, 0.16573810577392578, 0.18388351798057556, 0.16771730780601501, 0.15161003172397614, 0.14153669774532318, 0.165555939078331, 0.20757180452346802, 0.13748621940612793, 0.11731605231761932, 0.14309850335121155, 0.20334498584270477, 0.189954474568367, 0.24886687099933624, 0.17656387388706207, 0.14437223970890045, 0.1817687451839447, 0.15096130967140198, 0.18206247687339783, 0.15201972424983978, 0.13395853340625763, 0.1386323869228363, 0.11718781292438507, 0.11718723177909851, 0.11762100458145142, 0.1438247412443161, 0.1709771603345871, 0.1710289865732193, 0.1428329199552536, 0.13486219942569733, 0.16940909624099731, 0.20359613001346588, 0.20937928557395935, 0.2275700569152832, 0.1413251757621765, 0.19236284494400024, 0.14745773375034332, 0.22410182654857635, 0.197607159614563, 0.213570237159729, 0.18453674018383026, 0.24872852861881256, 0.11769818514585495, 0.14328013360500336, 0.15900887548923492, 0.1709095984697342, 0.17840905487537384, 0.15563929080963135, 0.2131536453962326, 0.15086929500102997, 0.1745627224445343, 0.18899014592170715, 0.2486877590417862, 0.15061190724372864, 0.21318495273590088, 0.17768578231334686, 0.1547534167766571, 0.17922282218933105, 0.14749100804328918, 0.18679434061050415, 0.17325271666049957, 0.14445549249649048, 0.1456502377986908, 0.18906760215759277, 0.14827628433704376, 0.1335967630147934, 0.17720086872577667, 0.1358746439218521, 0.16632528603076935, 0.15501783788204193, 0.23054814338684082, 0.1320471614599228, 0.18657469749450684, 0.17049045860767365, 0.11734973639249802, 0.1789007931947708, 0.1991647481918335, 0.1732926219701767, 0.13557694852352142, 0.15363182127475739, 0.17955084145069122, 0.1416015923023224, 0.16198566555976868, 0.1434493213891983, 0.14798671007156372, 0.1792704313993454, 0.13829244673252106, 0.13747450709342957, 0.142853245139122, 0.14419174194335938, 0.1755484640598297, 0.14746803045272827, 0.13935889303684235, 0.1885976642370224, 0.1558108776807785, 0.1174444705247879, 0.15234217047691345, 0.17422455549240112, 0.11755137145519257, 0.18109291791915894, 0.1537635773420334, 0.18857109546661377, 0.18390841782093048, 0.17545709013938904, 0.19069141149520874, 0.1446746438741684, 0.17422673106193542, 0.22222697734832764, 0.16577713191509247, 0.16812260448932648, 0.18831703066825867, 0.15613403916358948, 0.2023850530385971, 0.17916424572467804, 0.17353284358978271, 0.18986302614212036, 0.17466844618320465, 0.16863985359668732, 0.16468627750873566, 0.1788720339536667, 0.1420263946056366, 0.22487416863441467, 0.18922922015190125, 0.17547836899757385, 0.14471545815467834, 0.1593112051486969, 0.17236638069152832, 0.1838371306657791, 0.16373074054718018, 0.20724499225616455, 0.13891912996768951, 0.13073281943798065, 0.20311272144317627, 0.19587303698062897, 0.18206295371055603, 0.1171303391456604, 0.1826649159193039, 0.18935903906822205, 0.13398133218288422, 0.2037481665611267, 0.17899593710899353, 0.11737723648548126, 0.13874539732933044, 0.1701740026473999, 0.14846941828727722, 0.1174466460943222, 0.17235752940177917, 0.1537514477968216, 0.1834503412246704, 0.17759835720062256, 0.1491503119468689, 0.16549085080623627, 0.13724154233932495, 0.11737416684627533, 0.17637999355793, 0.11722292006015778, 0.16678422689437866, 0.14979906380176544, 0.11727163940668106, 0.14721499383449554, 0.17962686717510223, 0.20151986181735992, 0.20294582843780518, 0.14737685024738312, 0.17919448018074036, 0.14916090667247772, 0.18476749956607819, 0.19728800654411316, 0.2491844743490219, 0.16461540758609772, 0.1446688324213028, 0.14278703927993774, 0.23022723197937012, 0.17036283016204834, 0.15065866708755493, 0.20575979351997375, 0.1820625513792038, 0.1338333934545517, 0.18444477021694183, 0.1858537644147873, 0.11732696741819382, 0.11722871661186218, 0.14904476702213287, 0.15286406874656677, 0.17930985987186432, 0.11727500706911087, 0.13982298970222473, 0.19784605503082275, 0.11714661121368408, 0.17266666889190674, 0.16962043941020966, 0.20256981253623962, 0.18325966596603394, 0.2079540342092514, 0.17838376760482788, 0.2128094732761383, 0.1666354537010193, 0.2049388885498047, 0.1438140571117401, 0.13894644379615784, 0.17386701703071594, 0.17288875579833984, 0.1827029287815094, 0.18751753866672516, 0.11716631799936295, 0.18980133533477783, 0.11718892306089401, 0.18041054904460907, 0.15431731939315796, 0.11709744483232498, 0.14069266617298126, 0.14759616553783417, 0.18374867737293243, 0.11726756393909454, 0.14945489168167114, 0.2273641973733902, 0.17389105260372162, 0.17442187666893005, 0.11703193932771683, 0.2008831799030304, 0.15761955082416534, 0.15380533039569855, 0.15987536311149597, 0.19288142025470734, 0.15753795206546783, 0.11731234937906265, 0.16955268383026123, 0.1170346736907959, 0.19965000450611115, 0.21878087520599365, 0.15847483277320862, 0.16750216484069824, 0.1173100695014, 0.1840074211359024, 0.1472943127155304, 0.15644118189811707, 0.13022270798683167, 0.1755514293909073, 0.13711661100387573, 0.17953094840049744, 0.13718198239803314, 0.17302943766117096, 0.18037717044353485, 0.11690621823072433, 0.14091095328330994, 0.17113451659679413, 0.1956586390733719, 0.16574731469154358, 0.14391368627548218, 0.15737156569957733, 0.13406243920326233, 0.15296562016010284, 0.21410740911960602, 0.21448056399822235, 0.23008692264556885, 0.1797601878643036, 0.1429336667060852, 0.1168563961982727, 0.14473967254161835, 0.14724214375019073, 0.22620348632335663, 0.15160268545150757, 0.21220733225345612, 0.21210597455501556, 0.14674367010593414, 0.11690974235534668, 0.18755000829696655, 0.17607466876506805, 0.18590876460075378, 0.13619454205036163, 0.13925324380397797, 0.16930584609508514, 0.2041669636964798, 0.11692805588245392, 0.1809401661157608, 0.2033097892999649, 0.16101770102977753, 0.1375684291124344, 0.15846695005893707, 0.19131331145763397, 0.11684639751911163, 0.18538495898246765, 0.17822350561618805, 0.1497049182653427, 0.11696341633796692, 0.17211297154426575, 0.11697454750537872, 0.11709393560886383, 0.13272859156131744, 0.1417125016450882, 0.151667520403862, 0.11686135083436966, 0.1626226305961609, 0.18707741796970367, 0.21663898229599, 0.20727001130580902, 0.22958315908908844, 0.16499926149845123, 0.1907271295785904, 0.16113904118537903, 0.1440407931804657, 0.13807626068592072, 0.24951934814453125, 0.15429776906967163, 0.11693356186151505, 0.19206055998802185, 0.1738094687461853, 0.18782749772071838, 0.11675293743610382, 0.14017826318740845, 0.15478873252868652, 0.21449464559555054, 0.19751352071762085, 0.14170415699481964, 0.17551714181900024, 0.13730739057064056, 0.2153741866350174, 0.20892560482025146, 0.14985530078411102, 0.18693137168884277, 0.1832760125398636, 0.21273338794708252, 0.11700620502233505, 0.21069256961345673, 0.21961620450019836, 0.1168735921382904, 0.21510574221611023, 0.14008986949920654, 0.11703309416770935, 0.1605639010667801, 0.14243271946907043, 0.18865904211997986, 0.17891131341457367, 0.13911157846450806, 0.15815919637680054, 0.1636485457420349, 0.1452927440404892, 0.19842268526554108, 0.11676643043756485, 0.1759496033191681, 0.21248631179332733, 0.18700501322746277, 0.22173236310482025, 0.15605060756206512, 0.16556915640830994, 0.11697877943515778, 0.1650037169456482, 0.18067605793476105, 0.11687960475683212, 0.18127034604549408, 0.18580390512943268, 0.1464138925075531, 0.17044220864772797, 0.14797618985176086, 0.1719146966934204, 0.16278715431690216, 0.140423983335495, 0.1841922551393509, 0.16853633522987366, 0.1808784008026123, 0.17565609514713287, 0.18271969258785248, 0.15676450729370117, 0.146638885140419, 0.22251315414905548, 0.13117735087871552, 0.17061804234981537, 0.20905381441116333, 0.11679378151893616, 0.14653617143630981, 0.1403311789035797, 0.17613893747329712, 0.15644925832748413, 0.15423719584941864, 0.2245914787054062, 0.11677206307649612, 0.11706716567277908, 0.17941781878471375, 0.17345747351646423, 0.14430274069309235, 0.20665518939495087, 0.11684788763523102, 0.15210580825805664, 0.16988934576511383, 0.1923200637102127, 0.18143600225448608, 0.14716282486915588, 0.18404681980609894, 0.15702423453330994, 0.1616612821817398, 0.18497763574123383, 0.18074467778205872, 0.25010329484939575, 0.14841803908348083, 0.14089718461036682, 0.18748193979263306, 0.20362190902233124, 0.13981793820858002, 0.19252336025238037, 0.20966634154319763, 0.21008402109146118, 0.2157876193523407, 0.17065131664276123, 0.17230139672756195, 0.14592470228672028, 0.16355781257152557, 0.16806660592556, 0.19088459014892578, 0.195382758975029, 0.14090707898139954, 0.16093969345092773, 0.24929656088352203, 0.24964839220046997, 0.11686434596776962, 0.17462122440338135, 0.16877581179141998, 0.18477097153663635, 0.16018226742744446, 0.20742350816726685, 0.1720844954252243, 0.19201475381851196, 0.17623673379421234, 0.17433923482894897, 0.18901318311691284, 0.18703702092170715, 0.18802869319915771, 0.21467755734920502, 0.2494022697210312, 0.20546281337738037, 0.20143161714076996, 0.1447570025920868, 0.22518141567707062, 0.2252139300107956, 0.1768430769443512, 0.2494494765996933, 0.14498598873615265, 0.2049543261528015, 0.14126484096050262, 0.14130152761936188, 0.15323679149150848, 0.15137284994125366, 0.1417759656906128, 0.21293380856513977, 0.21964655816555023, 0.1173839420080185, 0.11701319366693497, 0.15132340788841248, 0.22362583875656128, 0.11696957051753998, 0.13032858073711395, 0.18345816433429718, 0.1169876679778099, 0.24913974106311798, 0.14900560677051544, 0.19122080504894257, 0.14437881112098694, 0.11688634008169174, 0.16649973392486572, 0.22091275453567505, 0.1767452210187912, 0.17624759674072266, 0.11711926758289337, 0.14403727650642395, 0.17134881019592285, 0.2492426633834839, 0.14453646540641785, 0.19177763164043427, 0.18578925728797913, 0.20265068113803864, 0.15043343603610992, 0.1491294950246811, 0.24925288558006287, 0.1503331959247589, 0.1775457262992859, 0.21196764707565308, 0.18478623032569885, 0.16979816555976868, 0.15280932188034058, 0.23326660692691803, 0.16977962851524353, 0.24935637414455414, 0.1931750327348709, 0.1355590522289276, 0.24914255738258362, 0.2492177039384842, 0.13268429040908813, 0.11687880009412766, 0.21524330973625183, 0.18757307529449463, 0.2111024409532547, 0.2491672933101654, 0.1519581824541092, 0.11695561558008194, 0.21108610928058624, 0.19516430795192719, 0.1707048863172531, 0.1648627519607544, 0.18443161249160767, 0.1171996146440506, 0.22508689761161804, 0.2176007628440857, 0.11691278964281082, 0.1527378261089325, 0.1709369719028473, 0.14803291857242584, 0.1653207391500473, 0.11701635271310806, 0.1798696666955948, 0.21985319256782532, 0.22445912659168243, 0.14419901371002197, 0.17349575459957123, 0.19565008580684662, 0.1854286640882492, 0.1574779748916626, 0.11707976460456848, 0.14625000953674316, 0.1606529951095581, 0.17875079810619354, 0.11735796928405762, 0.14618869125843048, 0.1809173822402954, 0.15791000425815582, 0.1470034271478653, 0.18255308270454407, 0.2491302639245987, 0.11696666479110718, 0.13948288559913635, 0.18046055734157562, 0.16747033596038818, 0.1488179713487625, 0.18591055274009705, 0.1705828309059143, 0.21347902715206146, 0.24915118515491486, 0.16162171959877014, 0.2489876002073288, 0.157803475856781, 0.1398988664150238, 0.11714429408311844, 0.1733027994632721, 0.13553957641124725, 0.18122528493404388, 0.2162301391363144, 0.2491215467453003, 0.1964554488658905, 0.1418994963169098, 0.11714060604572296, 0.18339912593364716, 0.1700727641582489, 0.19820553064346313, 0.11702483892440796, 0.17859932780265808, 0.11733287572860718, 0.1719350814819336, 0.20869556069374084, 0.22187267243862152, 0.13195008039474487, 0.14942938089370728, 0.1838618367910385, 0.16647674143314362, 0.19781307876110077, 0.13403280079364777, 0.16863183677196503, 0.19143252074718475, 0.14257699251174927, 0.17618513107299805, 0.19564205408096313, 0.14739905297756195, 0.11702673137187958, 0.1971931755542755, 0.16038116812705994, 0.1791963130235672, 0.17023500800132751, 0.11705446988344193, 0.20736104249954224, 0.1367546021938324, 0.18573246896266937, 0.24906601011753082, 0.14213769137859344, 0.19066160917282104, 0.22870875895023346, 0.17173388600349426, 0.11693036556243896, 0.14452838897705078, 0.21172548830509186, 0.20060613751411438, 0.17339874804019928, 0.16808265447616577, 0.15740767121315002, 0.1454256772994995, 0.17396892607212067, 0.21080638468265533, 0.2270975112915039, 0.20431794226169586, 0.16810475289821625, 0.11706680804491043, 0.1873733550310135, 0.2147301882505417, 0.13905808329582214, 0.15047648549079895, 0.18638168275356293, 0.13661393523216248, 0.1754601150751114, 0.21204282343387604, 0.20995697379112244, 0.19385240972042084, 0.15218837559223175, 0.15987035632133484, 0.2152746617794037, 0.13984495401382446, 0.21456129848957062, 0.11693594604730606, 0.11700930446386337, 0.21540656685829163, 0.11698402464389801, 0.1635342836380005, 0.1970556378364563, 0.1756235510110855, 0.20830491185188293, 0.1640772968530655, 0.16141624748706818, 0.13235242664813995, 0.24894101917743683, 0.17720937728881836, 0.11710604280233383, 0.22380729019641876, 0.2214922308921814, 0.11729879677295685, 0.2197120189666748, 0.21792589128017426, 0.18311505019664764, 0.20852234959602356, 0.1706961840391159, 0.2489992380142212, 0.13864994049072266, 0.14835390448570251, 0.15401166677474976, 0.17891743779182434, 0.21203356981277466, 0.1170409545302391, 0.13852277398109436, 0.11708276718854904, 0.1622234731912613]\n",
            "Val loss 0.16830484040988428\n",
            "Val auc roc 0.5007712895377129\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebac03a-ef5e-48bb-9ca9-bf37cdcbf2e3"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}