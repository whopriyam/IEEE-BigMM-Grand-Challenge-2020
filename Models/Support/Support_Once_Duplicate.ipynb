{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_Once_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab5e6556fe364ce4b5b00d82c3d77871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0923293c0a364bcdb3e05f7bfc9880b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f9c64377d564e9784f80fd461b6cbd4",
              "IPY_MODEL_81f4e57628054f26a46a1f5751f4f46a"
            ]
          }
        },
        "0923293c0a364bcdb3e05f7bfc9880b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f9c64377d564e9784f80fd461b6cbd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba730308c8b142c888f2036622fd00cc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2090,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2090,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f71ce0cfc7024dc98362324707520f5b"
          }
        },
        "81f4e57628054f26a46a1f5751f4f46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_149ab70007064b36affcf7765b34c276",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2090/2090 [19:42&lt;00:00,  1.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b07ad7fcc2494f9698b6fe1e3009d5"
          }
        },
        "ba730308c8b142c888f2036622fd00cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f71ce0cfc7024dc98362324707520f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "149ab70007064b36affcf7765b34c276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b07ad7fcc2494f9698b6fe1e3009d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f629a4259fa456994247da01e76daa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_755d5b1a2ade416b93b0d223974ecaf8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_855744aad62e40c69d8d2a52246caf80",
              "IPY_MODEL_1ecf89267f484fb495cdddb438b24fde"
            ]
          }
        },
        "755d5b1a2ade416b93b0d223974ecaf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "855744aad62e40c69d8d2a52246caf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_728a0ffbe40a4510a0925661034fb95a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2090,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2090,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03b736aa93b948fc8f91f6d58569e877"
          }
        },
        "1ecf89267f484fb495cdddb438b24fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb9f13f090b049cc80fa3a90a222d5ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2090/2090 [19:08&lt;00:00,  1.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_362744d0de404502b76955a7681b5ed8"
          }
        },
        "728a0ffbe40a4510a0925661034fb95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03b736aa93b948fc8f91f6d58569e877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb9f13f090b049cc80fa3a90a222d5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "362744d0de404502b76955a7681b5ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f38ed563ebf146159b1ff5d905f82311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_54d0cbbfa703415da2c01246b1f5e924",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cf87833a4d5f446da19ed5eaf6452c85",
              "IPY_MODEL_130b353626e049539673c613b2f5c9be"
            ]
          }
        },
        "54d0cbbfa703415da2c01246b1f5e924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf87833a4d5f446da19ed5eaf6452c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_504828c0629248bcbe2a80f2535f610d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2090,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2090,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8472d801306949fc804acdf451a08903"
          }
        },
        "130b353626e049539673c613b2f5c9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ce640f0e36e4cb7acb8e477ab31a35c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2090/2090 [19:02&lt;00:00,  1.83it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a7564982c1c4e3cbe99034625b3d07f"
          }
        },
        "504828c0629248bcbe2a80f2535f610d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8472d801306949fc804acdf451a08903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ce640f0e36e4cb7acb8e477ab31a35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a7564982c1c4e3cbe99034625b3d07f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bc1d892d-fa00-4ff7-8be2-89b2637e51be"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 7.29 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8df0f7a-54d6-483e-bc54-753cf4ab846f"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db1f187c-1597-4952-989c-1e8ace23acd9"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7522b159-24e5-4490-a6c2-0f7472b4f20f"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "27d2b2a1-12c1-404e-e609-35a848f442df"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "7d6e452d-a2c7-4122-9671-ac634df3b030"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a9872ee5-3c04-4720-8519-8bfe87b664af"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23c9e425-4c80-4afe-e63b-b3a55fc06407"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "4105d2d2-426e-4c51-bf31-f7595a392c28"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "170e8999-9b6a-44d1-80c5-c9bab59bc652"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    #MeToo: Sruthi Hariharan finds supporters in P...\n",
              "1                                        ption - no:2 \n",
              "2    College Student Raped Woman After She Attended...\n",
              "3    #MeToo In India Is Just A Tip Of An Iceberg An...\n",
              "4     Four-member panel of retired judges to conduc...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0faae0f1-e20e-490d-d155-8bd6c4c977dc"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:14<00:00, 28192577.77B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab68d7ed-4339-470e-ff21-d37420d8b911"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 668557.05B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c995e602-4122-4a46-ae24-c3c8cfceead3"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55c38a7b-f6d3-421a-f97e-1be83c627b2f"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0d5fa28-f521-4727-a086-f1d1a427dcc5"
      },
      "source": [
        "col_name = \"Support\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728,
          "referenced_widgets": [
            "ab5e6556fe364ce4b5b00d82c3d77871",
            "0923293c0a364bcdb3e05f7bfc9880b5",
            "2f9c64377d564e9784f80fd461b6cbd4",
            "81f4e57628054f26a46a1f5751f4f46a",
            "ba730308c8b142c888f2036622fd00cc",
            "f71ce0cfc7024dc98362324707520f5b",
            "149ab70007064b36affcf7765b34c276",
            "a6b07ad7fcc2494f9698b6fe1e3009d5",
            "6f629a4259fa456994247da01e76daa1",
            "755d5b1a2ade416b93b0d223974ecaf8",
            "855744aad62e40c69d8d2a52246caf80",
            "1ecf89267f484fb495cdddb438b24fde",
            "728a0ffbe40a4510a0925661034fb95a",
            "03b736aa93b948fc8f91f6d58569e877",
            "fb9f13f090b049cc80fa3a90a222d5ec",
            "362744d0de404502b76955a7681b5ed8",
            "f38ed563ebf146159b1ff5d905f82311",
            "54d0cbbfa703415da2c01246b1f5e924",
            "cf87833a4d5f446da19ed5eaf6452c85",
            "130b353626e049539673c613b2f5c9be",
            "504828c0629248bcbe2a80f2535f610d",
            "8472d801306949fc804acdf451a08903",
            "4ce640f0e36e4cb7acb8e477ab31a35c",
            "3a7564982c1c4e3cbe99034625b3d07f"
          ]
        },
        "outputId": "7e4842b4-11e3-4d74-cdb3-f6ce881794ee"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 8362\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 2117\n",
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab5e6556fe364ce4b5b00d82c3d77871",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2090.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1749\n",
            "Train Losses : [0.38987496495246887, 0.7061573266983032, 0.9070948362350464, 0.02251427248120308, 0.24160127341747284, 0.29521164298057556, 0.2770659327507019, 0.17689786851406097, 0.17381855845451355, 0.1656161993741989, 0.13096636533737183, 0.2083217203617096, 0.3470791280269623, 0.21382570266723633, 0.19708962738513947, 0.15260550379753113, 0.132442444562912, 0.1566123366355896, 0.16904032230377197, 0.17695794999599457, 0.19028453528881073, 0.2015850692987442, 0.2655862271785736, 0.17619045078754425, 0.18004319071769714, 0.1796271950006485, 0.20169490575790405, 0.1673583686351776, 0.14987486600875854, 0.16594354808330536, 0.12506628036499023, 0.11726304888725281, 0.3138938844203949, 0.11617764085531235, 0.13533097505569458, 0.07044757157564163, 0.22726276516914368, 0.1842653900384903, 0.24954742193222046, 0.19109061360359192, 0.15600800514221191, 0.08125223219394684, 0.26759111881256104, 0.2732306122779846, 0.20810914039611816, 0.19361580908298492, 0.17835074663162231, 0.20550647377967834, 0.1969924420118332, 0.1733887791633606, 0.17873883247375488, 0.18184293806552887, 0.2017265409231186, 0.18562808632850647, 0.18062810599803925, 0.1577197015285492, 0.23346616327762604, 0.17771807312965393, 0.22666418552398682, 0.13382697105407715, 0.23266716301441193, 0.1686759740114212, 0.13162092864513397, 0.20191912353038788, 0.19765841960906982, 0.1390192210674286, 0.23416709899902344, 0.13186094164848328, 0.16029787063598633, 0.17988702654838562, 0.10446562618017197, 0.22349239885807037, 0.15105515718460083, 0.1558501422405243, 0.06944328546524048, 0.11352432519197464, 0.2851925194263458, 0.2390962392091751, 0.1960107386112213, 0.11693877726793289, 0.16759386658668518, 0.21194209158420563, 0.05376376584172249, 0.33513593673706055, 0.3114798069000244, 0.28606700897216797, 0.1963988095521927, 0.19745463132858276, 0.19356434047222137, 0.1983472853899002, 0.1549033373594284, 0.23360271751880646, 0.15656518936157227, 0.16578587889671326, 0.18181337416172028, 0.1367994248867035, 0.17271126806735992, 0.14545896649360657, 0.16857270896434784, 0.17504817247390747, 0.21068009734153748, 0.19908325374126434, 0.17396976053714752, 0.19000360369682312, 0.20927321910858154, 0.16699275374412537, 0.19504988193511963, 0.1870567351579666, 0.223186194896698, 0.17772193253040314, 0.1390492171049118, 0.15532422065734863, 0.15932416915893555, 0.1857391744852066, 0.15343663096427917, 0.18537428975105286, 0.16344726085662842, 0.20624218881130219, 0.15653738379478455, 0.2037694901227951, 0.2275369018316269, 0.19398324191570282, 0.17135025560855865, 0.17588019371032715, 0.18821309506893158, 0.163376584649086, 0.15772902965545654, 0.21304528415203094, 0.13680873811244965, 0.18752260506153107, 0.20296943187713623, 0.1721366047859192, 0.15378011763095856, 0.1596318483352661, 0.20837557315826416, 0.18058477342128754, 0.2171236127614975, 0.1478319615125656, 0.14858190715312958, 0.16223116219043732, 0.15602241456508636, 0.20520928502082825, 0.209187850356102, 0.2209712117910385, 0.17277894914150238, 0.1806640625, 0.17921800911426544, 0.17008420825004578, 0.15246626734733582, 0.20078639686107635, 0.1685960441827774, 0.19580301642417908, 0.20036861300468445, 0.17861518263816833, 0.17918266355991364, 0.19931188225746155, 0.14527833461761475, 0.1580495685338974, 0.16065655648708344, 0.15527288615703583, 0.17671841382980347, 0.15551169216632843, 0.14260683953762054, 0.17787018418312073, 0.19754837453365326, 0.12899015843868256, 0.11515035480260849, 0.18689538538455963, 0.14314526319503784, 0.14837364852428436, 0.15538674592971802, 0.14285019040107727, 0.2035050392150879, 0.176840141415596, 0.20468711853027344, 0.19182269275188446, 0.20226866006851196, 0.12913048267364502, 0.12103638052940369, 0.2001011222600937, 0.19590914249420166, 0.21249356865882874, 0.15857186913490295, 0.18879812955856323, 0.2266520857810974, 0.13946257531642914, 0.18055081367492676, 0.14100931584835052, 0.13937486708164215, 0.17217028141021729, 0.12929202616214752, 0.15764138102531433, 0.21265201270580292, 0.18217366933822632, 0.17874445021152496, 0.20149482786655426, 0.14889279007911682, 0.15469138324260712, 0.18730278313159943, 0.1877739280462265, 0.1481606662273407, 0.15699300169944763, 0.145216703414917, 0.18116243183612823, 0.20112021267414093, 0.1804697960615158, 0.17181523144245148, 0.20284412801265717, 0.19683150947093964, 0.17918331921100616, 0.2363152951002121, 0.16140495240688324, 0.18471579253673553, 0.19988885521888733, 0.1904848963022232, 0.17381371557712555, 0.1646307110786438, 0.19587688148021698, 0.20154431462287903, 0.18972085416316986, 0.16160449385643005, 0.15723881125450134, 0.1860395073890686, 0.20170049369335175, 0.17329774796962738, 0.17209500074386597, 0.1886577159166336, 0.16816610097885132, 0.18039417266845703, 0.17086289823055267, 0.1716250479221344, 0.18737895786762238, 0.17269615828990936, 0.1914881467819214, 0.1750914454460144, 0.17235617339611053, 0.1840147227048874, 0.17808592319488525, 0.21166802942752838, 0.18814514577388763, 0.18914954364299774, 0.19345098733901978, 0.17330364882946014, 0.1598777174949646, 0.192324697971344, 0.1611672341823578, 0.16609111428260803, 0.16940951347351074, 0.20002996921539307, 0.16318833827972412, 0.18923956155776978, 0.18183140456676483, 0.17444932460784912, 0.18127672374248505, 0.1687418520450592, 0.17027002573013306, 0.16817936301231384, 0.1653437316417694, 0.165528804063797, 0.16573473811149597, 0.19806252419948578, 0.18589794635772705, 0.16818763315677643, 0.1611478626728058, 0.19299137592315674, 0.1728014498949051, 0.15129369497299194, 0.19871000945568085, 0.16665980219841003, 0.15918871760368347, 0.21576711535453796, 0.19992464780807495, 0.17769570648670197, 0.16075873374938965, 0.15903902053833008, 0.1644326001405716, 0.17956343293190002, 0.17499084770679474, 0.1535540223121643, 0.18321335315704346, 0.16350117325782776, 0.19863371551036835, 0.1619863510131836, 0.16314288973808289, 0.18004605174064636, 0.17427058517932892, 0.19458234310150146, 0.17221598327159882, 0.17703160643577576, 0.184044748544693, 0.16591262817382812, 0.19915832579135895, 0.15898540616035461, 0.16538548469543457, 0.20799638330936432, 0.1760587990283966, 0.1631343960762024, 0.17855025827884674, 0.15720635652542114, 0.16420207917690277, 0.16431519389152527, 0.18901295959949493, 0.1899641752243042, 0.18691644072532654, 0.16411975026130676, 0.1654985547065735, 0.1690221130847931, 0.16338984668254852, 0.18124797940254211, 0.20526330173015594, 0.15569838881492615, 0.17545394599437714, 0.16265052556991577, 0.20492178201675415, 0.17354439198970795, 0.1583314836025238, 0.15257404744625092, 0.20690321922302246, 0.15014465153217316, 0.15595312416553497, 0.19886282086372375, 0.14718909561634064, 0.17413043975830078, 0.1535663902759552, 0.1430889368057251, 0.1546989530324936, 0.18910691142082214, 0.178185373544693, 0.18123900890350342, 0.22081638872623444, 0.18126004934310913, 0.21969960629940033, 0.2160826325416565, 0.17916397750377655, 0.1936476230621338, 0.16907314956188202, 0.19288666546344757, 0.15500952303409576, 0.154668390750885, 0.1731569766998291, 0.18825170397758484, 0.16550615429878235, 0.1667199730873108, 0.1653529703617096, 0.17649540305137634, 0.16141515970230103, 0.16626322269439697, 0.16801877319812775, 0.15530900657176971, 0.19398635625839233, 0.1557324379682541, 0.17889145016670227, 0.17931202054023743, 0.16215480864048004, 0.1980832815170288, 0.16680596768856049, 0.17366713285446167, 0.17801408469676971, 0.16108454763889313, 0.16792401671409607, 0.18630589544773102, 0.18654485046863556, 0.20785722136497498, 0.17104889452457428, 0.17180922627449036, 0.18558453023433685, 0.20267896354198456, 0.15117332339286804, 0.18816325068473816, 0.19155244529247284, 0.18559524416923523, 0.1655980795621872, 0.16257885098457336, 0.15894261002540588, 0.19269275665283203, 0.17922194302082062, 0.18423885107040405, 0.17947518825531006, 0.16857939958572388, 0.17553280293941498, 0.17965973913669586, 0.18054726719856262, 0.17678898572921753, 0.17130883038043976, 0.17269085347652435, 0.17515915632247925, 0.17277081310749054, 0.18142694234848022, 0.1755828857421875, 0.17273178696632385, 0.16018161177635193, 0.1808774322271347, 0.1810389757156372, 0.16752171516418457, 0.17480574548244476, 0.15443551540374756, 0.1935179978609085, 0.16357190907001495, 0.1770981252193451, 0.18859796226024628, 0.20519863069057465, 0.1724136918783188, 0.1875743567943573, 0.1731773018836975, 0.19350549578666687, 0.15075525641441345, 0.1509343832731247, 0.17680443823337555, 0.15776149928569794, 0.16985531151294708, 0.16800850629806519, 0.14830797910690308, 0.19061313569545746, 0.20445984601974487, 0.16112686693668365, 0.1756051629781723, 0.16698454320430756, 0.1621992588043213, 0.17973017692565918, 0.1781415194272995, 0.1739087998867035, 0.16564379632472992, 0.1525822877883911, 0.18317876756191254, 0.17587688565254211, 0.17079918086528778, 0.17309045791625977, 0.16000090539455414, 0.14999333024024963, 0.1744314432144165, 0.17056626081466675, 0.1548478603363037, 0.18690413236618042, 0.15232454240322113, 0.16706615686416626, 0.20467796921730042, 0.21954700350761414, 0.1909131109714508, 0.19672325253486633, 0.14848139882087708, 0.1746780425310135, 0.1735410988330841, 0.1466083824634552, 0.18896299600601196, 0.15387780964374542, 0.1607033908367157, 0.16796565055847168, 0.17047417163848877, 0.17863096296787262, 0.18051160871982574, 0.1760430932044983, 0.17735934257507324, 0.17186374962329865, 0.15289448201656342, 0.16583286225795746, 0.19114235043525696, 0.16962282359600067, 0.1500062495470047, 0.16553863883018494, 0.17778080701828003, 0.17982831597328186, 0.17907889187335968, 0.15383855998516083, 0.17230476438999176, 0.1680130809545517, 0.1760215014219284, 0.17509403824806213, 0.16210754215717316, 0.15833285450935364, 0.15755924582481384, 0.14763185381889343, 0.17267289757728577, 0.14662055671215057, 0.14178523421287537, 0.19989068806171417, 0.14109916985034943, 0.1389118880033493, 0.19821108877658844, 0.17503325641155243, 0.2094842493534088, 0.1835210621356964, 0.1916840523481369, 0.1590442657470703, 0.18383145332336426, 0.21222367882728577, 0.19210630655288696, 0.190229594707489, 0.1561572551727295, 0.18564555048942566, 0.1819726973772049, 0.19583837687969208, 0.1903121918439865, 0.18091797828674316, 0.17006553709506989, 0.17593607306480408, 0.17835159599781036, 0.14716340601444244, 0.18463154137134552, 0.16626253724098206, 0.17416679859161377, 0.18204252421855927, 0.17420215904712677, 0.1601906418800354, 0.16567383706569672, 0.16728700697422028, 0.20468370616436005, 0.17968739569187164, 0.16935759782791138, 0.19647333025932312, 0.19445642828941345, 0.1830826997756958, 0.16260705888271332, 0.17724494636058807, 0.17439614236354828, 0.1652616560459137, 0.17479415237903595, 0.18212483823299408, 0.17215436697006226, 0.16820159554481506, 0.17646095156669617, 0.17595995962619781, 0.1734413206577301, 0.16984722018241882, 0.17954303324222565, 0.17979854345321655, 0.17753848433494568, 0.17660996317863464, 0.16013891994953156, 0.17046673595905304, 0.1727471649646759, 0.1740136593580246, 0.16682837903499603, 0.1704477220773697, 0.15934768319129944, 0.17723089456558228, 0.17373374104499817, 0.15329857170581818, 0.18726682662963867, 0.16560719907283783, 0.16987687349319458, 0.1749729961156845, 0.1854882836341858, 0.17681363224983215, 0.1610453724861145, 0.18953441083431244, 0.18005387485027313, 0.15378259122371674, 0.17659324407577515, 0.18267807364463806, 0.1765480935573578, 0.14991378784179688, 0.190787672996521, 0.18141622841358185, 0.18987387418746948, 0.14492067694664001, 0.16314692795276642, 0.16040371358394623, 0.18060339987277985, 0.2035730481147766, 0.15266771614551544, 0.16935087740421295, 0.19145908951759338, 0.18302907049655914, 0.19152936339378357, 0.1569036841392517, 0.13689513504505157, 0.14922671020030975, 0.15856289863586426, 0.16085653007030487, 0.18064020574092865, 0.16993491351604462, 0.15442906320095062, 0.1401563286781311, 0.1693202704191208, 0.18646208941936493, 0.17627118527889252, 0.20946472883224487, 0.12800423800945282, 0.1820041388273239, 0.20743724703788757, 0.23208048939704895, 0.1621103137731552, 0.15625019371509552, 0.14445024728775024, 0.13082973659038544, 0.13561920821666718, 0.17177367210388184, 0.2086183875799179, 0.17307743430137634, 0.17550337314605713, 0.15040327608585358, 0.1879492998123169, 0.2235564887523651, 0.17870783805847168, 0.22726336121559143, 0.18060386180877686, 0.15295931696891785, 0.1986353099346161, 0.18601319193840027, 0.19236689805984497, 0.20903950929641724, 0.16788312792778015, 0.19681192934513092, 0.19676190614700317, 0.18214289844036102, 0.19492840766906738, 0.1682540774345398, 0.17926664650440216, 0.17797894775867462, 0.1686471402645111, 0.18124546110630035, 0.1726517677307129, 0.17624962329864502, 0.17448748648166656, 0.17700383067131042, 0.18415333330631256, 0.17736925184726715, 0.17831583321094513, 0.17827926576137543, 0.17521826922893524, 0.15907393395900726, 0.18616268038749695, 0.1729886531829834, 0.17488078773021698, 0.1746244877576828, 0.17697139084339142, 0.1703927218914032, 0.16939470171928406, 0.18331335484981537, 0.18016988039016724, 0.16620942950248718, 0.1854115128517151, 0.1820245087146759, 0.17933513224124908, 0.16889749467372894, 0.17182254791259766, 0.1823364645242691, 0.1660824567079544, 0.16821537911891937, 0.16756205260753632, 0.17699502408504486, 0.1682475507259369, 0.18078327178955078, 0.16156451404094696, 0.1644315868616104, 0.18014129996299744, 0.17656826972961426, 0.16889385879039764, 0.16527247428894043, 0.17724022269248962, 0.180751234292984, 0.15815085172653198, 0.19909119606018066, 0.16831302642822266, 0.17491339147090912, 0.16856758296489716, 0.16963443160057068, 0.18117637932300568, 0.16242843866348267, 0.18746863305568695, 0.1782381683588028, 0.17123210430145264, 0.166559636592865, 0.17871743440628052, 0.1781996339559555, 0.16766321659088135, 0.19059795141220093, 0.18833404779434204, 0.16224758327007294, 0.16412870585918427, 0.16818484663963318, 0.17450420558452606, 0.16637614369392395, 0.18064768612384796, 0.17189499735832214, 0.17733782529830933, 0.17303262650966644, 0.1748945564031601, 0.1717039793729782, 0.18607741594314575, 0.17758990824222565, 0.1727055311203003, 0.1715017706155777, 0.1737106889486313, 0.1716727763414383, 0.17318974435329437, 0.1783774197101593, 0.1746283769607544, 0.17445124685764313, 0.17919419705867767, 0.17341554164886475, 0.1753499060869217, 0.1698630005121231, 0.1665162444114685, 0.18236586451530457, 0.17807133495807648, 0.16835196316242218, 0.17975598573684692, 0.17273594439029694, 0.16595834493637085, 0.1724686175584793, 0.1606072634458542, 0.16633257269859314, 0.1818782091140747, 0.17260922491550446, 0.16575296223163605, 0.17536133527755737, 0.1614883691072464, 0.17024220526218414, 0.18391329050064087, 0.17501534521579742, 0.19310592114925385, 0.17471984028816223, 0.158061683177948, 0.17391009628772736, 0.16444405913352966, 0.17510738968849182, 0.17187590897083282, 0.1899176687002182, 0.15644031763076782, 0.1726352423429489, 0.1697092205286026, 0.183490589261055, 0.1779131442308426, 0.16060559451580048, 0.15669533610343933, 0.15907946228981018, 0.16175316274166107, 0.18124902248382568, 0.16704197227954865, 0.1585872620344162, 0.16098350286483765, 0.1805046945810318, 0.19256119430065155, 0.1756390929222107, 0.17633315920829773, 0.16947534680366516, 0.1780659258365631, 0.1577179729938507, 0.18599818646907806, 0.16238339245319366, 0.17342758178710938, 0.19004686176776886, 0.16648747026920319, 0.1631614714860916, 0.15634743869304657, 0.17731067538261414, 0.16713683307170868, 0.15506021678447723, 0.18412095308303833, 0.1784610003232956, 0.16748063266277313, 0.16140352189540863, 0.15332861244678497, 0.18606676161289215, 0.1808742880821228, 0.1795213222503662, 0.17255251109600067, 0.14841538667678833, 0.14767548441886902, 0.15239597856998444, 0.17993859946727753, 0.15751193463802338, 0.18572689592838287, 0.163873553276062, 0.1526394784450531, 0.17240753769874573, 0.1918281763792038, 0.15070532262325287, 0.18591564893722534, 0.20866990089416504, 0.16949933767318726, 0.13839773833751678, 0.16466371715068817, 0.20332404971122742, 0.16033925116062164, 0.13690805435180664, 0.1876477599143982, 0.18485978245735168, 0.19918711483478546, 0.17933394014835358, 0.21035046875476837, 0.16320443153381348, 0.20605257153511047, 0.17419549822807312, 0.16287489235401154, 0.17977102100849152, 0.17357614636421204, 0.19056254625320435, 0.13903063535690308, 0.21451184153556824, 0.1810012310743332, 0.15927951037883759, 0.1753731220960617, 0.1614803522825241, 0.1576908677816391, 0.18122026324272156, 0.17942076921463013, 0.1662854701280594, 0.16573069989681244, 0.16540855169296265, 0.1682308316230774, 0.17236389219760895, 0.1681080162525177, 0.18839822709560394, 0.17761358618736267, 0.15365709364414215, 0.14611230790615082, 0.1740526407957077, 0.13210614025592804, 0.2038433700799942, 0.17399655282497406, 0.20961135625839233, 0.18471623957157135, 0.1451915204524994, 0.17026081681251526, 0.16346794366836548, 0.16190138459205627, 0.1951967477798462, 0.1933475136756897, 0.1486978679895401, 0.1734827607870102, 0.16254180669784546, 0.18138566613197327, 0.16440239548683167, 0.22439922392368317, 0.14999926090240479, 0.20766885578632355, 0.14677803218364716, 0.18284980952739716, 0.19479666650295258, 0.18087854981422424, 0.16644717752933502, 0.16982708871364594, 0.15989334881305695, 0.1501966118812561, 0.18196013569831848, 0.16914594173431396, 0.16199977695941925, 0.1474064737558365, 0.19031284749507904, 0.1632925122976303, 0.20647810399532318, 0.15706875920295715, 0.17858102917671204, 0.15821604430675507, 0.15647771954536438, 0.15330831706523895, 0.1402943730354309, 0.14945314824581146, 0.16471582651138306, 0.1579851508140564, 0.19313620030879974, 0.15300878882408142, 0.1653083860874176, 0.20125803351402283, 0.15752555429935455, 0.17408081889152527, 0.2150043398141861, 0.13089729845523834, 0.18767236173152924, 0.19241344928741455, 0.13808929920196533, 0.1290009766817093, 0.18224665522575378, 0.17201459407806396, 0.204024076461792, 0.1475963592529297, 0.14927057921886444, 0.19929499924182892, 0.23544616997241974, 0.1980915069580078, 0.12661847472190857, 0.20792588591575623, 0.17127612233161926, 0.17522379755973816, 0.1535518765449524, 0.1475256234407425, 0.17172925174236298, 0.15220466256141663, 0.17312227189540863, 0.22678615152835846, 0.17170749604701996, 0.13181985914707184, 0.1525658369064331, 0.2243528515100479, 0.17641638219356537, 0.15062996745109558, 0.14893756806850433, 0.17814862728118896, 0.1337270736694336, 0.15504944324493408, 0.15100152790546417, 0.13166405260562897, 0.18298205733299255, 0.14956910908222198, 0.19871388375759125, 0.17333686351776123, 0.151646226644516, 0.14958062767982483, 0.16705814003944397, 0.2045777589082718, 0.12348441779613495, 0.14591847360134125, 0.1710745096206665, 0.17946437001228333, 0.17376014590263367, 0.16953615844249725, 0.2021610140800476, 0.21392467617988586, 0.1544197052717209, 0.1511189490556717, 0.21023687720298767, 0.143838033080101, 0.17402607202529907, 0.1509290337562561, 0.15459521114826202, 0.17375408113002777, 0.17311063408851624, 0.17528676986694336, 0.1455477774143219, 0.11975198239088058, 0.17423082888126373, 0.14110130071640015, 0.2160443663597107, 0.17733624577522278, 0.21140405535697937, 0.21101444959640503, 0.14515486359596252, 0.21056202054023743, 0.2100813388824463, 0.12064798921346664, 0.12128082662820816, 0.1675475537776947, 0.14897052943706512, 0.17672783136367798, 0.14732825756072998, 0.2117851823568344, 0.1444791555404663, 0.20639187097549438, 0.24162688851356506, 0.18262025713920593, 0.20984847843647003, 0.1734951287508011, 0.19563642144203186, 0.2021998018026352, 0.17203813791275024, 0.17379017174243927, 0.17922209203243256, 0.17768718302249908, 0.17547492682933807, 0.17416180670261383, 0.17637576162815094, 0.17781803011894226, 0.16170918941497803, 0.1527671068906784, 0.16432589292526245, 0.17150960862636566, 0.18586716055870056, 0.1781083643436432, 0.17207138240337372, 0.15344201028347015, 0.15957573056221008, 0.15979492664337158, 0.18476001918315887, 0.19345612823963165, 0.1876477301120758, 0.18357090651988983, 0.17010989785194397, 0.20437544584274292, 0.1938752979040146, 0.17217709124088287, 0.15785999596118927, 0.15740478038787842, 0.1872701197862625, 0.19009707868099213, 0.17749689519405365, 0.16574497520923615, 0.17676372826099396, 0.17860710620880127, 0.18112590909004211, 0.17362599074840546, 0.18217338621616364, 0.16824358701705933, 0.16963988542556763, 0.17941591143608093, 0.16979703307151794, 0.16185159981250763, 0.1764441877603531, 0.16973069310188293, 0.17946399748325348, 0.17805512249469757, 0.1739983856678009, 0.1754390150308609, 0.18178103864192963, 0.1822759509086609, 0.16723760962486267, 0.17005805671215057, 0.18043206632137299, 0.17572222650051117, 0.1747625172138214, 0.18003849685192108, 0.17111261188983917, 0.17688661813735962, 0.16691067814826965, 0.17719726264476776, 0.1779937595129013, 0.1733473241329193, 0.18042142689228058, 0.1714656502008438, 0.16977842152118683, 0.1755795031785965, 0.17919841408729553, 0.17571432888507843, 0.18021349608898163, 0.16808761656284332, 0.16763445734977722, 0.16688767075538635, 0.17035426199436188, 0.17071256041526794, 0.1680459976196289, 0.17867302894592285, 0.18159878253936768, 0.18115192651748657, 0.16769199073314667, 0.17964190244674683, 0.16824546456336975, 0.1670016199350357, 0.18096128106117249, 0.1726231873035431, 0.16856472194194794, 0.16803093254566193, 0.17132706940174103, 0.1618981510400772, 0.1686064749956131, 0.17778640985488892, 0.1908315271139145, 0.16915978491306305, 0.17772094905376434, 0.1846032291650772, 0.1760786771774292, 0.16876249015331268, 0.17122532427310944, 0.1633388251066208, 0.16842541098594666, 0.17937135696411133, 0.17898714542388916, 0.17169174551963806, 0.1805533766746521, 0.17142389714717865, 0.16348974406719208, 0.1755431443452835, 0.16105034947395325, 0.16180194914340973, 0.1926300823688507, 0.16980502009391785, 0.1753733903169632, 0.16807682812213898, 0.17329247295856476, 0.1703944355249405, 0.1719369739294052, 0.15581440925598145, 0.1628682017326355, 0.15466906130313873, 0.16745321452617645, 0.18302735686302185, 0.1510528028011322, 0.2000633329153061, 0.16560091078281403, 0.18187090754508972, 0.15571920573711395, 0.17210732400417328, 0.156797394156456, 0.17422355711460114, 0.20791268348693848, 0.1712445616722107, 0.18428370356559753, 0.17847934365272522, 0.17089568078517914, 0.1723412573337555, 0.16612018644809723, 0.15859603881835938, 0.1681668609380722, 0.15707199275493622, 0.16532212495803833, 0.17589977383613586, 0.17491888999938965, 0.18205715715885162, 0.15635085105895996, 0.19909116625785828, 0.16486506164073944, 0.18444699048995972, 0.16522806882858276, 0.17690524458885193, 0.17005158960819244, 0.17834749817848206, 0.17532044649124146, 0.14130860567092896, 0.15964151918888092, 0.1942610740661621, 0.16093729436397552, 0.1925639659166336, 0.16475676000118256, 0.15810652077198029, 0.1390887200832367, 0.14897336065769196, 0.18948134779930115, 0.19961027801036835, 0.18348123133182526, 0.151691734790802, 0.1740369200706482, 0.170518696308136, 0.1545020043849945, 0.19572794437408447, 0.14985372126102448, 0.19105516374111176, 0.17248743772506714, 0.17051179707050323, 0.16715791821479797, 0.1616629660129547, 0.1674928516149521, 0.17153611779212952, 0.17209696769714355, 0.16050225496292114, 0.20290899276733398, 0.14906039834022522, 0.18357235193252563, 0.1613878607749939, 0.17678460478782654, 0.1578616350889206, 0.1931060552597046, 0.17913757264614105, 0.17608821392059326, 0.1688891053199768, 0.16944830119609833, 0.19019760191440582, 0.18327027559280396, 0.19922727346420288, 0.1770804226398468, 0.15993370115756989, 0.19379545748233795, 0.17339269816875458, 0.1703304797410965, 0.15604859590530396, 0.198795348405838, 0.16050799190998077, 0.17643800377845764, 0.18871240317821503, 0.19838301837444305, 0.16491781175136566, 0.1813528835773468, 0.16084106266498566, 0.15421120822429657, 0.17325109243392944, 0.18141652643680573, 0.18010790646076202, 0.18628528714179993, 0.17931538820266724, 0.16446079313755035, 0.1690329909324646, 0.20435188710689545, 0.1817077249288559, 0.17187823355197906, 0.1688004732131958, 0.20098134875297546, 0.15602700412273407, 0.17385640740394592, 0.15143081545829773, 0.18203702569007874, 0.1658468246459961, 0.16063106060028076, 0.18281352519989014, 0.15234892070293427, 0.19685795903205872, 0.17924953997135162, 0.17594587802886963, 0.18632878363132477, 0.19028116762638092, 0.16746950149536133, 0.15521883964538574, 0.17957067489624023, 0.16739843785762787, 0.19206266105175018, 0.18031765520572662, 0.1782248169183731, 0.17705634236335754, 0.1673363447189331, 0.18598884344100952, 0.17953000962734222, 0.17067663371562958, 0.16509932279586792, 0.17105312645435333, 0.17372547090053558, 0.18071366846561432, 0.1690048724412918, 0.17487698793411255, 0.17401264607906342, 0.1779797524213791, 0.17500530183315277, 0.17191308736801147, 0.17303894460201263, 0.1752612441778183, 0.1672230064868927, 0.17332683503627777, 0.1735517382621765, 0.17364239692687988, 0.17620313167572021, 0.17285652458667755, 0.16487431526184082, 0.17083095014095306, 0.17597925662994385, 0.16769593954086304, 0.17558911442756653, 0.17013975977897644, 0.18141593039035797, 0.17348900437355042, 0.17332468926906586, 0.17114713788032532, 0.17641563713550568, 0.18116284906864166, 0.1650942713022232, 0.1817087084054947, 0.1811053603887558, 0.17464224994182587, 0.1658789962530136, 0.1723569631576538, 0.16830259561538696, 0.17806044220924377, 0.17076514661312103, 0.17789198458194733, 0.17117494344711304, 0.17183195054531097, 0.17224185168743134, 0.16886857151985168, 0.17731161415576935, 0.17022939026355743, 0.17984315752983093, 0.16992680728435516, 0.1762388050556183, 0.17924544215202332, 0.16759945452213287, 0.17424191534519196, 0.17912675440311432, 0.17897926270961761, 0.172771155834198, 0.16855964064598083, 0.1719689965248108, 0.1765354871749878, 0.17348599433898926, 0.16964738070964813, 0.16978758573532104, 0.17661446332931519, 0.17334860563278198, 0.17918558418750763, 0.17228755354881287, 0.17230895161628723, 0.16938436031341553, 0.17203523218631744, 0.16810135543346405, 0.1761949211359024, 0.174693763256073, 0.175229012966156, 0.17155738174915314, 0.17553381621837616, 0.16975633800029755, 0.17938069999217987, 0.17302438616752625, 0.17089015245437622, 0.17064887285232544, 0.18184205889701843, 0.17490185797214508, 0.17533069849014282, 0.17377227544784546, 0.1707952469587326, 0.17958827316761017, 0.16940496861934662, 0.17675703763961792, 0.17021851241588593, 0.17504601180553436, 0.17695102095603943, 0.17628926038742065, 0.17678575217723846, 0.17000716924667358, 0.1732875108718872, 0.17457908391952515, 0.1716172695159912, 0.17193295061588287, 0.17084693908691406, 0.17538008093833923, 0.17062540352344513, 0.17561356723308563, 0.17461779713630676, 0.17802834510803223, 0.17274974286556244, 0.16975414752960205, 0.17583471536636353, 0.16999244689941406, 0.16866624355316162, 0.17578406631946564, 0.1693735271692276, 0.17637999355793, 0.1765313297510147, 0.1729711890220642, 0.1749068647623062, 0.17091242969036102, 0.17493008077144623, 0.17110572755336761, 0.16900865733623505, 0.1768461912870407, 0.1698818951845169, 0.17522329092025757, 0.17810691893100739, 0.17018169164657593, 0.17237471044063568, 0.1810903400182724, 0.17690914869308472, 0.18121786415576935, 0.1655055582523346, 0.16622480750083923, 0.16560865938663483, 0.1700318157672882, 0.17257001996040344, 0.1721799075603485, 0.17450638115406036, 0.1863725632429123, 0.1662353277206421, 0.1749296635389328, 0.17565281689167023, 0.16037119925022125, 0.17656755447387695, 0.188211128115654, 0.18784110248088837, 0.17816686630249023, 0.16131733357906342, 0.16854265332221985, 0.17849144339561462, 0.17662695050239563, 0.17825938761234283, 0.1763814389705658, 0.16772374510765076, 0.17030304670333862, 0.17174163460731506, 0.17040768265724182, 0.1629759818315506, 0.17290356755256653, 0.18299424648284912, 0.16707998514175415, 0.18121421337127686, 0.1729622483253479, 0.17198732495307922, 0.16888248920440674, 0.18456272780895233, 0.17281661927700043, 0.18695203959941864, 0.1704191267490387, 0.16177958250045776, 0.17230583727359772, 0.1771654337644577, 0.16604185104370117, 0.1790921837091446, 0.1858472377061844, 0.17397578060626984, 0.18322855234146118, 0.17617763578891754, 0.16467246413230896, 0.17644433677196503, 0.16649295389652252, 0.1792568415403366, 0.17670223116874695, 0.1729934811592102, 0.17983411252498627, 0.17333175241947174, 0.17421914637088776, 0.17785532772541046, 0.17857587337493896, 0.17522436380386353, 0.17744815349578857, 0.17024439573287964, 0.17065462470054626, 0.17327912151813507, 0.1734224408864975, 0.17681102454662323, 0.17297425866127014, 0.17820924520492554, 0.17274847626686096, 0.1721784919500351, 0.17132742702960968, 0.16843339800834656, 0.1717347502708435, 0.17648430168628693, 0.17364609241485596, 0.17106106877326965, 0.17752845585346222, 0.17238302528858185, 0.17703694105148315, 0.17358042299747467, 0.1722504198551178, 0.17182934284210205, 0.1671474575996399, 0.17600524425506592, 0.1644574999809265, 0.16866488754749298, 0.17776301503181458, 0.1645294427871704, 0.17321154475212097, 0.17505404353141785, 0.17278234660625458, 0.15917493402957916, 0.18658359348773956, 0.18511345982551575, 0.17251871526241302, 0.17526616156101227, 0.18325959146022797, 0.17570988833904266, 0.17603431642055511, 0.17578355967998505, 0.18307587504386902, 0.16221116483211517, 0.19008401036262512, 0.17292813956737518, 0.18116013705730438, 0.16768532991409302, 0.16780105233192444, 0.16695289313793182, 0.16074435412883759, 0.18256615102291107, 0.16606029868125916, 0.17426495254039764, 0.18363918364048004, 0.16760985553264618, 0.16598255932331085, 0.18243291974067688, 0.16732431948184967, 0.1728125810623169, 0.16514700651168823, 0.16592060029506683, 0.1642836630344391, 0.16392511129379272, 0.17316073179244995, 0.15433983504772186, 0.16293007135391235, 0.1729443371295929, 0.173109233379364, 0.1741175800561905, 0.20048263669013977, 0.17345619201660156, 0.20103232562541962, 0.18908636271953583, 0.17659708857536316, 0.16816310584545135, 0.15049682557582855, 0.1863124668598175, 0.17299170792102814, 0.17013417184352875, 0.18853597342967987, 0.16390953958034515, 0.184003084897995, 0.16191740334033966, 0.17519184947013855, 0.16429388523101807, 0.16274215281009674, 0.17643611133098602, 0.1628025323152542, 0.1753993034362793, 0.1742684543132782, 0.18707694113254547, 0.1893453449010849, 0.19701746106147766, 0.18452033400535583, 0.1534986048936844, 0.17676924169063568, 0.17606110870838165, 0.1546488106250763, 0.15455041825771332, 0.18039639294147491, 0.15387557446956635, 0.17353783547878265, 0.17426830530166626, 0.186196967959404, 0.16355842351913452, 0.15188626945018768, 0.17034822702407837, 0.18593177199363708, 0.17527824640274048, 0.16284161806106567, 0.1496243178844452, 0.16112977266311646, 0.18857795000076294, 0.1583605855703354, 0.1728752702474594, 0.1723313331604004, 0.15824900567531586, 0.15826261043548584, 0.172406405210495, 0.17500075697898865, 0.1761828362941742, 0.17182092368602753, 0.15877081453800201, 0.17107199132442474, 0.17095458507537842, 0.17651887238025665, 0.19307741522789001, 0.1552981287240982, 0.1906803846359253, 0.19149644672870636, 0.15513435006141663, 0.1927129030227661, 0.15721862018108368, 0.1585976928472519, 0.17921237647533417, 0.14098474383354187, 0.17686346173286438, 0.1547093242406845, 0.19549646973609924, 0.1710159033536911, 0.19411416351795197, 0.19387884438037872, 0.1730014532804489, 0.14035668969154358, 0.17339029908180237, 0.1747533529996872, 0.191673144698143, 0.1571086347103119, 0.1597505360841751, 0.17826712131500244, 0.1766727864742279, 0.17757749557495117, 0.15551169216632843, 0.19284577667713165, 0.17587241530418396, 0.1946699470281601, 0.15700282156467438, 0.14085370302200317, 0.19181914627552032, 0.15704099833965302, 0.15706130862236023, 0.19768314063549042, 0.15768441557884216, 0.21257087588310242, 0.1571018397808075, 0.17034150660037994, 0.19138631224632263, 0.17493726313114166, 0.19402742385864258, 0.17542164027690887, 0.1610371619462967, 0.17400291562080383, 0.1603890061378479, 0.19164644181728363, 0.1561076045036316, 0.15989622473716736, 0.1739126592874527, 0.1724093109369278, 0.14325256645679474, 0.16011522710323334, 0.1576412320137024, 0.1609722524881363, 0.14090172946453094, 0.1557019203901291, 0.17639140784740448, 0.1575244963169098, 0.21716691553592682, 0.17537900805473328, 0.1730073243379593, 0.15313638746738434, 0.15027731657028198, 0.15430492162704468, 0.17456570267677307, 0.1527773141860962, 0.1543758511543274, 0.17541643977165222, 0.20023445785045624, 0.1758088767528534, 0.13035273551940918, 0.17391832172870636, 0.17823055386543274, 0.20180048048496246, 0.22905240952968597, 0.17778325080871582, 0.15363679826259613, 0.17727920413017273, 0.17731580138206482, 0.2264319509267807, 0.20143035054206848, 0.15250155329704285, 0.19862738251686096, 0.15599660575389862, 0.17288702726364136, 0.17376500368118286, 0.15565471351146698, 0.17569030821323395, 0.176288902759552, 0.1945040374994278, 0.17598947882652283, 0.19360296428203583, 0.15677481889724731, 0.1562853902578354, 0.1390770822763443, 0.19438645243644714, 0.15702177584171295, 0.19115562736988068, 0.21346747875213623, 0.1727718710899353, 0.1573956459760666, 0.19346201419830322, 0.15726293623447418, 0.1752428412437439, 0.15953455865383148, 0.1732291430234909, 0.1596018224954605, 0.17327235639095306, 0.19122081995010376, 0.17599472403526306, 0.1753716915845871, 0.20820122957229614, 0.15722039341926575, 0.14420413970947266, 0.17384912073612213, 0.1442941427230835, 0.15754370391368866, 0.20760107040405273, 0.17566537857055664, 0.1571509689092636, 0.1571773886680603, 0.1433504819869995, 0.19142621755599976, 0.158839613199234, 0.15724581480026245, 0.17425887286663055, 0.15857186913490295, 0.1746298372745514, 0.17424272000789642, 0.19511516392230988, 0.21331895887851715, 0.19152890145778656, 0.2121018022298813, 0.19028879702091217, 0.17509497702121735, 0.17437639832496643, 0.17432481050491333, 0.159311443567276, 0.1596088856458664, 0.1754164695739746, 0.15939173102378845, 0.1898421347141266, 0.17336894571781158, 0.15917906165122986, 0.17386937141418457, 0.1881798803806305, 0.17240957915782928, 0.1739094853401184, 0.16118952631950378, 0.16096770763397217, 0.18616805970668793, 0.18714597821235657, 0.18567480146884918, 0.18719981610774994, 0.17203909158706665, 0.17486804723739624, 0.16270694136619568, 0.17344602942466736, 0.16275693476200104, 0.1835518628358841, 0.17347688972949982, 0.1628379076719284, 0.16367898881435394, 0.1822599619626999, 0.1721922904253006, 0.172637939453125, 0.18432390689849854, 0.1717034876346588, 0.1647123247385025, 0.18542981147766113, 0.1645134538412094, 0.15461218357086182, 0.1731586754322052, 0.175532728433609, 0.18379689753055573, 0.17304961383342743, 0.17082864046096802, 0.16140367090702057, 0.16354665160179138, 0.1747196912765503, 0.18531984090805054, 0.16531333327293396, 0.16243411600589752, 0.16920353472232819, 0.17374169826507568, 0.1637779027223587, 0.1760721355676651, 0.16345642507076263, 0.16980572044849396, 0.16193944215774536, 0.15024401247501373, 0.14941878616809845, 0.17634814977645874, 0.17565268278121948, 0.1882832646369934, 0.17240235209465027, 0.15630027651786804, 0.17489641904830933, 0.14449509978294373, 0.17801110446453094, 0.1739126294851303, 0.173226997256279, 0.15734443068504333, 0.17334610223770142, 0.17755724489688873, 0.1947811245918274, 0.14017796516418457, 0.1908065527677536, 0.15548117458820343, 0.1945519596338272, 0.17517565190792084, 0.13899727165699005, 0.1946197897195816, 0.19485433399677277, 0.13866150379180908, 0.21458809077739716, 0.1956213265657425, 0.19355392456054688, 0.173095703125, 0.17499327659606934, 0.17553967237472534, 0.17619074881076813, 0.15942592918872833, 0.1918347328901291, 0.17596937716007233, 0.1574196219444275, 0.17319053411483765, 0.17193244397640228, 0.1918613165616989, 0.173183411359787, 0.17213347554206848, 0.15880531072616577, 0.18655359745025635, 0.17511440813541412, 0.18896016478538513, 0.15911921858787537, 0.18979057669639587, 0.1588052362203598, 0.14779604971408844, 0.17482450604438782, 0.20222514867782593, 0.17417003214359283, 0.19072651863098145, 0.18696773052215576, 0.17435623705387115, 0.1643572449684143, 0.17745135724544525, 0.1980152577161789, 0.1866283267736435, 0.16204383969306946, 0.18345710635185242, 0.1840442568063736, 0.18211355805397034, 0.17376960813999176, 0.16548922657966614, 0.180740088224411, 0.16817644238471985, 0.18401408195495605, 0.16699351370334625, 0.17237351834774017, 0.1730833351612091, 0.1709669530391693, 0.18190835416316986, 0.16646736860275269, 0.1800745576620102, 0.16254650056362152, 0.17980481684207916, 0.17302891612052917, 0.17988258600234985, 0.17298026382923126, 0.17666751146316528, 0.17150746285915375, 0.17988842725753784, 0.17528784275054932, 0.18047213554382324, 0.17395135760307312, 0.1776464879512787, 0.17447176575660706, 0.17288313806056976, 0.17196954786777496, 0.173089399933815, 0.17411625385284424, 0.17190751433372498, 0.17327547073364258, 0.1751529574394226, 0.17285878956317902, 0.1719447523355484, 0.17398977279663086, 0.17466580867767334, 0.17176063358783722, 0.1743619740009308, 0.17350457608699799, 0.17510493099689484, 0.17374688386917114, 0.1715882569551468, 0.17455698549747467, 0.1738915592432022, 0.17542529106140137, 0.17115353047847748, 0.17139065265655518, 0.1728326827287674, 0.17427600920200348, 0.17351321876049042, 0.1743631660938263, 0.17215776443481445, 0.17088721692562103, 0.17393909394741058, 0.17433395981788635, 0.17177394032478333, 0.1748163402080536, 0.17241604626178741, 0.17231036722660065, 0.1707153022289276, 0.17356333136558533, 0.17104320228099823, 0.1735129952430725, 0.1719948798418045, 0.17279894649982452, 0.1728128045797348, 0.17276093363761902, 0.1779482513666153, 0.16832150518894196, 0.17901286482810974, 0.1734471321105957, 0.17298667132854462, 0.17577098309993744, 0.1661747395992279, 0.1692706197500229, 0.169268399477005, 0.16980314254760742, 0.16899128258228302, 0.16660411655902863, 0.16130325198173523, 0.16495642066001892, 0.1839192807674408, 0.18057654798030853, 0.16607363522052765, 0.18247675895690918, 0.1754017323255539, 0.17266437411308289, 0.1552000492811203, 0.1606733798980713, 0.16494007408618927, 0.1639707088470459, 0.16322650015354156, 0.17611539363861084, 0.1987149864435196, 0.18685568869113922, 0.160101056098938, 0.17591871321201324, 0.175543412566185, 0.18757808208465576, 0.17316393554210663, 0.17327740788459778, 0.18715530633926392, 0.18369360268115997, 0.18493908643722534, 0.15046100318431854, 0.1700519621372223, 0.17589454352855682, 0.150961235165596, 0.1640552133321762, 0.16106438636779785, 0.17248228192329407, 0.19947855174541473, 0.17275431752204895, 0.17449676990509033, 0.1720200479030609, 0.187910795211792, 0.15907740592956543, 0.17546528577804565, 0.18670399487018585, 0.16280916333198547, 0.17103636264801025, 0.15100771188735962, 0.1613054722547531, 0.18576805293560028, 0.16207052767276764, 0.1599440723657608, 0.19004136323928833, 0.15949109196662903, 0.1734323352575302, 0.17541632056236267, 0.17511771619319916, 0.20081323385238647, 0.18408873677253723, 0.16328102350234985, 0.1852094829082489, 0.16096556186676025, 0.1701468527317047, 0.17479655146598816, 0.18485218286514282, 0.1822199672460556, 0.18262378871440887, 0.1765449196100235, 0.16611871123313904, 0.18413838744163513, 0.16501228511333466, 0.16035686433315277, 0.16461148858070374, 0.16501030325889587, 0.18417346477508545, 0.17329205572605133, 0.16492794454097748, 0.1653950810432434, 0.18528378009796143, 0.1822836995124817, 0.1820821464061737, 0.15341943502426147, 0.17819485068321228, 0.1712043732404709, 0.164907768368721, 0.17082096636295319, 0.16199356317520142, 0.17634475231170654, 0.16057467460632324, 0.16360798478126526, 0.17314325273036957, 0.19741903245449066, 0.17355123162269592, 0.17682957649230957, 0.18419380486011505, 0.16609792411327362, 0.18691125512123108, 0.16418808698654175, 0.18571381270885468, 0.1965196132659912, 0.17122618854045868, 0.1833423525094986, 0.1738804131746292, 0.17524436116218567, 0.1549803763628006, 0.1806338131427765, 0.19265374541282654, 0.1766536980867386, 0.1636326164007187, 0.17128755152225494, 0.16902627050876617, 0.19000943005084991, 0.1807624101638794, 0.17298205196857452, 0.17532594501972198, 0.17279967665672302, 0.16950608789920807, 0.16807563602924347, 0.16676649451255798, 0.1735297292470932, 0.1843426525592804, 0.16820839047431946, 0.17161768674850464, 0.17580069601535797, 0.16438758373260498, 0.17472772300243378, 0.17981989681720734, 0.17042922973632812, 0.1733706146478653, 0.1682141125202179, 0.1690080761909485, 0.17044560611248016, 0.16844327747821808, 0.164687380194664, 0.17572639882564545, 0.17852433025836945, 0.16585645079612732, 0.16864994168281555, 0.17117834091186523, 0.17078061401844025, 0.17320309579372406, 0.16583967208862305, 0.1814851611852646, 0.1774379014968872, 0.17636412382125854, 0.17885833978652954, 0.1790967583656311, 0.17352920770645142, 0.17867045104503632, 0.17143575847148895, 0.17679409682750702, 0.1700490564107895, 0.16928231716156006, 0.18043726682662964, 0.1702713519334793, 0.18286795914173126, 0.1702764481306076, 0.1732950508594513, 0.18233948945999146, 0.17519788444042206, 0.18321681022644043, 0.1760292649269104, 0.17260681092739105, 0.17151035368442535, 0.17729800939559937, 0.16890905797481537, 0.16688278317451477, 0.17732101678848267, 0.17480045557022095, 0.17722965776920319, 0.16642871499061584, 0.16739241778850555, 0.1707392781972885, 0.17368386685848236, 0.17344999313354492, 0.16901347041130066, 0.17577269673347473, 0.17494253814220428, 0.17933599650859833, 0.17160563170909882, 0.1702347844839096, 0.1818477362394333, 0.17235684394836426, 0.16749587655067444, 0.1769818812608719, 0.18105433881282806, 0.18250672519207, 0.1649053692817688, 0.17111141979694366, 0.17390766739845276, 0.17074425518512726, 0.16780085861682892, 0.17526574432849884, 0.16661156713962555, 0.17817965149879456, 0.1779009848833084, 0.16867737472057343, 0.16840572655200958, 0.18398411571979523, 0.1670869141817093, 0.17082920670509338, 0.18413539230823517, 0.16927002370357513, 0.1664828211069107, 0.16628657281398773, 0.17505517601966858, 0.1747845560312271, 0.16951163113117218, 0.16390188038349152, 0.18502911925315857, 0.16933664679527283, 0.16473661363124847, 0.1687759906053543, 0.18018808960914612, 0.1709200143814087, 0.1680310070514679, 0.16403794288635254, 0.17686203122138977, 0.16937564313411713, 0.18048107624053955, 0.17560961842536926, 0.17885050177574158, 0.16601887345314026, 0.16918958723545074, 0.15953584015369415, 0.16320931911468506, 0.18065056204795837, 0.1663186103105545, 0.15750989317893982, 0.16944335401058197, 0.1754298359155655, 0.17214800417423248, 0.17224077880382538, 0.18415777385234833, 0.16959883272647858, 0.17102383077144623, 0.1642681062221527, 0.16498756408691406, 0.18198244273662567, 0.18698231875896454, 0.15187415480613708, 0.18637891113758087, 0.1631900519132614, 0.16430693864822388, 0.18437503278255463, 0.19849510490894318, 0.17678292095661163, 0.1773100197315216, 0.17537669837474823, 0.17616526782512665, 0.163911834359169, 0.17191265523433685, 0.17227737605571747, 0.1604776382446289, 0.183755025267601, 0.1720755696296692, 0.18563953042030334, 0.15799155831336975, 0.16989430785179138, 0.1652866005897522]\n",
            "Val loss 0.1736566699013142\n",
            "Val auc roc 0.5117495362377894\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f629a4259fa456994247da01e76daa1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2090.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1729\n",
            "Train Losses : [0.15190409123897552, 0.16424039006233215, 0.17466676235198975, 0.15084238350391388, 0.1601864993572235, 0.17940467596054077, 0.16384495794773102, 0.18425370752811432, 0.18561701476573944, 0.16186070442199707, 0.1700126975774765, 0.17709524929523468, 0.17636221647262573, 0.1741335391998291, 0.18608056008815765, 0.1801680028438568, 0.17032572627067566, 0.18571704626083374, 0.18504999577999115, 0.14739678800106049, 0.16396713256835938, 0.18915946781635284, 0.15861302614212036, 0.17062799632549286, 0.20264294743537903, 0.19316212832927704, 0.18473045527935028, 0.16941425204277039, 0.1858215481042862, 0.15824487805366516, 0.19917456805706024, 0.19832627475261688, 0.1709030121564865, 0.1631506234407425, 0.15319715440273285, 0.1694396287202835, 0.1895664483308792, 0.19420099258422852, 0.16173097491264343, 0.16935482621192932, 0.1924341917037964, 0.16670729219913483, 0.15667292475700378, 0.1659323126077652, 0.17069268226623535, 0.18549270927906036, 0.1669519990682602, 0.17064423859119415, 0.1621282696723938, 0.18075929582118988, 0.1817432940006256, 0.1779618114233017, 0.15708279609680176, 0.18572790920734406, 0.1878914088010788, 0.16617916524410248, 0.1688302755355835, 0.16576167941093445, 0.18781866133213043, 0.17695119976997375, 0.1879301816225052, 0.17866207659244537, 0.15788200497627258, 0.1772100031375885, 0.17739343643188477, 0.1647573858499527, 0.18157297372817993, 0.16864393651485443, 0.16784657537937164, 0.1652398556470871, 0.18236960470676422, 0.15797501802444458, 0.17046764492988586, 0.16939140856266022, 0.17977064847946167, 0.17890240252017975, 0.17456021904945374, 0.16938675940036774, 0.15751920640468597, 0.16798052191734314, 0.17129072546958923, 0.1801452785730362, 0.18116189539432526, 0.1817249059677124, 0.1565917432308197, 0.17097309231758118, 0.17772147059440613, 0.1688479483127594, 0.16361670196056366, 0.1764192134141922, 0.17588414251804352, 0.16395188868045807, 0.1608731597661972, 0.17033231258392334, 0.19325485825538635, 0.1597955971956253, 0.1737811118364334, 0.15943078696727753, 0.1547066867351532, 0.17420321702957153, 0.1682841032743454, 0.16918902099132538, 0.16485130786895752, 0.17157897353172302, 0.18891513347625732, 0.16708621382713318, 0.1965588629245758, 0.15235483646392822, 0.1679905652999878, 0.18250368535518646, 0.1592346876859665, 0.16172119975090027, 0.18367864191532135, 0.15115220844745636, 0.18223948776721954, 0.19859281182289124, 0.1616821140050888, 0.164576455950737, 0.17128950357437134, 0.15057732164859772, 0.17339874804019928, 0.19940520823001862, 0.15990763902664185, 0.16675706207752228, 0.16715958714485168, 0.1559900939464569, 0.16136012971401215, 0.17013898491859436, 0.1779848337173462, 0.17880764603614807, 0.18648025393486023, 0.19372259080410004, 0.1740315556526184, 0.15685461461544037, 0.16519010066986084, 0.18997593224048615, 0.1837923675775528, 0.16420520842075348, 0.15998704731464386, 0.180016428232193, 0.16865307092666626, 0.1571870595216751, 0.17351806163787842, 0.17129352688789368, 0.14907413721084595, 0.15572044253349304, 0.1483500599861145, 0.16716435551643372, 0.16269662976264954, 0.15593306720256805, 0.1552564650774002, 0.19502802193164825, 0.16922177374362946, 0.1620502769947052, 0.17770305275917053, 0.17948950827121735, 0.15962237119674683, 0.17959465086460114, 0.18251679837703705, 0.19641904532909393, 0.210589200258255, 0.17711839079856873, 0.16011863946914673, 0.1618078351020813, 0.15274716913700104, 0.16078375279903412, 0.18042528629302979, 0.18979895114898682, 0.18876530230045319, 0.17842228710651398, 0.15229003131389618, 0.18848678469657898, 0.17506380379199982, 0.16052956879138947, 0.1783703863620758, 0.1868627816438675, 0.17380176484584808, 0.15930797159671783, 0.18737450242042542, 0.1428443044424057, 0.1797247976064682, 0.17838513851165771, 0.17932124435901642, 0.17724137008190155, 0.20869679749011993, 0.17728881537914276, 0.17692922055721283, 0.19460810720920563, 0.14420722424983978, 0.19271893799304962, 0.15735934674739838, 0.1812538504600525, 0.17606937885284424, 0.18870224058628082, 0.1453574150800705, 0.1646360158920288, 0.17843182384967804, 0.18571726977825165, 0.17680442333221436, 0.1880192905664444, 0.16104844212532043, 0.17046573758125305, 0.18496060371398926, 0.18562108278274536, 0.18712611496448517, 0.17812174558639526, 0.18460384011268616, 0.20210705697536469, 0.1845499873161316, 0.18987344205379486, 0.18861830234527588, 0.16502900421619415, 0.16295325756072998, 0.1726093739271164, 0.18236614763736725, 0.16442659497261047, 0.1710232049226761, 0.1795247495174408, 0.1701926290988922, 0.16477525234222412, 0.18136966228485107, 0.18306280672550201, 0.16837699711322784, 0.1566481590270996, 0.17887817323207855, 0.1673385202884674, 0.17338436841964722, 0.15696483850479126, 0.18174903094768524, 0.17296074330806732, 0.162217915058136, 0.1700969934463501, 0.16048432886600494, 0.17368511855602264, 0.18628481030464172, 0.16813239455223083, 0.18022137880325317, 0.15940658748149872, 0.17834755778312683, 0.18811647593975067, 0.17275063693523407, 0.16468732059001923, 0.16429907083511353, 0.18601618707180023, 0.1754211187362671, 0.1835137903690338, 0.1641259342432022, 0.19251225888729095, 0.15588843822479248, 0.18337401747703552, 0.19197441637516022, 0.16937239468097687, 0.16864372789859772, 0.166978120803833, 0.16260220110416412, 0.16148842871189117, 0.17407390475273132, 0.18207354843616486, 0.18145811557769775, 0.18442073464393616, 0.19054476916790009, 0.17763720452785492, 0.1752757728099823, 0.1686098724603653, 0.18866467475891113, 0.1656510978937149, 0.16702918708324432, 0.1680379956960678, 0.16004589200019836, 0.17636427283287048, 0.17505806684494019, 0.17659598588943481, 0.1803099513053894, 0.1632842868566513, 0.1713590919971466, 0.17900188267230988, 0.16555750370025635, 0.1694297194480896, 0.1781836748123169, 0.17193245887756348, 0.16561125218868256, 0.15811532735824585, 0.1795044094324112, 0.16415822505950928, 0.1855892390012741, 0.18395785987377167, 0.16356678307056427, 0.1653205305337906, 0.16424095630645752, 0.15590403974056244, 0.15538465976715088, 0.16254667937755585, 0.18569806218147278, 0.15347890555858612, 0.16568323969841003, 0.16126911342144012, 0.16141626238822937, 0.1726178675889969, 0.18800088763237, 0.17573149502277374, 0.18407443165779114, 0.17234060168266296, 0.201105996966362, 0.17373156547546387, 0.14871570467948914, 0.16399310529232025, 0.17400607466697693, 0.16362252831459045, 0.1605004221200943, 0.16162045300006866, 0.1593899428844452, 0.18996135890483856, 0.1864061802625656, 0.1758715659379959, 0.1867133527994156, 0.18665021657943726, 0.14631721377372742, 0.15902911126613617, 0.1728539764881134, 0.17315922677516937, 0.1893385499715805, 0.17531971633434296, 0.15943938493728638, 0.18755513429641724, 0.15869764983654022, 0.1732475906610489, 0.1893453299999237, 0.17219612002372742, 0.18986715376377106, 0.1580764502286911, 0.1465533822774887, 0.1732902079820633, 0.17200639843940735, 0.17180383205413818, 0.15937133133411407, 0.1583579182624817, 0.15947286784648895, 0.1571841835975647, 0.19141189754009247, 0.19129902124404907, 0.17633719742298126, 0.15722884237766266, 0.17155200242996216, 0.19167521595954895, 0.1900019347667694, 0.18953043222427368, 0.15910303592681885, 0.17582851648330688, 0.1907166987657547, 0.1585826873779297, 0.16239717602729797, 0.18641908466815948, 0.16333626210689545, 0.14622481167316437, 0.1754390299320221, 0.18946926295757294, 0.16301532089710236, 0.17436036467552185, 0.17182235419750214, 0.18909931182861328, 0.17497174441814423, 0.1891450732946396, 0.18961818516254425, 0.1889064610004425, 0.15986500680446625, 0.18827979266643524, 0.16030879318714142, 0.1878460943698883, 0.20171046257019043, 0.16099032759666443, 0.17381560802459717, 0.18673430383205414, 0.17381179332733154, 0.18618537485599518, 0.18604736030101776, 0.17378102242946625, 0.17377221584320068, 0.17372530698776245, 0.15281498432159424, 0.1643427461385727, 0.17342323064804077, 0.17369653284549713, 0.17371848225593567, 0.17354938387870789, 0.17355643212795258, 0.16344021260738373, 0.1633780151605606, 0.15334002673625946, 0.16315725445747375, 0.16298289597034454, 0.18492068350315094, 0.17358896136283875, 0.16255313158035278, 0.17368805408477783, 0.16225501894950867, 0.15080375969409943, 0.16184782981872559, 0.18650929629802704, 0.161358043551445, 0.17383037507534027, 0.1736672967672348, 0.17384867370128632, 0.17390023171901703, 0.16056616604328156, 0.16038283705711365, 0.1881929337978363, 0.16009463369846344, 0.18855707347393036, 0.1740337610244751, 0.15975119173526764, 0.1742156594991684, 0.17392079532146454, 0.17389926314353943, 0.17400529980659485, 0.15947173535823822, 0.18936903774738312, 0.15940551459789276, 0.17414523661136627, 0.18946488201618195, 0.15928110480308533, 0.18951837718486786, 0.2054966241121292, 0.15945221483707428, 0.18916556239128113, 0.15965834259986877, 0.14614151418209076, 0.15978191792964935, 0.18891917169094086, 0.18886736035346985, 0.15973612666130066, 0.17405959963798523, 0.17399096488952637, 0.1597956120967865, 0.17394550144672394, 0.15968598425388336, 0.1459934115409851, 0.15950961410999298, 0.17400163412094116, 0.20571960508823395, 0.18971581757068634, 0.17401693761348724, 0.17419110238552094, 0.17404945194721222, 0.1739966869354248, 0.20515193045139313, 0.17398925125598907, 0.14606256783008575, 0.15981049835681915, 0.1739523708820343, 0.17397485673427582, 0.18879595398902893, 0.15976175665855408, 0.18851059675216675, 0.17380614578723907, 0.20359951257705688, 0.1469767987728119, 0.16029731929302216, 0.16030749678611755, 0.1738911271095276, 0.18823859095573425, 0.17393635213375092, 0.1738862842321396, 0.1880374699831009, 0.14750893414020538, 0.17394600808620453, 0.1879601776599884, 0.1605173945426941, 0.1739194244146347, 0.1476660966873169, 0.1739267110824585, 0.1603766679763794, 0.1881369948387146, 0.16030879318714142, 0.17394551634788513, 0.173954576253891, 0.17401307821273804, 0.18854016065597534, 0.1739255040884018, 0.1600385457277298, 0.1600092500448227, 0.1739644557237625, 0.18875126540660858, 0.18874523043632507, 0.17383207380771637, 0.17398026585578918, 0.17395895719528198, 0.17392422258853912, 0.16022133827209473, 0.173653706908226, 0.14710184931755066, 0.1881442815065384, 0.16013690829277039, 0.17397728562355042, 0.160125270485878, 0.15993589162826538, 0.20401722192764282, 0.1462990641593933, 0.17375557124614716, 0.15963925421237946, 0.1595882773399353, 0.1741005927324295, 0.17406810820102692, 0.17407244443893433, 0.1896703541278839, 0.18961192667484283, 0.14485123753547668, 0.174064040184021, 0.17408159375190735, 0.2062317430973053, 0.17406438291072845, 0.189606711268425, 0.18946070969104767, 0.15947461128234863, 0.15958485007286072, 0.1595904678106308, 0.18894170224666595, 0.1739664226770401, 0.18867477774620056, 0.16135427355766296, 0.16009223461151123, 0.17412468791007996, 0.1600111871957779, 0.1739177703857422, 0.16000790894031525, 0.1740359216928482, 0.15985901653766632, 0.1741141378879547, 0.17401985824108124, 0.1596345752477646, 0.14570336043834686, 0.1740342229604721, 0.17398595809936523, 0.20606471598148346, 0.18979458510875702, 0.15912072360515594, 0.17404021322727203, 0.15897974371910095, 0.17407681047916412, 0.17408215999603271, 0.17405639588832855, 0.2061452865600586, 0.17408308386802673, 0.17404016852378845, 0.15937092900276184, 0.15935856103897095, 0.15942318737506866, 0.20534560084342957, 0.17414341866970062, 0.1594734787940979, 0.15954048931598663, 0.15950506925582886, 0.1740294247865677, 0.1741342842578888, 0.1451888531446457, 0.17405611276626587, 0.17407065629959106, 0.18988840281963348, 0.17413799464702606, 0.18997958302497864, 0.15896762907505035, 0.15893183648586273, 0.17410942912101746, 0.1588921844959259, 0.14412999153137207, 0.1904236376285553, 0.1436225026845932, 0.19069623947143555, 0.17411284148693085, 0.17417652904987335, 0.15808460116386414, 0.15801289677619934, 0.15784873068332672, 0.15770350396633148, 0.17430764436721802, 0.19204650819301605, 0.1742832213640213, 0.1743616908788681, 0.1571408063173294, 0.1925545632839203, 0.1405154913663864, 0.1742815226316452, 0.156783789396286, 0.1744004338979721, 0.17442570626735687, 0.17440448701381683, 0.17446300387382507, 0.17443488538265228, 0.15621325373649597, 0.19384069740772247, 0.17443536221981049, 0.15606676042079926, 0.1744498759508133, 0.17443956434726715, 0.13847005367279053, 0.17445175349712372, 0.1556953638792038, 0.17451708018779755, 0.155463308095932, 0.21639586985111237, 0.1746196746826172, 0.1553509384393692, 0.1746053397655487, 0.17461343109607697, 0.17459988594055176, 0.1745874434709549, 0.1951226145029068, 0.19494713842868805, 0.17454411089420319, 0.19459618628025055, 0.1744433045387268, 0.19401076436042786, 0.13878898322582245, 0.13899797201156616, 0.1936529278755188, 0.15634892880916595, 0.17436203360557556, 0.17435893416404724, 0.17434266209602356, 0.17443759739398956, 0.17455841600894928, 0.1564662903547287, 0.17445267736911774, 0.17448562383651733, 0.17433254420757294, 0.17441336810588837, 0.1563316136598587, 0.21331201493740082, 0.21303321421146393, 0.17450207471847534, 0.1925741583108902, 0.17428649961948395, 0.1741998940706253, 0.1742064654827118, 0.14218059182167053, 0.1578388661146164, 0.158025324344635, 0.15784582495689392, 0.15781956911087036, 0.15774470567703247, 0.19164693355560303, 0.17439398169517517, 0.15756022930145264, 0.19198550283908844, 0.14137902855873108, 0.19220639765262604, 0.17418509721755981, 0.19223926961421967, 0.15731297433376312, 0.19211900234222412, 0.17429059743881226, 0.17430707812309265, 0.14148014783859253, 0.17410840094089508, 0.1572987139225006, 0.17439311742782593, 0.19218572974205017, 0.15726135671138763, 0.19222165644168854, 0.1743355393409729, 0.1573064774274826, 0.14114312827587128, 0.15709452331066132, 0.1924348920583725, 0.1570115089416504, 0.17427866160869598, 0.19305893778800964, 0.15671560168266296, 0.1742570847272873, 0.19284400343894958, 0.17438176274299622, 0.1744353026151657, 0.14010895788669586, 0.1741056591272354, 0.1398838460445404, 0.15451665222644806, 0.15669679641723633, 0.21401430666446686, 0.15756522119045258, 0.19209381937980652, 0.19884271919727325, 0.1769435554742813, 0.19211791455745697, 0.13902857899665833, 0.1534908264875412, 0.2138787806034088, 0.17693762481212616, 0.1393614560365677, 0.13934776186943054, 0.19460716843605042, 0.13910944759845734, 0.1922057867050171, 0.15802478790283203, 0.19719405472278595, 0.1533088982105255, 0.19322606921195984, 0.17274881899356842, 0.15688246488571167, 0.19537438452243805, 0.15450844168663025, 0.19535355269908905, 0.19314391911029816, 0.1756032407283783, 0.1920662522315979, 0.17451608180999756, 0.17391137778759003, 0.21216805279254913, 0.159469336271286, 0.17378702759742737, 0.191738098859787, 0.1418430358171463, 0.17489701509475708, 0.19138866662979126, 0.1561007797718048, 0.15984417498111725, 0.15799051523208618, 0.15625900030136108, 0.19044096767902374, 0.19265837967395782, 0.17360903322696686, 0.15965472161769867, 0.17227227985858917, 0.18978427350521088, 0.17623375356197357, 0.17296527326107025, 0.19288435578346252, 0.1751227229833603, 0.15651872754096985, 0.20681533217430115, 0.17377959191799164, 0.15721715986728668, 0.1605711281299591, 0.16000616550445557, 0.16002041101455688, 0.15975044667720795, 0.19192679226398468, 0.17217452824115753, 0.17641405761241913, 0.1592859923839569, 0.1578444391489029, 0.15894627571105957, 0.14433300495147705, 0.19014392793178558, 0.1896667629480362, 0.17360326647758484, 0.19031266868114471, 0.1907925009727478, 0.1729714721441269, 0.17579011619091034, 0.1882459968328476, 0.20658692717552185, 0.19096039235591888, 0.16028930246829987, 0.17434565722942352, 0.17471912503242493, 0.17401157319545746, 0.1902523934841156, 0.15876583755016327, 0.18911074101924896, 0.16144201159477234, 0.18915748596191406, 0.20164340734481812, 0.17425650358200073, 0.1620144248008728, 0.16128572821617126, 0.18712329864501953, 0.19943322241306305, 0.16185417771339417, 0.17367856204509735, 0.16220413148403168, 0.18585258722305298, 0.1739160418510437, 0.18524782359600067, 0.16253232955932617, 0.18513083457946777, 0.15259559452533722, 0.17456050217151642, 0.18337184190750122, 0.15298837423324585, 0.16397090256214142, 0.1842816025018692, 0.16430775821208954, 0.1850440949201584, 0.17394937574863434, 0.17305466532707214, 0.1740698665380478, 0.1631273329257965, 0.1730758249759674, 0.17430806159973145, 0.1737593710422516, 0.1641644835472107, 0.17306849360466003, 0.15285846590995789, 0.1730775237083435, 0.1524307131767273, 0.1857718974351883, 0.17428220808506012, 0.17305080592632294, 0.17361673712730408, 0.16212564706802368, 0.18588097393512726, 0.19788344204425812, 0.186274453997612, 0.1974598467350006, 0.16309423744678497, 0.15214359760284424, 0.16266196966171265, 0.18512672185897827, 0.16244906187057495, 0.16273389756679535, 0.18477976322174072, 0.16271887719631195, 0.18473851680755615, 0.18500538170337677, 0.17372559010982513, 0.17332419753074646, 0.15269029140472412, 0.17307338118553162, 0.16253700852394104, 0.18412798643112183, 0.15251076221466064, 0.19639694690704346, 0.15240487456321716, 0.18498486280441284, 0.1618669182062149, 0.18457582592964172, 0.17488209903240204, 0.1644042581319809, 0.17425429821014404, 0.16147319972515106, 0.1741725206375122, 0.17487221956253052, 0.18571892380714417, 0.1518830806016922, 0.15174491703510284, 0.16133812069892883, 0.17181938886642456, 0.17480424046516418, 0.184892475605011, 0.1717907041311264, 0.17531782388687134, 0.19899876415729523, 0.1634512096643448, 0.16345666348934174, 0.15982747077941895, 0.16374020278453827, 0.1848444640636444, 0.17144383490085602, 0.16030524671077728, 0.17223341763019562, 0.14960940182209015, 0.18519072234630585, 0.16365350782871246, 0.16304369270801544, 0.16292059421539307, 0.1725180596113205, 0.18692675232887268, 0.20178541541099548, 0.15813462436199188, 0.16325345635414124, 0.15860958397388458, 0.17481368780136108, 0.17170758545398712, 0.17135068774223328, 0.19076915085315704, 0.16061712801456451, 0.16067761182785034, 0.15926013886928558, 0.1748470515012741, 0.1896589994430542, 0.17129790782928467, 0.17059701681137085, 0.19035233557224274, 0.18940886855125427, 0.1594729870557785, 0.16016079485416412, 0.17614971101284027, 0.16067850589752197, 0.18642491102218628, 0.18897685408592224, 0.15813930332660675, 0.19031237065792084, 0.16031773388385773, 0.1607724279165268, 0.189588263630867, 0.17664894461631775, 0.1621127426624298, 0.17498870193958282, 0.17685334384441376, 0.15915748476982117, 0.1704481989145279, 0.16250649094581604, 0.1872865855693817, 0.16303464770317078, 0.17161360383033752, 0.1585252732038498, 0.1469736248254776, 0.19136027991771698, 0.17310132086277008, 0.1607787162065506, 0.1752830594778061, 0.18636250495910645, 0.17213165760040283, 0.18976187705993652, 0.17648768424987793, 0.1460905522108078, 0.18840934336185455, 0.17198923230171204, 0.15868370234966278, 0.17271503806114197, 0.17323538661003113, 0.1763046234846115, 0.15859158337116241, 0.19149760901927948, 0.1749032586812973, 0.18657997250556946, 0.16185800731182098, 0.17708802223205566, 0.15897329151630402, 0.17384247481822968, 0.18637964129447937, 0.20428510010242462, 0.17277689278125763, 0.18757565319538116, 0.15835978090763092, 0.16026270389556885, 0.18888448178768158, 0.147488072514534, 0.16032524406909943, 0.1704963743686676, 0.17281261086463928, 0.1734520047903061, 0.15789267420768738, 0.17330023646354675, 0.17727194726467133, 0.17595809698104858, 0.18525689840316772, 0.1909411996603012, 0.14737457036972046, 0.17635029554367065, 0.17580069601535797, 0.20266014337539673, 0.19014529883861542, 0.20204417407512665, 0.1718667894601822, 0.17224092781543732, 0.1607481837272644, 0.1762543022632599, 0.18912369012832642, 0.17317189276218414, 0.17723515629768372, 0.1749388873577118, 0.19844341278076172, 0.17712131142616272, 0.16244663298130035, 0.16152706742286682, 0.17269928753376007, 0.1867285668849945, 0.1606447547674179, 0.17479470372200012, 0.17328231036663055, 0.19601809978485107, 0.1867186278104782, 0.18497337400913239, 0.19472603499889374, 0.1542992740869522, 0.1546497792005539, 0.16222596168518066, 0.1835276186466217, 0.16481785476207733, 0.1661088764667511, 0.1837148219347, 0.18155233561992645, 0.15528157353401184, 0.175003319978714, 0.17355021834373474, 0.1552533209323883, 0.15509648621082306, 0.17201043665409088, 0.17218860983848572, 0.18231241405010223, 0.1848493218421936, 0.17534570395946503, 0.16318663954734802, 0.17428335547447205, 0.18396009504795074, 0.1721988022327423, 0.18297211825847626, 0.17497728765010834, 0.1845521181821823, 0.16233450174331665, 0.15465518832206726, 0.17571066319942474, 0.16334371268749237, 0.1632615327835083, 0.172749862074852, 0.16426530480384827, 0.16386878490447998, 0.1726684272289276, 0.18231020867824554, 0.17174692451953888, 0.1860101819038391, 0.18523292243480682, 0.1835009604692459, 0.1533956080675125, 0.1534004509449005, 0.17201034724712372, 0.1863480508327484, 0.175261452794075, 0.18231375515460968, 0.1615041196346283, 0.17494623363018036, 0.19541124999523163, 0.1731521338224411, 0.16321969032287598, 0.1713680475950241, 0.1842392534017563, 0.17243291437625885, 0.18181833624839783, 0.15421581268310547, 0.17166535556316376, 0.164270281791687, 0.19406768679618835, 0.19388309121131897, 0.16404390335083008, 0.17664822936058044, 0.15517836809158325, 0.1644749492406845, 0.1738848239183426, 0.17586123943328857, 0.17324307560920715, 0.1767071634531021, 0.1801169216632843, 0.1931043565273285, 0.16349723935127258, 0.17545868456363678, 0.1710817813873291, 0.18525369465351105, 0.17698968946933746, 0.1831914782524109, 0.18491536378860474, 0.1736503690481186, 0.1710355281829834, 0.17157196998596191, 0.16444611549377441, 0.1838359236717224, 0.16434288024902344, 0.18053501844406128, 0.18033696711063385, 0.1730891615152359, 0.15879042446613312, 0.16473953425884247, 0.18180543184280396, 0.16883663833141327, 0.16813725233078003, 0.16855281591415405, 0.1645866334438324, 0.1746247410774231, 0.176596999168396, 0.16469277441501617, 0.18259136378765106, 0.1733281910419464, 0.1819525510072708, 0.18263760209083557, 0.1674789935350418, 0.17518340051174164, 0.17338579893112183, 0.17583052814006805, 0.16670319437980652, 0.15765537321567535, 0.17530614137649536, 0.15725520253181458, 0.15693558752536774, 0.17360034584999084, 0.17313101887702942, 0.19230185449123383, 0.17380142211914062, 0.18318486213684082, 0.15550553798675537, 0.16386105120182037, 0.18343615531921387, 0.1550193578004837, 0.18331024050712585, 0.16403605043888092, 0.16428495943546295, 0.16340956091880798, 0.1539226919412613, 0.15351910889148712, 0.1845974177122116, 0.1735851913690567, 0.16274568438529968, 0.16279399394989014, 0.17381195724010468, 0.1736254245042801, 0.15090589225292206, 0.1618911325931549, 0.17366304993629456, 0.17338679730892181, 0.1492939442396164, 0.17370092868804932, 0.17396171391010284, 0.18768832087516785, 0.17379628121852875, 0.1878616213798523, 0.14755909144878387, 0.1879148632287979, 0.17404793202877045, 0.18804742395877838, 0.1879958063364029, 0.18792884051799774, 0.14745654165744781, 0.18762321770191193, 0.16088160872459412, 0.1743336170911789, 0.1884486824274063, 0.17399545013904572, 0.17449712753295898, 0.20178893208503723, 0.20143906772136688, 0.18702273070812225, 0.14917579293251038, 0.1742233783006668, 0.1742354929447174, 0.18609455227851868, 0.15024808049201965, 0.16225150227546692, 0.16154682636260986, 0.17350097000598907, 0.17399318516254425, 0.18580777943134308, 0.19870872795581818, 0.15070848166942596, 0.17396286129951477, 0.16225799918174744, 0.18542945384979248, 0.17332090437412262, 0.18610267341136932, 0.17355386912822723, 0.1860230416059494, 0.1857214719057083, 0.18552067875862122, 0.1736261397600174, 0.1848006397485733, 0.17370322346687317, 0.1736665964126587, 0.1534893810749054, 0.16334643959999084, 0.15368056297302246, 0.18408940732479095, 0.18393462896347046, 0.1633957475423813, 0.1843477040529251, 0.19465968012809753, 0.17363883554935455, 0.18416263163089752, 0.16351331770420074, 0.17379458248615265, 0.17342017590999603, 0.1738319993019104, 0.17497333884239197, 0.1724088042974472, 0.17240959405899048, 0.17266374826431274, 0.1923510730266571, 0.1649782657623291, 0.1722971796989441, 0.16570889949798584, 0.17493639886379242, 0.16436418890953064, 0.17351776361465454, 0.17230108380317688, 0.18127723038196564, 0.18371720612049103, 0.17478956282138824, 0.17329426109790802, 0.17154854536056519, 0.1732577234506607, 0.16442719101905823, 0.1905164271593094, 0.1722049117088318, 0.17439840734004974, 0.16511200368404388, 0.1747380942106247, 0.16772811114788055, 0.17559629678726196, 0.18266241252422333, 0.16503411531448364, 0.15810605883598328, 0.1724020093679428, 0.17459405958652496, 0.17431609332561493, 0.1671154648065567, 0.17440912127494812, 0.19046175479888916, 0.1831331104040146, 0.18045520782470703, 0.18044672906398773, 0.15795402228832245, 0.17221245169639587, 0.1744362860918045, 0.1722647100687027, 0.16716299951076508, 0.1741793304681778, 0.15826959908008575, 0.15812712907791138, 0.1751977950334549, 0.1803482472896576, 0.17421801388263702, 0.15737128257751465, 0.1907091736793518, 0.17348217964172363, 0.1807948797941208, 0.1824790984392166, 0.1741408109664917, 0.18269921839237213, 0.17253805696964264, 0.1730125993490219, 0.16676388680934906, 0.17432968318462372, 0.1648339182138443, 0.17379839718341827, 0.17443150281906128, 0.17408709228038788, 0.18982592225074768, 0.15805353224277496, 0.17402401566505432, 0.17288726568222046, 0.16544976830482483, 0.18078868091106415, 0.1665034294128418, 0.1652119755744934, 0.15790380537509918, 0.1650228351354599, 0.1824253350496292, 0.16591933369636536, 0.17269405722618103, 0.17406074702739716, 0.18275600671768188, 0.19136525690555573, 0.18249806761741638, 0.1734803318977356, 0.17325842380523682, 0.18253640830516815, 0.1648859977722168, 0.17374198138713837, 0.17334678769111633, 0.17392657697200775, 0.16479890048503876, 0.1821267157793045, 0.16599728167057037, 0.1732601374387741, 0.1733875870704651, 0.17357999086380005, 0.15791629254817963, 0.16599945724010468, 0.17372414469718933, 0.17314140498638153, 0.17339248955249786, 0.1817461997270584, 0.1734497845172882, 0.17321668565273285, 0.17337074875831604, 0.18152357637882233, 0.18214461207389832, 0.17291148006916046, 0.19023355841636658, 0.17400237917900085, 0.1733357310295105, 0.1653868705034256, 0.16525739431381226, 0.17242567241191864, 0.1805180162191391, 0.18008752167224884, 0.17318683862686157, 0.17183202505111694, 0.17248597741127014, 0.17269064486026764, 0.17534136772155762, 0.1715434491634369, 0.17916324734687805, 0.1747860610485077, 0.16589273512363434, 0.16848669946193695, 0.18128570914268494, 0.1680273562669754, 0.1603676974773407, 0.17963582277297974, 0.1719929724931717, 0.1686791330575943, 0.1663110852241516, 0.17388363182544708, 0.16863705217838287, 0.1796545535326004, 0.17828352749347687, 0.1640378087759018, 0.17252834141254425, 0.18820901215076447, 0.16753683984279633, 0.1743212640285492, 0.17226547002792358, 0.16515885293483734, 0.1657448261976242, 0.17860932648181915, 0.15929855406284332, 0.1721111536026001, 0.16900472342967987, 0.17210213840007782, 0.18051396310329437, 0.1585710644721985, 0.17018093168735504, 0.16449984908103943, 0.1793370544910431, 0.18990430235862732, 0.17343547940254211, 0.17253275215625763, 0.18127046525478363, 0.16309905052185059, 0.1767570525407791, 0.1775185465812683, 0.17787513136863708, 0.16815927624702454, 0.16902676224708557, 0.1763153374195099, 0.17631903290748596, 0.1674344539642334, 0.17470625042915344, 0.1719692051410675, 0.1700008660554886, 0.1642490178346634, 0.16433681547641754, 0.17589081823825836, 0.17513951659202576, 0.17602330446243286, 0.1720944494009018, 0.1642885059118271, 0.17555385828018188, 0.17620329558849335, 0.17415252327919006, 0.17202648520469666, 0.18184484541416168, 0.1822763979434967, 0.18491806089878082, 0.17141678929328918, 0.16436371207237244, 0.19087998569011688, 0.1648961454629898, 0.1631220430135727, 0.16737212240695953, 0.17900630831718445, 0.16476018726825714, 0.1757638156414032, 0.17048592865467072, 0.1715521514415741, 0.17019088566303253, 0.18367861211299896, 0.1640978753566742, 0.1640620231628418, 0.1904638409614563, 0.18362608551979065, 0.19015906751155853, 0.17921623587608337, 0.1651841402053833, 0.16327476501464844, 0.1784273236989975, 0.18263034522533417, 0.1648842841386795, 0.17809933423995972, 0.17754077911376953, 0.17552036046981812, 0.17666000127792358, 0.17036326229572296, 0.1734706610441208, 0.16958987712860107, 0.16518014669418335, 0.1696747988462448, 0.17850090563297272, 0.18166129291057587, 0.17064759135246277, 0.16911475360393524, 0.17186424136161804, 0.16836370527744293, 0.1772952675819397, 0.1678546816110611, 0.16989144682884216, 0.1653926819562912, 0.17451028525829315, 0.16953139007091522, 0.16834957897663116, 0.17317993938922882, 0.18117211759090424, 0.1742824912071228, 0.1677500307559967, 0.17282068729400635, 0.16490259766578674, 0.17348416149616241, 0.16226623952388763, 0.18513576686382294, 0.18512427806854248, 0.1640855222940445, 0.16980823874473572, 0.1834728866815567, 0.17089484632015228, 0.1780937761068344, 0.18322160840034485, 0.17017214000225067, 0.17689146101474762, 0.17637726664543152, 0.17687660455703735, 0.17218217253684998, 0.16600219905376434, 0.18145090341567993, 0.1798565685749054, 0.1651412546634674, 0.16349919140338898, 0.180644229054451, 0.18039175868034363, 0.16903257369995117, 0.16824539005756378, 0.17062246799468994, 0.18343926966190338, 0.17000989615917206, 0.17708121240139008, 0.17611916363239288, 0.16679072380065918, 0.17172183096408844, 0.17442168295383453, 0.17985400557518005, 0.18234682083129883, 0.16732154786586761, 0.1802845597267151, 0.16571907699108124, 0.16681183874607086, 0.17356988787651062, 0.17538662254810333, 0.165532648563385, 0.17523548007011414, 0.1667364090681076, 0.17815399169921875, 0.16788217425346375, 0.1692172884941101, 0.17102199792861938, 0.18337862193584442, 0.1787198930978775, 0.17782852053642273, 0.1697208285331726, 0.1683221459388733, 0.17745760083198547, 0.17508172988891602, 0.16575728356838226, 0.1703968048095703, 0.16843543946743011, 0.1757856011390686, 0.1713835597038269, 0.16405130922794342, 0.16310423612594604, 0.1789080947637558, 0.16850870847702026, 0.1652669906616211, 0.17874832451343536, 0.17715586721897125, 0.16113629937171936, 0.1771445870399475, 0.16778704524040222, 0.1778719276189804, 0.17133383452892303, 0.18026185035705566, 0.16663075983524323, 0.18858912587165833, 0.16866378486156464, 0.17088496685028076, 0.17744529247283936, 0.17875497043132782, 0.1705392748117447, 0.168931245803833, 0.18913434445858002, 0.1769396960735321, 0.16403768956661224, 0.1713559776544571, 0.17624159157276154, 0.18253032863140106, 0.17752310633659363, 0.1727869063615799, 0.16412188112735748, 0.17082878947257996, 0.1843356192111969, 0.18373289704322815, 0.1686214655637741, 0.18175776302814484, 0.16608388721942902, 0.16499380767345428, 0.1593264788389206, 0.17001928389072418, 0.1762445718050003, 0.17274923622608185, 0.16559243202209473, 0.16959881782531738, 0.17092201113700867, 0.15832637250423431, 0.1753081977367401, 0.17615610361099243, 0.18378946185112, 0.17052839696407318, 0.179117351770401, 0.1696656197309494, 0.17380310595035553, 0.17158260941505432, 0.1690196692943573, 0.16341450810432434, 0.17598222196102142, 0.17562448978424072, 0.1634645313024521, 0.1717149019241333, 0.1742527335882187, 0.16543707251548767, 0.1849459558725357, 0.1697685569524765, 0.17179302871227264, 0.16563831269741058, 0.16323083639144897, 0.16385899484157562, 0.1711379736661911, 0.18324147164821625, 0.1626027524471283, 0.17571863532066345, 0.18403193354606628, 0.1691354364156723, 0.16225527226924896, 0.16162189841270447, 0.16923579573631287, 0.1568959653377533, 0.15463146567344666, 0.1827733814716339, 0.17084798216819763, 0.16242748498916626, 0.16085563600063324, 0.16195441782474518, 0.16541029512882233, 0.1642564982175827, 0.16682109236717224, 0.15194179117679596, 0.16882812976837158, 0.1510668694972992, 0.177896648645401, 0.160854771733284, 0.18455983698368073, 0.17366689443588257, 0.17487308382987976, 0.19589799642562866, 0.14834323525428772, 0.1559624969959259, 0.14775337278842926, 0.2031266689300537, 0.17737054824829102, 0.19058139622211456, 0.18793244659900665, 0.15691368281841278, 0.1789359450340271, 0.16219216585159302, 0.1651507169008255, 0.17313678562641144, 0.16528528928756714, 0.17600752413272858, 0.18395397067070007, 0.1782458871603012, 0.17115122079849243, 0.1739230751991272, 0.1838182955980301, 0.1647905707359314, 0.16935579478740692, 0.19035646319389343, 0.1469799429178238, 0.16400866210460663, 0.1811072677373886, 0.16026078164577484, 0.16656282544136047, 0.18434561789035797, 0.19105690717697144, 0.1670483499765396, 0.18999117612838745, 0.18159328401088715, 0.16463042795658112, 0.14784330129623413, 0.1889372169971466, 0.14800786972045898, 0.1548023670911789, 0.1724291890859604, 0.2012091726064682, 0.16946949064731598, 0.16824989020824432, 0.1676086038351059, 0.18826420605182648, 0.1503153294324875, 0.16501817107200623, 0.14776606857776642, 0.18237850069999695, 0.15909478068351746, 0.1908993273973465, 0.1842016577720642, 0.17924873530864716, 0.14862410724163055, 0.19505764544010162, 0.1555716097354889, 0.1713671237230301, 0.1772015392780304, 0.14771707355976105, 0.14767515659332275, 0.16909438371658325, 0.18066862225532532, 0.16284304857254028, 0.1854732632637024, 0.16232848167419434, 0.14851316809654236, 0.1701977550983429, 0.1854804903268814, 0.19146332144737244, 0.19072796404361725, 0.19105570018291473, 0.2045670598745346, 0.16188453137874603, 0.15750724077224731, 0.20091739296913147, 0.17099665105342865, 0.16499796509742737, 0.18827739357948303, 0.17307361960411072, 0.1548871546983719, 0.1785876601934433, 0.15114741027355194, 0.14691044390201569, 0.16168378293514252, 0.14986833930015564, 0.16109484434127808, 0.16566364467144012, 0.1863323152065277, 0.17427322268486023, 0.187472403049469, 0.16308429837226868, 0.19669479131698608, 0.18971066176891327, 0.1669882833957672, 0.1812753826379776, 0.1806626170873642, 0.17431038618087769, 0.15900693833827972, 0.1823701113462448, 0.17433281242847443, 0.15737326443195343, 0.18533872067928314, 0.18687108159065247, 0.15855459868907928, 0.14623704552650452, 0.16552408039569855, 0.16396424174308777, 0.16964265704154968, 0.16712279617786407, 0.1719367653131485, 0.16671043634414673, 0.16432951390743256, 0.20601500570774078, 0.17196093499660492, 0.20633111894130707, 0.185134157538414, 0.17245973646640778, 0.17982985079288483, 0.15530186891555786, 0.17109304666519165, 0.18301516771316528, 0.1878584921360016, 0.1898503601551056, 0.17618167400360107, 0.17400844395160675, 0.17501075565814972, 0.17825016379356384, 0.14662982523441315, 0.17197486758232117, 0.1915765255689621, 0.1466980278491974, 0.1890891194343567, 0.1722446084022522, 0.16324801743030548, 0.18687184154987335, 0.19297128915786743, 0.16484808921813965, 0.1518627405166626, 0.20332661271095276, 0.15854810178279877, 0.158065065741539, 0.16868029534816742, 0.17985093593597412, 0.1746077686548233, 0.18221372365951538, 0.18683502078056335, 0.1654263585805893, 0.16914477944374084, 0.17351911962032318, 0.17264503240585327, 0.1771411895751953, 0.15230104327201843, 0.1832871437072754, 0.1654556542634964, 0.16573384404182434, 0.1694328337907791, 0.16226528584957123, 0.1838398575782776, 0.16413778066635132, 0.1590016931295395, 0.18976299464702606, 0.18910828232765198, 0.181400328874588, 0.18752779066562653, 0.1760014444589615, 0.16662853956222534, 0.16409283876419067, 0.15926772356033325, 0.14824038743972778, 0.17145952582359314, 0.17252954840660095, 0.15653133392333984, 0.1846274435520172, 0.15540413558483124, 0.15632477402687073, 0.19956742227077484, 0.19266070425510406, 0.19863657653331757, 0.1840188205242157, 0.18270234763622284, 0.1653682142496109, 0.16955254971981049, 0.169040247797966, 0.190522238612175, 0.1814461648464203, 0.17275361716747284, 0.15658150613307953, 0.17242905497550964, 0.17140795290470123, 0.18363294005393982, 0.18954645097255707, 0.1840059459209442, 0.18525564670562744, 0.18846914172172546, 0.1563287228345871, 0.16882874071598053, 0.16345441341400146, 0.17715193331241608, 0.14863047003746033, 0.14859220385551453, 0.17092296481132507, 0.1618541181087494, 0.16672955453395844, 0.16906046867370605, 0.1611538529396057, 0.19656114280223846, 0.1846645027399063, 0.18176652491092682, 0.16946417093276978, 0.14725713431835175, 0.16851726174354553, 0.18298594653606415, 0.15248055756092072, 0.17508050799369812, 0.20387212932109833, 0.1881510466337204, 0.1466781049966812, 0.182320699095726, 0.16083355247974396, 0.15826597809791565, 0.17975550889968872, 0.17171314358711243, 0.1521672010421753, 0.19341108202934265, 0.16394639015197754, 0.16404390335083008, 0.1688799113035202, 0.14628593623638153, 0.1649891436100006, 0.16574731469154358, 0.16802823543548584, 0.19072335958480835, 0.1595158874988556, 0.18770375847816467, 0.17830492556095123, 0.20574425160884857, 0.1759999841451645, 0.15488864481449127, 0.15518920123577118, 0.19054439663887024, 0.1947421431541443, 0.14555548131465912, 0.19484874606132507, 0.16637249290943146, 0.145649716258049, 0.19055593013763428, 0.161325603723526, 0.16632169485092163, 0.1553478091955185, 0.19363215565681458, 0.16618479788303375, 0.19666363298892975, 0.19681376218795776, 0.14584150910377502, 0.2047383338212967, 0.19292563199996948, 0.1543574035167694, 0.19260884821414948, 0.1629817932844162, 0.17836904525756836, 0.1713152527809143, 0.15414978563785553, 0.17839327454566956, 0.17444345355033875, 0.175178661942482, 0.15909025073051453, 0.16947884857654572, 0.1935015767812729, 0.2026347666978836, 0.163730189204216, 0.1700330674648285, 0.19092310965061188, 0.19295932352542877, 0.17417503893375397, 0.18130990862846375, 0.20077700912952423, 0.17618250846862793, 0.1770028918981552, 0.1572490781545639, 0.16029766201972961, 0.1822117269039154, 0.16919885575771332, 0.16641156375408173, 0.16602353751659393, 0.18145118653774261, 0.16477449238300323, 0.1776031106710434, 0.190996453166008, 0.1508878767490387, 0.1682717353105545, 0.1914295107126236, 0.18850699067115784, 0.18096809089183807, 0.17246392369270325, 0.17652671039104462, 0.1512201875448227, 0.19789990782737732, 0.16532514989376068, 0.16752538084983826, 0.1514490246772766, 0.15136674046516418, 0.1636594980955124, 0.17682486772537231, 0.18096068501472473, 0.1576748639345169, 0.16698144376277924, 0.16725654900074005, 0.18286339938640594, 0.16767618060112, 0.16848349571228027, 0.17919954657554626, 0.156963050365448, 0.17492081224918365, 0.17479154467582703, 0.16226014494895935, 0.16031339764595032, 0.16235201060771942, 0.16749435663223267, 0.1740536093711853, 0.16844147443771362, 0.1685098111629486, 0.19050081074237823, 0.1715240627527237, 0.18830354511737823, 0.18149098753929138, 0.19148540496826172, 0.19414280354976654, 0.14862783253192902, 0.18079638481140137, 0.17423592507839203, 0.17188699543476105, 0.18032079935073853, 0.16984963417053223, 0.16286751627922058, 0.15859916806221008, 0.15902721881866455, 0.17841680347919464, 0.14891043305397034, 0.1486826092004776, 0.17681579291820526, 0.15613524615764618, 0.16618718206882477, 0.2023288458585739, 0.1790287345647812, 0.16427266597747803, 0.202593594789505, 0.17687678337097168, 0.18990910053253174, 0.20235997438430786, 0.20213215053081512, 0.16087758541107178, 0.1693911850452423, 0.16721555590629578, 0.16868682205677032, 0.19090548157691956, 0.1661619246006012, 0.15616677701473236, 0.16680845618247986, 0.1695399433374405, 0.17426586151123047, 0.1623867005109787, 0.14975972473621368, 0.17179584503173828, 0.1557493805885315, 0.16967585682868958, 0.18765424191951752, 0.1664048731327057, 0.16726306080818176, 0.15902818739414215, 0.17477765679359436, 0.1605565994977951, 0.16629855334758759, 0.18881264328956604, 0.1713213175535202, 0.1589803248643875, 0.17738722264766693, 0.16688111424446106, 0.16718690097332, 0.15518194437026978, 0.15476952493190765, 0.1922886222600937, 0.16714318096637726, 0.1869044452905655, 0.1928107887506485, 0.1736365109682083, 0.16711601614952087, 0.1758480817079544, 0.17479726672172546, 0.18453051149845123, 0.1811835765838623, 0.14855994284152985, 0.16571274399757385, 0.19466115534305573, 0.20146243274211884, 0.20122459530830383, 0.16103865206241608, 0.15657933056354523, 0.1899542510509491, 0.1800103634595871, 0.16636325418949127, 0.18449531495571136, 0.17123158276081085, 0.1945180594921112, 0.1498146802186966, 0.1845400184392929, 0.16960006952285767, 0.19972772896289825, 0.18390437960624695, 0.1905684620141983, 0.17128844559192657, 0.1670084446668625, 0.17152969539165497, 0.16165278851985931, 0.19832096993923187, 0.1786969155073166, 0.1581641137599945, 0.15144531428813934, 0.18088649213314056, 0.16172143816947937, 0.18998734652996063, 0.16429471969604492, 0.16379112005233765, 0.18353205919265747, 0.18940432369709015, 0.16387403011322021, 0.19142287969589233, 0.1830064058303833, 0.15967655181884766, 0.1664586216211319, 0.17269940674304962, 0.16213110089302063, 0.16027264297008514, 0.1971919983625412, 0.1649959236383438, 0.15872403979301453, 0.1754072606563568, 0.17053470015525818, 0.1786508858203888, 0.1609015017747879, 0.166203111410141, 0.16254383325576782, 0.19743308424949646, 0.16420280933380127, 0.17263709008693695, 0.19739596545696259, 0.1973048597574234, 0.17903555929660797, 0.1670941710472107, 0.17327840626239777, 0.18091364204883575, 0.19630448520183563, 0.18006040155887604, 0.1825755089521408, 0.16638033092021942, 0.17342795431613922, 0.17880794405937195, 0.19442729651927948, 0.1719677746295929, 0.16766604781150818, 0.17690248787403107, 0.18750350177288055, 0.17126867175102234, 0.1858726292848587, 0.17144279181957245, 0.169453963637352, 0.1714678853750229, 0.15643048286437988, 0.17136284708976746, 0.15653793513774872, 0.15904971957206726, 0.18736310303211212, 0.16675160825252533, 0.17500945925712585, 0.1632109433412552, 0.179713636636734, 0.1790485680103302, 0.16675497591495514, 0.1885223388671875, 0.18320149183273315, 0.15592972934246063, 0.17945413291454315, 0.1719585359096527, 0.16341662406921387, 0.1715809851884842, 0.17996032536029816, 0.19278956949710846, 0.18063132464885712, 0.16373522579669952, 0.18023620545864105, 0.1772124171257019, 0.17143800854682922, 0.17853261530399323, 0.18818624317646027, 0.16777858138084412, 0.1721498966217041, 0.1770985722541809, 0.16210691630840302, 0.16703389585018158, 0.15598677098751068, 0.16659225523471832, 0.16214367747306824, 0.1783750355243683, 0.18620961904525757, 0.18744848668575287, 0.1876271665096283, 0.16635344922542572, 0.16643387079238892, 0.1860724538564682, 0.16792437434196472, 0.18546932935714722, 0.16996490955352783, 0.1769614815711975, 0.18642017245292664, 0.17231646180152893, 0.17603875696659088, 0.17891915142536163, 0.16692885756492615, 0.17156647145748138, 0.16389092803001404, 0.17540721595287323, 0.18330536782741547, 0.17564691603183746, 0.1859501600265503, 0.1905757337808609, 0.17253080010414124, 0.18106724321842194, 0.15805116295814514, 0.18523260951042175, 0.1614130139350891, 0.177219957113266, 0.16720335185527802, 0.183145672082901, 0.16341295838356018, 0.17554552853107452, 0.17709769308567047, 0.18036438524723053, 0.16330528259277344, 0.18325379490852356, 0.1675635576248169, 0.1880379617214203, 0.1640181839466095, 0.1723700612783432, 0.17209698259830475, 0.1851026713848114, 0.16032202541828156]\n",
            "Val loss 0.17325025777172276\n",
            "Val auc roc 0.5\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f38ed563ebf146159b1ff5d905f82311",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2090.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1728\n",
            "Train Losses : [0.18709175288677216, 0.16316697001457214, 0.17655248939990997, 0.16996800899505615, 0.16962307691574097, 0.17461319267749786, 0.18140561878681183, 0.17409734427928925, 0.17574302852153778, 0.17375202476978302, 0.16804750263690948, 0.18117906153202057, 0.1728481501340866, 0.1869407296180725, 0.17773215472698212, 0.1734323352575302, 0.184251606464386, 0.16770850121974945, 0.17045670747756958, 0.17202796041965485, 0.16062255203723907, 0.16538788378238678, 0.17079059779644012, 0.17900387942790985, 0.16060467064380646, 0.1727375090122223, 0.1708369255065918, 0.16899575293064117, 0.16935871541500092, 0.16867968440055847, 0.1643734723329544, 0.18403105437755585, 0.17488466203212738, 0.18649296462535858, 0.18123510479927063, 0.1618323177099228, 0.17279702425003052, 0.16856200993061066, 0.16888967156410217, 0.17819616198539734, 0.18693296611309052, 0.16698290407657623, 0.16131208837032318, 0.16361412405967712, 0.16918163001537323, 0.16486161947250366, 0.1741926670074463, 0.17667090892791748, 0.17838652431964874, 0.16701017320156097, 0.1740906536579132, 0.16431322693824768, 0.17161925137043, 0.16343945264816284, 0.16471213102340698, 0.18323417007923126, 0.1869761198759079, 0.16453680396080017, 0.17243309319019318, 0.1628018617630005, 0.16043902933597565, 0.16804209351539612, 0.16037648916244507, 0.16685597598552704, 0.16032792627811432, 0.18717746436595917, 0.17103311419487, 0.17164954543113708, 0.18724463880062103, 0.16395775973796844, 0.1657150536775589, 0.17941652238368988, 0.17851196229457855, 0.16021384298801422, 0.16575777530670166, 0.18513166904449463, 0.1711830347776413, 0.17677822709083557, 0.1637810319662094, 0.167438805103302, 0.17733769118785858, 0.17110684514045715, 0.17341022193431854, 0.1680023968219757, 0.16966211795806885, 0.17005851864814758, 0.1696225255727768, 0.1671684831380844, 0.1676509976387024, 0.18749253451824188, 0.17159470915794373, 0.18358473479747772, 0.1739935278892517, 0.16621212661266327, 0.17859646677970886, 0.16951778531074524, 0.18488401174545288, 0.16864310204982758, 0.17857788503170013, 0.16329427063465118, 0.16756446659564972, 0.165944442152977, 0.18035626411437988, 0.16866780817508698, 0.17352072894573212, 0.16476333141326904, 0.16427376866340637, 0.1822839379310608, 0.17075754702091217, 0.1795973777770996, 0.1819099634885788, 0.17552030086517334, 0.1876719892024994, 0.16903646290302277, 0.16412188112735748, 0.1804041713476181, 0.17567996680736542, 0.1637919396162033, 0.17077985405921936, 0.17370793223381042, 0.16795530915260315, 0.17309342324733734, 0.1780458390712738, 0.18110427260398865, 0.16525986790657043, 0.18061938881874084, 0.17147694528102875, 0.16731911897659302, 0.16776151955127716, 0.17261956632137299, 0.17687241733074188, 0.17070040106773376, 0.1704026460647583, 0.17754527926445007, 0.1808692216873169, 0.17053356766700745, 0.18219764530658722, 0.164232075214386, 0.16760894656181335, 0.17424802482128143, 0.18449407815933228, 0.16770446300506592, 0.17081333696842194, 0.165279820561409, 0.18474821746349335, 0.18772277235984802, 0.1704503744840622, 0.1763729453086853, 0.18770632147789001, 0.17356881499290466, 0.1627945899963379, 0.17010915279388428, 0.17794938385486603, 0.16714589297771454, 0.17277610301971436, 0.1764468550682068, 0.16797205805778503, 0.17047859728336334, 0.17840951681137085, 0.16153672337532043, 0.1746497005224228, 0.17216257750988007, 0.16828849911689758, 0.16697335243225098, 0.1773054599761963, 0.1801205724477768, 0.16566920280456543, 0.1781032830476761, 0.16942915320396423, 0.16262386739253998, 0.16495004296302795, 0.18477889895439148, 0.18180377781391144, 0.16690045595169067, 0.1739734411239624, 0.1683732569217682, 0.17326220870018005, 0.16222785413265228, 0.17845453321933746, 0.1718139499425888, 0.18035542964935303, 0.1729603111743927, 0.1637001782655716, 0.1877148300409317, 0.17317552864551544, 0.1707056760787964, 0.1731029748916626, 0.16620385646820068, 0.16988220810890198, 0.16980740427970886, 0.17808133363723755, 0.17818714678287506, 0.17623543739318848, 0.183401957154274, 0.184001162648201, 0.17486616969108582, 0.1790272444486618, 0.16649509966373444, 0.18414823710918427, 0.1834896057844162, 0.1598307341337204, 0.17563419044017792, 0.17614395916461945, 0.17920926213264465, 0.16203033924102783, 0.17660178244113922, 0.17896269261837006, 0.18282409012317657, 0.1875808984041214, 0.16381855309009552, 0.17088618874549866, 0.18009896576404572, 0.1813274472951889, 0.17479871213436127, 0.18395540118217468, 0.15999802947044373, 0.179835706949234, 0.18500101566314697, 0.18336045742034912, 0.1622016876935959, 0.1681850701570511, 0.17255328595638275, 0.177203968167305, 0.17397141456604004, 0.17659614980220795, 0.1601121425628662, 0.18737193942070007, 0.17590004205703735, 0.1672576516866684, 0.17619219422340393, 0.17127612233161926, 0.17569231986999512, 0.17406880855560303, 0.16890019178390503, 0.1737985461950302, 0.16016501188278198, 0.16362737119197845, 0.17204022407531738, 0.17633217573165894, 0.1803503781557083, 0.16138407588005066, 0.17760629951953888, 0.162828266620636, 0.17827902734279633, 0.17838488519191742, 0.16938042640686035, 0.16542483866214752, 0.17369431257247925, 0.1702096164226532, 0.18731273710727692, 0.18018868565559387, 0.1873079240322113, 0.17972612380981445, 0.16510693728923798, 0.16020990908145905, 0.1712496429681778, 0.1716526597738266, 0.1830136626958847, 0.18510089814662933, 0.17618988454341888, 0.17545294761657715, 0.17468585073947906, 0.17262539267539978, 0.1856638342142105, 0.1729140430688858, 0.1758236289024353, 0.18711958825588226, 0.17431925237178802, 0.16313117742538452, 0.17155440151691437, 0.1853989064693451, 0.1715284138917923, 0.17362286150455475, 0.17717516422271729, 0.16045339405536652, 0.1620779037475586, 0.18357257544994354, 0.16996155679225922, 0.1683996021747589, 0.17860530316829681, 0.17391365766525269, 0.18376345932483673, 0.16696962714195251, 0.167024627327919, 0.16932769119739532, 0.1794237494468689, 0.18176749348640442, 0.17534954845905304, 0.18343839049339294, 0.17689655721187592, 0.1724168211221695, 0.17044861614704132, 0.1757148951292038, 0.1702560931444168, 0.16588057577610016, 0.16060438752174377, 0.1694565713405609, 0.1731390506029129, 0.172807976603508, 0.16317912936210632, 0.16052976250648499, 0.1713840514421463, 0.1690552532672882, 0.16234932839870453, 0.1706133335828781, 0.16044184565544128, 0.16257904469966888, 0.1657363325357437, 0.1662864238023758, 0.1683451384305954, 0.173105388879776, 0.17348700761795044, 0.16019664704799652, 0.17225082218647003, 0.17902493476867676, 0.1680770218372345, 0.17049425840377808, 0.16714493930339813, 0.17006918787956238, 0.16477642953395844, 0.16000302135944366, 0.17916251718997955, 0.1698366105556488, 0.16773749887943268, 0.1726795732975006, 0.17691361904144287, 0.16906096041202545, 0.16930562257766724, 0.16763584315776825, 0.17679424583911896, 0.1694633960723877, 0.1671454757452011, 0.18008890748023987, 0.16464288532733917, 0.1790257841348648, 0.17074565589427948, 0.15963619947433472, 0.17688393592834473, 0.16841275990009308, 0.1816300004720688, 0.1659121960401535, 0.18319754302501678, 0.1680864691734314, 0.17468692362308502, 0.16827736794948578, 0.18796655535697937, 0.16445370018482208, 0.1781267374753952, 0.1763639897108078, 0.16670756042003632, 0.16310371458530426, 0.17151600122451782, 0.1704205870628357, 0.17011985182762146, 0.16873164474964142, 0.1879841387271881, 0.16861163079738617, 0.16897174715995789, 0.17166072130203247, 0.17847920954227448, 0.17332640290260315, 0.16654227674007416, 0.1787901520729065, 0.16072699427604675, 0.18558712303638458, 0.17795731127262115, 0.16512048244476318, 0.17360959947109222, 0.16511042416095734, 0.1835373044013977, 0.1777602732181549, 0.16114462912082672, 0.15944138169288635, 0.15940718352794647, 0.17168590426445007, 0.16696058213710785, 0.16349662840366364, 0.16672392189502716, 0.1741921752691269, 0.1681860238313675, 0.1685040146112442, 0.17432492971420288, 0.1812390685081482, 0.1716133952140808, 0.15922562777996063, 0.17507657408714294, 0.16918587684631348, 0.17148318886756897, 0.1884627789258957, 0.15913131833076477, 0.18651941418647766, 0.18847864866256714, 0.17624816298484802, 0.1704590767621994, 0.1591086983680725, 0.17086508870124817, 0.1717999428510666, 0.17443400621414185, 0.16681773960590363, 0.1885477751493454, 0.16202056407928467, 0.1885697841644287, 0.1742815226316452, 0.17934200167655945, 0.1657760888338089, 0.1693849116563797, 0.17288541793823242, 0.17606714367866516, 0.17988227307796478, 0.15907900035381317, 0.16566145420074463, 0.16552937030792236, 0.17849944531917572, 0.1729152947664261, 0.16935738921165466, 0.18543681502342224, 0.1705130785703659, 0.18448388576507568, 0.17382140457630157, 0.16401581466197968, 0.17052072286605835, 0.16440004110336304, 0.179380401968956, 0.1805819720029831, 0.16995151340961456, 0.1590239256620407, 0.17121201753616333, 0.17109142243862152, 0.17635521292686462, 0.166175976395607, 0.18308225274085999, 0.18616777658462524, 0.17637261748313904, 0.17126892507076263, 0.15898549556732178, 0.16722874343395233, 0.17398881912231445, 0.1830931156873703, 0.18867211043834686, 0.18364813923835754, 0.18025656044483185, 0.16713829338550568, 0.18054503202438354, 0.18435069918632507, 0.17415419220924377, 0.1660008579492569, 0.18441887199878693, 0.16424544155597687, 0.1693870723247528, 0.17720302939414978, 0.18403847515583038, 0.18402673304080963, 0.17003275454044342, 0.15910695493221283, 0.15911509096622467, 0.17628292739391327, 0.1833375096321106, 0.1613427847623825, 0.1733040064573288, 0.1673261672258377, 0.1715160757303238, 0.18081721663475037, 0.16457659006118774, 0.16318155825138092, 0.1704617589712143, 0.17476652562618256, 0.1885918229818344, 0.16592589020729065, 0.17811737954616547, 0.16617095470428467, 0.18290267884731293, 0.1644587516784668, 0.16582447290420532, 0.18001797795295715, 0.17861506342887878, 0.1729859560728073, 0.1589931696653366, 0.1803513914346695, 0.1642758846282959, 0.18865235149860382, 0.1678227335214615, 0.15900388360023499, 0.1735789030790329, 0.16579340398311615, 0.1701391339302063, 0.18090736865997314, 0.1802043914794922, 0.1589524745941162, 0.1615380495786667, 0.17295442521572113, 0.17466378211975098, 0.18874219059944153, 0.18387505412101746, 0.17124849557876587, 0.1721944957971573, 0.16441991925239563, 0.17513436079025269, 0.17775742709636688, 0.1639881432056427, 0.17850379645824432, 0.1845506727695465, 0.17720094323158264, 0.170152947306633, 0.16748298704624176, 0.17422068119049072, 0.1627478450536728, 0.16797807812690735, 0.17203031480312347, 0.1867285817861557, 0.16910742223262787, 0.1665036529302597, 0.1833062320947647, 0.17637930810451508, 0.1654045581817627, 0.1690371036529541, 0.16723081469535828, 0.17272377014160156, 0.16688527166843414, 0.15892337262630463, 0.16801322996616364, 0.16345453262329102, 0.18876014649868011, 0.17131377756595612, 0.16100116074085236, 0.1724260002374649, 0.17674660682678223, 0.16225102543830872, 0.17378494143486023, 0.17973926663398743, 0.17384758591651917, 0.17791469395160675, 0.15879076719284058, 0.16691124439239502, 0.18593092262744904, 0.17010577023029327, 0.1726475954055786, 0.1712208092212677, 0.17594702541828156, 0.17899002134799957, 0.16645295917987823, 0.17543068528175354, 0.18145175278186798, 0.18326956033706665, 0.17840544879436493, 0.1618524044752121, 0.17346380650997162, 0.1587093472480774, 0.18897920846939087, 0.17163929343223572, 0.17039118707180023, 0.16768215596675873, 0.18069981038570404, 0.17101755738258362, 0.17579197883605957, 0.18010130524635315, 0.16629330813884735, 0.1619901806116104, 0.17974981665611267, 0.17286746203899384, 0.17760644853115082, 0.16216886043548584, 0.17247258126735687, 0.1747482568025589, 0.17946137487888336, 0.16967912018299103, 0.17757384479045868, 0.1759796440601349, 0.1835094690322876, 0.1624927818775177, 0.16681158542633057, 0.17840923368930817, 0.17457745969295502, 0.17710882425308228, 0.1623649299144745, 0.16620364785194397, 0.17983675003051758, 0.17390182614326477, 0.1628313809633255, 0.16806869208812714, 0.1752810925245285, 0.1864597648382187, 0.18139216303825378, 0.16487319767475128, 0.17191563546657562, 0.17926296591758728, 0.18209338188171387, 0.1650984287261963, 0.17483463883399963, 0.17801271378993988, 0.16884249448776245, 0.16621680557727814, 0.17377012968063354, 0.17059901356697083, 0.1752714067697525, 0.17388910055160522, 0.17367136478424072, 0.16417475044727325, 0.16622017323970795, 0.165928915143013, 0.17620402574539185, 0.1585901379585266, 0.17175346612930298, 0.17740723490715027, 0.15852700173854828, 0.189157173037529, 0.16374747455120087, 0.1689489334821701, 0.17190267145633698, 0.17739243805408478, 0.17008258402347565, 0.18485547602176666, 0.1584160327911377, 0.1830371618270874, 0.1776345819234848, 0.16302897036075592, 0.18138235807418823, 0.18089956045150757, 0.1717442125082016, 0.17902380228042603, 0.17402604222297668, 0.1630260944366455, 0.16664311289787292, 0.16144371032714844, 0.18925942480564117, 0.18586522340774536, 0.17769929766654968, 0.17497752606868744, 0.1741584688425064, 0.18919800221920013, 0.1711864322423935, 0.17448745667934418, 0.18537355959415436, 0.1720796525478363, 0.18388235569000244, 0.1768255978822708, 0.16493064165115356, 0.18255357444286346, 0.18486084043979645, 0.16687865555286407, 0.16426101326942444, 0.1705494523048401, 0.18455883860588074, 0.1781567633152008, 0.18105962872505188, 0.1704951822757721, 0.16417014598846436, 0.1783834844827652, 0.16307447850704193, 0.17577669024467468, 0.1676165908575058, 0.16726301610469818, 0.16767334938049316, 0.17320695519447327, 0.15870064496994019, 0.16423849761486053, 0.17976033687591553, 0.16820180416107178, 0.17560841143131256, 0.1774051934480667, 0.16834764182567596, 0.16395503282546997, 0.18557332456111908, 0.1776028424501419, 0.18477369844913483, 0.1769668012857437, 0.1586606800556183, 0.16886034607887268, 0.16345034539699554, 0.1692369133234024, 0.18531271815299988, 0.18903329968452454, 0.18711145222187042, 0.18405012786388397, 0.17734013497829437, 0.1889909952878952, 0.17413868010044098, 0.17380985617637634, 0.18174739181995392, 0.1692221760749817, 0.16026872396469116, 0.17156384885311127, 0.18884938955307007, 0.17171725630760193, 0.17893095314502716, 0.16994857788085938, 0.17515206336975098, 0.16844218969345093, 0.1766148954629898, 0.18277502059936523, 0.18213269114494324, 0.16543734073638916, 0.1689571738243103, 0.17341239750385284, 0.18007534742355347, 0.18259069323539734, 0.17628228664398193, 0.17920823395252228, 0.16541875898838043, 0.17579153180122375, 0.17768941819667816, 0.17901207506656647, 0.17101122438907623, 0.18460847437381744, 0.17619441449642181, 0.16829641163349152, 0.16792096197605133, 0.1645754873752594, 0.17865188419818878, 0.16884659230709076, 0.17980439960956573, 0.16424530744552612, 0.18012607097625732, 0.1805075854063034, 0.17788183689117432, 0.1761033833026886, 0.16231347620487213, 0.1590901017189026, 0.1840001940727234, 0.1780986338853836, 0.16980020701885223, 0.1701711118221283, 0.17358630895614624, 0.17250998318195343, 0.17242850363254547, 0.18135537207126617, 0.17569328844547272, 0.17192304134368896, 0.18276607990264893, 0.18352998793125153, 0.17071276903152466, 0.17842869460582733, 0.16530655324459076, 0.15914969146251678, 0.16742657124996185, 0.1884322613477707, 0.16619740426540375, 0.17545022070407867, 0.17748430371284485, 0.1674206703901291, 0.17162327468395233, 0.1641674041748047, 0.1629868596792221, 0.17248085141181946, 0.16929368674755096, 0.1771193891763687, 0.16158662736415863, 0.1809784471988678, 0.17381852865219116, 0.17943184077739716, 0.18132337927818298, 0.16798080503940582, 0.18485000729560852, 0.16677215695381165, 0.16511063277721405, 0.15912793576717377, 0.17293022572994232, 0.17911207675933838, 0.1640355885028839, 0.15909850597381592, 0.1736229956150055, 0.16771820187568665, 0.16747698187828064, 0.17154204845428467, 0.16694918274879456, 0.1836039423942566, 0.17802686989307404, 0.1818549633026123, 0.16790783405303955, 0.1756933480501175, 0.18353834748268127, 0.1631699502468109, 0.18477900326251984, 0.17562003433704376, 0.1680697351694107, 0.15890903770923615, 0.17092888057231903, 0.16575954854488373, 0.17638377845287323, 0.1671069711446762, 0.17533667385578156, 0.17470866441726685, 0.1609647423028946, 0.18881776928901672, 0.1718427687883377, 0.18073387444019318, 0.18424610793590546, 0.17698954045772552, 0.17850612103939056, 0.18085737526416779, 0.1773880124092102, 0.16814817488193512, 0.17212840914726257, 0.16534380614757538, 0.1747501939535141, 0.16682061553001404, 0.17036151885986328, 0.15885204076766968, 0.16939672827720642, 0.1701333224773407, 0.17624551057815552, 0.17184673249721527, 0.16685354709625244, 0.17360568046569824, 0.17918770015239716, 0.1748240739107132, 0.17949645221233368, 0.17378439009189606, 0.16426238417625427, 0.18009711802005768, 0.17813685536384583, 0.17611297965049744, 0.17703960835933685, 0.18228259682655334, 0.17169953882694244, 0.15882013738155365, 0.16750726103782654, 0.16876502335071564, 0.17903119325637817, 0.18105283379554749, 0.18223387002944946, 0.16678954660892487, 0.1888565719127655, 0.16489921510219574, 0.18121092021465302, 0.15882518887519836, 0.17993712425231934, 0.18212898075580597, 0.17699818313121796, 0.17896975576877594, 0.17677490413188934, 0.168182834982872, 0.18452104926109314, 0.17606937885284424, 0.18296854197978973, 0.16916562616825104, 0.1663924604654312, 0.16324537992477417, 0.17083118855953217, 0.1650116741657257, 0.18562030792236328, 0.16224101185798645, 0.17531509697437286, 0.17431758344173431, 0.182420551776886, 0.16856542229652405, 0.16829675436019897, 0.17759644985198975, 0.16372789442539215, 0.17881332337856293, 0.1589370220899582, 0.18100489675998688, 0.1589221954345703, 0.17734146118164062, 0.17504215240478516, 0.17461194097995758, 0.17122362554073334, 0.16565430164337158, 0.16339173913002014, 0.17990219593048096, 0.17604753375053406, 0.17348450422286987, 0.1764700710773468, 0.17082646489143372, 0.18174047768115997, 0.17758731544017792, 0.1588553637266159, 0.18879148364067078, 0.1694691926240921, 0.16643112897872925, 0.18879231810569763, 0.1653762310743332, 0.1766434907913208, 0.16321806609630585, 0.16412986814975739, 0.1724231094121933, 0.162038654088974, 0.17923305928707123, 0.17628099024295807, 0.16341139376163483, 0.1687609702348709, 0.17767347395420074, 0.16577371954917908, 0.1763034462928772, 0.16982652246952057, 0.18303224444389343, 0.16921919584274292, 0.18883685767650604, 0.17371788620948792, 0.17756247520446777, 0.16880521178245544, 0.1760154664516449, 0.17904521524906158, 0.17645283043384552, 0.17289038002490997, 0.1651574671268463, 0.17153599858283997, 0.1761949360370636, 0.17925713956356049, 0.17222817242145538, 0.17795847356319427, 0.16837310791015625, 0.16404850780963898, 0.17828209698200226, 0.16903989017009735, 0.18880169093608856, 0.17146655917167664, 0.17120060324668884, 0.18271444737911224, 0.17148594558238983, 0.16458430886268616, 0.18040511012077332, 0.16727787256240845, 0.17578035593032837, 0.1670752316713333, 0.1772012859582901, 0.17194919288158417, 0.171443909406662, 0.16947686672210693, 0.17492590844631195, 0.16984708607196808, 0.16582965850830078, 0.17829151451587677, 0.1658918559551239, 0.16857975721359253, 0.17670787870883942, 0.18369008600711823, 0.174835667014122, 0.18879757821559906, 0.16547851264476776, 0.16706982254981995, 0.1620427817106247, 0.16142083704471588, 0.16879762709140778, 0.1643451303243637, 0.16917204856872559, 0.1684725135564804, 0.16579845547676086, 0.17269957065582275, 0.16477586328983307, 0.16789992153644562, 0.16893792152404785, 0.16247211396694183, 0.1638779491186142, 0.17208129167556763, 0.16798627376556396, 0.1766614317893982, 0.17760464549064636, 0.18410323560237885, 0.1670016050338745, 0.1759636253118515, 0.15868689119815826, 0.16834278404712677, 0.17624466121196747, 0.16558097302913666, 0.18396548926830292, 0.16102097928524017, 0.18900570273399353, 0.16608373820781708, 0.1654430627822876, 0.1835198849439621, 0.17146337032318115, 0.16842268407344818, 0.18066459894180298, 0.17448361217975616, 0.17983689904212952, 0.1737675666809082, 0.1684260368347168, 0.17596405744552612, 0.17017614841461182, 0.17114613950252533, 0.1691911816596985, 0.16796082258224487, 0.17505908012390137, 0.18900232017040253, 0.16939786076545715, 0.16180075705051422, 0.18433283269405365, 0.17082639038562775, 0.1586853563785553, 0.17467153072357178, 0.17222294211387634, 0.158682182431221, 0.18901300430297852, 0.15866652131080627, 0.18705187737941742, 0.16968409717082977, 0.16274629533290863, 0.16881927847862244, 0.16195379197597504, 0.16888262331485748, 0.16299231350421906, 0.17380833625793457, 0.16788959503173828, 0.16162410378456116, 0.16279412806034088, 0.17713415622711182, 0.17048218846321106, 0.15856704115867615, 0.1709764450788498, 0.1585204303264618, 0.16567163169384003, 0.16232548654079437, 0.17002230882644653, 0.1823141723871231, 0.16542547941207886, 0.17856696248054504, 0.1653725951910019, 0.18928726017475128, 0.18548312783241272, 0.1627901941537857, 0.18141919374465942, 0.16845852136611938, 0.1674622893333435, 0.18103067576885223, 0.169533371925354, 0.18367008864879608, 0.1615464836359024, 0.18190930783748627, 0.17036375403404236, 0.1892964094877243, 0.16402609646320343, 0.15839135646820068, 0.16762986779212952, 0.1807122826576233, 0.15841205418109894, 0.16559238731861115, 0.16920028626918793, 0.1712612509727478, 0.18265219032764435, 0.17727211117744446, 0.18001000583171844, 0.17191152274608612, 0.1769416630268097, 0.15835785865783691, 0.17694339156150818, 0.16641515493392944, 0.17127808928489685, 0.17318114638328552, 0.17063584923744202, 0.17606253921985626, 0.16340504586696625, 0.17181451618671417, 0.18338361382484436, 0.17908427119255066, 0.17586562037467957, 0.17409442365169525, 0.16732272505760193, 0.18176159262657166, 0.1582687497138977, 0.17479801177978516, 0.1782797873020172, 0.16216157376766205, 0.17157387733459473, 0.17284856736660004, 0.16298459470272064, 0.16400232911109924, 0.1770048439502716, 0.1806327998638153, 0.18956339359283447, 0.1767093539237976, 0.1768800914287567, 0.17620860040187836, 0.1776733100414276, 0.16728736460208893, 0.17905108630657196, 0.17947877943515778, 0.15823671221733093, 0.17783379554748535, 0.16976813971996307, 0.1728389859199524, 0.17849667370319366, 0.17972198128700256, 0.17684222757816315, 0.17288662493228912, 0.16176460683345795, 0.17990492284297943, 0.17148736119270325, 0.17530478537082672, 0.18410110473632812, 0.1721837967634201, 0.16767223179340363, 0.17907677590847015, 0.17984330654144287, 0.15833818912506104, 0.1893974095582962, 0.18309636414051056, 0.16998812556266785, 0.15837432444095612, 0.175320565700531, 0.1583666205406189, 0.16955170035362244, 0.1733100563287735, 0.1774023026227951, 0.17797015607357025, 0.16892489790916443, 0.16702347993850708, 0.16702485084533691, 0.1751083880662918, 0.1758260726928711, 0.16162273287773132, 0.1780024617910385, 0.18045184016227722, 0.1803274005651474, 0.1643250733613968, 0.1686919927597046, 0.18265551328659058, 0.16981728374958038, 0.16702041029930115, 0.18502424657344818, 0.16473884880542755, 0.17151248455047607, 0.16944898664951324, 0.16268911957740784, 0.16973663866519928, 0.17736540734767914, 0.18011653423309326, 0.17606255412101746, 0.16589082777500153, 0.17206940054893494, 0.17806901037693024, 0.17876318097114563, 0.16204582154750824, 0.17651429772377014, 0.1660735011100769, 0.17401963472366333, 0.18251770734786987, 0.16783297061920166, 0.16823740303516388, 0.15823613107204437, 0.15821248292922974, 0.18209196627140045, 0.1748538315296173, 0.1783590018749237, 0.17181824147701263, 0.1895531564950943, 0.17569679021835327, 0.16104918718338013, 0.1867855042219162, 0.18955036997795105, 0.15818770229816437, 0.17837412655353546, 0.1618778556585312, 0.1807803213596344, 0.1676306575536728, 0.16555961966514587, 0.18953783810138702, 0.18018706142902374, 0.17163236439228058, 0.170829638838768, 0.16243094205856323, 0.1620548665523529, 0.1582418531179428, 0.1645708531141281, 0.17497792840003967, 0.1665744036436081, 0.18950603902339935, 0.17339377105236053, 0.15822041034698486, 0.18542711436748505, 0.17006167769432068, 0.18050405383110046, 0.16079777479171753, 0.17699715495109558, 0.18955890834331512, 0.1668010950088501, 0.16852952539920807, 0.1683541238307953, 0.18953987956047058, 0.17362165451049805, 0.166781485080719, 0.18283745646476746, 0.1648811399936676, 0.174112930893898, 0.17773161828517914, 0.18512952327728271, 0.17713747918605804, 0.17573823034763336, 0.17560721933841705, 0.16415336728096008, 0.17067572474479675, 0.1747979372739792, 0.17522312700748444, 0.16354414820671082, 0.16933853924274445, 0.16401943564414978, 0.17060621082782745, 0.16402140259742737, 0.18030788004398346, 0.16916701197624207, 0.17961233854293823, 0.17527958750724792, 0.1702372431755066, 0.16120661795139313, 0.18953900039196014, 0.16244329512119293, 0.17547111213207245, 0.18570953607559204, 0.17350494861602783, 0.15816378593444824, 0.16276566684246063, 0.1772952526807785, 0.17636355757713318, 0.1663675159215927, 0.17976076900959015, 0.17690537869930267, 0.1839546263217926, 0.17476600408554077, 0.15814752876758575, 0.17866642773151398, 0.16686993837356567, 0.17568889260292053, 0.17106691002845764, 0.15814566612243652, 0.1866902858018875, 0.17356833815574646, 0.16499628126621246, 0.17616696655750275, 0.17681477963924408, 0.18961244821548462, 0.1749681830406189, 0.1775379627943039, 0.16836629807949066, 0.18629173934459686, 0.17725305259227753, 0.17202429473400116, 0.16512897610664368, 0.18432600796222687, 0.18960817158222198, 0.1895659863948822, 0.16382187604904175, 0.1612347960472107, 0.16946855187416077, 0.17843745648860931, 0.17213751375675201, 0.1671835035085678, 0.16889357566833496, 0.15820235013961792, 0.17333295941352844, 0.18080680072307587, 0.17877498269081116, 0.17026320099830627, 0.18952372670173645, 0.18651725351810455, 0.17244191467761993, 0.17358870804309845, 0.18953824043273926, 0.18176762759685516, 0.16969788074493408, 0.18949994444847107, 0.17391476035118103, 0.1648285835981369, 0.18948718905448914, 0.1663910299539566, 0.1807919293642044, 0.16304264962673187, 0.16973713040351868, 0.17664436995983124, 0.17275570333003998, 0.1831514537334442, 0.1678062081336975, 0.1764252781867981, 0.17558138072490692, 0.17157261073589325, 0.1760307103395462, 0.1759810447692871, 0.15835481882095337, 0.17210251092910767, 0.17154242098331451, 0.17843349277973175, 0.1866607517004013, 0.17191512882709503, 0.18937169015407562, 0.16782937943935394, 0.15833941102027893, 0.15836934745311737, 0.17836728692054749, 0.17111973464488983, 0.18934762477874756, 0.15835513174533844, 0.16880430281162262, 0.17171166837215424, 0.18428029119968414, 0.17983469367027283, 0.17634741961956024, 0.15835317969322205, 0.16799862682819366, 0.16951623558998108, 0.17491689324378967, 0.18319985270500183, 0.17352688312530518, 0.17083589732646942, 0.16902706027030945, 0.16707384586334229, 0.1677170991897583, 0.17408181726932526, 0.16213150322437286, 0.1637522429227829, 0.17581628262996674, 0.16536349058151245, 0.1791498064994812, 0.1678539216518402, 0.17792999744415283, 0.18346619606018066, 0.1821228563785553, 0.17477703094482422, 0.1626547873020172, 0.17577601969242096, 0.16489483416080475, 0.17428959906101227, 0.16279514133930206, 0.18449939787387848, 0.185997873544693, 0.16730742156505585, 0.16829058527946472, 0.17485560476779938, 0.16760820150375366, 0.15828199684619904, 0.17401796579360962, 0.16860955953598022, 0.1860022395849228, 0.18450003862380981, 0.1627146452665329, 0.16737601161003113, 0.16568005084991455, 0.15827803313732147, 0.1582687497138977, 0.1722760945558548, 0.17341648042201996, 0.16856881976127625, 0.17259986698627472, 0.16578099131584167, 0.17294007539749146, 0.18104243278503418, 0.1675773561000824, 0.1716308891773224, 0.16914993524551392, 0.18953166902065277, 0.17593659460544586, 0.16319702565670013, 0.1779414862394333, 0.1641303151845932, 0.171485036611557, 0.1700684130191803, 0.17623339593410492, 0.16836467385292053, 0.17891116440296173, 0.17406894266605377, 0.15816761553287506, 0.1810777485370636, 0.16225320100784302, 0.18960054218769073, 0.17111189663410187, 0.1763530671596527, 0.1686587780714035, 0.17448720335960388, 0.1852608472108841, 0.17420677840709686, 0.18405383825302124, 0.18088017404079437, 0.15815706551074982, 0.16541115939617157, 0.17281055450439453, 0.16413389146327972, 0.1821792870759964, 0.179190993309021, 0.16747206449508667, 0.16317611932754517, 0.16461101174354553, 0.1615810990333557, 0.18960121273994446, 0.16771133244037628, 0.1581614464521408, 0.18317905068397522, 0.1612555831670761, 0.1798379272222519, 0.17653436958789825, 0.18432168662548065, 0.1621795892715454, 0.15812359750270844, 0.17241008579730988, 0.18240949511528015, 0.17028029263019562, 0.17524133622646332, 0.1704520434141159, 0.1774151474237442, 0.17688320577144623, 0.18667834997177124, 0.16254791617393494, 0.17045332491397858, 0.16914986073970795, 0.17078596353530884, 0.1687498241662979, 0.16488218307495117, 0.1762133091688156, 0.16388145089149475, 0.17192210257053375, 0.17937414348125458, 0.1694614440202713, 0.17579729855060577, 0.1842624992132187, 0.16685260832309723, 0.184334859251976, 0.1580498367547989, 0.1634150594472885, 0.17943674325942993, 0.18125975131988525, 0.17948615550994873, 0.16969671845436096, 0.17577706277370453, 0.17760559916496277, 0.1837068498134613, 0.17522649466991425, 0.16820159554481506, 0.16913220286369324, 0.16934174299240112, 0.17863623797893524, 0.16808807849884033, 0.1748027354478836, 0.18465614318847656, 0.1675930768251419, 0.17341135442256927, 0.15806718170642853, 0.17927458882331848, 0.17995727062225342, 0.16483807563781738, 0.1657516360282898, 0.16747447848320007, 0.18296393752098083, 0.17185521125793457, 0.16638876497745514, 0.18093878030776978, 0.16454733908176422, 0.17487604916095734, 0.16944418847560883, 0.18421797454357147, 0.17559315264225006, 0.16679921746253967, 0.18306787312030792, 0.1669577956199646, 0.18225277960300446, 0.18635690212249756, 0.17746633291244507, 0.16709913313388824, 0.17502738535404205, 0.1844174563884735, 0.16081202030181885, 0.1749412566423416, 0.169687882065773, 0.18187661468982697, 0.15808174014091492, 0.15805992484092712, 0.18097946047782898, 0.18062365055084229, 0.170232892036438, 0.18065974116325378, 0.17696307599544525, 0.17239250242710114, 0.16371475160121918, 0.18155528604984283, 0.17073982954025269, 0.18130478262901306, 0.18963311612606049, 0.1602228879928589, 0.16777555644512177, 0.17097708582878113, 0.17126920819282532, 0.1775743067264557, 0.17435114085674286, 0.15814289450645447, 0.16432932019233704, 0.1788707673549652, 0.16580702364444733, 0.17382609844207764, 0.16574105620384216, 0.1730094850063324, 0.16735489666461945, 0.17654021084308624, 0.16889087855815887, 0.16914582252502441, 0.15807604789733887, 0.1896897405385971, 0.1678573340177536, 0.15806083381175995, 0.17827032506465912, 0.1713574081659317, 0.178477481007576, 0.17028255760669708, 0.16999924182891846, 0.16320422291755676, 0.1644093096256256, 0.1896882951259613, 0.16811928153038025, 0.18107753992080688, 0.18533319234848022, 0.16921338438987732, 0.17564402520656586, 0.18971769511699677, 0.17559903860092163, 0.16524526476860046, 0.18576593697071075, 0.16161136329174042, 0.17665213346481323, 0.1793377548456192, 0.17965167760849, 0.16968724131584167, 0.17268843948841095, 0.17750638723373413, 0.18040451407432556, 0.17056919634342194, 0.16955825686454773, 0.172735333442688, 0.15810725092887878, 0.17029471695423126, 0.17000162601470947, 0.15808989107608795, 0.16666056215763092, 0.17867127060890198, 0.17322085797786713, 0.16932092607021332, 0.17737215757369995, 0.17957213521003723, 0.17126783728599548, 0.1746678203344345, 0.17018970847129822, 0.17293474078178406, 0.17473384737968445, 0.17042532563209534, 0.17874422669410706, 0.16847334802150726, 0.1779422163963318, 0.17555592954158783, 0.16546806693077087, 0.16638727486133575, 0.168633371591568, 0.1668783724308014, 0.17238067090511322, 0.16815254092216492, 0.17360803484916687, 0.17180794477462769, 0.17595292627811432, 0.17930415272712708, 0.1696022003889084, 0.1679757982492447, 0.1708899885416031, 0.17528067529201508, 0.16203400492668152, 0.15801818668842316, 0.18973201513290405, 0.161944180727005, 0.17030487954616547, 0.15800684690475464, 0.18977011740207672, 0.16300569474697113, 0.18104493618011475, 0.17572081089019775, 0.17602618038654327, 0.17678898572921753, 0.1736636757850647, 0.17779357731342316, 0.16666139662265778, 0.1834833025932312, 0.15800830721855164, 0.17303426563739777, 0.17461156845092773, 0.16217610239982605, 0.17714335024356842, 0.1667250245809555, 0.17427368462085724, 0.18980301916599274, 0.16808746755123138, 0.1847062110900879, 0.17272064089775085, 0.1684870421886444, 0.171043261885643, 0.16568602621555328, 0.16221404075622559, 0.18419060111045837, 0.18307913839817047, 0.17979268729686737, 0.16615641117095947, 0.16979625821113586, 0.16675323247909546, 0.18471893668174744, 0.16296426951885223, 0.16955380141735077, 0.17037412524223328, 0.17630068957805634, 0.16183049976825714, 0.16928012669086456, 0.1743716299533844, 0.16789452731609344, 0.17340251803398132, 0.16789239645004272, 0.1631041169166565, 0.1780942976474762, 0.17799216508865356, 0.1683724820613861, 0.15799036622047424, 0.16025421023368835, 0.17671754956245422, 0.1653694361448288, 0.157958522439003, 0.18239188194274902, 0.17810633778572083, 0.1826564073562622, 0.1743537038564682, 0.16540628671646118, 0.16707909107208252, 0.18983067572116852, 0.17593109607696533, 0.16617418825626373, 0.16988085210323334, 0.1653614193201065, 0.1834724247455597, 0.17454668879508972, 0.15791459381580353, 0.16696414351463318, 0.16751685738563538, 0.17480918765068054, 0.1747504621744156, 0.1898760199546814, 0.17223666608333588, 0.18700462579727173, 0.17640560865402222, 0.17949558794498444, 0.18343929946422577, 0.1782357096672058, 0.1699097752571106, 0.18259908258914948, 0.16983000934123993, 0.1760544776916504, 0.16884659230709076, 0.164079949259758, 0.16847974061965942, 0.16452151536941528, 0.16666732728481293, 0.17391172051429749, 0.1732015311717987, 0.16498777270317078, 0.1765190064907074, 0.16664999723434448, 0.17191562056541443, 0.17782753705978394, 0.1699308604001999, 0.17723293602466583, 0.16580750048160553, 0.15787255764007568, 0.16940657794475555, 0.1625416874885559, 0.17505775392055511, 0.1899440884590149, 0.16902410984039307, 0.17948338389396667, 0.18992415070533752, 0.1578606516122818, 0.15782864391803741, 0.1708609163761139, 0.17219200730323792, 0.16462154686450958, 0.16947875916957855, 0.18207797408103943, 0.17547878623008728, 0.18287555873394012, 0.16283729672431946, 0.1767113208770752, 0.17306293547153473, 0.1676877737045288, 0.18059034645557404, 0.16270306706428528, 0.15782195329666138, 0.17776021361351013, 0.1727486550807953, 0.16815058887004852, 0.18998709321022034, 0.1696639508008957, 0.1723279356956482, 0.1899707317352295, 0.1661229282617569, 0.16881316900253296, 0.1780318319797516, 0.15782193839550018, 0.16168823838233948, 0.16266292333602905, 0.1842014193534851, 0.17286936938762665, 0.17252004146575928, 0.17187991738319397, 0.16194775700569153, 0.17302195727825165, 0.1899542361497879, 0.17610977590084076, 0.1854867786169052, 0.16797511279582977, 0.15783795714378357, 0.1713116616010666, 0.17242369055747986, 0.17580321431159973, 0.1899758279323578, 0.1714622676372528, 0.17167071998119354, 0.18151317536830902, 0.1857813149690628, 0.17931541800498962, 0.1708938479423523, 0.1832001656293869, 0.1739477664232254, 0.1784605234861374, 0.1759004443883896, 0.16438357532024384, 0.1756742298603058, 0.1687651425600052, 0.16336987912654877, 0.16636474430561066, 0.17350713908672333, 0.17908945679664612, 0.16483385860919952, 0.1770852506160736, 0.18993595242500305, 0.17160886526107788, 0.1663128137588501, 0.17947672307491302, 0.18109117448329926, 0.1768033653497696, 0.17942267656326294, 0.1677461564540863, 0.18067891895771027, 0.1728641837835312, 0.168424591422081, 0.17534035444259644, 0.1842324584722519, 0.1718505322933197, 0.15789154171943665, 0.173081636428833, 0.17900200188159943, 0.18095716834068298, 0.17099446058273315, 0.184335395693779, 0.1763433814048767, 0.17670154571533203, 0.18308404088020325, 0.16667167842388153, 0.17473500967025757, 0.15789717435836792, 0.16234727203845978, 0.17248481512069702, 0.1669681817293167, 0.1781199872493744, 0.17586763203144073, 0.17006056010723114, 0.1766306459903717, 0.18216952681541443, 0.1683836132287979, 0.16294099390506744, 0.16881470382213593, 0.17808715999126434, 0.18990880250930786, 0.18474406003952026, 0.1728544682264328, 0.16094474494457245, 0.17774698138237, 0.17081844806671143, 0.1776973158121109, 0.18357433378696442, 0.16941474378108978, 0.17086391150951385, 0.17967356741428375, 0.17701345682144165, 0.15791086852550507, 0.182955801486969, 0.16132552921772003, 0.17892557382583618, 0.18985380232334137, 0.16667337715625763, 0.16145329177379608, 0.15790432691574097, 0.1633148342370987, 0.16769933700561523, 0.17973896861076355, 0.1898442506790161, 0.17169883847236633, 0.18473251163959503, 0.157931387424469, 0.16060446202754974, 0.16430993378162384, 0.1710820347070694, 0.17118944227695465, 0.16975624859333038, 0.16569654643535614, 0.1898411065340042, 0.1704244762659073, 0.16231976449489594, 0.1795058250427246, 0.1635631024837494, 0.17322543263435364, 0.17619259655475616, 0.17831550538539886, 0.17728681862354279, 0.17224334180355072, 0.18984608352184296, 0.16701921820640564, 0.17917127907276154, 0.173639178276062, 0.17593975365161896, 0.1713399738073349, 0.17316386103630066, 0.17522485554218292, 0.16998536884784698, 0.16549113392829895, 0.17402257025241852, 0.17054741084575653, 0.17862579226493835, 0.16463029384613037, 0.17459455132484436, 0.15793025493621826, 0.16597628593444824, 0.16435526311397552, 0.17449426651000977, 0.17678529024124146, 0.18030160665512085, 0.17148445546627045, 0.18315142393112183, 0.18113106489181519, 0.1841014176607132, 0.18671457469463348, 0.17532286047935486, 0.17975258827209473, 0.1776982694864273, 0.18427447974681854, 0.15794935822486877, 0.1813170611858368, 0.15794064104557037, 0.1733814775943756, 0.18371616303920746, 0.1723487377166748, 0.17831191420555115, 0.1781090945005417, 0.18982884287834167, 0.15794804692268372, 0.1686355173587799, 0.1744006723165512, 0.18061526119709015, 0.1627587080001831, 0.16699855029582977, 0.17913958430290222, 0.1732649803161621, 0.17777195572853088, 0.17871496081352234, 0.18981684744358063, 0.16877342760562897, 0.16568535566329956, 0.1656031459569931, 0.17010946571826935, 0.1579635590314865, 0.17653681337833405, 0.1865042746067047, 0.18982073664665222, 0.17176800966262817, 0.18068698048591614, 0.18249531090259552, 0.17041462659835815, 0.1699393391609192, 0.1579550951719284, 0.1621188223361969, 0.1762409806251526, 0.17633876204490662, 0.17560121417045593, 0.16206499934196472, 0.16840282082557678, 0.1675969809293747, 0.15794900059700012, 0.18979354202747345, 0.16542059183120728, 0.1764998733997345, 0.15794077515602112, 0.173980250954628, 0.17974041402339935, 0.1704607605934143, 0.1654752939939499, 0.15793384611606598, 0.17016206681728363, 0.18109381198883057, 0.16793310642242432, 0.16792255640029907, 0.16643807291984558, 0.17551067471504211, 0.1579218953847885, 0.18591462075710297, 0.16421611607074738, 0.16624556481838226, 0.18430474400520325, 0.17134499549865723, 0.16947858035564423, 0.17514681816101074, 0.166566401720047, 0.16843785345554352, 0.18248288333415985, 0.1761738359928131, 0.1842726618051529, 0.16690392792224884, 0.15791192650794983, 0.15790802240371704, 0.1752820909023285, 0.1638646274805069, 0.18492545187473297, 0.18013164401054382, 0.18986576795578003, 0.16361182928085327, 0.1664220243692398, 0.1731960028409958, 0.1804683804512024, 0.1847195327281952, 0.1681155115365982, 0.16998963057994843, 0.16906946897506714, 0.17598991096019745, 0.17133596539497375, 0.16888855397701263, 0.17843103408813477, 0.18985983729362488, 0.1898857206106186, 0.1791139394044876, 0.17567767202854156, 0.1749507337808609, 0.16247816383838654, 0.1727183610200882, 0.15792794525623322, 0.1782447248697281, 0.17615386843681335, 0.171144038438797, 0.18499931693077087, 0.18984973430633545, 0.17510679364204407, 0.18316875398159027, 0.17760080099105835, 0.182985320687294, 0.17166456580162048, 0.16291311383247375, 0.17985770106315613, 0.17932653427124023, 0.17158347368240356, 0.17229416966438293, 0.18140609562397003, 0.16474337875843048, 0.18305432796478271, 0.16603614389896393, 0.16298994421958923, 0.1788840889930725, 0.16878017783164978, 0.17890313267707825, 0.17950311303138733, 0.18985724449157715, 0.16594432294368744, 0.17711634933948517, 0.16940820217132568, 0.17735746502876282, 0.16185294091701508, 0.17633067071437836, 0.1898360252380371, 0.1669108122587204, 0.17311054468154907, 0.1657002568244934, 0.17540127038955688, 0.16335131227970123, 0.17504721879959106, 0.16791382431983948, 0.1579507738351822, 0.17706753313541412, 0.17859186232089996, 0.1843733787536621, 0.15794984996318817, 0.1898241639137268, 0.16165004670619965, 0.15791921317577362, 0.17962288856506348, 0.16336600482463837, 0.16383236646652222, 0.1677398830652237, 0.17580586671829224, 0.17524249851703644, 0.18340373039245605, 0.15794290602207184, 0.1752772480249405, 0.1579435020685196, 0.1745377480983734, 0.17385932803153992, 0.15791013836860657, 0.1579083502292633, 0.18433237075805664, 0.17368584871292114, 0.1722513884305954, 0.17134405672550201, 0.1623518019914627, 0.18045185506343842, 0.1762717068195343, 0.1811780333518982, 0.17606665194034576, 0.1804874688386917, 0.17895957827568054, 0.17232339084148407, 0.16891862452030182, 0.1708911657333374, 0.16965529322624207, 0.17537930607795715, 0.18074600398540497, 0.17008179426193237, 0.1579236537218094, 0.17652194201946259, 0.1790264993906021, 0.17981821298599243, 0.173204243183136, 0.17084601521492004, 0.16231699287891388, 0.16598834097385406, 0.15790152549743652, 0.1898648887872696, 0.17962676286697388, 0.16995252668857574, 0.17506009340286255, 0.16884076595306396, 0.1811053603887558, 0.1657114028930664, 0.169489324092865, 0.177130326628685, 0.17081815004348755, 0.16630806028842926, 0.1741052269935608, 0.18304583430290222, 0.17440441250801086, 0.16757838428020477, 0.17719420790672302, 0.18098050355911255, 0.18208256363868713, 0.17597614228725433, 0.1679602563381195, 0.16768001019954681, 0.18518424034118652, 0.18401232361793518, 0.18059490621089935, 0.1579270213842392, 0.16977226734161377, 0.17111417651176453, 0.15991152822971344, 0.15793530642986298, 0.178405299782753, 0.17415238916873932, 0.16068057715892792, 0.17779770493507385, 0.17698529362678528, 0.1755329668521881, 0.16646738350391388, 0.18070079386234283, 0.16331376135349274, 0.1676723062992096, 0.175027534365654, 0.18010887503623962, 0.17955677211284637, 0.1843368411064148, 0.1833496391773224, 0.16578984260559082, 0.16268329322338104, 0.17562061548233032, 0.16810481250286102, 0.17646582424640656, 0.16907157003879547, 0.17982324957847595, 0.17579270899295807, 0.17234911024570465, 0.17455700039863586, 0.17862093448638916, 0.17128950357437134, 0.16798412799835205, 0.16886387765407562, 0.18031004071235657, 0.17078693211078644, 0.15793125331401825, 0.17066918313503265, 0.18450671434402466, 0.1608344465494156, 0.17425107955932617, 0.17018617689609528, 0.16880999505519867, 0.16969290375709534, 0.1770228147506714, 0.1678648144006729, 0.16416530311107635, 0.18309102952480316, 0.17156723141670227, 0.1748451292514801, 0.15793278813362122, 0.17141593992710114]\n",
            "Val loss 0.1732689918270634\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a25213-d5b0-4f74-c930-834529a6710c"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}