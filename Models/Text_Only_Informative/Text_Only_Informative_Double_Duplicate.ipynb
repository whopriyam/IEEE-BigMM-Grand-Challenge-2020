{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Only_Informative_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35a5f5760cce4497ada6470e90493fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb0e04a11bf243ad92b9b20beb9f811d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_909b8dbbe8794f0ca9ad0867ee542488",
              "IPY_MODEL_c327333d0fd74d4ebce526267945134c"
            ]
          }
        },
        "cb0e04a11bf243ad92b9b20beb9f811d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "909b8dbbe8794f0ca9ad0867ee542488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b02ef76cc93240abbcb2b916a1cb762e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2465,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2465,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97490bfffc7749149ae806a46dbbb9d8"
          }
        },
        "c327333d0fd74d4ebce526267945134c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_354abf3161e142ddb5752964fbb28e8e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2465/2465 [44:21&lt;00:00,  1.08s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ca5353ef7f841e78de58309423237f8"
          }
        },
        "b02ef76cc93240abbcb2b916a1cb762e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97490bfffc7749149ae806a46dbbb9d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "354abf3161e142ddb5752964fbb28e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ca5353ef7f841e78de58309423237f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6376bc98d52f41ff8d3a309b01dc6d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b1e85c08d2243599b19c91f9e745b93",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_279e3453cc944617bd991f54e493fb12",
              "IPY_MODEL_d657feb7a2664313922f5e2ee825d067"
            ]
          }
        },
        "4b1e85c08d2243599b19c91f9e745b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "279e3453cc944617bd991f54e493fb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_85cf759bc900412296ce4bf85c4e1653",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2465,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2465,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6bb6f915e58c42c0b0dc68d4d590eb4c"
          }
        },
        "d657feb7a2664313922f5e2ee825d067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a94cfaa93b8e447c8b3da700bde4068d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2465/2465 [42:55&lt;00:00,  1.04s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2a53982c92643f5a1b7add4d1cb83aa"
          }
        },
        "85cf759bc900412296ce4bf85c4e1653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6bb6f915e58c42c0b0dc68d4d590eb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94cfaa93b8e447c8b3da700bde4068d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2a53982c92643f5a1b7add4d1cb83aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7e09f47f1594acda7583b0901e04e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6f2fdb7d04e4a2db26913b2a8ae9635",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ea917f8df434c138d9dcaca2bea15a0",
              "IPY_MODEL_b56f57857792453f8148e4e40979ec28"
            ]
          }
        },
        "b6f2fdb7d04e4a2db26913b2a8ae9635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ea917f8df434c138d9dcaca2bea15a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b194c9bb7fa41718eb7f654788ad0f3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2465,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2465,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd7fe069079547caa943e74acd815218"
          }
        },
        "b56f57857792453f8148e4e40979ec28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e58a8f349fd5462caa3857d3e6cb0afe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2465/2465 [43:04&lt;00:00,  1.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8eaf48083bb41498f29a9776829d46c"
          }
        },
        "5b194c9bb7fa41718eb7f654788ad0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd7fe069079547caa943e74acd815218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e58a8f349fd5462caa3857d3e6cb0afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8eaf48083bb41498f29a9776829d46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "835fc571-4d27-47d5-b56d-b7d3330848f7"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/33)\u001b[K\rremote: Counting objects:   6% (2/33)\u001b[K\rremote: Counting objects:   9% (3/33)\u001b[K\rremote: Counting objects:  12% (4/33)\u001b[K\rremote: Counting objects:  15% (5/33)\u001b[K\rremote: Counting objects:  18% (6/33)\u001b[K\rremote: Counting objects:  21% (7/33)\u001b[K\rremote: Counting objects:  24% (8/33)\u001b[K\rremote: Counting objects:  27% (9/33)\u001b[K\rremote: Counting objects:  30% (10/33)\u001b[K\rremote: Counting objects:  33% (11/33)\u001b[K\rremote: Counting objects:  36% (12/33)\u001b[K\rremote: Counting objects:  39% (13/33)\u001b[K\rremote: Counting objects:  42% (14/33)\u001b[K\rremote: Counting objects:  45% (15/33)\u001b[K\rremote: Counting objects:  48% (16/33)\u001b[K\rremote: Counting objects:  51% (17/33)\u001b[K\rremote: Counting objects:  54% (18/33)\u001b[K\rremote: Counting objects:  57% (19/33)\u001b[K\rremote: Counting objects:  60% (20/33)\u001b[K\rremote: Counting objects:  63% (21/33)\u001b[K\rremote: Counting objects:  66% (22/33)\u001b[K\rremote: Counting objects:  69% (23/33)\u001b[K\rremote: Counting objects:  72% (24/33)\u001b[K\rremote: Counting objects:  75% (25/33)\u001b[K\rremote: Counting objects:  78% (26/33)\u001b[K\rremote: Counting objects:  81% (27/33)\u001b[K\rremote: Counting objects:  84% (28/33)\u001b[K\rremote: Counting objects:  87% (29/33)\u001b[K\rremote: Counting objects:  90% (30/33)\u001b[K\rremote: Counting objects:  93% (31/33)\u001b[K\rremote: Counting objects:  96% (32/33)\u001b[K\rremote: Counting objects: 100% (33/33)\u001b[K\rremote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 15.53 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5bcf847-6f18-4951-e749-c49e83acc4aa"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d4a6585-de3e-4e7a-8842-34c947dd1431"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8e269cd8-170c-4d98-8c17-f4f3a6b63580"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "828c1372-0e82-477e-ff01-98a16097809d"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "47e20903-d7ba-4626-8dbf-88c00c271d77"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "54b1ce8f-0c45-486a-ab00-5fe820ea6ea0"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "94b59488-1abd-4150-80af-439e0d2e13a0"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "0fc5ba4a-fe30-4944-9285-4f03e9540bbb"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8d473433-cee4-40a7-b578-da36e97d394f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Comedy legend @ArsenioHall discusses #BillCosb...\n",
              "1    #MeToo movement: Tatas may not renew Suhel Set...\n",
              "2    #MeToo #MeTooIndia Everyday new names....#Alok...\n",
              "3    SC refuses urgent hearing on cases that have c...\n",
              "4    #MeToo movement: SC declines urgent hearing on...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eab69869-327e-4df7-b04e-cf7cca697eee"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:36<00:00, 11062218.61B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d46c55b5-b984-4439-9058-477a4979de11"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 318105.43B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e6f54e52-2016-46d6-f68e-951618dcde80"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30be979f-dae0-41dd-c513-feac9b4d0734"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d2d8e2c-3dfc-41fa-8021-58ee34c8f686"
      },
      "source": [
        "col_name = \"Text_Only_Informative\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 0 # or 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796,
          "referenced_widgets": [
            "35a5f5760cce4497ada6470e90493fb4",
            "cb0e04a11bf243ad92b9b20beb9f811d",
            "909b8dbbe8794f0ca9ad0867ee542488",
            "c327333d0fd74d4ebce526267945134c",
            "b02ef76cc93240abbcb2b916a1cb762e",
            "97490bfffc7749149ae806a46dbbb9d8",
            "354abf3161e142ddb5752964fbb28e8e",
            "9ca5353ef7f841e78de58309423237f8",
            "6376bc98d52f41ff8d3a309b01dc6d7b",
            "4b1e85c08d2243599b19c91f9e745b93",
            "279e3453cc944617bd991f54e493fb12",
            "d657feb7a2664313922f5e2ee825d067",
            "85cf759bc900412296ce4bf85c4e1653",
            "6bb6f915e58c42c0b0dc68d4d590eb4c",
            "a94cfaa93b8e447c8b3da700bde4068d",
            "d2a53982c92643f5a1b7add4d1cb83aa",
            "a7e09f47f1594acda7583b0901e04e67",
            "b6f2fdb7d04e4a2db26913b2a8ae9635",
            "5ea917f8df434c138d9dcaca2bea15a0",
            "b56f57857792453f8148e4e40979ec28",
            "5b194c9bb7fa41718eb7f654788ad0f3",
            "bd7fe069079547caa943e74acd815218",
            "e58a8f349fd5462caa3857d3e6cb0afe",
            "a8eaf48083bb41498f29a9776829d46c"
          ]
        },
        "outputId": "359fd4e2-9bfb-4b78-e783-d70409f67398"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 0. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 9862\n",
            "Old data length : 1596\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 2502\n",
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35a5f5760cce4497ada6470e90493fb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2465.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1736\n",
            "Train Losses : [0.2901975214481354, 0.24697345495224, 0.23549364507198334, 0.2955130338668823, 0.2258475422859192, 0.16137708723545074, 0.21300996840000153, 0.18500493466854095, 0.13587987422943115, 0.08580268919467926, 0.19874106347560883, 0.12354767322540283, 0.23276983201503754, 0.41231030225753784, 0.09595965594053268, 0.045584797859191895, 0.21398049592971802, 0.21733558177947998, 0.20250146090984344, 0.11319774389266968, 0.16013361513614655, 0.1574137955904007, 0.16050724685192108, 0.12192612886428833, 0.16961413621902466, 0.16178877651691437, 0.15191906690597534, 0.20064377784729004, 0.1690170019865036, 0.20889323949813843, 0.18294034898281097, 0.19231963157653809, 0.14504820108413696, 0.2004600614309311, 0.21589577198028564, 0.18301910161972046, 0.21575218439102173, 0.17146669328212738, 0.1844637542963028, 0.1846664696931839, 0.1808416247367859, 0.1724972128868103, 0.16269271075725555, 0.20045211911201477, 0.19413691759109497, 0.1639689952135086, 0.19086788594722748, 0.15268783271312714, 0.17866083979606628, 0.1677670180797577, 0.17755268514156342, 0.1682245135307312, 0.16039349138736725, 0.1461121141910553, 0.1897319108247757, 0.19304582476615906, 0.14161807298660278, 0.1301077902317047, 0.1301507204771042, 0.16465839743614197, 0.16114483773708344, 0.18324600160121918, 0.155486062169075, 0.1001281589269638, 0.09739474952220917, 0.3116336762905121, 0.14504723250865936, 0.19604723155498505, 0.22309066355228424, 0.2788309156894684, 0.2696707546710968, 0.20165719091892242, 0.14182719588279724, 0.13928519189357758, 0.1023913323879242, 0.21766354143619537, 0.08923378586769104, 0.15782438218593597, 0.11064133793115616, 0.23931805789470673, 0.200288787484169, 0.21747350692749023, 0.10711118578910828, 0.20834121108055115, 0.10643211752176285, 0.20211248099803925, 0.1387799233198166, 0.31075286865234375, 0.22382068634033203, 0.1871127337217331, 0.22569048404693604, 0.2164168506860733, 0.13594557344913483, 0.16152890026569366, 0.125733882188797, 0.21442541480064392, 0.3326674699783325, 0.20087198913097382, 0.19338174164295197, 0.12905491888523102, 0.1597411036491394, 0.2573825418949127, 0.21998657286167145, 0.20692072808742523, 0.20879192650318146, 0.222957044839859, 0.15787942707538605, 0.18871954083442688, 0.14649701118469238, 0.19188767671585083, 0.14711764454841614, 0.15195564925670624, 0.17249377071857452, 0.13455437123775482, 0.1334281861782074, 0.22146248817443848, 0.17135527729988098, 0.1406181901693344, 0.17732436954975128, 0.23519212007522583, 0.1549413651227951, 0.1753293126821518, 0.1965789496898651, 0.2613707184791565, 0.1661384105682373, 0.21869532763957977, 0.18230855464935303, 0.14364127814769745, 0.2096366286277771, 0.21961380541324615, 0.22255970537662506, 0.17576724290847778, 0.18203212320804596, 0.14224448800086975, 0.18012216687202454, 0.16263149678707123, 0.16912557184696198, 0.16113056242465973, 0.1442750096321106, 0.20729048550128937, 0.1637570559978485, 0.150291308760643, 0.17263063788414001, 0.17867538332939148, 0.1764974296092987, 0.18229517340660095, 0.1805116981267929, 0.20201215147972107, 0.18190893530845642, 0.17706628143787384, 0.15631164610385895, 0.1608525961637497, 0.1620005965232849, 0.17325322329998016, 0.1373244971036911, 0.15520761907100677, 0.1884995400905609, 0.18816924095153809, 0.18422527611255646, 0.16830343008041382, 0.15303461253643036, 0.17444130778312683, 0.19849304854869843, 0.18675298988819122, 0.203165665268898, 0.1507754921913147, 0.19387073814868927, 0.1790444552898407, 0.17606644332408905, 0.19320961833000183, 0.16595932841300964, 0.19888770580291748, 0.1985008269548416, 0.17593511939048767, 0.1971655786037445, 0.13470180332660675, 0.16451475024223328, 0.17079982161521912, 0.15811055898666382, 0.1501304656267166, 0.21194833517074585, 0.15977050364017487, 0.20159783959388733, 0.16559408605098724, 0.20438313484191895, 0.17750874161720276, 0.16813434660434723, 0.16933000087738037, 0.17084051668643951, 0.1940165013074875, 0.16002367436885834, 0.18158957362174988, 0.18081295490264893, 0.22287680208683014, 0.16879357397556305, 0.14871230721473694, 0.1573021560907364, 0.2075045108795166, 0.156662717461586, 0.1858920156955719, 0.17493604123592377, 0.18295186758041382, 0.15141256153583527, 0.16313086450099945, 0.2256501168012619, 0.17574408650398254, 0.14134098589420319, 0.22433891892433167, 0.18144801259040833, 0.16520297527313232, 0.18886782228946686, 0.16062632203102112, 0.17938856780529022, 0.17680451273918152, 0.15732114017009735, 0.18923319876194, 0.15657182037830353, 0.18484970927238464, 0.1755288988351822, 0.1800224483013153, 0.18737925589084625, 0.1353939026594162, 0.16270755231380463, 0.1837942898273468, 0.15450507402420044, 0.16291725635528564, 0.1951414793729782, 0.17258290946483612, 0.1558617651462555, 0.16719093918800354, 0.16943849623203278, 0.16090929508209229, 0.1880878508090973, 0.1450834721326828, 0.1529998779296875, 0.15142659842967987, 0.13987329602241516, 0.16189835965633392, 0.16800914704799652, 0.12331216037273407, 0.21008944511413574, 0.23114623129367828, 0.22351348400115967, 0.13445715606212616, 0.12964631617069244, 0.20396974682807922, 0.1549626886844635, 0.15500591695308685, 0.13519230484962463, 0.22743572294712067, 0.11738341301679611, 0.19585953652858734, 0.18732348084449768, 0.1852007806301117, 0.11266682296991348, 0.19618509709835052, 0.12353350967168808, 0.16003182530403137, 0.18343496322631836, 0.13241562247276306, 0.17241010069847107, 0.15471330285072327, 0.1591208279132843, 0.15885336697101593, 0.12163405865430832, 0.18233148753643036, 0.1230776458978653, 0.09963119775056839, 0.15467166900634766, 0.2591158151626587, 0.2047044187784195, 0.2962828576564789, 0.1756374090909958, 0.2773253619670868, 0.12974923849105835, 0.10692037642002106, 0.18901483714580536, 0.18101264536380768, 0.14196205139160156, 0.14555612206459045, 0.1254606395959854, 0.21729816496372223, 0.13703182339668274, 0.13403774797916412, 0.17772352695465088, 0.2620832622051239, 0.12377305328845978, 0.21201281249523163, 0.15575794875621796, 0.2116398960351944, 0.11985853314399719, 0.2077653706073761, 0.15802514553070068, 0.1840856969356537, 0.17949983477592468, 0.17683355510234833, 0.24042192101478577, 0.12409041076898575, 0.13356676697731018, 0.23843292891979218, 0.20360903441905975, 0.15034815669059753, 0.17476236820220947, 0.22922450304031372, 0.21901075541973114, 0.13553403317928314, 0.15656678378582, 0.2162436544895172, 0.19465544819831848, 0.17455878853797913, 0.17098893225193024, 0.14767105877399445, 0.19525089859962463, 0.16555263102054596, 0.1695730984210968, 0.13921593129634857, 0.1666613072156906, 0.1534879207611084, 0.19804386794567108, 0.19817613065242767, 0.15493568778038025, 0.18979881703853607, 0.2038772851228714, 0.165738046169281, 0.1521122306585312, 0.17447887361049652, 0.18268132209777832, 0.16165588796138763, 0.19587428867816925, 0.1617622673511505, 0.18310914933681488, 0.1677175611257553, 0.17889238893985748, 0.17373043298721313, 0.18166689574718475, 0.20819534361362457, 0.15688739717006683, 0.17235442996025085, 0.2043841928243637, 0.173782616853714, 0.17921224236488342, 0.15682518482208252, 0.18003033101558685, 0.1694577932357788, 0.18490095436573029, 0.17100539803504944, 0.1572333425283432, 0.16842810809612274, 0.2034090906381607, 0.1587853729724884, 0.17171598970890045, 0.17016570270061493, 0.17474818229675293, 0.14390279352664948, 0.16332104802131653, 0.17383234202861786, 0.19154782593250275, 0.17172804474830627, 0.16593049466609955, 0.2027757614850998, 0.20208121836185455, 0.18983614444732666, 0.143226757645607, 0.1505935937166214, 0.14274194836616516, 0.19843202829360962, 0.1501123160123825, 0.15507209300994873, 0.17368918657302856, 0.19300493597984314, 0.20187361538410187, 0.18794699013233185, 0.15265335142612457, 0.1609797328710556, 0.19224245846271515, 0.16938045620918274, 0.18938207626342773, 0.2185501605272293, 0.14432188868522644, 0.18944913148880005, 0.19637435674667358, 0.19941829144954681, 0.1414070427417755, 0.1745067834854126, 0.14244398474693298, 0.1588892638683319, 0.18849605321884155, 0.1837272346019745, 0.18427503108978271, 0.17447160184383392, 0.1638229638338089, 0.15452270209789276, 0.1875876635313034, 0.159907728433609, 0.16208429634571075, 0.17326019704341888, 0.18400956690311432, 0.21183277666568756, 0.1790999174118042, 0.16134759783744812, 0.16562913358211517, 0.19238565862178802, 0.20632410049438477, 0.15058185160160065, 0.18319866061210632, 0.17785553634166718, 0.1603643000125885, 0.18356651067733765, 0.20467479526996613, 0.18220743536949158, 0.1491139680147171, 0.1623535007238388, 0.1499888300895691, 0.1707746386528015, 0.15911799669265747, 0.16184230148792267, 0.16128477454185486, 0.1830320656299591, 0.1496896594762802, 0.19320908188819885, 0.1431707888841629, 0.19478413462638855, 0.16175523400306702, 0.2119378298521042, 0.19429834187030792, 0.18900780379772186, 0.17042483389377594, 0.1657448261976242, 0.1432609260082245, 0.18358078598976135, 0.15140891075134277, 0.15838374197483063, 0.175816148519516, 0.14160186052322388, 0.16102728247642517, 0.1839285045862198, 0.2095329463481903, 0.1965208798646927, 0.18428656458854675, 0.21620780229568481, 0.17290422320365906, 0.19679845869541168, 0.21040451526641846, 0.1664866805076599, 0.16289225220680237, 0.20465955138206482, 0.1596866399049759, 0.1640380173921585, 0.19933627545833588, 0.17449027299880981, 0.17342977225780487, 0.19383074343204498, 0.15921497344970703, 0.16963057219982147, 0.1900656521320343, 0.1895408183336258, 0.16319765150547028, 0.1505294144153595, 0.1617349535226822, 0.17723983526229858, 0.1629127860069275, 0.16326335072517395, 0.16757139563560486, 0.17552582919597626, 0.1635277420282364, 0.16189466416835785, 0.17171716690063477, 0.16378040611743927, 0.17986351251602173, 0.18095606565475464, 0.17234690487384796, 0.18146491050720215, 0.1958899050951004, 0.17933891713619232, 0.17365306615829468, 0.17675454914569855, 0.16472287476062775, 0.1721401959657669, 0.16791243851184845, 0.15637962520122528, 0.17885717749595642, 0.17381498217582703, 0.17292653024196625, 0.17219272255897522, 0.15773172676563263, 0.16647936403751373, 0.1823560744524002, 0.19317571818828583, 0.19081345200538635, 0.17296253144741058, 0.18844608962535858, 0.16940496861934662, 0.16944481432437897, 0.1680862158536911, 0.16032223403453827, 0.15644927322864532, 0.1518140584230423, 0.19789747893810272, 0.18170741200447083, 0.14984305202960968, 0.20138907432556152, 0.1575937122106552, 0.18432600796222687, 0.16779102385044098, 0.16384108364582062, 0.17425636947155, 0.20786239206790924, 0.17801308631896973, 0.1715579777956009, 0.15626227855682373, 0.19020803272724152, 0.19933366775512695, 0.18768709897994995, 0.17181523144245148, 0.14843949675559998, 0.17472697794437408, 0.17051994800567627, 0.18347598612308502, 0.16798698902130127, 0.20863501727581024, 0.1771010309457779, 0.14425687491893768, 0.18483877182006836, 0.1492367833852768, 0.14723047614097595, 0.18502643704414368, 0.1984565109014511, 0.20636282861232758, 0.14606258273124695, 0.1727290153503418, 0.1805863380432129, 0.18100352585315704, 0.1596945971250534, 0.18789784610271454, 0.1933896541595459, 0.19955754280090332, 0.17219781875610352, 0.1518261730670929, 0.1717069298028946, 0.164171501994133, 0.1835416853427887, 0.18882940709590912, 0.1887417584657669, 0.1727759689092636, 0.153517946600914, 0.18804453313350677, 0.16013650596141815, 0.18607063591480255, 0.17993710935115814, 0.16738024353981018, 0.16141901910305023, 0.1944212019443512, 0.1629103422164917, 0.18010686337947845, 0.1936977505683899, 0.1750270128250122, 0.16117320954799652, 0.17195317149162292, 0.1644877791404724, 0.19132447242736816, 0.16015584766864777, 0.16158629953861237, 0.1686428189277649, 0.18107806146144867, 0.15806944668293, 0.1684863418340683, 0.18010108172893524, 0.18247072398662567, 0.17083315551280975, 0.169393852353096, 0.18406443297863007, 0.177279531955719, 0.16673247516155243, 0.16302263736724854, 0.16014955937862396, 0.16114678978919983, 0.1769038587808609, 0.1646437793970108, 0.18846336007118225, 0.1547534316778183, 0.15408413112163544, 0.1721460074186325, 0.1536642163991928, 0.1685338020324707, 0.18070459365844727, 0.17837531864643097, 0.17352813482284546, 0.1836051195859909, 0.1510792076587677, 0.1661728173494339, 0.17353101074695587, 0.1645984649658203, 0.17998860776424408, 0.20670098066329956, 0.1447642594575882, 0.1794443577528, 0.15718670189380646, 0.18989941477775574, 0.18162359297275543, 0.188625767827034, 0.1445830762386322, 0.14442861080169678, 0.14402607083320618, 0.19132396578788757, 0.17793308198451996, 0.15580786764621735, 0.18360638618469238, 0.19567976891994476, 0.1863408237695694, 0.15816237032413483, 0.18875087797641754, 0.20177437365055084, 0.1902254819869995, 0.1743110865354538, 0.18615184724330902, 0.17059217393398285, 0.16371780633926392, 0.14463923871517181, 0.14442990720272064, 0.20745685696601868, 0.1715812236070633, 0.14407280087471008, 0.17318852245807648, 0.2082408219575882, 0.1752859205007553, 0.157278373837471, 0.1440429538488388, 0.16624991595745087, 0.17130115628242493, 0.17370349168777466, 0.17619550228118896, 0.16000474989414215, 0.1715368777513504, 0.1411007046699524, 0.20034408569335938, 0.18913571536540985, 0.16524019837379456, 0.1959555447101593, 0.16907408833503723, 0.19153285026550293, 0.17692561447620392, 0.17494255304336548, 0.16408300399780273, 0.17925170063972473, 0.20850937068462372, 0.18057984113693237, 0.17004366219043732, 0.19039656221866608, 0.20303809642791748, 0.17299042642116547, 0.17696751654148102, 0.1502831131219864, 0.16511781513690948, 0.17104417085647583, 0.16870321333408356, 0.17092232406139374, 0.1794317066669464, 0.16269727051258087, 0.16994412243366241, 0.16854870319366455, 0.19355162978172302, 0.17568184435367584, 0.18923790752887726, 0.17721706628799438, 0.1829765886068344, 0.1773027777671814, 0.1939944326877594, 0.17679621279239655, 0.17554780840873718, 0.17676709592342377, 0.17301110923290253, 0.17396235466003418, 0.16341105103492737, 0.17961111664772034, 0.18177203834056854, 0.17100736498832703, 0.1841566115617752, 0.17901979386806488, 0.1793966442346573, 0.1814737617969513, 0.1771029680967331, 0.16817720234394073, 0.18610747158527374, 0.1748649626970291, 0.16765816509723663, 0.16939197480678558, 0.1792895346879959, 0.1706860512495041, 0.17070819437503815, 0.17463253438472748, 0.17417556047439575, 0.17059843242168427, 0.17794671654701233, 0.16472148895263672, 0.1804307997226715, 0.16606228053569794, 0.16685748100280762, 0.17697779834270477, 0.17004743218421936, 0.16717825829982758, 0.17901591956615448, 0.1704930067062378, 0.1866065114736557, 0.16064435243606567, 0.1874498426914215, 0.18745237588882446, 0.17414933443069458, 0.1771107167005539, 0.1821233630180359, 0.18498803675174713, 0.177031010389328, 0.16989506781101227, 0.18043012917041779, 0.17119799554347992, 0.16630366444587708, 0.16834013164043427, 0.16613027453422546, 0.17832495272159576, 0.16539591550827026, 0.1730192005634308, 0.17858590185642242, 0.17499440908432007, 0.17276661098003387, 0.1726972460746765, 0.16381828486919403, 0.16342830657958984, 0.16617819666862488, 0.16871574521064758, 0.17376062273979187, 0.16579891741275787, 0.16596508026123047, 0.16767063736915588, 0.1761549413204193, 0.15905311703681946, 0.18814939260482788, 0.1809769570827484, 0.1890871524810791, 0.18740130960941315, 0.15384697914123535, 0.1706693470478058, 0.19168825447559357, 0.17201325297355652, 0.17609308660030365, 0.15888521075248718, 0.17441585659980774, 0.18407312035560608, 0.19598029553890228, 0.17671513557434082, 0.16155457496643066, 0.16464082896709442, 0.1672876924276352, 0.17710015177726746, 0.15371187031269073, 0.16173158586025238, 0.15985587239265442, 0.1591312140226364, 0.1511458158493042, 0.18497724831104279, 0.1611970067024231, 0.2013385146856308, 0.1803244799375534, 0.1479058414697647, 0.1824919730424881, 0.18498454988002777, 0.19213984906673431, 0.14662963151931763, 0.1888929158449173, 0.17315497994422913, 0.1700950413942337, 0.1595497727394104, 0.16251738369464874, 0.17135366797447205, 0.17770534753799438, 0.1601574420928955, 0.1954166740179062, 0.176946222782135, 0.17440518736839294, 0.17817442119121552, 0.1887711137533188, 0.19570131599903107, 0.18910197913646698, 0.17588724195957184, 0.17459717392921448, 0.1622387319803238, 0.15813620388507843, 0.18713614344596863, 0.18634110689163208, 0.158555805683136, 0.18788625299930573, 0.17362259328365326, 0.17145082354545593, 0.16190537810325623, 0.17699943482875824, 0.18822675943374634, 0.18734276294708252, 0.18779882788658142, 0.17459990084171295, 0.16178429126739502, 0.1724589616060257, 0.16222243010997772, 0.17213715612888336, 0.1624314785003662, 0.1737625151872635, 0.17727716267108917, 0.16834445297718048, 0.16305391490459442, 0.17571543157100677, 0.1625281572341919, 0.1606968194246292, 0.15289358794689178, 0.1839490830898285, 0.16963206231594086, 0.17822682857513428, 0.17756864428520203, 0.18493683636188507, 0.15073725581169128, 0.1592097133398056, 0.17708489298820496, 0.16532357037067413, 0.16639026999473572, 0.17456334829330444, 0.18093383312225342, 0.16243134438991547, 0.18561144173145294, 0.1579478532075882, 0.17607298493385315, 0.1598672866821289, 0.1772298812866211, 0.17256711423397064, 0.20545537769794464, 0.1590857356786728, 0.18986091017723083, 0.18562833964824677, 0.14576998353004456, 0.1923852115869522, 0.17665719985961914, 0.19204330444335938, 0.16258105635643005, 0.1731102019548416, 0.17534160614013672, 0.17221421003341675, 0.20255662500858307, 0.1742483526468277, 0.1596541404724121, 0.20066644251346588, 0.1616976112127304, 0.17483267188072205, 0.15062148869037628, 0.1828458607196808, 0.17416881024837494, 0.18679311871528625, 0.16517159342765808, 0.18664929270744324, 0.16255928575992584, 0.17116355895996094, 0.17521798610687256, 0.1761997938156128, 0.16977570950984955, 0.17174364626407623, 0.15346887707710266, 0.17126448452472687, 0.16548241674900055, 0.1629328727722168, 0.16729342937469482, 0.18340106308460236, 0.16173028945922852, 0.16221560537815094, 0.16938653588294983, 0.18714307248592377, 0.16980300843715668, 0.15838763117790222, 0.18992972373962402, 0.1590631604194641, 0.17127300798892975, 0.1619902104139328, 0.19312651455402374, 0.14824116230010986, 0.1878877580165863, 0.15614008903503418, 0.2028108388185501, 0.15804672241210938, 0.16722528636455536, 0.1624489277601242, 0.18011902272701263, 0.16718579828739166, 0.18621204793453217, 0.2026284635066986, 0.1578386276960373, 0.16350387036800385, 0.16349287331104279, 0.19082540273666382, 0.19468872249126434, 0.15034376084804535, 0.19004572927951813, 0.1626042127609253, 0.19767975807189941, 0.18863245844841003, 0.16864491999149323, 0.17117728292942047, 0.16441132128238678, 0.1917746365070343, 0.18298828601837158, 0.16178537905216217, 0.16431395709514618, 0.16164922714233398, 0.16360649466514587, 0.17112882435321808, 0.1641034632921219, 0.15694956481456757, 0.19058477878570557, 0.1624521017074585, 0.15715788304805756, 0.1570667177438736, 0.16345231235027313, 0.16761496663093567, 0.17825908958911896, 0.16056308150291443, 0.1857949048280716, 0.15553772449493408, 0.16629862785339355, 0.18267343938350677, 0.1542329043149948, 0.16994741559028625, 0.18905851244926453, 0.15729673206806183, 0.17803341150283813, 0.1828879863023758, 0.19418932497501373, 0.18772917985916138, 0.16562505066394806, 0.1795126050710678, 0.17667250335216522, 0.16762912273406982, 0.16698364913463593, 0.16169053316116333, 0.18759053945541382, 0.15091702342033386, 0.1802206188440323, 0.16343559324741364, 0.1645655781030655, 0.2001933753490448, 0.15896116197109222, 0.1868496835231781, 0.16095171868801117, 0.19951720535755157, 0.19926708936691284, 0.18599216639995575, 0.17631137371063232, 0.19319678843021393, 0.1951204091310501, 0.17055952548980713, 0.17365513741970062, 0.18409350514411926, 0.18522028625011444, 0.16213226318359375, 0.17558763921260834, 0.15777160227298737, 0.17199598252773285, 0.16323356330394745, 0.1740073561668396, 0.18381114304065704, 0.18163226544857025, 0.18231268227100372, 0.17811639606952667, 0.18077696859836578, 0.17873355746269226, 0.17842069268226624, 0.18330959975719452, 0.1632867306470871, 0.16167239844799042, 0.17545896768569946, 0.1774253100156784, 0.18443860113620758, 0.17692716419696808, 0.18455275893211365, 0.1834130585193634, 0.18180973827838898, 0.17751237750053406, 0.17736700177192688, 0.17329509556293488, 0.1768009215593338, 0.17686304450035095, 0.17681267857551575, 0.17737454175949097, 0.17366176843643188, 0.17255839705467224, 0.1739359050989151, 0.17491981387138367, 0.16964086890220642, 0.17018592357635498, 0.17499612271785736, 0.1755487471818924, 0.16819548606872559, 0.17585715651512146, 0.17646148800849915, 0.1710883378982544, 0.1770855337381363, 0.1690431535243988, 0.17577128112316132, 0.17413575947284698, 0.17862771451473236, 0.174717515707016, 0.17115409672260284, 0.17166033387184143, 0.17209799587726593, 0.17536994814872742, 0.17179986834526062, 0.17632509768009186, 0.1766504943370819, 0.1724308580160141, 0.1736496388912201, 0.1737351417541504, 0.1719626784324646, 0.17921042442321777, 0.17738866806030273, 0.17166569828987122, 0.17515486478805542, 0.1709316372871399, 0.1729363054037094, 0.17187923192977905, 0.1666681468486786, 0.17102186381816864, 0.17887002229690552, 0.18033704161643982, 0.17460370063781738, 0.18185755610466003, 0.17828768491744995, 0.17191632091999054, 0.17318235337734222, 0.16829945147037506, 0.17014341056346893, 0.17786820232868195, 0.17845867574214935, 0.17403832077980042, 0.177283376455307, 0.16817867755889893, 0.16625042259693146, 0.17920073866844177, 0.1753438115119934, 0.17648153007030487, 0.17153781652450562, 0.1808435320854187, 0.17596806585788727, 0.1737627238035202, 0.16924038529396057, 0.17540590465068817, 0.1732511669397354, 0.170503169298172, 0.172616109251976, 0.17169642448425293, 0.1746690273284912, 0.17139457166194916, 0.1716804802417755, 0.17550629377365112, 0.17415526509284973, 0.1752760112285614, 0.17295938730239868, 0.17163507640361786, 0.17074967920780182, 0.17544934153556824, 0.1658683568239212, 0.17492176592350006, 0.1702110916376114, 0.17225465178489685, 0.18039551377296448, 0.1710631102323532, 0.17094136774539948, 0.17070762813091278, 0.17661786079406738, 0.17353226244449615, 0.17017175257205963, 0.17230640351772308, 0.16559600830078125, 0.17224977910518646, 0.1838766485452652, 0.17703115940093994, 0.18277885019779205, 0.17780669033527374, 0.18308553099632263, 0.1735428422689438, 0.17650824785232544, 0.17064563930034637, 0.1685510128736496, 0.16778527200222015, 0.17324933409690857, 0.17947538197040558, 0.1781252920627594, 0.1801450103521347, 0.1743442565202713, 0.1790921837091446, 0.16266556084156036, 0.17450308799743652, 0.17093168199062347, 0.17267856001853943, 0.17422007024288177, 0.17242486774921417, 0.1720312237739563, 0.17449191212654114, 0.17422181367874146, 0.1698259562253952, 0.17467975616455078, 0.17416150867938995, 0.1703634411096573, 0.17117127776145935, 0.1657281368970871, 0.17654286324977875, 0.168838232755661, 0.17015226185321808, 0.17255398631095886, 0.17205913364887238, 0.17123787105083466, 0.17064665257930756, 0.16786037385463715, 0.17339898645877838, 0.17677289247512817, 0.16931478679180145, 0.16620919108390808, 0.1766413152217865, 0.16451086103916168, 0.17225216329097748, 0.18565461039543152, 0.17054356634616852, 0.18808415532112122, 0.18674170970916748, 0.182628333568573, 0.16093742847442627, 0.1759573072195053, 0.1870414912700653, 0.17498323321342468, 0.17222540080547333, 0.16860291361808777, 0.1855672299861908, 0.17454326152801514, 0.17556512355804443, 0.17494609951972961, 0.17110686004161835, 0.17314717173576355, 0.1818275898694992, 0.1832049936056137, 0.1761699914932251, 0.16539925336837769, 0.17866063117980957, 0.17287683486938477, 0.1671772301197052, 0.18202035129070282, 0.17252473533153534, 0.17254896461963654, 0.1787320077419281, 0.17484170198440552, 0.17990513145923615, 0.17144539952278137, 0.17067600786685944, 0.1730344593524933, 0.17720064520835876, 0.16606563329696655, 0.172472283244133, 0.17655514180660248, 0.17602035403251648, 0.17692497372627258, 0.16992807388305664, 0.1709652692079544, 0.1791091114282608, 0.17433103919029236, 0.17338033020496368, 0.17516762018203735, 0.17161402106285095, 0.17470350861549377, 0.1779567003250122, 0.17431367933750153, 0.17812436819076538, 0.17761117219924927, 0.17036747932434082, 0.17112940549850464, 0.17874932289123535, 0.1807388812303543, 0.17672325670719147, 0.177174374461174, 0.17318525910377502, 0.16968877613544464, 0.17003966867923737, 0.17117854952812195, 0.17397525906562805, 0.1741718053817749, 0.17182905972003937, 0.1741974949836731, 0.17339107394218445, 0.17422325909137726, 0.17874453961849213, 0.1771673560142517, 0.17040234804153442, 0.1754503697156906, 0.17415039241313934, 0.17193308472633362, 0.1763317883014679, 0.1739775389432907, 0.17034661769866943, 0.17309600114822388, 0.17149308323860168, 0.17545510828495026, 0.17330898344516754, 0.17556406557559967, 0.1739961802959442, 0.17059020698070526, 0.17393502593040466, 0.17619635164737701, 0.16780363023281097, 0.17225316166877747, 0.17964854836463928, 0.17474834620952606, 0.16866232454776764, 0.17483103275299072, 0.17200279235839844, 0.1780785769224167, 0.17838221788406372, 0.1799626648426056, 0.17127083241939545, 0.17122848331928253, 0.17169669270515442, 0.17710842192173004, 0.17732493579387665, 0.1746416985988617, 0.17675267159938812, 0.17544318735599518, 0.17461837828159332, 0.17808383703231812, 0.17018920183181763, 0.17365948855876923, 0.16927677392959595, 0.17142242193222046, 0.1776372194290161, 0.16946765780448914, 0.17221061885356903, 0.16640540957450867, 0.170961394906044, 0.17967751622200012, 0.18456409871578217, 0.16773545742034912, 0.18652240931987762, 0.17417632043361664, 0.18453583121299744, 0.18002891540527344, 0.17305974662303925, 0.1754264533519745, 0.18008562922477722, 0.17056001722812653, 0.16995565593242645, 0.17101967334747314, 0.17698633670806885, 0.17712020874023438, 0.1800469011068344, 0.17719809710979462, 0.16549141705036163, 0.18119598925113678, 0.176917165517807, 0.1629313826560974, 0.17372822761535645, 0.17693476378917694, 0.1754194051027298, 0.16898466646671295, 0.1750534623861313, 0.17312057316303253, 0.17941579222679138, 0.16484187543392181, 0.1824008673429489, 0.1786065697669983, 0.16165339946746826, 0.17031104862689972, 0.1672607660293579, 0.18674540519714355, 0.17329782247543335, 0.18668018281459808, 0.1694970726966858, 0.16517460346221924, 0.18321651220321655, 0.17311374843120575, 0.1730494499206543, 0.17382977902889252, 0.16982294619083405, 0.17028038203716278, 0.17385146021842957, 0.17643047869205475, 0.1634196639060974, 0.17200560867786407, 0.16657370328903198, 0.1729760318994522, 0.17071227729320526, 0.1801815629005432, 0.17447689175605774, 0.17310959100723267, 0.17762982845306396, 0.18147197365760803, 0.1772703230381012, 0.1790868490934372, 0.17569418251514435, 0.16468480229377747, 0.17187821865081787, 0.17219661176204681, 0.1729964017868042, 0.1661122590303421, 0.17044608294963837, 0.16941235959529877, 0.1782575100660324, 0.16911301016807556, 0.1672697812318802, 0.18142546713352203, 0.1723347306251526, 0.17630502581596375, 0.18094810843467712, 0.16555435955524445, 0.169341579079628, 0.18116404116153717, 0.17985771596431732, 0.1763385683298111, 0.16863682866096497, 0.17861823737621307, 0.17205609381198883, 0.17572207748889923, 0.17945261299610138, 0.17571289837360382, 0.17554453015327454, 0.1718113124370575, 0.16881684958934784, 0.172882080078125, 0.17923566699028015, 0.167705237865448, 0.1858724057674408, 0.17129696905612946, 0.17774249613285065, 0.17254388332366943, 0.17914965748786926, 0.17112064361572266, 0.17511871457099915, 0.17501863837242126, 0.17444036900997162, 0.17754992842674255, 0.17366090416908264, 0.1703622192144394, 0.17173202335834503, 0.1732415109872818, 0.16853533685207367, 0.17017292976379395, 0.17116965353488922, 0.16844911873340607, 0.1688160002231598, 0.17189809679985046, 0.16865849494934082, 0.168096125125885, 0.1735408455133438, 0.17664262652397156, 0.17680193483829498, 0.18010137975215912, 0.18002773821353912, 0.17516246438026428, 0.17225874960422516, 0.1732376515865326, 0.1731259971857071, 0.1709165871143341, 0.1781756579875946, 0.17917092144489288, 0.18197943270206451, 0.17159511148929596, 0.1746024191379547, 0.17221380770206451, 0.17099779844284058, 0.16926735639572144, 0.17572900652885437, 0.1766297072172165, 0.16848690807819366, 0.16955861449241638, 0.17196346819400787, 0.1783188134431839, 0.17545998096466064, 0.17117543518543243, 0.16940636932849884, 0.1750245839357376, 0.16742849349975586, 0.17527291178703308, 0.16850577294826508, 0.18209606409072876, 0.1741582453250885, 0.16936227679252625, 0.17036712169647217, 0.18069729208946228, 0.17523467540740967, 0.16826435923576355, 0.16805176436901093, 0.17769083380699158, 0.16909398138523102, 0.17724545300006866, 0.18102698028087616, 0.16615889966487885, 0.16720758378505707, 0.17640799283981323, 0.1813509613275528, 0.17366230487823486, 0.1690528839826584, 0.18104006350040436, 0.16694200038909912, 0.17421793937683105, 0.17907658219337463, 0.181104838848114, 0.16746672987937927, 0.17285744845867157, 0.17583516240119934, 0.17817150056362152, 0.1658829152584076, 0.1786019206047058, 0.17260640859603882, 0.18555112183094025, 0.18044523894786835, 0.16429896652698517, 0.1713322401046753, 0.17214877903461456, 0.17291055619716644, 0.17676103115081787, 0.17866474390029907, 0.1749068647623062, 0.17037518322467804, 0.17580708861351013, 0.1667845994234085, 0.1733882874250412, 0.1709747016429901, 0.17164070904254913, 0.1792350709438324, 0.17103847861289978, 0.1620769500732422, 0.17216534912586212, 0.17915856838226318, 0.16916243731975555, 0.17393091320991516, 0.1654299944639206, 0.17117008566856384, 0.17089144885540009, 0.18848825991153717, 0.176088348031044, 0.1691964864730835, 0.16975083947181702, 0.1641930639743805, 0.16882973909378052, 0.17185932397842407, 0.19075147807598114, 0.1646302342414856, 0.17004844546318054, 0.16515761613845825, 0.17162327468395233, 0.17662717401981354, 0.16613152623176575, 0.18438400328159332, 0.17895233631134033, 0.1791873276233673, 0.1878238469362259, 0.15953776240348816, 0.1690150499343872, 0.17347100377082825, 0.1868116557598114, 0.17364920675754547, 0.1635013073682785, 0.1831260323524475, 0.1891552060842514, 0.17130334675312042, 0.16014346480369568, 0.16873304545879364, 0.1724063903093338, 0.17186729609966278, 0.15895086526870728, 0.18176518380641937, 0.1757003217935562, 0.180738165974617, 0.17616446316242218, 0.16568110883235931, 0.1712867021560669, 0.17326666414737701, 0.17390473186969757, 0.15729330480098724, 0.17103363573551178, 0.1746353954076767, 0.16497716307640076, 0.16854418814182281, 0.1917337030172348, 0.1564234346151352, 0.15624278783798218, 0.17931102216243744, 0.1719330996274948, 0.18099312484264374, 0.1676481068134308, 0.16390906274318695, 0.17331141233444214, 0.15450279414653778, 0.165970578789711, 0.17401066422462463, 0.15689872205257416, 0.1641264259815216, 0.16709190607070923, 0.18476691842079163, 0.16239185631275177, 0.16693659126758575, 0.184928759932518, 0.1954757124185562, 0.1847672313451767, 0.16269855201244354, 0.16584423184394836, 0.17895358800888062, 0.1545911729335785, 0.2014390379190445, 0.19054293632507324, 0.19316543638706207, 0.1760059893131256, 0.14936687052249908, 0.16191455721855164, 0.1639087051153183, 0.1887255162000656, 0.16233958303928375, 0.18309560418128967, 0.15933367609977722, 0.19185996055603027, 0.17111024260520935, 0.17407332360744476, 0.1491338461637497, 0.18010714650154114, 0.17277836799621582, 0.1621861606836319, 0.17685146629810333, 0.1698317974805832, 0.20179806649684906, 0.1482352316379547, 0.20162414014339447, 0.17002668976783752, 0.15948688983917236, 0.1487017571926117, 0.15711766481399536, 0.17501850426197052, 0.15911327302455902, 0.15891090035438538, 0.15792112052440643, 0.18539094924926758, 0.18277426064014435, 0.14639130234718323, 0.18202438950538635, 0.17655791342258453, 0.15840476751327515, 0.16381338238716125, 0.18844395875930786, 0.1713533252477646, 0.16449256241321564, 0.17311440408229828, 0.17120765149593353, 0.20862461626529694, 0.15894800424575806, 0.1429591178894043, 0.17621612548828125, 0.1674797534942627, 0.16865409910678864, 0.14206790924072266, 0.1793123483657837, 0.17477546632289886, 0.15569983422756195, 0.15966013073921204, 0.1768818199634552, 0.15109321475028992, 0.15891100466251373, 0.1500730812549591, 0.1379455178976059, 0.19849702715873718, 0.17890283465385437, 0.15168185532093048, 0.18900597095489502, 0.16836605966091156, 0.17246294021606445, 0.17362745106220245, 0.2039513736963272, 0.17038480937480927, 0.13445277512073517, 0.1515335589647293, 0.19675716757774353, 0.176687091588974, 0.1337851732969284, 0.14909884333610535, 0.15230727195739746, 0.20957709848880768, 0.19923296570777893, 0.2023477703332901, 0.14947766065597534, 0.22341464459896088, 0.17558786273002625, 0.1524783819913864, 0.156211256980896, 0.17950677871704102, 0.15815648436546326, 0.1497129648923874, 0.15313942730426788, 0.169753298163414, 0.14732706546783447, 0.1523595005273819, 0.18279264867305756, 0.19574639201164246, 0.18244798481464386, 0.1646208018064499, 0.19640164077281952, 0.19272108376026154, 0.19801504909992218, 0.1705806404352188, 0.1582556813955307, 0.17991957068443298, 0.16928182542324066, 0.14857232570648193, 0.18997597694396973, 0.18210047483444214, 0.165115088224411, 0.18384496867656708, 0.2046961784362793, 0.15973952412605286, 0.18358726799488068, 0.16029390692710876, 0.18339039385318756, 0.17989246547222137, 0.17361460626125336, 0.1378987729549408, 0.15594381093978882, 0.1797441989183426, 0.13762617111206055, 0.18087974190711975, 0.19987566769123077, 0.15876822173595428, 0.1665278971195221, 0.16875921189785004, 0.15456801652908325, 0.18957531452178955, 0.14545348286628723, 0.1790662556886673, 0.181425541639328, 0.17685112357139587, 0.1899203509092331, 0.15403616428375244, 0.15121422708034515, 0.15915681421756744, 0.17795804142951965, 0.2191811203956604, 0.1555756777524948, 0.16836726665496826, 0.15274053812026978, 0.14906135201454163, 0.16413742303848267, 0.1817825436592102, 0.13490372896194458, 0.1988678276538849, 0.18293917179107666, 0.19426052272319794, 0.14675332605838776, 0.16259662806987762, 0.19359952211380005, 0.18951457738876343, 0.18476900458335876, 0.15679970383644104, 0.17154324054718018, 0.19769716262817383, 0.1702537089586258, 0.20088131725788116, 0.18215622007846832, 0.17155882716178894, 0.15319235622882843, 0.16901367902755737, 0.13614360988140106, 0.1473294496536255, 0.2182786613702774, 0.19387514889240265, 0.1837458610534668, 0.15733876824378967, 0.17657102644443512, 0.19174955785274506, 0.17931132018566132, 0.1994742602109909, 0.17915520071983337, 0.19343502819538116, 0.19017881155014038, 0.15770922601222992, 0.1840938925743103, 0.16070285439491272, 0.16840197145938873, 0.15777587890625, 0.14159566164016724, 0.17238372564315796, 0.15098413825035095, 0.19312892854213715, 0.15187068283557892, 0.17454034090042114, 0.18989130854606628, 0.1752934455871582, 0.1760062426328659, 0.15677300095558167, 0.19630637764930725, 0.1717555969953537, 0.16003163158893585, 0.192833811044693, 0.15290682017803192, 0.1686856746673584, 0.16010843217372894, 0.19116032123565674, 0.2101457118988037, 0.18585455417633057, 0.1949521154165268, 0.15560382604599, 0.17256194353103638, 0.17511647939682007, 0.17119620740413666, 0.17314453423023224, 0.16932973265647888, 0.15707159042358398, 0.1632729470729828, 0.20500938594341278, 0.1711629182100296, 0.1670038402080536, 0.1646086424589157, 0.1905660480260849, 0.1710965484380722, 0.17602337896823883, 0.1824243813753128, 0.16072571277618408, 0.16099222004413605, 0.14819540083408356, 0.14799760282039642, 0.20245124399662018, 0.16225789487361908, 0.1894102841615677, 0.1704249531030655, 0.16419027745723724, 0.17747856676578522, 0.17060525715351105, 0.1980680525302887, 0.16654632985591888, 0.16533467173576355, 0.2029569298028946, 0.18361122906208038, 0.14769431948661804, 0.16749601066112518, 0.16617876291275024, 0.1579454094171524, 0.16812998056411743, 0.18092495203018188, 0.15268924832344055, 0.16489920020103455, 0.16111738979816437, 0.16356630623340607, 0.1709568202495575, 0.17526690661907196, 0.14566996693611145, 0.1609088033437729, 0.1702197790145874, 0.18381263315677643, 0.16910412907600403, 0.1905735731124878, 0.16375719010829926, 0.19676974415779114, 0.15219271183013916, 0.16450102627277374, 0.15626071393489838, 0.16782255470752716, 0.16138292849063873, 0.17915712296962738, 0.14073003828525543, 0.14013537764549255, 0.14789654314517975, 0.15195713937282562, 0.17408117651939392, 0.19935035705566406, 0.1455777883529663, 0.15916748344898224, 0.18167728185653687, 0.15463590621948242, 0.1664234697818756, 0.17245246469974518, 0.17140457034111023, 0.15918533504009247, 0.17769448459148407, 0.20262251794338226, 0.17270198464393616, 0.1688012331724167, 0.17719562351703644, 0.1712041050195694, 0.15628190338611603, 0.18406450748443604, 0.17820776998996735, 0.1948719024658203, 0.22445645928382874, 0.17042405903339386, 0.1671023964881897, 0.1760055273771286, 0.15423667430877686, 0.19795887172222137, 0.15634751319885254, 0.17802128195762634, 0.1993420422077179, 0.1851450651884079, 0.1730421483516693, 0.17618584632873535, 0.177311971783638, 0.17273113131523132, 0.21643030643463135, 0.21544715762138367, 0.1705137938261032, 0.1562882959842682, 0.16556188464164734, 0.15254001319408417, 0.19730627536773682, 0.17127619683742523, 0.20944026112556458, 0.1667531579732895, 0.14378465712070465, 0.17088176310062408, 0.14450180530548096, 0.1680249571800232, 0.16702061891555786, 0.17818382382392883, 0.20680290460586548, 0.17130930721759796, 0.2020387202501297, 0.17575208842754364, 0.14528504014015198, 0.18411406874656677, 0.15793131291866302, 0.15914185345172882, 0.14523203670978546, 0.19753433763980865, 0.15683089196681976, 0.17842747271060944, 0.20680354535579681, 0.1578194946050644, 0.19240960478782654, 0.16823306679725647, 0.17954252660274506, 0.19582709670066833, 0.17761199176311493, 0.18492670357227325, 0.18289870023727417, 0.1715342402458191, 0.20481513440608978, 0.1624860018491745, 0.15918327867984772, 0.1612371951341629, 0.16004769504070282, 0.19075417518615723, 0.16326867043972015, 0.17978699505329132, 0.16340695321559906, 0.18028844892978668, 0.17619940638542175, 0.14760182797908783, 0.1897110939025879, 0.16592898964881897, 0.17812971770763397, 0.15798527002334595, 0.16539672017097473, 0.18733133375644684, 0.1467715948820114, 0.16688093543052673, 0.1652088761329651, 0.1611364781856537, 0.15932482481002808, 0.17015406489372253, 0.1702841967344284, 0.1648218184709549, 0.1556343287229538, 0.16116803884506226, 0.15840184688568115, 0.19401507079601288, 0.1881248950958252, 0.18507494032382965, 0.20828796923160553, 0.16865278780460358, 0.15461526811122894, 0.1729055941104889, 0.1579323709011078, 0.1437067687511444, 0.17339365184307098, 0.1572381556034088, 0.19074447453022003, 0.19480308890342712, 0.1603969782590866, 0.18116851150989532, 0.14283090829849243, 0.16005276143550873, 0.17562341690063477, 0.16143503785133362, 0.19417889416217804, 0.17302587628364563, 0.16315558552742004, 0.15403926372528076, 0.18553440272808075, 0.1666717529296875, 0.1549210101366043, 0.1721421778202057, 0.1952202469110489, 0.1526753306388855, 0.16969117522239685, 0.15269525349140167, 0.18214461207389832, 0.18224507570266724, 0.1912178248167038, 0.1959894597530365, 0.15383976697921753, 0.1705680787563324, 0.14005839824676514, 0.18707355856895447, 0.18010877072811127, 0.21252086758613586, 0.1699988842010498, 0.17007458209991455, 0.1746872067451477, 0.1872829794883728, 0.18195784091949463, 0.17822027206420898, 0.19697560369968414, 0.17144328355789185, 0.19492207467556, 0.1732560694217682, 0.15505512058734894, 0.16900242865085602, 0.19042664766311646, 0.1914171427488327, 0.19205591082572937, 0.1460169553756714, 0.17540884017944336, 0.16731224954128265, 0.14686238765716553, 0.20322494208812714, 0.14724195003509521, 0.1732044517993927, 0.16473530232906342, 0.15708240866661072, 0.17890562117099762, 0.1748875379562378, 0.15751805901527405, 0.14750295877456665, 0.16728590428829193, 0.16453824937343597, 0.16111747920513153, 0.16653135418891907, 0.17074799537658691, 0.1813223958015442, 0.14550389349460602, 0.16917988657951355, 0.1754092127084732, 0.17329801619052887, 0.14377287030220032, 0.15477171540260315, 0.17932729423046112, 0.18200527131557465, 0.15229134261608124, 0.15689463913440704, 0.17192968726158142, 0.21235384047031403, 0.1971549242734909, 0.18842542171478271, 0.16863152384757996, 0.18906155228614807, 0.17070333659648895, 0.1818544566631317, 0.19349658489227295, 0.19044938683509827, 0.19631820917129517, 0.19563210010528564, 0.19818679988384247, 0.17887629568576813, 0.15745534002780914, 0.18092304468154907, 0.16919264197349548, 0.15590566396713257, 0.16758586466312408, 0.16373080015182495, 0.19384464621543884, 0.17170777916908264, 0.16615353524684906, 0.1615014225244522, 0.15328063070774078, 0.17454800009727478, 0.15714846551418304, 0.17169663310050964, 0.1605091542005539, 0.16256658732891083, 0.16287389397621155, 0.20551158487796783, 0.17853538691997528, 0.1899634301662445, 0.14514070749282837, 0.17164720594882965, 0.1575668454170227, 0.15704594552516937, 0.18441243469715118, 0.17638781666755676, 0.1793672889471054, 0.1804731786251068, 0.15821583569049835, 0.17798057198524475, 0.17412348091602325, 0.17000457644462585, 0.19135843217372894, 0.17121779918670654, 0.15868093073368073, 0.1602630466222763, 0.19337211549282074, 0.18155479431152344, 0.18858438730239868, 0.1883525848388672, 0.15922555327415466, 0.17207038402557373, 0.14450395107269287, 0.17762602865695953, 0.15976768732070923, 0.17602728307247162, 0.18709176778793335, 0.17619477212429047, 0.17002630233764648, 0.1782684326171875, 0.1632745862007141, 0.1590338498353958, 0.15770462155342102, 0.17482532560825348, 0.17486658692359924, 0.19142282009124756, 0.17634496092796326, 0.20766030251979828, 0.1439182311296463, 0.20717677474021912, 0.14423218369483948, 0.18949216604232788, 0.16285991668701172, 0.18570923805236816, 0.17616775631904602, 0.189649298787117, 0.1704472154378891, 0.18528860807418823, 0.17827993631362915, 0.18098679184913635, 0.17323347926139832, 0.18234528601169586, 0.17374339699745178, 0.1719127893447876, 0.1615385264158249, 0.15846236050128937, 0.18178756535053253, 0.19091077148914337, 0.1861131638288498, 0.17570152878761292, 0.1731862723827362, 0.17533807456493378, 0.1786787509918213, 0.1620589792728424, 0.1744760274887085, 0.16019342839717865, 0.19015197455883026, 0.1510261446237564, 0.15852491557598114, 0.16928690671920776, 0.16537699103355408, 0.1723310500383377, 0.17187903821468353, 0.1831129789352417, 0.18582886457443237, 0.17535750567913055, 0.17564812302589417, 0.15023528039455414, 0.1747438609600067, 0.1737021952867508, 0.17419472336769104, 0.1863459199666977, 0.1614445298910141, 0.14972428977489471, 0.1772007793188095, 0.1492745727300644, 0.18254795670509338, 0.20097912847995758, 0.16035740077495575, 0.15901146829128265, 0.17210760712623596, 0.16405245661735535, 0.19005145132541656, 0.2016151398420334, 0.16205163300037384, 0.15587803721427917, 0.17513896524906158, 0.1778595894575119, 0.16008739173412323, 0.17860360443592072, 0.14827679097652435, 0.175751730799675, 0.18933480978012085, 0.14775343239307404, 0.1733844131231308, 0.16008903086185455, 0.1722681075334549, 0.17668557167053223, 0.15729182958602905, 0.1461791694164276, 0.18387481570243835, 0.1454547941684723, 0.16300711035728455, 0.17521144449710846, 0.15811805427074432, 0.17021313309669495, 0.18881604075431824, 0.1666487604379654, 0.1425599306821823, 0.19500991702079773, 0.17917555570602417, 0.15460523962974548, 0.1778329610824585, 0.1764684021472931, 0.1676217019557953, 0.16062946617603302, 0.1889912337064743, 0.15697288513183594, 0.1917945295572281, 0.19315561652183533, 0.15455128252506256, 0.17334647476673126, 0.1579633504152298, 0.19475236535072327, 0.15909044444561005, 0.14053437113761902, 0.17745020985603333, 0.19950801134109497, 0.17201551795005798, 0.1929149478673935, 0.21223308145999908, 0.1703258454799652, 0.16068829596042633, 0.16563522815704346, 0.19015558063983917, 0.15768055617809296, 0.14187727868556976, 0.2100217193365097, 0.15543696284294128, 0.19710250198841095, 0.16053830087184906, 0.20929190516471863, 0.14271335303783417, 0.17294733226299286, 0.17818330228328705, 0.1719072163105011, 0.16183285415172577, 0.1692267209291458, 0.1708240658044815, 0.14310991764068604, 0.17460286617279053, 0.15494689345359802, 0.15772829949855804, 0.15557502210140228, 0.15594062209129333, 0.1580139845609665, 0.17587198317050934, 0.16035814583301544, 0.15442027151584625, 0.17281821370124817, 0.19353093206882477, 0.1754242479801178, 0.17034122347831726, 0.1665225476026535, 0.13844379782676697, 0.19519123435020447, 0.17002974450588226, 0.17343400418758392, 0.19766934216022491, 0.19260476529598236, 0.13797146081924438, 0.1721603125333786, 0.18943893909454346, 0.15921863913536072, 0.20017968118190765, 0.1382531374692917, 0.16048815846443176, 0.1572754681110382, 0.151017963886261, 0.1550147980451584, 0.15582671761512756, 0.15167197585105896, 0.175171360373497, 0.13564938306808472, 0.1596664935350418, 0.15604889392852783, 0.17491818964481354, 0.15276490151882172, 0.19827131927013397, 0.22442421317100525, 0.13170690834522247, 0.17496925592422485, 0.15640096366405487, 0.171144500374794, 0.17723558843135834, 0.17567917704582214, 0.19465598464012146, 0.13029395043849945, 0.15511968731880188, 0.20388811826705933, 0.17832767963409424, 0.17636385560035706, 0.16888973116874695, 0.14718541502952576, 0.18035610020160675, 0.17439711093902588, 0.1702805906534195, 0.1293802708387375, 0.15619590878486633, 0.22906257212162018, 0.17905040085315704, 0.1710159033536911, 0.194301575422287, 0.17196673154830933, 0.17628483474254608, 0.2033827304840088, 0.1736307591199875, 0.1307961642742157, 0.197262242436409, 0.1312537044286728, 0.20329253375530243, 0.154832661151886, 0.20051109790802002, 0.1531107872724533, 0.1752934455871582, 0.22391434013843536, 0.15085862576961517, 0.17721417546272278, 0.22245968878269196, 0.17376470565795898, 0.1753375083208084, 0.20030853152275085, 0.14878825843334198, 0.19585058093070984, 0.20042090117931366, 0.16949254274368286, 0.19592821598052979, 0.13784419000148773, 0.17680826783180237, 0.17159511148929596, 0.1590755581855774, 0.1546204835176468, 0.17914779484272003, 0.15808923542499542, 0.16721989214420319, 0.15333276987075806, 0.17335395514965057, 0.15252389013767242, 0.15511202812194824, 0.15983092784881592, 0.18009276688098907, 0.13817711174488068, 0.19512292742729187, 0.19247671961784363, 0.13746021687984467, 0.19463379681110382, 0.1557825207710266, 0.19456690549850464, 0.15455682575702667, 0.15246830880641937, 0.17858904600143433, 0.15175873041152954, 0.19053588807582855, 0.19658689200878143, 0.16977226734161377, 0.1577698290348053, 0.19954994320869446, 0.17341913282871246, 0.15030260384082794, 0.17329220473766327, 0.1752673089504242, 0.15297609567642212, 0.1481924206018448, 0.21765467524528503, 0.15757034718990326, 0.1526569277048111, 0.1796073168516159, 0.14690424501895905, 0.15672048926353455, 0.15774749219417572, 0.1816781610250473, 0.17821039259433746, 0.13544119894504547, 0.17854297161102295, 0.15635821223258972, 0.19923390448093414, 0.17293958365917206, 0.16796524822711945, 0.13402362167835236, 0.1991630345582962, 0.17771022021770477, 0.17118144035339355, 0.20167745649814606, 0.1446896195411682, 0.20355011522769928, 0.1779877245426178, 0.19427071511745453, 0.1863018423318863, 0.17228133976459503, 0.1721714287996292, 0.15287688374519348, 0.19960491359233856, 0.20506882667541504, 0.15009838342666626, 0.19772838056087494, 0.15092137455940247, 0.17076675593852997, 0.16307644546031952, 0.1580888330936432, 0.18104638159275055, 0.17360782623291016, 0.19040590524673462, 0.14867854118347168, 0.1538439244031906, 0.18246006965637207, 0.19361984729766846, 0.15267318487167358, 0.169363334774971, 0.18893703818321228, 0.1531846821308136, 0.17809391021728516, 0.16200734674930573, 0.20093822479248047, 0.16563774645328522, 0.18374605476856232, 0.15200740098953247, 0.18076081573963165, 0.16133789718151093, 0.1565266102552414, 0.1786547303199768, 0.2138662189245224, 0.16987338662147522, 0.15325133502483368, 0.15066805481910706, 0.16151349246501923, 0.17142713069915771, 0.13922931253910065, 0.18144269287586212, 0.19384105503559113, 0.19411587715148926, 0.18828122317790985, 0.15816284716129303, 0.17078207433223724, 0.18121594190597534, 0.15943551063537598, 0.15482984483242035, 0.1515907496213913, 0.14722029864788055, 0.1650150865316391, 0.1674766093492508, 0.1567736566066742, 0.15511144697666168, 0.18065056204795837, 0.17306716740131378, 0.20287054777145386, 0.15498928725719452, 0.19786621630191803, 0.21635085344314575, 0.1865968257188797, 0.17432264983654022, 0.1797577142715454, 0.19145186245441437, 0.2026960700750351, 0.18438398838043213, 0.18875116109848022, 0.1408616006374359, 0.18007725477218628, 0.1634727567434311, 0.21037301421165466, 0.1810869723558426, 0.18182213604450226, 0.18028151988983154, 0.17461851239204407, 0.20224319398403168, 0.20631957054138184, 0.19316288828849792, 0.17004850506782532, 0.17903460562229156, 0.2032608687877655, 0.16499802470207214, 0.14859074354171753, 0.15437938272953033, 0.17063988745212555, 0.16479773819446564, 0.16994121670722961, 0.14989079535007477, 0.1615877002477646, 0.17234274744987488, 0.15553125739097595, 0.1698993444442749, 0.16644912958145142, 0.1706559956073761, 0.15968109667301178, 0.19044262170791626, 0.1762678623199463, 0.1691187471151352, 0.16283464431762695, 0.16558344662189484, 0.17944253981113434, 0.15637724101543427, 0.16341587901115417, 0.16365493834018707, 0.15431752800941467, 0.18128138780593872, 0.1731942892074585, 0.16452710330486298, 0.16834194958209991, 0.18794821202754974, 0.15482686460018158, 0.18389229476451874, 0.1539832502603531, 0.17102967202663422, 0.16254539787769318, 0.1945246458053589, 0.15798930823802948, 0.16388337314128876, 0.144686758518219, 0.14645399153232574, 0.16823960840702057, 0.16892056167125702, 0.20848308503627777, 0.16143490374088287, 0.17167213559150696, 0.2089306265115738, 0.20406481623649597, 0.174143984913826, 0.19396771490573883, 0.16120384633541107, 0.16194762289524078, 0.16728416085243225, 0.19680103659629822, 0.16948837041854858, 0.17885428667068481, 0.17480874061584473, 0.16618363559246063, 0.16428890824317932, 0.1928572654724121, 0.17469292879104614, 0.15252773463726044, 0.19574174284934998, 0.15082745254039764, 0.17899839580059052, 0.15954677760601044, 0.15474259853363037, 0.18649952113628387, 0.18318068981170654, 0.1969863772392273, 0.17224235832691193, 0.16970978677272797, 0.15232506394386292, 0.15276795625686646, 0.1778954714536667, 0.15978237986564636, 0.1573156714439392, 0.17891642451286316, 0.1588636189699173, 0.1783536672592163, 0.17148372530937195, 0.1548794060945511, 0.16366825997829437, 0.17941313982009888, 0.19608156383037567, 0.15951254963874817, 0.182012677192688, 0.188175767660141, 0.15455205738544464, 0.17096392810344696, 0.1540759950876236, 0.18095296621322632, 0.16294290125370026, 0.19152137637138367, 0.15291254222393036, 0.18994875252246857]\n",
            "Val loss 0.1718484349012375\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6376bc98d52f41ff8d3a309b01dc6d7b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2465.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1727\n",
            "Train Losses : [0.1711106300354004, 0.1923205405473709, 0.17474325001239777, 0.16359050571918488, 0.1680677980184555, 0.17254848778247833, 0.17865025997161865, 0.164555624127388, 0.17659485340118408, 0.16268715262413025, 0.1444600224494934, 0.15361647307872772, 0.16467076539993286, 0.19855667650699615, 0.15364550054073334, 0.1577310711145401, 0.19604207575321198, 0.14308175444602966, 0.19430828094482422, 0.18152177333831787, 0.17244099080562592, 0.1572963446378708, 0.17675063014030457, 0.1876523792743683, 0.1716015487909317, 0.16555236279964447, 0.17820647358894348, 0.16890378296375275, 0.17175820469856262, 0.16620208323001862, 0.17918457090854645, 0.15583543479442596, 0.16040387749671936, 0.14241456985473633, 0.15912193059921265, 0.17351007461547852, 0.1593511998653412, 0.1602795273065567, 0.16081294417381287, 0.1864745169878006, 0.14002282917499542, 0.18296536803245544, 0.13909868896007538, 0.18220554292201996, 0.17239408195018768, 0.1599961817264557, 0.18282124400138855, 0.1969704031944275, 0.16140064597129822, 0.17121531069278717, 0.2182602882385254, 0.17446023225784302, 0.18360526859760284, 0.1953246295452118, 0.21808624267578125, 0.20239542424678802, 0.13696707785129547, 0.17996425926685333, 0.1510177105665207, 0.17548079788684845, 0.13768817484378815, 0.1555504947900772, 0.13754259049892426, 0.1372240036725998, 0.18302907049655914, 0.15300174057483673, 0.17286063730716705, 0.14664146304130554, 0.16326867043972015, 0.13494127988815308, 0.1560104340314865, 0.15280723571777344, 0.17451022565364838, 0.1697613000869751, 0.13221515715122223, 0.1978384405374527, 0.19784267246723175, 0.17754562199115753, 0.1494271159172058, 0.15477555990219116, 0.20443184673786163, 0.19801758229732513, 0.22695741057395935, 0.1836315393447876, 0.15041011571884155, 0.17744594812393188, 0.20010533928871155, 0.1926250010728836, 0.17591272294521332, 0.17482133209705353, 0.18005254864692688, 0.14925560355186462, 0.1607499122619629, 0.1509608030319214, 0.20520900189876556, 0.16991831362247467, 0.1331443041563034, 0.19975224137306213, 0.1534940004348755, 0.17424191534519196, 0.17809124290943146, 0.17058585584163666, 0.1750095933675766, 0.16794267296791077, 0.20218193531036377, 0.17989695072174072, 0.17831306159496307, 0.18155181407928467, 0.2039162665605545, 0.16932721436023712, 0.15720956027507782, 0.1968027949333191, 0.15056291222572327, 0.15286944806575775, 0.19498151540756226, 0.16973315179347992, 0.15956170856952667, 0.19974730908870697, 0.1973935067653656, 0.17229798436164856, 0.18722043931484222, 0.18872930109500885, 0.19933897256851196, 0.17106753587722778, 0.1379736065864563, 0.1710750162601471, 0.15407918393611908, 0.16724562644958496, 0.1386822611093521, 0.1818181276321411, 0.1535160094499588, 0.17280805110931396, 0.21464334428310394, 0.21445003151893616, 0.1930246502161026, 0.1753779798746109, 0.1797703057527542, 0.15343494713306427, 0.14038105309009552, 0.17297565937042236, 0.21174472570419312, 0.19752702116966248, 0.1762305349111557, 0.16921024024486542, 0.1524210423231125, 0.1699274480342865, 0.1535593569278717, 0.1973058134317398, 0.17757314443588257, 0.15922501683235168, 0.15353865921497345, 0.14281386137008667, 0.17485757172107697, 0.1944229006767273, 0.1626131534576416, 0.15928532183170319, 0.19273842871189117, 0.160503089427948, 0.16786615550518036, 0.18334494531154633, 0.167585089802742, 0.177146315574646, 0.16796137392520905, 0.1698044091463089, 0.14195288717746735, 0.14179465174674988, 0.18442057073116302, 0.16550438106060028, 0.17650765180587769, 0.15820258855819702, 0.15069538354873657, 0.17361930012702942, 0.14008967578411102, 0.18165843188762665, 0.2134045660495758, 0.13931071758270264, 0.15533238649368286, 0.17888492345809937, 0.15237592160701752, 0.170487180352211, 0.13813306391239166, 0.21575763821601868, 0.15962602198123932, 0.1891247183084488, 0.1797034740447998, 0.1567288488149643, 0.16018106043338776, 0.17715121805667877, 0.1538868248462677, 0.21725021302700043, 0.17831191420555115, 0.15501244366168976, 0.15840190649032593, 0.15274350345134735, 0.19531795382499695, 0.15897402167320251, 0.19820933043956757, 0.2177717685699463, 0.13648033142089844, 0.19595369696617126, 0.16592608392238617, 0.15636786818504333, 0.17933256924152374, 0.1768043488264084, 0.1755240261554718, 0.17226830124855042, 0.16146789491176605, 0.17261871695518494, 0.17501820623874664, 0.171171635389328, 0.21635785698890686, 0.1733863800764084, 0.17853815853595734, 0.17343769967556, 0.18888098001480103, 0.18989451229572296, 0.19396746158599854, 0.1782599836587906, 0.15192089974880219, 0.13987022638320923, 0.18898485600948334, 0.17432239651679993, 0.16030529141426086, 0.14050117135047913, 0.17858923971652985, 0.17955166101455688, 0.19153161346912384, 0.19506052136421204, 0.15824541449546814, 0.15918906033039093, 0.185610830783844, 0.17195945978164673, 0.15429528057575226, 0.17456477880477905, 0.1668967306613922, 0.1875889003276825, 0.1936696618795395, 0.1500869244337082, 0.21117429435253143, 0.16825324296951294, 0.15882445871829987, 0.15968742966651917, 0.19120530784130096, 0.15426044166088104, 0.1503940373659134, 0.1588279753923416, 0.17765824496746063, 0.16941720247268677, 0.15772664546966553, 0.17785395681858063, 0.15261009335517883, 0.21056988835334778, 0.15423016250133514, 0.17386415600776672, 0.15817198157310486, 0.1678324192762375, 0.16696499288082123, 0.17537058889865875, 0.16751818358898163, 0.15441910922527313, 0.17517279088497162, 0.17988695204257965, 0.1592356264591217, 0.17812123894691467, 0.17971907556056976, 0.17421786487102509, 0.19209937751293182, 0.1771233081817627, 0.15910416841506958, 0.18737928569316864, 0.16239623725414276, 0.15801486372947693, 0.15341190993785858, 0.1943565011024475, 0.16238394379615784, 0.17672695219516754, 0.18644863367080688, 0.17877419292926788, 0.15750089287757874, 0.20479212701320648, 0.17894145846366882, 0.19180434942245483, 0.16742931306362152, 0.17485959827899933, 0.15640445053577423, 0.14115752279758453, 0.15854346752166748, 0.14100295305252075, 0.17229579389095306, 0.17977803945541382, 0.18184807896614075, 0.21246808767318726, 0.16930660605430603, 0.15723015367984772, 0.15349702537059784, 0.19411858916282654, 0.15637633204460144, 0.1894105225801468, 0.15912707149982452, 0.18182644248008728, 0.15825431048870087, 0.15480385720729828, 0.19192615151405334, 0.1911526322364807, 0.18591418862342834, 0.1581558734178543, 0.15735101699829102, 0.19724902510643005, 0.16346172988414764, 0.18397918343544006, 0.1929784119129181, 0.15550489723682404, 0.19083157181739807, 0.15874914824962616, 0.1889367401599884, 0.19277921319007874, 0.18623802065849304, 0.14155597984790802, 0.15908734500408173, 0.18853454291820526, 0.17595094442367554, 0.1589307188987732, 0.16685310006141663, 0.16107431054115295, 0.1703699231147766, 0.15223371982574463, 0.16392403841018677, 0.1660449355840683, 0.15734952688217163, 0.14936204254627228, 0.1722288727760315, 0.187751904129982, 0.21023191511631012, 0.1542072892189026, 0.19389232993125916, 0.165329247713089, 0.15922203660011292, 0.19588321447372437, 0.20910823345184326, 0.18272727727890015, 0.1835484653711319, 0.20781844854354858, 0.18504582345485687, 0.16380494832992554, 0.16358038783073425, 0.17536212503910065, 0.15157382190227509, 0.1666588932275772, 0.1455380767583847, 0.14549557864665985, 0.16898129880428314, 0.17167453467845917, 0.18979249894618988, 0.1828295737504959, 0.1539764255285263, 0.17764168977737427, 0.17901922762393951, 0.17672009766101837, 0.18669329583644867, 0.17620918154716492, 0.18054009974002838, 0.1624222844839096, 0.18171477317810059, 0.17890134453773499, 0.14512814581394196, 0.14514866471290588, 0.16087673604488373, 0.19701945781707764, 0.20622391998767853, 0.15563707053661346, 0.19803503155708313, 0.15625041723251343, 0.17137373983860016, 0.16152890026569366, 0.17388591170310974, 0.17567932605743408, 0.1724904179573059, 0.19146892428398132, 0.17798055708408356, 0.1830037385225296, 0.14554964005947113, 0.2053893804550171, 0.1776100993156433, 0.18289393186569214, 0.14622630178928375, 0.18412861227989197, 0.18123924732208252, 0.16433830559253693, 0.1805424839258194, 0.20394861698150635, 0.1594778150320053, 0.15022431313991547, 0.15873396396636963, 0.17350268363952637, 0.19283442199230194, 0.16105233132839203, 0.17483209073543549, 0.16933931410312653, 0.1575198620557785, 0.1574455052614212, 0.20296578109264374, 0.18389707803726196, 0.1617802530527115, 0.20232820510864258, 0.20186614990234375, 0.14858576655387878, 0.19151760637760162, 0.18349125981330872, 0.16998746991157532, 0.18315182626247406, 0.17027099430561066, 0.1504741907119751, 0.18076495826244354, 0.15432128310203552, 0.1664634644985199, 0.15076172351837158, 0.1635587513446808, 0.15949447453022003, 0.16037781536579132, 0.1855591982603073, 0.16459141671657562, 0.16769921779632568, 0.1869317591190338, 0.20056235790252686, 0.16241516172885895, 0.17397184669971466, 0.15808819234371185, 0.19305011630058289, 0.14911967515945435, 0.18507477641105652, 0.18481194972991943, 0.16037043929100037, 0.14909325540065765, 0.15923668444156647, 0.15620157122612, 0.14830496907234192, 0.16672731935977936, 0.16021984815597534, 0.17221763730049133, 0.15750955045223236, 0.1882099062204361, 0.1616767793893814, 0.146115243434906, 0.16733556985855103, 0.14529870450496674, 0.17698748409748077, 0.14432403445243835, 0.18141773343086243, 0.16210773587226868, 0.16290804743766785, 0.18245439231395721, 0.2014128416776657, 0.1760932207107544, 0.19157934188842773, 0.187185138463974, 0.15379036962985992, 0.18373936414718628, 0.16850590705871582, 0.19833038747310638, 0.16002196073532104, 0.19565775990486145, 0.1867181807756424, 0.19644932448863983, 0.19141197204589844, 0.16067096590995789, 0.14154517650604248, 0.19808311760425568, 0.19312062859535217, 0.14976303279399872, 0.1630665361881256, 0.18460199236869812, 0.1837788224220276, 0.156843364238739, 0.14927469193935394, 0.18047183752059937, 0.14893341064453125, 0.16732831299304962, 0.1942547857761383, 0.19777141511440277, 0.1709083467721939, 0.18709248304367065, 0.1498594880104065, 0.142801433801651, 0.15274426341056824, 0.17042122781276703, 0.15410849452018738, 0.17974773049354553, 0.18788869678974152, 0.1423155814409256, 0.1785946637392044, 0.1643194854259491, 0.17960496246814728, 0.18442606925964355, 0.2105139046907425, 0.1878739893436432, 0.17079250514507294, 0.2097318321466446, 0.16245825588703156, 0.18577897548675537, 0.14274053275585175, 0.1762089878320694, 0.18200263381004333, 0.18078996241092682, 0.16451361775398254, 0.19604630768299103, 0.1768745481967926, 0.17474275827407837, 0.19660060107707977, 0.15352694690227509, 0.1794486939907074, 0.15398773550987244, 0.16376322507858276, 0.16719123721122742, 0.1574341356754303, 0.1932838261127472, 0.1692165732383728, 0.17800244688987732, 0.1517210304737091, 0.16320978105068207, 0.16725139319896698, 0.1913621872663498, 0.17735011875629425, 0.1877814084291458, 0.17396527528762817, 0.1978396624326706, 0.18036344647407532, 0.1672426462173462, 0.19044092297554016, 0.16413448750972748, 0.16120466589927673, 0.15494585037231445, 0.20559899508953094, 0.1912863552570343, 0.17420710623264313, 0.14593690633773804, 0.1741923987865448, 0.17236460745334625, 0.18169760704040527, 0.1832331120967865, 0.16671237349510193, 0.16937506198883057, 0.18774127960205078, 0.1779068559408188, 0.18067048490047455, 0.1745191067457199, 0.1773860603570938, 0.18065686523914337, 0.17682214081287384, 0.17389951646327972, 0.17551270127296448, 0.16230076551437378, 0.18596653640270233, 0.17699816823005676, 0.17891930043697357, 0.16158398985862732, 0.1734689474105835, 0.18920005857944489, 0.16175875067710876, 0.17586001753807068, 0.17196667194366455, 0.18554408848285675, 0.16052065789699554, 0.17213107645511627, 0.17145472764968872, 0.18459801375865936, 0.1866050511598587, 0.160640686750412, 0.1752074956893921, 0.16445909440517426, 0.1775134652853012, 0.1745002120733261, 0.17130525410175323, 0.18776296079158783, 0.1495821177959442, 0.1618991494178772, 0.18813666701316833, 0.1867920458316803, 0.16298046708106995, 0.17358055710792542, 0.1603630781173706, 0.17024348676204681, 0.1612270623445511, 0.18692539632320404, 0.18384210765361786, 0.16179126501083374, 0.18645891547203064, 0.1854102909564972, 0.18861013650894165, 0.16215811669826508, 0.15009188652038574, 0.19920141994953156, 0.17412924766540527, 0.15989814698696136, 0.16119715571403503, 0.1853460669517517, 0.15887977182865143, 0.1623401939868927, 0.16066491603851318, 0.15939733386039734, 0.16331429779529572, 0.1601496785879135, 0.17548930644989014, 0.1872282773256302, 0.19044499099254608, 0.20013806223869324, 0.15893882513046265, 0.16358420252799988, 0.14955462515354156, 0.17072640359401703, 0.18624624609947205, 0.1865054816007614, 0.162673681974411, 0.16143934428691864, 0.18318118155002594, 0.17414698004722595, 0.17408543825149536, 0.1493150144815445, 0.1742643564939499, 0.18605364859104156, 0.159417062997818, 0.1617571860551834, 0.20066151022911072, 0.18342119455337524, 0.15762701630592346, 0.18674461543560028, 0.17420025169849396, 0.17851285636425018, 0.18491575121879578, 0.17182466387748718, 0.17429649829864502, 0.18597684800624847, 0.18581891059875488, 0.1814175844192505, 0.15091387927532196, 0.16018547117710114, 0.17144252359867096, 0.17541272938251495, 0.1620381772518158, 0.16340337693691254, 0.1513276845216751, 0.18821264803409576, 0.1626923829317093, 0.17417941987514496, 0.1743093878030777, 0.1673867553472519, 0.18905578553676605, 0.17107149958610535, 0.1905381977558136, 0.15076836943626404, 0.1629968136548996, 0.16056925058364868, 0.1698746383190155, 0.1721160113811493, 0.18758980929851532, 0.16539672017097473, 0.15699166059494019, 0.16598668694496155, 0.17158234119415283, 0.16340690851211548, 0.16463938355445862, 0.18334722518920898, 0.16371870040893555, 0.17781947553157806, 0.14857041835784912, 0.16209004819393158, 0.17731641232967377, 0.18707166612148285, 0.2024213671684265, 0.17536434531211853, 0.14759936928749084, 0.1760990470647812, 0.16228359937667847, 0.14722885191440582, 0.15766267478466034, 0.1571817547082901, 0.16887232661247253, 0.17324165999889374, 0.18785002827644348, 0.19297221302986145, 0.1608920842409134, 0.16003483533859253, 0.15805992484092712, 0.19358402490615845, 0.1613219827413559, 0.20582902431488037, 0.14493794739246368, 0.15521898865699768, 0.17392218112945557, 0.15999768674373627, 0.17448048293590546, 0.14422550797462463, 0.20708656311035156, 0.17297424376010895, 0.1730250120162964, 0.20729580521583557, 0.18702946603298187, 0.15885543823242188, 0.15948453545570374, 0.17036326229572296, 0.15518340468406677, 0.1776551753282547, 0.144328773021698, 0.1924256533384323, 0.175772562623024, 0.1579412966966629, 0.15476982295513153, 0.15882891416549683, 0.19349601864814758, 0.15365475416183472, 0.17410486936569214, 0.17317622900009155, 0.15932928025722504, 0.15292882919311523, 0.18926942348480225, 0.1592976301908493, 0.15905553102493286, 0.1545730084180832, 0.14229394495487213, 0.18696944415569305, 0.18758583068847656, 0.17526070773601532, 0.1714877486228943, 0.15345346927642822, 0.1723187267780304, 0.17986303567886353, 0.17087610065937042, 0.17766939103603363, 0.17477095127105713, 0.1904299110174179, 0.17479699850082397, 0.1980423629283905, 0.15721829235553741, 0.17367301881313324, 0.1937420517206192, 0.17633415758609772, 0.16857947409152985, 0.17888081073760986, 0.14194351434707642, 0.17113704979419708, 0.17278143763542175, 0.15479421615600586, 0.175475612282753, 0.1723775714635849, 0.15110372006893158, 0.21008671820163727, 0.1864779144525528, 0.1760731339454651, 0.15505069494247437, 0.17485487461090088, 0.1748167723417282, 0.173740416765213, 0.17150844633579254, 0.17302396893501282, 0.15493468940258026, 0.1952894926071167, 0.17377062141895294, 0.15506736934185028, 0.14289560914039612, 0.16008539497852325, 0.1903960108757019, 0.17538054287433624, 0.1723632663488388, 0.20907214283943176, 0.15837471187114716, 0.14272737503051758, 0.17342935502529144, 0.14264819025993347, 0.17403076589107513, 0.18263721466064453, 0.16640087962150574, 0.16678451001644135, 0.19052700698375702, 0.17655673623085022, 0.17286276817321777, 0.17845271527767181, 0.18612146377563477, 0.14261268079280853, 0.19474750757217407, 0.17760704457759857, 0.16061705350875854, 0.19471268355846405, 0.1665351539850235, 0.1844208687543869, 0.16018547117710114, 0.16013094782829285, 0.18003599345684052, 0.1756024956703186, 0.20791397988796234, 0.1571975201368332, 0.17356397211551666, 0.17691650986671448, 0.15336044132709503, 0.17967700958251953, 0.15704016387462616, 0.15926778316497803, 0.174538716673851, 0.20731708407402039, 0.1775856465101242, 0.15171116590499878, 0.17884831130504608, 0.17033110558986664, 0.15857280790805817, 0.1727869063615799, 0.15529434382915497, 0.14401891827583313, 0.15580451488494873, 0.20764215290546417, 0.2076757401227951, 0.19016137719154358, 0.1920774281024933, 0.14409981667995453, 0.17922478914260864, 0.17778463661670685, 0.17509877681732178, 0.18850892782211304, 0.17866630852222443, 0.17047594487667084, 0.15942347049713135, 0.1957566887140274, 0.15743158757686615, 0.17838197946548462, 0.20531205832958221, 0.15945999324321747, 0.1720009595155716, 0.1523573100566864, 0.1709793359041214, 0.18983294069766998, 0.17758400738239288, 0.17125427722930908, 0.1815611571073532, 0.168585866689682, 0.1600281000137329, 0.1897270530462265, 0.1727864146232605, 0.14687077701091766, 0.16008387506008148, 0.17403250932693481, 0.20357990264892578, 0.19276082515716553, 0.190144881606102, 0.1680578738451004, 0.16222608089447021, 0.16893452405929565, 0.16232477128505707, 0.16296832263469696, 0.16492363810539246, 0.16532650589942932, 0.1697983741760254, 0.17445243895053864, 0.19076021015644073, 0.17986822128295898, 0.14764279127120972, 0.16732914745807648, 0.16909317672252655, 0.18490982055664062, 0.1770123541355133, 0.15464313328266144, 0.18379712104797363, 0.18440835177898407, 0.15283724665641785, 0.16238175332546234, 0.17167991399765015, 0.17716945707798004, 0.15313029289245605, 0.19832846522331238, 0.18668046593666077, 0.15491773188114166, 0.1686500608921051, 0.1898128092288971, 0.16027376055717468, 0.1731625199317932, 0.183110311627388, 0.18146859109401703, 0.14831103384494781, 0.1922171711921692, 0.14823581278324127, 0.16377009451389313, 0.18203410506248474, 0.16069331765174866, 0.14779819548130035, 0.18300434947013855, 0.17947454750537872, 0.1543847769498825, 0.1648970991373062, 0.17251551151275635, 0.16285313665866852, 0.17449136078357697, 0.169554203748703, 0.16475418210029602, 0.18131375312805176, 0.19300809502601624, 0.18112631142139435, 0.1671053022146225, 0.14498628675937653, 0.15328508615493774, 0.17411813139915466, 0.15766721963882446, 0.17422865331172943, 0.15316028892993927, 0.1724117249250412, 0.160649374127388, 0.18865253031253815, 0.17088167369365692, 0.15904462337493896, 0.1931726187467575, 0.17661194503307343, 0.1548350751399994, 0.1670239269733429, 0.1808798909187317, 0.1768319010734558, 0.20960721373558044, 0.19356071949005127, 0.14249780774116516, 0.19025684893131256, 0.18550905585289001, 0.17552562057971954, 0.1574592888355255, 0.14315281808376312, 0.18248189985752106, 0.17055641114711761, 0.1874321550130844, 0.1702454388141632, 0.1934928596019745, 0.1433887928724289, 0.1527281552553177, 0.161045104265213, 0.19177742302417755, 0.1946893185377121, 0.18213047087192535, 0.14340147376060486, 0.17479106783866882, 0.15520597994327545, 0.152046337723732, 0.18921610713005066, 0.16663779318332672, 0.17812596261501312, 0.20875400304794312, 0.1777850091457367, 0.19250914454460144, 0.14304274320602417, 0.18941406905651093, 0.1432110071182251, 0.20817448198795319, 0.1885630190372467, 0.16896353662014008, 0.14372940361499786, 0.15678416192531586, 0.16924221813678741, 0.1691548228263855, 0.1747298538684845, 0.1880280077457428, 0.19290152192115784, 0.1914813369512558, 0.1786898821592331, 0.16068023443222046, 0.19427047669887543, 0.1540963351726532, 0.1570305973291397, 0.14487457275390625, 0.18706056475639343, 0.1825994998216629, 0.187935471534729, 0.17244952917099, 0.18593057990074158, 0.16921156644821167, 0.18698261678218842, 0.19143418967723846, 0.1718532294034958, 0.15593183040618896, 0.1684286743402481, 0.18493302166461945, 0.17954382300376892, 0.16260050237178802, 0.17722709476947784, 0.1922963261604309, 0.16858351230621338, 0.158615842461586, 0.1659114509820938, 0.17917606234550476, 0.1709452122449875, 0.15723399817943573, 0.15616482496261597, 0.17695006728172302, 0.1594500094652176, 0.16569094359874725, 0.15662647783756256, 0.17010492086410522, 0.19591976702213287, 0.14736737310886383, 0.17008116841316223, 0.1774502396583557, 0.16336919367313385, 0.16366945207118988, 0.1848708838224411, 0.1664256751537323, 0.20361532270908356, 0.17624062299728394, 0.14700624346733093, 0.16325941681861877, 0.18542788922786713, 0.16635312139987946, 0.15377986431121826, 0.1611459106206894, 0.19354744255542755, 0.16083215177059174, 0.1653553992509842, 0.18303535878658295, 0.17097245156764984, 0.19677317142486572, 0.18478919565677643, 0.16297227144241333, 0.16178712248802185, 0.1547614485025406, 0.15858350694179535, 0.18697671592235565, 0.1860322505235672, 0.17079965770244598, 0.1702462136745453, 0.1740572601556778, 0.16667208075523376, 0.1851399689912796, 0.17984186112880707, 0.20375625789165497, 0.18279993534088135, 0.16356788575649261, 0.18080635368824005, 0.16345196962356567, 0.18341386318206787, 0.17895865440368652, 0.16507062315940857, 0.16460654139518738, 0.16831733286380768, 0.1764495074748993, 0.16729892790317535, 0.16293896734714508, 0.17041407525539398, 0.15365581214427948, 0.16159068048000336, 0.1568022072315216, 0.1565227508544922, 0.17630761861801147, 0.17194226384162903, 0.18202462792396545, 0.1634458303451538, 0.1594603955745697, 0.1798599362373352, 0.18186117708683014, 0.18626634776592255, 0.18549345433712006, 0.16286201775074005, 0.18648627400398254, 0.1528473198413849, 0.16340500116348267, 0.1702076643705368, 0.18165311217308044, 0.1470763385295868, 0.16512343287467957, 0.17860820889472961, 0.18600305914878845, 0.17524443566799164, 0.19598878920078278, 0.17186658084392548, 0.18817752599716187, 0.17338773608207703, 0.16562087833881378, 0.19768652319908142, 0.16749802231788635, 0.16869372129440308, 0.15970534086227417, 0.16741041839122772, 0.14822079241275787, 0.17879031598567963, 0.2019176185131073, 0.2034812569618225, 0.17458388209342957, 0.16453510522842407, 0.15616895258426666, 0.15893536806106567, 0.1551995873451233, 0.15988773107528687, 0.19469933211803436, 0.18780800700187683, 0.15781116485595703, 0.17415554821491241, 0.18688146770000458, 0.1593204289674759, 0.14823313057422638, 0.17669567465782166, 0.15201151371002197, 0.17514283955097198, 0.18919464945793152, 0.18520745635032654, 0.15974116325378418, 0.19407692551612854, 0.17591457068920135, 0.1696007251739502, 0.17228777706623077, 0.19161012768745422, 0.15908928215503693, 0.18112823367118835, 0.16773533821105957, 0.18768015503883362, 0.16256944835186005, 0.16252578794956207, 0.16199377179145813, 0.16679273545742035, 0.14853790402412415, 0.16580255329608917, 0.17042718827724457, 0.18225516378879547, 0.18295645713806152, 0.16842618584632874, 0.1745208203792572, 0.1862080693244934, 0.1918635070323944, 0.15941773355007172, 0.17692512273788452, 0.1781287044286728, 0.17387385666370392, 0.15800386667251587, 0.18222253024578094, 0.15896975994110107, 0.1849929541349411, 0.20259779691696167, 0.17466388642787933, 0.1639166921377182, 0.16354972124099731, 0.1587192565202713, 0.16307929158210754, 0.20203973352909088, 0.1762584000825882, 0.15715478360652924, 0.1768522709608078, 0.16347438097000122, 0.17537614703178406, 0.17374810576438904, 0.17969413101673126, 0.16131074726581573, 0.1806809902191162, 0.18002626299858093, 0.14855098724365234, 0.19345982372760773, 0.16026714444160461, 0.18510845303535461, 0.15243567526340485, 0.1487932652235031, 0.1540832370519638, 0.1940542310476303, 0.15594495832920074, 0.17911463975906372, 0.17264369130134583, 0.1727287322282791, 0.18703223764896393, 0.17743363976478577, 0.1481911689043045, 0.1625881791114807, 0.14791172742843628, 0.1594507396221161, 0.2026040405035019, 0.15282341837882996, 0.18479667603969574, 0.18387727439403534, 0.20276017487049103, 0.19021277129650116, 0.2023547887802124, 0.1783563792705536, 0.19163751602172852, 0.18178342282772064, 0.14927397668361664, 0.175381138920784, 0.19517521560192108, 0.1657751053571701, 0.15800325572490692, 0.16276228427886963, 0.1503823846578598, 0.18016375601291656, 0.19676728546619415, 0.16961778700351715, 0.1921921968460083, 0.15075789391994476, 0.18166625499725342, 0.18642695248126984, 0.15840815007686615, 0.19815514981746674, 0.16902287304401398, 0.19209352135658264, 0.16640515625476837, 0.16243276000022888, 0.178611621260643, 0.16524375975131989, 0.15936167538166046, 0.17551642656326294, 0.19282814860343933, 0.15881004929542542, 0.17604754865169525, 0.18123243749141693, 0.17975445091724396, 0.1667766571044922, 0.15307356417179108, 0.1704264134168625, 0.16580457985401154, 0.15318498015403748, 0.19158151745796204, 0.17063727974891663, 0.1800803244113922, 0.17404145002365112, 0.16038191318511963, 0.16335226595401764, 0.1777389943599701, 0.1688431054353714, 0.19636157155036926, 0.15255601704120636, 0.17758530378341675, 0.1699485182762146, 0.16422756016254425, 0.16999559104442596, 0.19672033190727234, 0.15782536566257477, 0.16477948427200317, 0.17295747995376587, 0.19762565195560455, 0.18754348158836365, 0.18178580701351166, 0.16672731935977936, 0.17930138111114502, 0.17928490042686462, 0.1942717283964157, 0.19657957553863525, 0.19554325938224792, 0.18396607041358948, 0.17424193024635315, 0.17573831975460052, 0.17660090327262878, 0.15991733968257904, 0.1743784099817276, 0.17843599617481232, 0.1760428100824356, 0.16386546194553375, 0.17540216445922852, 0.1644962728023529, 0.17047974467277527, 0.17124372720718384, 0.19011977314949036, 0.17976148426532745, 0.17688949406147003, 0.18174414336681366, 0.17844615876674652, 0.1763753741979599, 0.17776614427566528, 0.17986930906772614, 0.1770123839378357, 0.15790094435214996, 0.1708287000656128, 0.16623561084270477, 0.1613711714744568, 0.17436997592449188, 0.1611490696668625, 0.18994908034801483, 0.16375146806240082, 0.17831550538539886, 0.16755712032318115, 0.18775422871112823, 0.15823309123516083, 0.18427938222885132, 0.18322449922561646, 0.18098999559879303, 0.17070630192756653, 0.18876607716083527, 0.1807766556739807, 0.1720694750547409, 0.159113347530365, 0.1574355959892273, 0.1592927724123001, 0.17316082119941711, 0.18153052031993866, 0.1878359317779541, 0.15879900753498077, 0.195777028799057, 0.16635388135910034, 0.15858463943004608, 0.16089564561843872, 0.1650293469429016, 0.1802530437707901, 0.1616217941045761, 0.15772782266139984, 0.16043952107429504, 0.18291302025318146, 0.1581539511680603, 0.17335037887096405, 0.18431983888149261, 0.17612287402153015, 0.1699947565793991, 0.1668715924024582, 0.1717529147863388, 0.17084340751171112, 0.17182375490665436, 0.16644427180290222, 0.16678617894649506, 0.1673542708158493, 0.18664032220840454, 0.16242486238479614, 0.17369802296161652, 0.17481933534145355, 0.17383970320224762, 0.1831715703010559, 0.17828458547592163, 0.16351327300071716, 0.17525261640548706, 0.18046624958515167, 0.19270767271518707, 0.18958961963653564, 0.17423558235168457, 0.15626990795135498, 0.17426367104053497, 0.17162922024726868, 0.1538463532924652, 0.16839154064655304, 0.18834266066551208, 0.17505188286304474, 0.18451301753520966, 0.17032162845134735, 0.17748624086380005, 0.17347390949726105, 0.15667949616909027, 0.1557980477809906, 0.15557602047920227, 0.17011024057865143, 0.1802765429019928, 0.1875726729631424, 0.16444481909275055, 0.18050648272037506, 0.17063599824905396, 0.16140131652355194, 0.18725232779979706, 0.16980594396591187, 0.15876372158527374, 0.1680220365524292, 0.19449517130851746, 0.1651519238948822, 0.1609332114458084, 0.19063518941402435, 0.18930192291736603, 0.18173058331012726, 0.1649201512336731, 0.1579420417547226, 0.1666749268770218, 0.1607930213212967, 0.15412814915180206, 0.19251981377601624, 0.19237995147705078, 0.15392711758613586, 0.17607752978801727, 0.18880464136600494, 0.17174281179904938, 0.15849685668945312, 0.18472780287265778, 0.17406374216079712, 0.1737455278635025, 0.16170044243335724, 0.17303875088691711, 0.17104999721050262, 0.18524132668972015, 0.15774735808372498, 0.16460178792476654, 0.16440629959106445, 0.1640043556690216, 0.18072493374347687, 0.16102935373783112, 0.16956882178783417, 0.19521957635879517, 0.17900434136390686, 0.17798620462417603, 0.16700085997581482, 0.18246901035308838, 0.1589941829442978, 0.16505979001522064, 0.18453483283519745, 0.1856805980205536, 0.18022823333740234, 0.18841660022735596, 0.1737421154975891, 0.17383207380771637, 0.18197663128376007, 0.16659419238567352, 0.17334440350532532, 0.17901119589805603, 0.15967021882534027, 0.18775372207164764, 0.19362299144268036, 0.18472371995449066, 0.1678261160850525, 0.1924227774143219, 0.16248126327991486, 0.16629303991794586, 0.18440930545330048, 0.16084791719913483, 0.15584056079387665, 0.19136002659797668, 0.190637469291687, 0.17806966602802277, 0.1818850338459015, 0.16858722269535065, 0.16426409780979156, 0.18576215207576752, 0.18017978966236115, 0.17256107926368713, 0.17358511686325073, 0.162880539894104, 0.15942901372909546, 0.17867371439933777, 0.16930843889713287, 0.16625915467739105, 0.17688560485839844, 0.15969105064868927, 0.1606733202934265, 0.19282497465610504, 0.16576656699180603, 0.15964952111244202, 0.15639282763004303, 0.16366901993751526, 0.16270199418067932, 0.18782854080200195, 0.18073058128356934, 0.17025408148765564, 0.15416598320007324, 0.19000819325447083, 0.15838009119033813, 0.17350353300571442, 0.16561515629291534, 0.16996055841445923, 0.15899188816547394, 0.16766931116580963, 0.17490506172180176, 0.1898084431886673, 0.15911301970481873, 0.17552390694618225, 0.15673349797725677, 0.17146612703800201, 0.1754923164844513, 0.18311092257499695, 0.16162386536598206, 0.15963111817836761, 0.1830238401889801, 0.18334722518920898, 0.15581931173801422, 0.16534145176410675, 0.17060652375221252, 0.19289721548557281, 0.15752212703227997, 0.1763530671596527, 0.19285975396633148, 0.18402263522148132, 0.19278894364833832, 0.15578600764274597, 0.1627182513475418, 0.1886332482099533, 0.17363613843917847, 0.18159493803977966, 0.19181205332279205, 0.17610490322113037, 0.15917159616947174, 0.18010789155960083, 0.16064943373203278, 0.1753576695919037, 0.1669892519712448, 0.15733420848846436, 0.1790764182806015, 0.17446187138557434, 0.18600749969482422, 0.19094949960708618, 0.16134525835514069, 0.15712684392929077, 0.19097360968589783, 0.18027591705322266, 0.17245988547801971, 0.17627733945846558, 0.15525488555431366, 0.16640982031822205, 0.16857363283634186, 0.16892321407794952, 0.17697815597057343, 0.17874957621097565, 0.17263475060462952, 0.1782994568347931, 0.17811186611652374, 0.15713229775428772, 0.15702897310256958, 0.1757400631904602, 0.16095945239067078, 0.16970810294151306, 0.1565713733434677, 0.15887366235256195, 0.166087806224823, 0.16609016060829163, 0.1808154284954071, 0.15524286031723022, 0.19373297691345215, 0.16447274386882782, 0.16570676863193512, 0.1849486529827118, 0.16723927855491638, 0.1807362139225006, 0.1630149781703949, 0.1627698540687561, 0.1589110940694809, 0.1646415740251541, 0.1676325798034668, 0.17306941747665405, 0.17442534863948822, 0.17807859182357788, 0.17940591275691986, 0.1589660346508026, 0.1658133566379547, 0.18174488842487335, 0.1548500657081604, 0.19701963663101196, 0.17370271682739258, 0.159784197807312, 0.1518712043762207, 0.152455672621727, 0.1896645873785019, 0.1757686883211136, 0.18080419301986694, 0.18686142563819885, 0.19679905474185944, 0.17948126792907715, 0.1848493367433548, 0.16008587181568146, 0.16353800892829895, 0.17041239142417908, 0.1677134782075882, 0.18261727690696716, 0.17561212182044983, 0.1714983731508255, 0.18774722516536713, 0.17263686656951904, 0.17401191592216492, 0.15813972055912018, 0.16558118164539337, 0.16812975704669952, 0.17953987419605255, 0.17162616550922394, 0.17794889211654663, 0.15267106890678406, 0.18553824722766876, 0.17689929902553558, 0.1965174376964569, 0.1708885133266449, 0.16746056079864502, 0.17918667197227478, 0.16430741548538208, 0.15979458391666412, 0.1767357438802719, 0.19737635552883148, 0.1956510841846466, 0.15422503650188446, 0.1899341493844986, 0.17464770376682281, 0.15787547826766968, 0.2000761181116104, 0.1547006517648697, 0.18866249918937683, 0.1719716191291809, 0.19289954006671906, 0.16899845004081726, 0.17024382948875427, 0.16771256923675537, 0.17393717169761658, 0.1659984439611435, 0.15751676261425018, 0.1765008121728897, 0.15534499287605286, 0.17351235449314117, 0.16603030264377594, 0.16710087656974792, 0.15519298613071442, 0.16930048167705536, 0.1685241460800171, 0.18572020530700684, 0.17437170445919037, 0.1745033711194992, 0.18316303193569183, 0.16137373447418213, 0.19016292691230774, 0.16669389605522156, 0.17618218064308167, 0.18405431509017944, 0.19909736514091492, 0.1590706706047058, 0.15878435969352722, 0.18833187222480774, 0.18586640059947968, 0.17432601749897003, 0.16525590419769287, 0.1528906673192978, 0.17731723189353943, 0.1649733930826187, 0.1658214032649994, 0.19081084430217743, 0.19021368026733398, 0.16113857924938202, 0.15492478013038635, 0.18072564899921417, 0.174148827791214, 0.16713504493236542, 0.19386135041713715, 0.18740716576576233, 0.16334161162376404, 0.18569263815879822, 0.15479439496994019, 0.168009951710701, 0.159433051943779, 0.1544533371925354, 0.18231485784053802, 0.19384163618087769, 0.1882179230451584, 0.15954570472240448, 0.18063901364803314, 0.16058696806430817, 0.17858687043190002, 0.1726730316877365, 0.1744125932455063, 0.1652020961046219, 0.15565550327301025, 0.1649332195520401, 0.19483844935894012, 0.19467048346996307, 0.1610586792230606, 0.16328583657741547, 0.16976304352283478, 0.1711912304162979, 0.18856249749660492, 0.15855805575847626, 0.17856056988239288, 0.181894913315773, 0.18967707455158234, 0.16821792721748352, 0.18787577748298645, 0.16491849720478058, 0.17824696004390717, 0.16592203080654144, 0.15743188560009003, 0.1657048761844635, 0.1756119579076767, 0.17900869250297546, 0.1536475569009781, 0.15677180886268616, 0.16278496384620667, 0.17957143485546112, 0.17453856766223907, 0.16719985008239746, 0.18708817660808563, 0.18187934160232544, 0.15553376078605652, 0.1875779926776886, 0.18285387754440308, 0.16852553188800812, 0.174763485789299, 0.19518040120601654, 0.17057347297668457, 0.15396961569786072, 0.15389427542686462, 0.15562637150287628, 0.1886252611875534, 0.19572831690311432, 0.15922601521015167, 0.19609104096889496, 0.15503478050231934, 0.2004377394914627, 0.18096579611301422, 0.16299666464328766, 0.19856387376785278, 0.16750012338161469, 0.16016902029514313, 0.1767505556344986, 0.18759949505329132, 0.15655454993247986, 0.17011810839176178, 0.18066512048244476, 0.15466827154159546, 0.19551008939743042, 0.15493059158325195, 0.1775609403848648, 0.16021159291267395, 0.18820619583129883, 0.19389820098876953, 0.1592908799648285, 0.18946143984794617, 0.18086272478103638, 0.17624162137508392, 0.18184144794940948, 0.18967391550540924, 0.1587553471326828, 0.17771588265895844, 0.17800219357013702, 0.17565372586250305, 0.17792858183383942, 0.16026714444160461, 0.1895422637462616, 0.1564890742301941, 0.17111754417419434, 0.16939958930015564, 0.16297748684883118, 0.15648820996284485, 0.1722395122051239, 0.18098753690719604, 0.17417548596858978, 0.19262707233428955, 0.19254188239574432, 0.19034872949123383, 0.17404720187187195, 0.18829716742038727, 0.16252267360687256, 0.1699739247560501, 0.1746649444103241, 0.1918206661939621, 0.16070398688316345, 0.1877828687429428, 0.18496377766132355, 0.16465504467487335, 0.169459268450737, 0.19285859167575836, 0.17013737559318542, 0.1804911345243454, 0.18369022011756897, 0.16955547034740448, 0.15873034298419952, 0.18704771995544434, 0.1859951764345169, 0.15970741212368011, 0.17272154986858368, 0.17505891621112823, 0.1703466922044754, 0.18339315056800842, 0.18901506066322327, 0.16973574459552765, 0.15675799548625946, 0.16081616282463074, 0.1701238453388214, 0.17292483150959015, 0.1690368801355362, 0.15945947170257568, 0.17148976027965546, 0.1764620691537857, 0.158818781375885, 0.1843840628862381, 0.18158970773220062, 0.17405277490615845, 0.1898094117641449, 0.18352119624614716, 0.18180614709854126, 0.16540269553661346, 0.16647613048553467, 0.17520053684711456, 0.1796865612268448, 0.15793579816818237, 0.15773285925388336, 0.18065708875656128, 0.17818321287631989, 0.17334988713264465, 0.1632196307182312, 0.18302494287490845, 0.156885027885437, 0.1897617131471634, 0.16985496878623962, 0.17391464114189148, 0.16676805913448334, 0.16794206202030182, 0.18089738488197327, 0.15696895122528076, 0.16057182848453522, 0.15632663667201996, 0.17607896029949188, 0.17365235090255737, 0.17835725843906403, 0.18057140707969666, 0.18144544959068298, 0.1929061859846115, 0.1653883159160614, 0.17844949662685394, 0.19317783415317535, 0.17959748208522797, 0.17687419056892395, 0.17386400699615479, 0.16753101348876953, 0.1624191701412201, 0.17236264050006866, 0.15455172955989838, 0.16890262067317963, 0.16046108305454254, 0.16638977825641632, 0.17117741703987122, 0.18396764993667603, 0.17534226179122925, 0.16758380830287933, 0.167105570435524, 0.1693429946899414, 0.15565098822116852, 0.17258073389530182, 0.18947584927082062, 0.17520016431808472, 0.15519611537456512, 0.19324788451194763, 0.17178940773010254, 0.17529501020908356, 0.16271033883094788, 0.1701774150133133, 0.16619862616062164, 0.1724979281425476, 0.16920876502990723, 0.16366642713546753, 0.17385435104370117, 0.18589061498641968, 0.1653161495923996, 0.18430890142917633, 0.15699653327465057, 0.18085405230522156, 0.1543254554271698, 0.17333771288394928, 0.16250769793987274, 0.17145857214927673, 0.19251875579357147, 0.15241748094558716, 0.18751899898052216, 0.16619668900966644, 0.18297019600868225, 0.15214034914970398, 0.17830215394496918, 0.1714903861284256, 0.19512806832790375, 0.1887425184249878, 0.19210772216320038, 0.17682313919067383, 0.1719440370798111, 0.16358792781829834, 0.17234188318252563, 0.17965644598007202, 0.1544991284608841, 0.17013688385486603, 0.17019502818584442, 0.16024526953697205, 0.1660487800836563, 0.1664111465215683, 0.15743666887283325, 0.19477955996990204, 0.19102025032043457, 0.15263056755065918, 0.1672041118144989, 0.18087075650691986, 0.15770237147808075, 0.16593818366527557, 0.16928167641162872, 0.18802082538604736, 0.16679036617279053, 0.19875048100948334, 0.172352135181427, 0.16856390237808228, 0.18596859276294708, 0.15354813635349274, 0.15368762612342834, 0.18570788204669952, 0.15624290704727173, 0.16135931015014648, 0.1656237542629242, 0.16586364805698395, 0.18323460221290588, 0.1731254756450653, 0.1700442135334015, 0.1524960845708847, 0.15885436534881592, 0.16170871257781982, 0.19653938710689545, 0.16859164834022522, 0.18134623765945435, 0.1917571872472763, 0.14490871131420135, 0.15099161863327026, 0.17605532705783844, 0.18327569961547852, 0.15059882402420044, 0.19021567702293396, 0.1821538507938385, 0.16925093531608582, 0.17797328531742096, 0.14997591078281403, 0.19397486746311188, 0.1760970950126648, 0.1498471051454544, 0.18193627893924713, 0.17252376675605774, 0.180814728140831, 0.17335094511508942, 0.15647146105766296, 0.18010064959526062, 0.1910698115825653, 0.15372876822948456, 0.16037419438362122, 0.20119547843933105, 0.16051529347896576, 0.15794286131858826, 0.18174658715724945, 0.19274328649044037, 0.17516160011291504, 0.1731111854314804, 0.16004633903503418, 0.16235700249671936, 0.14917902648448944, 0.1631990522146225, 0.20068301260471344, 0.18448853492736816, 0.17856043577194214, 0.18212968111038208, 0.18080756068229675, 0.16771085560321808, 0.16890229284763336, 0.16564132273197174, 0.172129824757576, 0.18062981963157654, 0.17465201020240784, 0.17816512286663055, 0.2014404982328415, 0.1897941678762436, 0.1704554259777069, 0.17159947752952576, 0.18289820849895477, 0.1641460508108139, 0.16357263922691345, 0.14992225170135498, 0.1619463860988617, 0.17013423144817352, 0.18461790680885315, 0.20027263462543488, 0.17297779023647308, 0.14972129464149475, 0.17178037762641907, 0.17736868560314178, 0.1803772747516632, 0.15452374517917633, 0.15970580279827118, 0.16227197647094727, 0.18100214004516602, 0.2023402750492096, 0.18056140840053558, 0.16815485060214996, 0.15312612056732178, 0.18107959628105164, 0.1664837896823883, 0.17262116074562073, 0.1698293834924698, 0.15374614298343658, 0.176895409822464, 0.1946548968553543, 0.16152769327163696, 0.1720924824476242, 0.1589382141828537, 0.19585448503494263, 0.14867664873600006, 0.18167799711227417, 0.1746503710746765, 0.16510352492332458, 0.14869756996631622, 0.14853860437870026, 0.16000209748744965, 0.14870573580265045, 0.16805022954940796, 0.19319158792495728, 0.19587969779968262, 0.1793016642332077, 0.17494511604309082, 0.17056512832641602, 0.17943055927753448, 0.17462696135044098, 0.15240153670310974, 0.18167224526405334, 0.16574636101722717, 0.18909765779972076, 0.18297897279262543, 0.16744837164878845, 0.1802225559949875, 0.16326650977134705, 0.16896973550319672, 0.16421952843666077, 0.18670891225337982, 0.17050611972808838, 0.16669431328773499, 0.1702990084886551, 0.1663564145565033, 0.147310271859169, 0.17727115750312805, 0.16365158557891846, 0.18272005021572113, 0.17916113138198853, 0.17111161351203918, 0.18384182453155518, 0.17140676081180573, 0.15629398822784424, 0.19895727932453156, 0.1687343418598175, 0.17546500265598297, 0.17615456879138947, 0.16174136102199554, 0.16949695348739624, 0.14612990617752075, 0.1615656167268753, 0.17522242665290833, 0.15802167356014252, 0.16052140295505524, 0.16045358777046204, 0.1968877762556076, 0.14444907009601593, 0.15250955522060394, 0.17544128000736237, 0.20796893537044525, 0.1925985962152481, 0.1977696269750595, 0.19429317116737366, 0.1822046935558319, 0.1784045398235321, 0.14409710466861725, 0.19113479554653168, 0.1562887579202652, 0.1927500069141388, 0.15854698419570923, 0.17169153690338135, 0.1849711686372757, 0.18652528524398804, 0.17163656651973724, 0.18550336360931396, 0.175106942653656, 0.17464369535446167, 0.16044937074184418, 0.18680866062641144, 0.16083113849163055, 0.20528988540172577, 0.18899762630462646, 0.1542380154132843, 0.16133011877536774, 0.1461474746465683, 0.17337428033351898, 0.18425385653972626, 0.1613679826259613, 0.18941497802734375, 0.19251015782356262, 0.16186322271823883, 0.1767909973859787, 0.17507800459861755, 0.1672445386648178, 0.18311092257499695, 0.19463950395584106, 0.1761917620897293, 0.17917406558990479, 0.19057877361774445, 0.1693856120109558, 0.18259455263614655, 0.18477597832679749, 0.20277142524719238, 0.17574353516101837, 0.1885475218296051, 0.1665707230567932, 0.157830610871315, 0.18252649903297424, 0.16867801547050476, 0.1489993929862976, 0.16905851662158966, 0.1884033977985382, 0.16970613598823547, 0.1695757508277893, 0.17285487055778503, 0.1999683976173401, 0.18044395744800568, 0.16115418076515198, 0.1640506237745285, 0.1630147099494934, 0.17366774380207062, 0.18362155556678772, 0.1821059286594391, 0.17400424182415009, 0.1503538340330124, 0.19896800816059113, 0.19885428249835968, 0.17218558490276337, 0.18595610558986664, 0.1903623342514038, 0.1736193150281906, 0.17364567518234253, 0.17581842839717865, 0.15225747227668762, 0.16041478514671326, 0.18066789209842682, 0.16763633489608765, 0.1789073348045349, 0.17420203983783722, 0.16580717265605927, 0.16383017599582672, 0.152456596493721, 0.1815248429775238, 0.16457727551460266, 0.19689494371414185, 0.19092532992362976, 0.1662735790014267, 0.1521785408258438, 0.16622211039066315, 0.16117413341999054, 0.18572883307933807, 0.18268311023712158, 0.17926299571990967, 0.15177835524082184, 0.16050156950950623, 0.17955158650875092, 0.17723935842514038, 0.17807337641716003, 0.1597033441066742, 0.1913670301437378, 0.1849886178970337, 0.17745065689086914, 0.16976317763328552, 0.17603261768817902, 0.18671497702598572, 0.1741109937429428, 0.1763789802789688, 0.17612983286380768, 0.15987692773342133, 0.18346650898456573, 0.15882889926433563, 0.17212188243865967, 0.17615830898284912, 0.18611349165439606, 0.16028356552124023, 0.17287249863147736, 0.16079221665859222, 0.17462415993213654, 0.16922447085380554, 0.1675494909286499, 0.18004055321216583, 0.1739310622215271, 0.18623021245002747, 0.1575554758310318, 0.16780254244804382, 0.17209458351135254, 0.16585136950016022, 0.17633862793445587, 0.1524447351694107, 0.15565195679664612, 0.1741277277469635, 0.16305316984653473, 0.15678155422210693, 0.18787646293640137, 0.16685010492801666, 0.15459533035755157, 0.19775353372097015, 0.18542088568210602, 0.1910828948020935, 0.1828843504190445, 0.1633865237236023, 0.17174243927001953, 0.1552737057209015, 0.1773049384355545, 0.15731032192707062, 0.17524093389511108, 0.18913434445858002, 0.18073564767837524, 0.18487083911895752, 0.1733217090368271, 0.17297033965587616, 0.16313084959983826, 0.175956591963768, 0.16892029345035553, 0.1710381954908371, 0.15266527235507965, 0.1715027540922165, 0.16063864529132843, 0.163139209151268, 0.1767510622739792, 0.18541647493839264, 0.1750345528125763, 0.16534090042114258, 0.15622538328170776, 0.15706822276115417, 0.19699178636074066, 0.1815917044878006, 0.16477476060390472, 0.17461076378822327, 0.15402185916900635, 0.18229378759860992, 0.18406058847904205, 0.1520557999610901, 0.18959146738052368, 0.17427842319011688, 0.19209885597229004, 0.18475523591041565, 0.16077333688735962, 0.18031011521816254, 0.15941853821277618, 0.17799687385559082, 0.17886066436767578, 0.176510751247406, 0.17441222071647644, 0.17273028194904327, 0.16010934114456177, 0.17190217971801758, 0.15338073670864105, 0.17178399860858917, 0.1718963384628296, 0.15903395414352417, 0.15331122279167175, 0.18551169335842133, 0.16566503047943115, 0.19566011428833008, 0.18579821288585663, 0.1927989274263382, 0.15592683851718903, 0.18033739924430847, 0.15402084589004517, 0.17706429958343506, 0.1615777462720871, 0.15728744864463806, 0.16382744908332825, 0.18199698626995087, 0.16858796775341034, 0.18530787527561188, 0.1655735820531845, 0.16038472950458527, 0.16760550439357758, 0.17164847254753113, 0.16008137166500092, 0.18150168657302856, 0.16982275247573853, 0.15894745290279388, 0.1638226956129074, 0.1779472976922989, 0.17360977828502655, 0.16961707174777985, 0.1767505705356598, 0.18340390920639038, 0.16048398613929749, 0.1928132325410843, 0.1520453840494156, 0.1812726855278015, 0.17164544761180878, 0.16774137318134308, 0.16345053911209106, 0.1971183866262436, 0.170523539185524, 0.1969088315963745, 0.1547824591398239, 0.16632044315338135, 0.1537247598171234, 0.18683819472789764, 0.19632777571678162, 0.15165124833583832, 0.16321326792240143, 0.16201746463775635, 0.17563454806804657, 0.17448721826076508, 0.19618012011051178, 0.15462641417980194, 0.1674899011850357, 0.1860940158367157, 0.16206985712051392, 0.1748640537261963, 0.18489807844161987, 0.18362924456596375, 0.16951806843280792, 0.1580963134765625, 0.16499589383602142, 0.16543589532375336, 0.19085712730884552, 0.19593198597431183, 0.19505473971366882, 0.17630982398986816, 0.1678857058286667, 0.20160600543022156, 0.1748068779706955, 0.17690598964691162, 0.17189784348011017, 0.17943772673606873, 0.17639334499835968, 0.1549781858921051, 0.15426339209079742, 0.1715174913406372, 0.1763686090707779, 0.18176326155662537, 0.16742075979709625, 0.15061713755130768, 0.16962169110774994, 0.1590629518032074, 0.15361498296260834, 0.19750991463661194, 0.1660929024219513, 0.17202472686767578, 0.171070858836174, 0.16364413499832153, 0.18572522699832916, 0.1854316145181656, 0.18653251230716705, 0.1705690622329712, 0.1969447135925293, 0.1781459003686905, 0.17473718523979187, 0.19501686096191406, 0.1811242550611496, 0.19143381714820862, 0.1733592003583908, 0.17501848936080933, 0.16721609234809875, 0.1551644504070282, 0.15402185916900635, 0.17166286706924438, 0.18354813754558563, 0.14627481997013092, 0.171700581908226, 0.20888927578926086, 0.1696634590625763, 0.16157153248786926, 0.18757785856723785, 0.1685819774866104, 0.17955443263053894, 0.15803690254688263, 0.17290231585502625, 0.18332448601722717, 0.18790428340435028, 0.1667580008506775, 0.18874143064022064, 0.18368762731552124, 0.19930841028690338, 0.1790631264448166, 0.1737208068370819, 0.16977976262569427, 0.18708738684654236, 0.16423636674880981, 0.19288545846939087, 0.1822926551103592, 0.1808495670557022, 0.1968715786933899, 0.15491655468940735, 0.17608270049095154, 0.18745695054531097, 0.17756155133247375, 0.1593550145626068, 0.16260823607444763, 0.17548832297325134, 0.1644832193851471, 0.1640368103981018, 0.16319593787193298, 0.18446865677833557, 0.15500430762767792, 0.1676802784204483, 0.17303206026554108, 0.1813662201166153, 0.18519045412540436, 0.18309791386127472, 0.17203398048877716, 0.1679701954126358, 0.16254284977912903, 0.18170563876628876, 0.18258024752140045, 0.1753528118133545, 0.1578858643770218, 0.1985853910446167, 0.18024499714374542, 0.1700880229473114, 0.1861228048801422, 0.1692914515733719, 0.16595745086669922, 0.1678905040025711, 0.1678282767534256, 0.16404053568840027, 0.17251871526241302, 0.18161575496196747, 0.16563734412193298, 0.17629270255565643, 0.16544806957244873, 0.1689479500055313, 0.15462933480739594, 0.16501981019973755, 0.1712440848350525, 0.16945506632328033, 0.154107004404068, 0.17161643505096436, 0.17308498919010162, 0.18423117697238922, 0.15600204467773438, 0.17643994092941284, 0.17651575803756714, 0.1712171733379364, 0.15589040517807007, 0.1758188009262085, 0.1709509789943695, 0.16717056930065155, 0.19640950858592987, 0.1696646511554718, 0.16991287469863892, 0.16860279440879822, 0.17664478719234467, 0.1685304045677185, 0.18048003315925598, 0.1522306501865387, 0.1903417855501175, 0.17484940588474274, 0.19147856533527374, 0.18803933262825012, 0.1658005267381668, 0.18746092915534973, 0.19733650982379913, 0.1971571147441864, 0.16462580859661102, 0.16799746453762054, 0.1963825225830078, 0.15278315544128418, 0.1549801379442215, 0.1648091822862625, 0.18121939897537231, 0.1600046455860138, 0.15799015760421753, 0.17256329953670502, 0.19085682928562164, 0.17016185820102692, 0.18378782272338867, 0.17260512709617615, 0.18373416364192963, 0.1869070678949356, 0.15568795800209045, 0.19224077463150024, 0.18176880478858948, 0.16928158700466156, 0.19807428121566772, 0.1569846123456955, 0.15891654789447784, 0.18619996309280396, 0.1697639375925064, 0.18133968114852905, 0.16453367471694946, 0.18141432106494904, 0.1796564757823944, 0.16579321026802063, 0.1615920066833496, 0.17314296960830688, 0.16032476723194122, 0.19082048535346985, 0.17655642330646515, 0.1677435040473938, 0.1624811887741089, 0.17232374846935272, 0.18194705247879028, 0.16945073008537292, 0.16776739060878754, 0.16164419054985046, 0.17088519036769867, 0.16061151027679443, 0.16455528140068054, 0.17980606853961945, 0.1811622679233551, 0.15454106032848358, 0.18224866688251495, 0.16855469346046448, 0.1777810901403427, 0.1639624685049057, 0.15822942554950714, 0.19416069984436035, 0.17729884386062622, 0.18239809572696686, 0.1760888248682022, 0.158691868185997, 0.15987376868724823, 0.18375790119171143]\n",
            "Val loss 0.1719178671836853\n",
            "Val auc roc 0.5\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7e09f47f1594acda7583b0901e04e67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2465.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1726\n",
            "Train Losses : [0.17509473860263824, 0.1827239990234375, 0.1735062301158905, 0.17943879961967468, 0.1667972356081009, 0.17067740857601166, 0.16171343624591827, 0.15463484823703766, 0.18110321462154388, 0.1939181685447693, 0.15580767393112183, 0.17774687707424164, 0.17705674469470978, 0.17071163654327393, 0.1701239049434662, 0.17788101732730865, 0.17319343984127045, 0.16299638152122498, 0.18593071401119232, 0.15465323626995087, 0.1691075712442398, 0.19383253157138824, 0.16436384618282318, 0.16837964951992035, 0.17334581911563873, 0.15468332171440125, 0.1746876984834671, 0.18684621155261993, 0.18029853701591492, 0.16944268345832825, 0.16561566293239594, 0.1643170267343521, 0.15184468030929565, 0.17187181115150452, 0.16978278756141663, 0.1546146720647812, 0.1828189641237259, 0.16603972017765045, 0.1655186414718628, 0.17227546870708466, 0.17156405746936798, 0.19403831660747528, 0.16356194019317627, 0.18515166640281677, 0.18616974353790283, 0.1546933501958847, 0.17192745208740234, 0.18714095652103424, 0.15445393323898315, 0.1811559498310089, 0.1597549319267273, 0.1598818600177765, 0.1737140566110611, 0.18147528171539307, 0.19400902092456818, 0.15766550600528717, 0.1883794367313385, 0.1893301010131836, 0.1751227229833603, 0.17570467293262482, 0.15292824804782867, 0.17527365684509277, 0.16750799119472504, 0.16504372656345367, 0.16633383929729462, 0.16004735231399536, 0.16879969835281372, 0.1985262781381607, 0.18171800673007965, 0.18545927107334137, 0.1738022416830063, 0.16954979300498962, 0.17438963055610657, 0.17752820253372192, 0.1772323101758957, 0.17566417157649994, 0.18028220534324646, 0.16177897155284882, 0.17509014904499054, 0.164642333984375, 0.1530163288116455, 0.18066933751106262, 0.17580005526542664, 0.16815946996212006, 0.15467673540115356, 0.16574950516223907, 0.194177508354187, 0.17186035215854645, 0.17970030009746552, 0.16986708343029022, 0.1530538946390152, 0.17172594368457794, 0.1768144965171814, 0.17410099506378174, 0.17536434531211853, 0.1696237176656723, 0.19429954886436462, 0.1679994761943817, 0.176255464553833, 0.17969703674316406, 0.1620510071516037, 0.17401036620140076, 0.18513818085193634, 0.17058852314949036, 0.17123067378997803, 0.17071720957756042, 0.18556658923625946, 0.17395631968975067, 0.1724512279033661, 0.19283896684646606, 0.19081926345825195, 0.16735781729221344, 0.16438250243663788, 0.16668297350406647, 0.16185207664966583, 0.17831793427467346, 0.16914427280426025, 0.16027525067329407, 0.1743285059928894, 0.1642906814813614, 0.19014474749565125, 0.15957197546958923, 0.1876673400402069, 0.1815527230501175, 0.168705016374588, 0.17689640820026398, 0.15455563366413116, 0.17850135266780853, 0.1665310114622116, 0.16005203127861023, 0.15918977558612823, 0.17996281385421753, 0.17064811289310455, 0.1627141833305359, 0.17281325161457062, 0.17791391909122467, 0.16046465933322906, 0.1767577826976776, 0.16640396416187286, 0.18883632123470306, 0.1940876543521881, 0.15442439913749695, 0.18068596720695496, 0.16557781398296356, 0.16132043302059174, 0.16854627430438995, 0.1790345460176468, 0.15867726504802704, 0.17872653901576996, 0.18456223607063293, 0.15903043746948242, 0.15452536940574646, 0.17009587585926056, 0.19410942494869232, 0.15442800521850586, 0.17882125079631805, 0.16963352262973785, 0.17197634279727936, 0.1711024045944214, 0.17471224069595337, 0.17171449959278107, 0.19419117271900177, 0.18124711513519287, 0.19419342279434204, 0.1858483999967575, 0.18850745260715485, 0.18011653423309326, 0.18272803723812103, 0.1545281559228897, 0.18522803485393524, 0.1873851716518402, 0.18199360370635986, 0.19397732615470886, 0.16509297490119934, 0.1640218198299408, 0.17196261882781982, 0.18204133212566376, 0.18102402985095978, 0.16698606312274933, 0.16546128690242767, 0.1722581386566162, 0.18795259296894073, 0.18251612782478333, 0.1938520222902298, 0.18442176282405853, 0.1616097241640091, 0.16270814836025238, 0.19110751152038574, 0.1744607537984848, 0.18586046993732452, 0.17074482142925262, 0.17163068056106567, 0.15476840734481812, 0.17764519155025482, 0.16729994118213654, 0.18081842362880707, 0.1750749796628952, 0.1767345815896988, 0.18005892634391785, 0.16548584401607513, 0.19388075172901154, 0.1548762023448944, 0.1936563104391098, 0.1724010854959488, 0.180466428399086, 0.1628597229719162, 0.16666822135448456, 0.16788312792778015, 0.17735330760478973, 0.16873663663864136, 0.1711948662996292, 0.17717799544334412, 0.17239491641521454, 0.16536031663417816, 0.16576990485191345, 0.1728411316871643, 0.18137569725513458, 0.18309688568115234, 0.17998012900352478, 0.18170827627182007, 0.18701626360416412, 0.1629725843667984, 0.1760251373052597, 0.16628161072731018, 0.17645026743412018, 0.15490013360977173, 0.16712686419487, 0.17933543026447296, 0.17441584169864655, 0.1650606244802475, 0.18277978897094727, 0.16220210492610931, 0.1769585758447647, 0.15492568910121918, 0.17406688630580902, 0.18723420798778534, 0.1622166484594345, 0.15473002195358276, 0.1652289479970932, 0.15469759702682495, 0.1688718944787979, 0.19293706119060516, 0.15472723543643951, 0.19395531713962555, 0.16260558366775513, 0.17184960842132568, 0.16352355480194092, 0.16636407375335693, 0.15094341337680817, 0.18208296597003937, 0.16634249687194824, 0.15464900434017181, 0.1775294691324234, 0.17417308688163757, 0.17867408692836761, 0.19388727843761444, 0.16615979373455048, 0.17438846826553345, 0.1665695607662201, 0.17603445053100586, 0.18011528253555298, 0.17279350757598877, 0.15461421012878418, 0.18647746741771698, 0.16696394979953766, 0.1790531426668167, 0.16424089670181274, 0.19417282938957214, 0.16669298708438873, 0.16263380646705627, 0.16456085443496704, 0.15454933047294617, 0.1817045956850052, 0.16910511255264282, 0.1829378306865692, 0.19397875666618347, 0.18775980174541473, 0.15460868179798126, 0.17129936814308167, 0.1821075677871704, 0.17680686712265015, 0.16938002407550812, 0.19585055112838745, 0.1614757776260376, 0.16950365900993347, 0.15764518082141876, 0.15982219576835632, 0.1942097544670105, 0.15824295580387115, 0.17689062654972076, 0.1635509431362152, 0.16290195286273956, 0.16959574818611145, 0.16650599241256714, 0.17104431986808777, 0.1588309109210968, 0.1759253740310669, 0.15457236766815186, 0.1745385378599167, 0.18687191605567932, 0.1756478250026703, 0.15574951469898224, 0.16511528193950653, 0.16647909581661224, 0.1543334573507309, 0.17504002153873444, 0.19006229937076569, 0.17283351719379425, 0.1660226583480835, 0.1542470008134842, 0.17710810899734497, 0.16839924454689026, 0.1904207319021225, 0.1652713268995285, 0.16709381341934204, 0.18541093170642853, 0.18405909836292267, 0.15712004899978638, 0.1646999567747116, 0.16687482595443726, 0.17353397607803345, 0.16805364191532135, 0.16144196689128876, 0.17574846744537354, 0.1707712560892105, 0.1540277600288391, 0.15934866666793823, 0.1944768726825714, 0.15402141213417053, 0.1642499417066574, 0.15402284264564514, 0.1888676881790161, 0.1740877330303192, 0.1747099608182907, 0.17619480192661285, 0.18929748237133026, 0.16827812790870667, 0.15392827987670898, 0.16241563856601715, 0.1701544225215912, 0.16654357314109802, 0.17484942078590393, 0.1703244298696518, 0.18157444894313812, 0.17803455889225006, 0.173751100897789, 0.16229183971881866, 0.15291231870651245, 0.1784156709909439, 0.164137601852417, 0.18887969851493835, 0.1596645563840866, 0.1755933314561844, 0.17143650352954865, 0.16885411739349365, 0.187580868601799, 0.16938330233097076, 0.1697607785463333, 0.17091526091098785, 0.15648387372493744, 0.1536204218864441, 0.19514884054660797, 0.17055867612361908, 0.19502738118171692, 0.16231794655323029, 0.1632855087518692, 0.1788797825574875, 0.1636972576379776, 0.16151133179664612, 0.17249612510204315, 0.1624649614095688, 0.1866523176431656, 0.16607259213924408, 0.17211765050888062, 0.17055663466453552, 0.15351077914237976, 0.17934805154800415, 0.180807963013649, 0.18098357319831848, 0.19520099461078644, 0.18113166093826294, 0.19274498522281647, 0.1535276621580124, 0.16993877291679382, 0.1752697378396988, 0.1614113450050354, 0.18771231174468994, 0.1534997522830963, 0.1534496396780014, 0.17123864591121674, 0.15682214498519897, 0.18536025285720825, 0.1534104198217392, 0.18563172221183777, 0.15431246161460876, 0.18076065182685852, 0.18461871147155762, 0.1665373593568802, 0.17279702425003052, 0.1535412073135376, 0.1805674284696579, 0.17923535406589508, 0.18884289264678955, 0.17793217301368713, 0.17380626499652863, 0.18925869464874268, 0.1883603036403656, 0.1772819310426712, 0.1953059732913971, 0.16855907440185547, 0.1614307314157486, 0.16539186239242554, 0.19524720311164856, 0.151846781373024, 0.1844777911901474, 0.17796121537685394, 0.15431654453277588, 0.17611582577228546, 0.17793147265911102, 0.16623958945274353, 0.18114829063415527, 0.15692217648029327, 0.1649499088525772, 0.17763759195804596, 0.1835232824087143, 0.16103509068489075, 0.1673346906900406, 0.1823575347661972, 0.17413514852523804, 0.1732931286096573, 0.17510826885700226, 0.1679190993309021, 0.1845812052488327, 0.1647179275751114, 0.15427058935165405, 0.1550857126712799, 0.16809862852096558, 0.16745340824127197, 0.18070727586746216, 0.16809029877185822, 0.17943193018436432, 0.18128947913646698, 0.17616982758045197, 0.19505271315574646, 0.17431695759296417, 0.18809159100055695, 0.15379886329174042, 0.18108901381492615, 0.15357054769992828, 0.17946122586727142, 0.18511705100536346, 0.17432168126106262, 0.16909675300121307, 0.17786121368408203, 0.16925597190856934, 0.15936587750911713, 0.18449613451957703, 0.16194820404052734, 0.1639155149459839, 0.18867820501327515, 0.1536460518836975, 0.19061149656772614, 0.16415530443191528, 0.16510048508644104, 0.17552994191646576, 0.17878636717796326, 0.17194777727127075, 0.17345036566257477, 0.1763293743133545, 0.16244453191757202, 0.16295509040355682, 0.19491438567638397, 0.1859283745288849, 0.1754421591758728, 0.16977694630622864, 0.19181913137435913, 0.16647295653820038, 0.16627855598926544, 0.17826774716377258, 0.1689750999212265, 0.16415151953697205, 0.18076929450035095, 0.1895478367805481, 0.16463159024715424, 0.17314957082271576, 0.16656087338924408, 0.19621708989143372, 0.180682972073555, 0.17575733363628387, 0.18317252397537231, 0.17503070831298828, 0.18051575124263763, 0.16572706401348114, 0.18090280890464783, 0.19142258167266846, 0.16244302690029144, 0.18735627830028534, 0.1762390434741974, 0.18372909724712372, 0.1731187254190445, 0.16214413940906525, 0.15952500700950623, 0.18632079660892487, 0.164574533700943, 0.17969092726707458, 0.19465193152427673, 0.15704569220542908, 0.17788255214691162, 0.18093900382518768, 0.18932947516441345, 0.17463909089565277, 0.16755907237529755, 0.15325981378555298, 0.1540660858154297, 0.18508367240428925, 0.17462840676307678, 0.17240257561206818, 0.194455087184906, 0.17538559436798096, 0.1654457151889801, 0.1722295880317688, 0.17565789818763733, 0.1594811975955963, 0.16362091898918152, 0.18155084550380707, 0.1543244868516922, 0.17334496974945068, 0.16659855842590332, 0.15763388574123383, 0.17918318510055542, 0.18240322172641754, 0.17281001806259155, 0.1685107946395874, 0.17674744129180908, 0.16734983026981354, 0.16707922518253326, 0.16848021745681763, 0.19430242478847504, 0.1736093908548355, 0.18580631911754608, 0.1673736572265625, 0.163778156042099, 0.1703580617904663, 0.159005805850029, 0.1722995936870575, 0.17321263253688812, 0.18355420231819153, 0.16420117020606995, 0.17423079907894135, 0.1543140858411789, 0.1832820326089859, 0.16598345339298248, 0.1770925670862198, 0.16298142075538635, 0.15983742475509644, 0.18437862396240234, 0.16399087011814117, 0.1659545600414276, 0.15737447142601013, 0.17129981517791748, 0.17352713644504547, 0.1777472198009491, 0.18176567554473877, 0.15402577817440033, 0.1679774671792984, 0.16796885430812836, 0.15405970811843872, 0.194456547498703, 0.18275104463100433, 0.1640220284461975, 0.18144775927066803, 0.152467280626297, 0.19454777240753174, 0.17432324588298798, 0.18921132385730743, 0.17230570316314697, 0.15403862297534943, 0.16330541670322418, 0.19047117233276367, 0.16496962308883667, 0.17911040782928467, 0.17029644548892975, 0.18687395751476288, 0.18965043127536774, 0.17003758251667023, 0.173138827085495, 0.17322179675102234, 0.17808426916599274, 0.16156157851219177, 0.18353179097175598, 0.19528692960739136, 0.16669175028800964, 0.17697636783123016, 0.17150795459747314, 0.1658526510000229, 0.18758615851402283, 0.16181214153766632, 0.17427025735378265, 0.16196227073669434, 0.18389536440372467, 0.17384561896324158, 0.16237381100654602, 0.16149604320526123, 0.17546789348125458, 0.17694197595119476, 0.15411193668842316, 0.16164079308509827, 0.1558936983346939, 0.1836172491312027, 0.1918134242296219, 0.15421801805496216, 0.1715206503868103, 0.1797703057527542, 0.19452083110809326, 0.15411341190338135, 0.18440477550029755, 0.17938078939914703, 0.17479170858860016, 0.16203473508358002, 0.15828722715377808, 0.15836888551712036, 0.19440346956253052, 0.16765441000461578, 0.1822441965341568, 0.18258550763130188, 0.17238742113113403, 0.16388839483261108, 0.15794873237609863, 0.17040741443634033, 0.15423451364040375, 0.1604195386171341, 0.1667362004518509, 0.18097996711730957, 0.15506884455680847, 0.1803954839706421, 0.16649654507637024, 0.18312117457389832, 0.17735357582569122, 0.15407142043113708, 0.1727019101381302, 0.17955493927001953, 0.19156703352928162, 0.17354066669940948, 0.1963239312171936, 0.15550978481769562, 0.16275686025619507, 0.15417632460594177, 0.16425588726997375, 0.16032275557518005, 0.18432413041591644, 0.15398737788200378, 0.192891463637352, 0.17799274623394012, 0.17611484229564667, 0.16350367665290833, 0.18096576631069183, 0.17367616295814514, 0.17131799459457397, 0.18064162135124207, 0.1660279482603073, 0.1675032377243042, 0.15566733479499817, 0.1717972606420517, 0.17390425503253937, 0.19469431042671204, 0.15683235228061676, 0.1612510085105896, 0.1679249256849289, 0.16307969391345978, 0.1864168345928192, 0.15404775738716125, 0.1701449602842331, 0.17176945507526398, 0.19476380944252014, 0.17510122060775757, 0.18328140676021576, 0.16332855820655823, 0.16218535602092743, 0.15745387971401215, 0.18712075054645538, 0.1744968444108963, 0.18028047680854797, 0.19463536143302917, 0.1858886182308197, 0.1539236605167389, 0.1755581647157669, 0.16745461523532867, 0.16494326293468475, 0.18408755958080292, 0.1847134679555893, 0.1833248883485794, 0.16390973329544067, 0.1763342320919037, 0.15410062670707703, 0.19464154541492462, 0.1946784406900406, 0.15396521985530853, 0.19000454246997833, 0.17087696492671967, 0.1651567816734314, 0.18785470724105835, 0.1555270105600357, 0.16861951351165771, 0.164528489112854, 0.17900347709655762, 0.17188720405101776, 0.16486519575119019, 0.1772027462720871, 0.1583939492702484, 0.16730424761772156, 0.17356646060943604, 0.15398387610912323, 0.17804881930351257, 0.16062207520008087, 0.17080657184123993, 0.16866733133792877, 0.16605685651302338, 0.18970011174678802, 0.1797202229499817, 0.17447669804096222, 0.16975392401218414, 0.1650068759918213, 0.1599298119544983, 0.17182056605815887, 0.16105452179908752, 0.1724255234003067, 0.17433065176010132, 0.1750408560037613, 0.16131706535816193, 0.18483154475688934, 0.17332576215267181, 0.1947149783372879, 0.15804076194763184, 0.18270784616470337, 0.18104785680770874, 0.16264280676841736, 0.17777970433235168, 0.1644742488861084, 0.1941446214914322, 0.16227731108665466, 0.16993099451065063, 0.18288099765777588, 0.16620834171772003, 0.17663633823394775, 0.17348429560661316, 0.16158170998096466, 0.19471076130867004, 0.15393607318401337, 0.1801673322916031, 0.17766626179218292, 0.17446017265319824, 0.153899148106575, 0.18622751533985138, 0.16197587549686432, 0.1548043191432953, 0.18628861010074615, 0.1731361597776413, 0.1718081533908844, 0.17476952075958252, 0.15415754914283752, 0.18173561990261078, 0.16874298453330994, 0.17481063306331635, 0.18588370084762573, 0.15451443195343018, 0.167946919798851, 0.18071021139621735, 0.1723601520061493, 0.18261121213436127, 0.15399162471294403, 0.17010800540447235, 0.18010416626930237, 0.17778068780899048, 0.1831642985343933, 0.184196338057518, 0.1726987212896347, 0.19099821150302887, 0.19484485685825348, 0.19471348822116852, 0.17241311073303223, 0.17273405194282532, 0.18360772728919983, 0.15762050449848175, 0.18541750311851501, 0.17483839392662048, 0.1724318563938141, 0.15423044562339783, 0.16989752650260925, 0.1541557013988495, 0.1742745041847229, 0.17636573314666748, 0.16970635950565338, 0.17901670932769775, 0.19014869630336761, 0.1874261498451233, 0.1740395426750183, 0.16824021935462952, 0.1846480667591095, 0.1955781877040863, 0.1788807362318039, 0.17208673059940338, 0.15415510535240173, 0.17759445309638977, 0.17321327328681946, 0.17132949829101562, 0.167763352394104, 0.18069182336330414, 0.15293192863464355, 0.19197919964790344, 0.1742265820503235, 0.16681599617004395, 0.1783866584300995, 0.161390483379364, 0.17083516716957092, 0.18530674278736115, 0.1747095286846161, 0.1541813611984253, 0.17678098380565643, 0.1541115939617157, 0.16583752632141113, 0.19444432854652405, 0.1855018138885498, 0.17841488122940063, 0.19315221905708313, 0.15073657035827637, 0.17226731777191162, 0.18574543297290802, 0.17542968690395355, 0.16716767847537994, 0.16023394465446472, 0.15728624165058136, 0.16249337792396545, 0.18941208720207214, 0.16829349100589752, 0.1840299367904663, 0.18091590702533722, 0.18416045606136322, 0.1726152002811432, 0.1769692450761795, 0.171847864985466, 0.1660175621509552, 0.16857083141803741, 0.1576719731092453, 0.1695616990327835, 0.16197390854358673, 0.15862777829170227, 0.19429747760295868, 0.15532545745372772, 0.16494931280612946, 0.19118039309978485, 0.18444208800792694, 0.1669551134109497, 0.1563563048839569, 0.1787564903497696, 0.15844862163066864, 0.17732016742229462, 0.17438197135925293, 0.1681700497865677, 0.16140200197696686, 0.16388677060604095, 0.17198412120342255, 0.19434437155723572, 0.1865287572145462, 0.18026389181613922, 0.1542518585920334, 0.15948207676410675, 0.15937726199626923, 0.19025394320487976, 0.17503739893436432, 0.185799703001976, 0.1803470253944397, 0.16408376395702362, 0.1541876643896103, 0.16328124701976776, 0.17453353106975555, 0.19433410465717316, 0.15421175956726074, 0.16898410022258759, 0.17091171443462372, 0.15418656170368195, 0.1693023294210434, 0.15420116484165192, 0.15412704646587372, 0.15969425439834595, 0.17561714351177216, 0.17345348000526428, 0.18463745713233948, 0.1839975118637085, 0.16849997639656067, 0.17537227272987366, 0.18043042719364166, 0.15404492616653442, 0.17227745056152344, 0.17049264907836914, 0.15403594076633453, 0.18621696531772614, 0.1946878433227539, 0.18243053555488586, 0.19455333054065704, 0.18484677374362946, 0.17734161019325256, 0.15930376946926117, 0.16806481778621674, 0.16060781478881836, 0.16308459639549255, 0.18249952793121338, 0.1539328694343567, 0.1724269539117813, 0.15350621938705444, 0.18505269289016724, 0.16728390753269196, 0.1540919840335846, 0.16022756695747375, 0.16492275893688202, 0.18154406547546387, 0.15402476489543915, 0.1887975037097931, 0.18755634129047394, 0.16612979769706726, 0.18303044140338898, 0.1947232335805893, 0.18345339596271515, 0.16462524235248566, 0.16361317038536072, 0.17525123059749603, 0.1621740609407425, 0.16830092668533325, 0.17338652908802032, 0.19458891451358795, 0.1624567061662674, 0.1849994659423828, 0.16538691520690918, 0.16957177221775055, 0.16929063200950623, 0.1539217233657837, 0.15571652352809906, 0.1539035588502884, 0.15676173567771912, 0.1541946977376938, 0.15383031964302063, 0.16639447212219238, 0.1676582247018814, 0.16115619242191315, 0.17664897441864014, 0.1538086086511612, 0.15379248559474945, 0.16165737807750702, 0.18917424976825714, 0.15369604527950287, 0.1539260596036911, 0.16295041143894196, 0.16350701451301575, 0.16295801103115082, 0.17018617689609528, 0.1663993000984192, 0.17830227315425873, 0.1751885861158371, 0.1698305308818817, 0.15316332876682281, 0.18069404363632202, 0.18177220225334167, 0.18098093569278717, 0.1644124537706375, 0.18990673124790192, 0.1804424524307251, 0.18600329756736755, 0.18249329924583435, 0.18239960074424744, 0.18496720492839813, 0.1860981583595276, 0.1488035023212433, 0.18054348230361938, 0.1951943039894104, 0.1682785451412201, 0.1697775274515152, 0.1592475324869156, 0.15697678923606873, 0.18379279971122742, 0.16370460391044617, 0.17586536705493927, 0.1810687631368637, 0.16401682794094086, 0.1607135683298111, 0.1605999618768692, 0.17831043899059296, 0.1637662649154663, 0.16509172320365906, 0.17419809103012085, 0.1763114631175995, 0.17824065685272217, 0.18982341885566711, 0.1669495850801468, 0.15714502334594727, 0.164678692817688, 0.18075768649578094, 0.19526372849941254, 0.16308215260505676, 0.20022907853126526, 0.15363861620426178, 0.17259646952152252, 0.16244465112686157, 0.1744174212217331, 0.15356263518333435, 0.17500437796115875, 0.1562308520078659, 0.17476776242256165, 0.1846153438091278, 0.15368573367595673, 0.17869508266448975, 0.17281925678253174, 0.15352411568164825, 0.17564056813716888, 0.15478982031345367, 0.1529478132724762, 0.16824866831302643, 0.1534525603055954, 0.1771480292081833, 0.16642403602600098, 0.16560886800289154, 0.15347205102443695, 0.18896366655826569, 0.17979836463928223, 0.19170598685741425, 0.16228197515010834, 0.16656531393527985, 0.18185466527938843, 0.15993987023830414, 0.18214155733585358, 0.16056153178215027, 0.1533006876707077, 0.1688503473997116, 0.15940825641155243, 0.16237770020961761, 0.17072682082653046, 0.17989590764045715, 0.16638046503067017, 0.1517987847328186, 0.17889682948589325, 0.18356956541538239, 0.15913692116737366, 0.1896325945854187, 0.18851076066493988, 0.16336241364479065, 0.16557790338993073, 0.16668574512004852, 0.18021127581596375, 0.1677704006433487, 0.17081324756145477, 0.1758490800857544, 0.16531360149383545, 0.16653543710708618, 0.17105171084403992, 0.19291548430919647, 0.17286567389965057, 0.18676942586898804, 0.18999749422073364, 0.15337656438350677, 0.1585940569639206, 0.17431223392486572, 0.17290090024471283, 0.15546546876430511, 0.1922760307788849, 0.15741825103759766, 0.1532520353794098, 0.177352175116539, 0.1844278872013092, 0.1650812327861786, 0.1817121058702469, 0.15331923961639404, 0.15552356839179993, 0.153297558426857, 0.17190057039260864, 0.17229372262954712, 0.18222977221012115, 0.1532081812620163, 0.17880560457706451, 0.17986264824867249, 0.17715086042881012, 0.17818613350391388, 0.1955079287290573, 0.1533176302909851, 0.1699085235595703, 0.1580895036458969, 0.18250592052936554, 0.16955994069576263, 0.17133377492427826, 0.17396914958953857, 0.15313313901424408, 0.16352578997612, 0.1654205322265625, 0.187448188662529, 0.16749228537082672, 0.1505345106124878, 0.1558307260274887, 0.19217415153980255, 0.16196207702159882, 0.19566407799720764, 0.18138237297534943, 0.18320226669311523, 0.18397901952266693, 0.17305400967597961, 0.16330020129680634, 0.17389129102230072, 0.16288520395755768, 0.17082639038562775, 0.18403060734272003, 0.19581973552703857, 0.1648205816745758, 0.17630955576896667, 0.17823266983032227, 0.17978917062282562, 0.1955065280199051, 0.1634262055158615, 0.18769089877605438, 0.18987390398979187, 0.1605953425168991, 0.1644974797964096, 0.18771055340766907, 0.1613507866859436, 0.16374032199382782, 0.17765925824642181, 0.17983396351337433, 0.1891382783651352, 0.16891542077064514, 0.187318816781044, 0.17110058665275574, 0.18508280813694, 0.18606004118919373, 0.16852428019046783, 0.15363557636737823, 0.18437440693378448, 0.1572229117155075, 0.19546478986740112, 0.18165989220142365, 0.18118950724601746, 0.1534550040960312, 0.16851544380187988, 0.18790048360824585, 0.15984952449798584, 0.15327636897563934, 0.17088104784488678, 0.1804007887840271, 0.16543303430080414, 0.1585818976163864, 0.16916422545909882, 0.18265342712402344, 0.19441671669483185, 0.17468735575675964, 0.18389686942100525, 0.16049638390541077, 0.16720759868621826, 0.1660909801721573, 0.17665211856365204, 0.17869339883327484, 0.1532129943370819, 0.16582290828227997, 0.15989434719085693, 0.19549302756786346, 0.15696774423122406, 0.15892525017261505, 0.1955154836177826, 0.1956522911787033, 0.1711973398923874, 0.1865382194519043, 0.17495331168174744, 0.1954474300146103, 0.15920737385749817, 0.16659846901893616, 0.16115739941596985, 0.15580031275749207, 0.17684856057167053, 0.16418594121932983, 0.18212087452411652, 0.16564594209194183, 0.1768609881401062, 0.1668597310781479, 0.1725671887397766, 0.1751115322113037, 0.17759138345718384, 0.1737719476222992, 0.1757572591304779, 0.16420595347881317, 0.17709243297576904, 0.16835050284862518, 0.181422159075737, 0.18253818154335022, 0.1758541464805603, 0.19121891260147095, 0.17205512523651123, 0.15917393565177917, 0.1588679403066635, 0.18410100042819977, 0.16752243041992188, 0.1733570694923401, 0.17668084800243378, 0.1652727574110031, 0.18229126930236816, 0.15766620635986328, 0.18517746031284332, 0.193370521068573, 0.15854884684085846, 0.17127563059329987, 0.16431684792041779, 0.18969830870628357, 0.19075962901115417, 0.1953880339860916, 0.17390578985214233, 0.16212090849876404, 0.15331894159317017, 0.1595962792634964, 0.1621791422367096, 0.16246284544467926, 0.179719939827919, 0.18378856778144836, 0.16545717418193817, 0.18364585936069489, 0.17666925489902496, 0.1658552885055542, 0.15329480171203613, 0.15502606332302094, 0.1668252944946289, 0.18423543870449066, 0.16760091483592987, 0.1789693981409073, 0.1697813868522644, 0.1638699620962143, 0.1665973663330078, 0.17645542323589325, 0.19099672138690948, 0.18224111199378967, 0.16551652550697327, 0.1834811270236969, 0.1728004515171051, 0.17785745859146118, 0.15865416824817657, 0.1815285086631775, 0.17134366929531097, 0.1507718861103058, 0.1533668488264084, 0.1716274619102478, 0.18032477796077728, 0.16335654258728027, 0.18162301182746887, 0.19601090252399445, 0.17683573067188263, 0.1613355129957199, 0.1694958359003067, 0.1772756278514862, 0.17627699673175812, 0.17752769589424133, 0.1559332013130188, 0.18235328793525696, 0.18507768213748932, 0.18799550831317902, 0.17288392782211304, 0.19138787686824799, 0.15992066264152527, 0.19543898105621338, 0.16645359992980957, 0.17930778861045837, 0.17566949129104614, 0.17478810250759125, 0.18227322399616241, 0.15328426659107208, 0.1749171018600464, 0.19037526845932007, 0.1736118197441101, 0.1534072607755661, 0.17294611036777496, 0.1860586404800415, 0.17522312700748444, 0.16054826974868774, 0.18084192276000977, 0.1866198182106018, 0.1667676717042923, 0.16838666796684265, 0.1762717068195343, 0.16774828732013702, 0.18950840830802917, 0.15841929614543915, 0.18705067038536072, 0.15588785707950592, 0.187332883477211, 0.18368634581565857, 0.16543394327163696, 0.19527840614318848, 0.17793980240821838, 0.1811486929655075, 0.17223872244358063, 0.1628294736146927, 0.18729394674301147, 0.18792259693145752, 0.17263062298297882, 0.1779257208108902, 0.16755391657352448, 0.16174417734146118, 0.19520190358161926, 0.18368756771087646, 0.19541384279727936, 0.17767693102359772, 0.1722690463066101, 0.1534920632839203, 0.1832728087902069, 0.16234417259693146, 0.17038549482822418, 0.17313343286514282, 0.19506682455539703, 0.15894880890846252, 0.17843759059906006, 0.1535307765007019, 0.1843721866607666, 0.19503501057624817, 0.17612729966640472, 0.16161102056503296, 0.18018795549869537, 0.16812731325626373, 0.1771620661020279, 0.15354987978935242, 0.15370655059814453, 0.1700606346130371, 0.17353111505508423, 0.17824265360832214, 0.19512055814266205, 0.1773410141468048, 0.1762237697839737, 0.15961062908172607, 0.16343708336353302, 0.15378665924072266, 0.17090222239494324, 0.15357479453086853, 0.17232991755008698, 0.17087477445602417, 0.16024935245513916, 0.16376706957817078, 0.1648823767900467, 0.15848186612129211, 0.15410420298576355, 0.1732194572687149, 0.19555801153182983, 0.1698581874370575, 0.17698688805103302, 0.15726453065872192, 0.17409008741378784, 0.18851380050182343, 0.1709015816450119, 0.1765621453523636, 0.1552216112613678, 0.1952783316373825, 0.1751457005739212, 0.16320231556892395, 0.17312000691890717, 0.16590526700019836, 0.1760215163230896, 0.16876575350761414, 0.17395450174808502, 0.15341858565807343, 0.16955238580703735, 0.1825399547815323, 0.16100789606571198, 0.1533668488264084, 0.17854180932044983, 0.18440622091293335, 0.16787876188755035, 0.1817716360092163, 0.16680783033370972, 0.18526007235050201, 0.16386818885803223, 0.15340232849121094, 0.16361522674560547, 0.16560056805610657, 0.15973597764968872, 0.16335633397102356, 0.15964533388614655, 0.18465492129325867, 0.19563204050064087, 0.18101301789283752, 0.15859366953372955, 0.1729324758052826, 0.17581544816493988, 0.17309655249118805, 0.17866893112659454, 0.18516172468662262, 0.19107088446617126, 0.18849711120128632, 0.18101181089878082, 0.18180720508098602, 0.1791367530822754, 0.1953769475221634, 0.1623377501964569, 0.16422832012176514, 0.16849280893802643, 0.16050812602043152, 0.17489928007125854, 0.16710253059864044, 0.1694256067276001, 0.1832883059978485, 0.16912074387073517, 0.16754525899887085, 0.19536016881465912, 0.1800135374069214, 0.16296859085559845, 0.18750016391277313, 0.17309722304344177, 0.1684437096118927, 0.17063987255096436, 0.1535375714302063, 0.1822848916053772, 0.17634263634681702, 0.16341499984264374, 0.1751663088798523, 0.15626712143421173, 0.17494983971118927, 0.19522298872470856, 0.19519153237342834, 0.1825348436832428, 0.18395549058914185, 0.16560864448547363, 0.16536284983158112, 0.16766668856143951, 0.15665291249752045, 0.16454462707042694, 0.15349224209785461, 0.17182376980781555, 0.1869223266839981, 0.17492319643497467, 0.18223291635513306, 0.15370239317417145, 0.1771085560321808, 0.19521401822566986, 0.17130805552005768, 0.17953579127788544, 0.18280641734600067, 0.16488738358020782, 0.16093561053276062, 0.15848180651664734, 0.17283593118190765, 0.16926352679729462, 0.1607012152671814, 0.17896611988544464, 0.1807619333267212, 0.1606631875038147, 0.19048641622066498, 0.18329331278800964, 0.1684979796409607, 0.18381568789482117, 0.19507278501987457, 0.16246557235717773, 0.16546909511089325, 0.1531367152929306, 0.15363410115242004, 0.16754740476608276, 0.18348895013332367, 0.17110340297222137, 0.15346622467041016, 0.1738128513097763, 0.15358510613441467, 0.17023004591464996, 0.18681327998638153, 0.1534859538078308, 0.18770162761211395, 0.1671602725982666, 0.1978944092988968, 0.17061151564121246, 0.15354134142398834, 0.17659080028533936, 0.19163328409194946, 0.16231794655323029, 0.19515381753444672, 0.16958938539028168, 0.15896201133728027, 0.17898128926753998, 0.16930340230464935, 0.15688173472881317, 0.15348193049430847, 0.16567981243133545, 0.186885267496109, 0.18292702734470367, 0.17205053567886353, 0.17604082822799683, 0.17113009095191956, 0.15840761363506317, 0.16177736222743988, 0.1805548518896103, 0.15896031260490417, 0.19532394409179688, 0.16297762095928192, 0.18284468352794647, 0.16476209461688995, 0.1738259494304657, 0.16230536997318268, 0.18063712120056152, 0.18520107865333557, 0.17849275469779968, 0.1634438931941986, 0.17885661125183105, 0.1687585413455963, 0.1607251763343811, 0.15339311957359314, 0.17009882628917694, 0.17441877722740173, 0.19527636468410492, 0.16359765827655792, 0.1649988740682602, 0.16639038920402527, 0.15346556901931763, 0.19539950788021088, 0.19529637694358826, 0.18420925736427307, 0.18184809386730194, 0.18635804951190948, 0.173562690615654, 0.19198651611804962, 0.16952572762966156, 0.1602305918931961, 0.15343542397022247, 0.171317920088768, 0.18452434241771698, 0.1840192675590515, 0.18748442828655243, 0.1703738570213318, 0.16787730157375336, 0.19525296986103058, 0.16049854457378387, 0.16806572675704956, 0.18803244829177856, 0.180341437458992, 0.15749339759349823, 0.19516818225383759, 0.1687280833721161, 0.153686985373497, 0.1683436930179596, 0.15345622599124908, 0.16542589664459229, 0.1633438616991043, 0.1666231006383896, 0.17979122698307037, 0.15346847474575043, 0.17841306328773499, 0.17051899433135986, 0.19453726708889008, 0.17477881908416748, 0.1809060424566269, 0.17863506078720093, 0.1794511377811432, 0.16640742123126984, 0.17006933689117432, 0.15342292189598083, 0.15811792016029358, 0.1839471459388733, 0.17403051257133484, 0.17270830273628235, 0.15424492955207825, 0.18386802077293396, 0.19536331295967102, 0.19542290270328522, 0.17434726655483246, 0.15929876267910004, 0.15567174553871155, 0.171425461769104, 0.17960073053836823, 0.18385417759418488, 0.1886291205883026, 0.17569880187511444, 0.17116354405879974, 0.17013759911060333, 0.182453453540802, 0.18127037584781647, 0.16174349188804626, 0.16962280869483948, 0.1843753457069397, 0.15552078187465668, 0.16114288568496704, 0.18248413503170013, 0.16729794442653656, 0.17739678919315338, 0.18564949929714203, 0.1537226140499115, 0.153400719165802, 0.17692770063877106, 0.1620013266801834, 0.17573049664497375, 0.19521144032478333, 0.17319419980049133, 0.16234531998634338, 0.1902746558189392, 0.171273872256279, 0.15354008972644806, 0.17659886181354523, 0.1636841893196106, 0.19516944885253906, 0.17770306766033173, 0.17517505586147308, 0.17083412408828735, 0.1768919974565506, 0.16452302038669586, 0.17495164275169373, 0.17691190540790558, 0.15898412466049194, 0.17136725783348083, 0.17268212139606476, 0.18320514261722565, 0.18715420365333557, 0.19083626568317413, 0.19014285504817963, 0.1628127545118332, 0.1893194317817688, 0.17756319046020508, 0.19120368361473083, 0.19513528048992157, 0.18507961928844452, 0.17572477459907532, 0.17076870799064636, 0.1783449947834015, 0.1653154045343399, 0.16918092966079712, 0.18160854279994965, 0.195209339261055, 0.1867208182811737, 0.15182636678218842, 0.16414254903793335, 0.16017766296863556, 0.16100190579891205, 0.17926596105098724, 0.1536789834499359, 0.15243728458881378, 0.18097253143787384, 0.17575643956661224, 0.17565494775772095, 0.1663307249546051, 0.15366655588150024, 0.18214163184165955, 0.18777687847614288, 0.19367948174476624, 0.15354733169078827, 0.16132910549640656, 0.16790631413459778, 0.16193147003650665, 0.16179817914962769, 0.16553005576133728, 0.18928149342536926, 0.17735373973846436, 0.19520698487758636, 0.1808495670557022, 0.16177348792552948, 0.15349017083644867, 0.17197521030902863, 0.19090579450130463, 0.16309687495231628, 0.18934974074363708, 0.16877850890159607, 0.1959858536720276, 0.1853809356689453, 0.1732960194349289, 0.17724570631980896, 0.1653348207473755, 0.1795797497034073, 0.18631123006343842, 0.1658644676208496, 0.17847156524658203, 0.18661034107208252, 0.15316280722618103, 0.18360407650470734, 0.17329446971416473, 0.17683197557926178, 0.1951216757297516, 0.17211665213108063, 0.15361283719539642, 0.1849220097064972, 0.1597086787223816, 0.17574867606163025, 0.17237471044063568, 0.15844622254371643, 0.17081107199192047, 0.18259741365909576, 0.19519992172718048, 0.1750182807445526, 0.1725921332836151, 0.1600835919380188, 0.17351949214935303, 0.16534487903118134, 0.15349270403385162, 0.19518786668777466, 0.18046116828918457, 0.15311312675476074, 0.15870331227779388, 0.18289059400558472, 0.1952536702156067, 0.17100468277931213, 0.18126240372657776, 0.1731121987104416, 0.1576683223247528, 0.18178875744342804, 0.18188782036304474, 0.18814080953598022, 0.1804443895816803, 0.1679968386888504, 0.17010582983493805, 0.17703521251678467, 0.176111102104187, 0.16880707442760468, 0.1536259949207306, 0.15356937050819397, 0.16101740300655365, 0.18840043246746063, 0.157967209815979, 0.15352773666381836, 0.17589175701141357, 0.18349522352218628, 0.1608288437128067, 0.17481762170791626, 0.19645340740680695, 0.1685550957918167, 0.17121870815753937, 0.18173809349536896, 0.171844482421875, 0.18838506937026978, 0.19525112211704254, 0.17126041650772095, 0.17827802896499634, 0.18132060766220093, 0.16540205478668213, 0.1536378413438797, 0.15363439917564392, 0.17042866349220276, 0.17402178049087524, 0.1774907261133194, 0.1861306130886078, 0.17086881399154663, 0.17750637233257294, 0.15369468927383423, 0.1855640411376953, 0.1790591925382614, 0.17490001022815704, 0.15370285511016846, 0.1733350157737732, 0.17148509621620178, 0.15357725322246552, 0.18142938613891602, 0.19526705145835876, 0.19533276557922363, 0.16374973952770233, 0.1690644770860672, 0.180012047290802, 0.19203969836235046, 0.1868181675672531, 0.15349431335926056, 0.15743230283260345, 0.19118449091911316, 0.1631755232810974, 0.17230765521526337, 0.17866982519626617, 0.16675205528736115, 0.18482044339179993, 0.16866344213485718, 0.18386146426200867, 0.17044298350811005, 0.1761171817779541, 0.16715557873249054, 0.18505002558231354, 0.19521048665046692, 0.1950386017560959, 0.15594471991062164, 0.17684726417064667, 0.16398711502552032, 0.17043437063694, 0.1845685839653015, 0.15264341235160828, 0.1775067150592804, 0.15847842395305634, 0.19217932224273682, 0.1729024350643158, 0.16577155888080597, 0.18191607296466827, 0.1678447127342224, 0.18456724286079407, 0.16132144629955292, 0.1588309109210968, 0.16809438169002533, 0.16327506303787231, 0.19522446393966675, 0.15361595153808594, 0.15355336666107178, 0.1588495820760727, 0.16268695890903473, 0.17122666537761688, 0.16088643670082092, 0.17769260704517365, 0.19528231024742126, 0.15846724808216095, 0.1816294938325882, 0.1534944474697113, 0.17174381017684937, 0.1732611209154129, 0.19262708723545074, 0.16112245619297028, 0.16318291425704956, 0.15713131427764893, 0.18092870712280273, 0.1860756278038025, 0.18803894519805908, 0.15731993317604065, 0.16102367639541626, 0.1609335094690323, 0.15677344799041748, 0.19217073917388916, 0.18852598965168, 0.1743546724319458, 0.15896010398864746, 0.1720435917377472, 0.1761963814496994, 0.18713901937007904, 0.18287599086761475, 0.16519929468631744, 0.17084629833698273, 0.16948701441287994, 0.16422049701213837, 0.17695923149585724, 0.1746193915605545, 0.15893889963626862, 0.16108736395835876, 0.17218798398971558, 0.15349677205085754, 0.15360817313194275, 0.16991575062274933, 0.16121329367160797, 0.18072426319122314, 0.16204744577407837, 0.16828137636184692, 0.17985482513904572, 0.18627507984638214, 0.19533196091651917, 0.16153913736343384, 0.17878010869026184, 0.16675490140914917, 0.18071772158145905, 0.1952158361673355, 0.15344783663749695, 0.17217665910720825, 0.17371773719787598, 0.17387178540229797, 0.1675613522529602, 0.19247163832187653, 0.18230873346328735, 0.18397562205791473, 0.1874157190322876, 0.1637609452009201, 0.16087573766708374, 0.19514121115207672, 0.17014369368553162, 0.1952630877494812, 0.19535574316978455, 0.17005738615989685, 0.19507849216461182, 0.17562727630138397, 0.1792549192905426, 0.17370997369289398, 0.1670985221862793, 0.17676502466201782, 0.19506558775901794, 0.16503290832042694, 0.16277214884757996, 0.1649332195520401, 0.16276708245277405, 0.15359944105148315, 0.16182942688465118, 0.19708944857120514, 0.1749878078699112, 0.15362626314163208, 0.16205771267414093, 0.18398930132389069, 0.18248100578784943, 0.1633572280406952, 0.19077296555042267, 0.16575810313224792, 0.15199357271194458, 0.19497941434383392, 0.18106193840503693, 0.1847797930240631, 0.1801946759223938, 0.1872870922088623, 0.15792900323867798, 0.18119162321090698, 0.1874508559703827, 0.179531529545784, 0.18237176537513733, 0.16129249334335327, 0.16282492876052856, 0.17472140491008759, 0.16876277327537537, 0.16905461251735687, 0.17736417055130005, 0.15985539555549622, 0.15735656023025513, 0.18480506539344788, 0.17496806383132935, 0.19492870569229126, 0.16997435688972473, 0.17551741003990173, 0.16567161679267883, 0.16669334471225739, 0.1657421588897705, 0.17337797582149506, 0.16266535222530365, 0.18162213265895844, 0.1716068983078003, 0.1746116429567337, 0.18232862651348114, 0.16582639515399933, 0.18859416246414185, 0.165591761469841, 0.1949804127216339, 0.16919885575771332, 0.16915801167488098, 0.16462600231170654, 0.17930372059345245, 0.15361471474170685, 0.17182496190071106, 0.177284836769104, 0.1681731939315796, 0.17944267392158508, 0.1739712655544281, 0.1912442445755005, 0.17062141001224518, 0.1592434048652649, 0.15359386801719666, 0.17737071216106415, 0.17695191502571106, 0.19508472084999084, 0.19508768618106842, 0.18668439984321594, 0.1630961149930954, 0.1624661684036255, 0.17062148451805115, 0.18040621280670166, 0.17106401920318604, 0.18417203426361084, 0.16461417078971863, 0.184116929769516, 0.1916348934173584, 0.1712234914302826, 0.16881971061229706, 0.15921655297279358, 0.16721312701702118, 0.17746928334236145, 0.1771005243062973, 0.16704446077346802, 0.16155993938446045, 0.16543138027191162, 0.1949508786201477, 0.17354567348957062, 0.1812305748462677, 0.17301906645298004, 0.15371690690517426, 0.16004647314548492, 0.1846086084842682, 0.18564949929714203, 0.18031038343906403, 0.18382392823696136, 0.1625247597694397, 0.16022194921970367, 0.1950095146894455, 0.17374497652053833, 0.17528949677944183, 0.15369176864624023, 0.1995363086462021, 0.16364477574825287, 0.15974432229995728, 0.15869328379631042, 0.17860817909240723, 0.18122857809066772, 0.17881466448307037, 0.18350589275360107, 0.183281809091568, 0.16819459199905396, 0.18584173917770386, 0.17604058980941772, 0.1906493455171585, 0.17428423464298248, 0.16794046759605408, 0.1784234642982483, 0.1794327348470688, 0.1736532747745514, 0.16255001723766327, 0.17446748912334442, 0.15260525047779083, 0.18406203389167786, 0.17295555770397186, 0.1773373931646347, 0.1688687652349472, 0.16113239526748657, 0.18096137046813965, 0.1539376825094223, 0.16720126569271088, 0.1648915857076645, 0.15188641846179962, 0.16453567147254944, 0.16943852603435516, 0.16794130206108093, 0.17641866207122803, 0.1642526239156723, 0.15368178486824036, 0.1609116941690445, 0.17883461713790894, 0.16512630879878998, 0.15859290957450867, 0.1950996071100235, 0.17015911638736725, 0.1776220053434372, 0.19491955637931824, 0.17976385354995728, 0.187681645154953, 0.18599922955036163, 0.17918209731578827, 0.17148014903068542, 0.1537492722272873, 0.15773530304431915, 0.16246984899044037, 0.19500355422496796, 0.16396455466747284, 0.1815963089466095, 0.175515279173851, 0.16423523426055908, 0.1775728315114975, 0.17503318190574646, 0.17898842692375183, 0.15376009047031403, 0.18418282270431519, 0.16515342891216278, 0.1810007095336914, 0.18631456792354584, 0.16747571527957916, 0.15369726717472076, 0.15882976353168488, 0.16148512065410614, 0.16505931317806244, 0.1766413301229477, 0.19518552720546722, 0.17803312838077545, 0.1614067554473877, 0.16396775841712952, 0.16172954440116882, 0.17667223513126373, 0.15370608866214752, 0.1715913861989975, 0.1588948518037796, 0.1698060780763626, 0.18977895379066467, 0.1953418105840683, 0.15974831581115723, 0.17211785912513733, 0.1724260300397873, 0.16915754973888397, 0.174978569149971, 0.17154741287231445, 0.17621250450611115, 0.17664815485477448, 0.15347710251808167, 0.19469782710075378, 0.1951572597026825, 0.160814106464386, 0.1686815470457077, 0.1731514185667038, 0.1580117642879486, 0.17172911763191223, 0.153580904006958, 0.17373116314411163, 0.16693127155303955, 0.15359048545360565, 0.18297767639160156, 0.15359099209308624, 0.17956019937992096, 0.1732899248600006, 0.15356284379959106, 0.18498502671718597, 0.17404970526695251, 0.17114630341529846, 0.16258256137371063, 0.19502347707748413, 0.16751757264137268, 0.17255356907844543, 0.15370464324951172, 0.1767558455467224, 0.16133946180343628, 0.17201098799705505, 0.17999747395515442, 0.184262216091156, 0.1619318425655365, 0.15355797111988068, 0.170414000749588, 0.16076120734214783, 0.15368559956550598, 0.17299476265907288, 0.16042295098304749, 0.1685902178287506, 0.16105718910694122, 0.17766301333904266, 0.1649629920721054, 0.19512200355529785, 0.16242004930973053, 0.18015627562999725, 0.15348564088344574, 0.17521366477012634, 0.16924788057804108, 0.18105699121952057, 0.16255168616771698, 0.18509206175804138, 0.15927909314632416, 0.18177779018878937, 0.16547054052352905, 0.19515874981880188, 0.15969884395599365, 0.17225344479084015, 0.1681164801120758, 0.15206168591976166, 0.18193717300891876, 0.1803639680147171, 0.1806117743253708, 0.1624620258808136, 0.18021388351917267, 0.19752874970436096, 0.18629032373428345, 0.18520915508270264, 0.1648419350385666, 0.16736146807670593, 0.17536690831184387, 0.19513541460037231, 0.18824255466461182, 0.18947052955627441, 0.1737421154975891, 0.17459550499916077, 0.17772530019283295, 0.18589764833450317, 0.18705573678016663, 0.15353906154632568, 0.15369994938373566, 0.16996552050113678, 0.18190725147724152, 0.1757233887910843, 0.19520798325538635, 0.17222541570663452, 0.17363570630550385, 0.17925266921520233, 0.17020834982395172, 0.1534910947084427, 0.16744893789291382, 0.16278918087482452, 0.18479889631271362, 0.17816071212291718, 0.1695842742919922, 0.15837478637695312, 0.163327157497406, 0.18675337731838226, 0.171311154961586, 0.18980851769447327, 0.17060507833957672, 0.15286849439144135, 0.1702461689710617, 0.1724625825881958, 0.1888047158718109, 0.16854919493198395, 0.17219462990760803, 0.1668262630701065, 0.15909843146800995, 0.1785280406475067, 0.1907830685377121, 0.1665572226047516, 0.1717580407857895, 0.17185208201408386, 0.19023311138153076, 0.16822564601898193, 0.17834854125976562, 0.15350131690502167, 0.1671110987663269, 0.1765856146812439, 0.17162707448005676, 0.1654803603887558, 0.19016015529632568, 0.18053174018859863, 0.1654629409313202, 0.15949121117591858, 0.1648755967617035, 0.16621267795562744, 0.16198338568210602, 0.16595786809921265, 0.16783444583415985, 0.1796511858701706, 0.1679501086473465, 0.1805894672870636, 0.16224460303783417, 0.18360523879528046, 0.1629989594221115, 0.17461299896240234, 0.15649211406707764, 0.1602982133626938, 0.18578432500362396, 0.17373429238796234, 0.16648225486278534, 0.17871123552322388, 0.17816999554634094, 0.19186286628246307, 0.1928187608718872, 0.15359008312225342, 0.17177191376686096, 0.1912652850151062, 0.19511497020721436, 0.16350294649600983, 0.1534731388092041, 0.17444156110286713, 0.19362841546535492, 0.17468635737895966, 0.1865621656179428, 0.18726302683353424, 0.16663359105587006, 0.1737525463104248, 0.19193783402442932, 0.18926839530467987, 0.17527362704277039, 0.18386343121528625, 0.16196300089359283, 0.18952074646949768, 0.18468737602233887, 0.19503289461135864, 0.1534949690103531, 0.16784843802452087, 0.16837915778160095, 0.16054539382457733, 0.1529081165790558, 0.15588496625423431, 0.18571779131889343, 0.15361672639846802, 0.17558738589286804, 0.15590766072273254, 0.16496847569942474, 0.16318999230861664, 0.16595368087291718, 0.18136227130889893, 0.17087844014167786, 0.15545794367790222, 0.16455385088920593, 0.1579064130783081, 0.17425352334976196, 0.15283873677253723, 0.1674613356590271, 0.17893287539482117, 0.1750299632549286, 0.18318112194538116, 0.15393729507923126, 0.1950879991054535, 0.1951347291469574, 0.16569317877292633, 0.1728544682264328, 0.17495615780353546, 0.17823798954486847, 0.18567503988742828, 0.1716427505016327, 0.18238210678100586, 0.18918126821517944, 0.17663128674030304, 0.15968109667301178, 0.1536276936531067, 0.18825465440750122, 0.19517911970615387, 0.1806662678718567, 0.16093675792217255, 0.18292361497879028, 0.16660046577453613, 0.19501996040344238, 0.1574103683233261, 0.16361677646636963, 0.18166548013687134, 0.15975533425807953, 0.17046236991882324, 0.16485877335071564, 0.15353325009346008, 0.17217567563056946, 0.17802846431732178, 0.16372351348400116, 0.17281600832939148, 0.18708735704421997, 0.17414097487926483, 0.17222578823566437, 0.1677922010421753, 0.18248042464256287, 0.1829347163438797, 0.1647522747516632, 0.18017040193080902, 0.15366889536380768, 0.17233097553253174, 0.1846301555633545, 0.15810030698776245, 0.15365877747535706, 0.17490220069885254, 0.19583845138549805, 0.16951189935207367, 0.16616056859493256, 0.19511568546295166, 0.16214148700237274, 0.17155995965003967, 0.17033545672893524, 0.1535213142633438, 0.15352590382099152, 0.17249786853790283, 0.1951942890882492, 0.1891850233078003, 0.17646612226963043, 0.15878641605377197, 0.17390215396881104, 0.16214494407176971, 0.17164212465286255, 0.16513891518115997, 0.17511232197284698, 0.18491438031196594, 0.1770087480545044, 0.17225876450538635, 0.1827254742383957, 0.17145858705043793, 0.18719369173049927, 0.16743431985378265, 0.16741688549518585, 0.15932901203632355, 0.1734527349472046, 0.17405302822589874, 0.16743320226669312, 0.17053139209747314, 0.15612563490867615, 0.19279778003692627, 0.18039900064468384, 0.17292942106723785, 0.16777068376541138, 0.16141369938850403, 0.17373217642307281, 0.17734651267528534, 0.1827687919139862, 0.15817596018314362, 0.16585949063301086, 0.1814628392457962, 0.17690445482730865, 0.1785643845796585, 0.15560948848724365, 0.1817992925643921, 0.16935521364212036, 0.17462706565856934, 0.1766946017742157, 0.16169820725917816, 0.17816521227359772, 0.17993256449699402, 0.19570302963256836, 0.17955876886844635, 0.1534840166568756, 0.15436427295207977, 0.16896964609622955, 0.16943326592445374, 0.1778179407119751, 0.15350766479969025, 0.1823827177286148, 0.17737898230552673, 0.15811586380004883, 0.17411677539348602, 0.17654548585414886, 0.16406036913394928, 0.17121005058288574, 0.153462216258049, 0.16016782820224762, 0.18654470145702362, 0.18403135240077972, 0.16223613917827606, 0.18544358015060425, 0.17771829664707184, 0.178619846701622, 0.1852605938911438, 0.17228804528713226, 0.16904889047145844, 0.15064457058906555, 0.16198958456516266, 0.189785897731781, 0.16025274991989136, 0.17215761542320251, 0.19419239461421967, 0.18040916323661804, 0.16316139698028564, 0.19145281612873077, 0.18722140789031982, 0.15348337590694427, 0.1663096398115158, 0.17443086206912994, 0.16196246445178986, 0.1581219732761383, 0.15689688920974731, 0.18228429555892944, 0.15352468192577362, 0.1661779135465622, 0.17895905673503876, 0.1842212826013565, 0.17899516224861145, 0.17771951854228973, 0.17049038410186768, 0.17415477335453033, 0.17178305983543396, 0.1718578338623047, 0.196466326713562, 0.17668233811855316, 0.1951734870672226, 0.17052628099918365, 0.1919422149658203, 0.19444051384925842, 0.17242519557476044, 0.1608918309211731, 0.15355603396892548, 0.17896482348442078, 0.17994657158851624, 0.17127829790115356, 0.18325094878673553, 0.16166526079177856, 0.16839319467544556, 0.18633481860160828, 0.1931953728199005, 0.15350887179374695, 0.1775175780057907, 0.17049482464790344, 0.1581292599439621, 0.19271385669708252, 0.17742957174777985, 0.16700120270252228, 0.18017590045928955, 0.16818729043006897, 0.1651877909898758, 0.17347675561904907, 0.15774588286876678, 0.18156473338603973, 0.16244329512119293, 0.1701509803533554, 0.17275525629520416, 0.16287116706371307, 0.16040782630443573, 0.16618505120277405, 0.17807641625404358, 0.16935165226459503, 0.17744508385658264, 0.15348446369171143, 0.17031462490558624, 0.16685301065444946, 0.16816593706607819, 0.18986310064792633, 0.16413187980651855, 0.16291610896587372, 0.1700238287448883, 0.17763330042362213, 0.18729498982429504, 0.15569329261779785, 0.16034378111362457, 0.1559634953737259, 0.16602541506290436, 0.1707487404346466, 0.16242057085037231, 0.16797365248203278, 0.18312954902648926, 0.16580744087696075, 0.1797717660665512, 0.16685694456100464, 0.16406415402889252, 0.16705641150474548, 0.18034890294075012, 0.16117872297763824, 0.18454572558403015, 0.18003606796264648]\n",
            "Val loss 0.17192187638282777\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f56c5b4b-545d-446e-8c8e-66183f78debe"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}