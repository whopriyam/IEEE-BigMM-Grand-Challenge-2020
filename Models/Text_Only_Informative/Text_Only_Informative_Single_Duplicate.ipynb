{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Only_Informative_Single_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbbb154c2504437da24242f7d552d2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ab77dd69b6834ac697c9951892b9af65",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ac747db997d4e21a2e33ad537d923b5",
              "IPY_MODEL_c6adfa702418430689060bc4fc0f1b5b"
            ]
          }
        },
        "ab77dd69b6834ac697c9951892b9af65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ac747db997d4e21a2e33ad537d923b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_18eeee1709d046a9a5a5f9e0d506689f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69ddf0ad6b434b319377e692b051b86f"
          }
        },
        "c6adfa702418430689060bc4fc0f1b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e09fd11aa124fa4a9898c4242a07d66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:09&lt;00:00, 25.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f34557d31e8a4144b1b48c07dcbf6794"
          }
        },
        "18eeee1709d046a9a5a5f9e0d506689f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69ddf0ad6b434b319377e692b051b86f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e09fd11aa124fa4a9898c4242a07d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f34557d31e8a4144b1b48c07dcbf6794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f72ce2bd03c41ccaffd17a4fcfdbc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8b461063760a41ed836cc9d205f40ff7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9cd89202b09647eeae1ae7410996db23",
              "IPY_MODEL_7bcbd9a5488f4431ac2137cca8c60835"
            ]
          }
        },
        "8b461063760a41ed836cc9d205f40ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cd89202b09647eeae1ae7410996db23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd6e66f4185b4c3bac714530189ac73d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2036,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2036,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_323118a0bd3d43f595e3a224b59cb3a6"
          }
        },
        "7bcbd9a5488f4431ac2137cca8c60835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9483d05c9f104eadabd19481714b5a22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2036/2036 [18:55&lt;00:00,  1.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67d127cea2134db7a980c2324c669b72"
          }
        },
        "bd6e66f4185b4c3bac714530189ac73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "323118a0bd3d43f595e3a224b59cb3a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9483d05c9f104eadabd19481714b5a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67d127cea2134db7a980c2324c669b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b6a48a4b8cd4ee6b1338dab4119f35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb703b5dd1a448f799daf84f4ec20c2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a15342390b61457e823f9f08b65059a0",
              "IPY_MODEL_26748136a9ec43abbb4ef3e6b9ababfd"
            ]
          }
        },
        "cb703b5dd1a448f799daf84f4ec20c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a15342390b61457e823f9f08b65059a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96f3638540a241a3b559a2197e8160e2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2036,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2036,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0baf94a9eb694d09ac07e788a2eeeb50"
          }
        },
        "26748136a9ec43abbb4ef3e6b9ababfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81cd4fa949c64272b72e0ad0856947dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2036/2036 [18:48&lt;00:00,  1.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d06ae742c87b4c1a940c8fa7451beb4c"
          }
        },
        "96f3638540a241a3b559a2197e8160e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0baf94a9eb694d09ac07e788a2eeeb50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81cd4fa949c64272b72e0ad0856947dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d06ae742c87b4c1a940c8fa7451beb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d953dce3659f42739ca836c2b8874206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_173d5279ff774278a58fdc947bd1be5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0123c43cab9e41ea9c5d927cec71eacd",
              "IPY_MODEL_f211dabc9c3541ea9e6e69abb05b89f4"
            ]
          }
        },
        "173d5279ff774278a58fdc947bd1be5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0123c43cab9e41ea9c5d927cec71eacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8d300c2befc041b1beb511e7ac315abf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2036,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2036,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63554a6925da405c88a82f8068aea282"
          }
        },
        "f211dabc9c3541ea9e6e69abb05b89f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c449dd7229554dfbafc72deceb2cdfb0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2036/2036 [18:45&lt;00:00,  1.81it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a38ed527fae4563b989ecdee219b32c"
          }
        },
        "8d300c2befc041b1beb511e7ac315abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63554a6925da405c88a82f8068aea282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c449dd7229554dfbafc72deceb2cdfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a38ed527fae4563b989ecdee219b32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1fa8c0ad-5747-453b-aeb3-5cff3b57ca2c"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 41.81 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee2d075f-fba2-4e6d-def3-403c3abfeec1"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c927a71-2b78-43ee-ecca-086ae56e261d"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3ffe2735-452b-40d1-ace4-55a1c8c59311"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cd0d1d07-1cc2-4ef0-b8f0-a3c9df3ed0b5"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "6cfe9fe7-3718-4438-d7c7-823831dfe11d"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6b6ca189-e402-4953-b260-2a835086e426"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4e05c04f-202d-41bb-bf66-238bff2bedde"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "314ad313-bfc3-4c22-f05e-86e6c984ca44"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b44cf7e2-984d-406c-c5e9-2bfc39c1edf9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    @Alyssa_Milano @chelseahandler #MeTooMovement....\n",
              "1    #podestaemails Changed everything in my life. ...\n",
              "2    Bollywood director denies #MeToo claims, threa...\n",
              "3    Missouri State student reportedly raped hours ...\n",
              "4                Meet the women worried about #MeToo  \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b060fdf7-a520-41ed-a02c-ff133000f308"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:27<00:00, 14988478.46B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99547c7b-8bc5-4076-ab2c-fdf58544cba8"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1143355.14B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "fa48ed7d-f083-4c0c-973b-ae22c983b271"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea63dfdb-4684-406b-9e03-e296e71417f8"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a195ac0-36bb-4bf9-9bae-6bf73a842dc1"
      },
      "source": [
        "col_name = \"Text_Only_Informative\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 0 # or 0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862,
          "referenced_widgets": [
            "cbbb154c2504437da24242f7d552d2f7",
            "ab77dd69b6834ac697c9951892b9af65",
            "7ac747db997d4e21a2e33ad537d923b5",
            "c6adfa702418430689060bc4fc0f1b5b",
            "18eeee1709d046a9a5a5f9e0d506689f",
            "69ddf0ad6b434b319377e692b051b86f",
            "2e09fd11aa124fa4a9898c4242a07d66",
            "f34557d31e8a4144b1b48c07dcbf6794",
            "5f72ce2bd03c41ccaffd17a4fcfdbc93",
            "8b461063760a41ed836cc9d205f40ff7",
            "9cd89202b09647eeae1ae7410996db23",
            "7bcbd9a5488f4431ac2137cca8c60835",
            "bd6e66f4185b4c3bac714530189ac73d",
            "323118a0bd3d43f595e3a224b59cb3a6",
            "9483d05c9f104eadabd19481714b5a22",
            "67d127cea2134db7a980c2324c669b72",
            "5b6a48a4b8cd4ee6b1338dab4119f35e",
            "cb703b5dd1a448f799daf84f4ec20c2c",
            "a15342390b61457e823f9f08b65059a0",
            "26748136a9ec43abbb4ef3e6b9ababfd",
            "96f3638540a241a3b559a2197e8160e2",
            "0baf94a9eb694d09ac07e788a2eeeb50",
            "81cd4fa949c64272b72e0ad0856947dd",
            "d06ae742c87b4c1a940c8fa7451beb4c",
            "d953dce3659f42739ca836c2b8874206",
            "173d5279ff774278a58fdc947bd1be5b",
            "0123c43cab9e41ea9c5d927cec71eacd",
            "f211dabc9c3541ea9e6e69abb05b89f4",
            "8d300c2befc041b1beb511e7ac315abf",
            "63554a6925da405c88a82f8068aea282",
            "c449dd7229554dfbafc72deceb2cdfb0",
            "5a38ed527fae4563b989ecdee219b32c"
          ]
        },
        "outputId": "661220fe-9b5b-41af-967c-e41c68b75dda"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 8145\n",
            "Old data length : 1596\n",
            "minority class is 0. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 2026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbbb154c2504437da24242f7d552d2f7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f72ce2bd03c41ccaffd17a4fcfdbc93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2036.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1854\n",
            "Train Losses : [0.07322349399328232, 0.4015718102455139, 1.8767727613449097, 0.741566002368927, 3.050506830215454, 0.0009571779519319534, 1.2508964538574219, 4.10059118270874, 2.4794397354125977, 1.3247363567352295, 0.6201984286308289, 0.24926207959651947, 0.5244596004486084, 0.027583424001932144, 1.6933571100234985, 0.3427911698818207, 0.40660032629966736, 0.07753097265958786, 0.016253385692834854, 0.09515876322984695, 0.43379130959510803, 0.5282780528068542, 0.04368271306157112, 0.43563005328178406, 0.1674318164587021, 0.06865935772657394, 0.35500529408454895, 0.2825755178928375, 0.17373086512088776, 0.4951581060886383, 0.2829406261444092, 0.09855110198259354, 0.11239250004291534, 0.09595955163240433, 0.09852353483438492, 0.23842909932136536, 0.050110265612602234, 0.40678268671035767, 0.0542769730091095, 0.46357086300849915, 0.05816717445850372, 0.08764293044805527, 0.06204114854335785, 0.03021683357656002, 0.49217742681503296, 0.1084810420870781, 0.25124719738960266, 0.1731318086385727, 0.0656646266579628, 0.38536661863327026, 0.02482355758547783, 0.3229469358921051, 0.46972015500068665, 0.26373347640037537, 0.060375481843948364, 0.19583158195018768, 0.10839416086673737, 0.3711349368095398, 0.12294984608888626, 0.6999978423118591, 0.7322845458984375, 0.539230227470398, 0.041967861354351044, 0.31972476840019226, 0.5131596326828003, 0.25230515003204346, 0.11502096056938171, 0.15232358872890472, 0.20176954567432404, 0.14687921106815338, 0.393140584230423, 0.33313417434692383, 0.20181860029697418, 0.1089400127530098, 0.1964753121137619, 0.10270309448242188, 0.16883663833141327, 0.2633863687515259, 0.15451815724372864, 0.18807418644428253, 0.22695133090019226, 0.11474199593067169, 0.4499731957912445, 0.14929306507110596, 0.33306074142456055, 0.1825663149356842, 0.11517323553562164, 0.20441000163555145, 0.1873319447040558, 0.18843135237693787, 0.21366645395755768, 0.09007313847541809, 0.17782199382781982, 0.09239880740642548, 0.0998658835887909, 0.08371614664793015, 0.2092924416065216, 0.0792427659034729, 0.2274148166179657, 0.5086172223091125, 0.3508795201778412, 0.22962351143360138, 0.24889762699604034, 0.2234683334827423, 0.3218541443347931, 0.11391758173704147, 0.40228477120399475, 0.238352432847023, 0.14493244886398315, 0.29467692971229553, 0.2258487045764923, 0.35750141739845276, 0.11372271925210953, 0.300716370344162, 0.14272505044937134, 0.21677738428115845, 0.24884992837905884, 0.11781223118305206, 0.34321004152297974, 0.23417825996875763, 0.3407434821128845, 0.1850513368844986, 0.2920716404914856, 0.11015022546052933, 0.13199718296527863, 0.1689230054616928, 0.15296556055545807, 0.19976913928985596, 0.39245638251304626, 0.18195712566375732, 0.2198650985956192, 0.18038347363471985, 0.2079138308763504, 0.20216220617294312, 0.2448936402797699, 0.2629925012588501, 0.407267302274704, 0.09683091193437576, 0.07990974932909012, 0.17303135991096497, 0.170949786901474, 0.12905898690223694, 0.1402169167995453, 0.21389664709568024, 0.1798209398984909, 0.27193599939346313, 0.19061724841594696, 0.1511409431695938, 0.3097422122955322, 0.267155259847641, 0.0929589495062828, 0.26206550002098083, 0.3173237144947052, 0.13290826976299286, 0.0649581253528595, 0.14908654987812042, 0.2453300654888153, 0.35126399993896484, 0.28655555844306946, 0.15745587646961212, 0.28742027282714844, 0.12603561580181122, 0.18843407928943634, 0.2812405526638031, 0.09367513656616211, 0.3814305067062378, 0.11404454708099365, 0.2500958740711212, 0.25542789697647095, 0.24717792868614197, 0.08637535572052002, 0.12973985075950623, 0.2843652069568634, 0.17672008275985718, 0.23643508553504944, 0.20509281754493713, 0.19378605484962463, 0.18434830009937286, 0.14684604108333588, 0.1592940390110016, 0.16215385496616364, 0.24926644563674927, 0.15315726399421692, 0.16396592557430267, 0.1483483463525772, 0.3828623294830322, 0.22835427522659302, 0.2277379184961319, 0.2715737819671631, 0.1676700860261917, 0.1670973300933838, 0.12202243506908417, 0.2513093948364258, 0.13167187571525574, 0.2222694456577301, 0.15139730274677277, 0.21769514679908752, 0.26456984877586365, 0.24874578416347504, 0.2486358880996704, 0.15403732657432556, 0.11435429751873016, 0.22906285524368286, 0.16291840374469757, 0.2085813581943512, 0.20567840337753296, 0.1723017543554306, 0.16023828089237213, 0.3136190176010132, 0.12790706753730774, 0.24029216170310974, 0.16026043891906738, 0.12005791068077087, 0.1519969254732132, 0.16505037248134613, 0.21554522216320038, 0.22784708440303802, 0.2662810981273651, 0.30735355615615845, 0.27565714716911316, 0.14554910361766815, 0.15150408446788788, 0.30050766468048096, 0.19591133296489716, 0.33453652262687683, 0.14215654134750366, 0.18730920553207397, 0.1281837821006775, 0.19659844040870667, 0.1695202738046646, 0.08094155788421631, 0.14608052372932434, 0.24689161777496338, 0.0918610617518425, 0.09751418232917786, 0.21438676118850708, 0.08390563726425171, 0.21720272302627563, 0.18447574973106384, 0.3251841068267822, 0.13082943856716156, 0.22728890180587769, 0.12596087157726288, 0.2954536974430084, 0.2351733148097992, 0.10551216453313828, 0.22833159565925598, 0.1925930380821228, 0.20604287087917328, 0.2312520295381546, 0.21627506613731384, 0.1756410151720047, 0.26405566930770874, 0.29621589183807373, 0.20062080025672913, 0.19045978784561157, 0.15690141916275024, 0.1515083760023117, 0.15626253187656403, 0.15095312893390656, 0.1322365254163742, 0.2321644127368927, 0.15757496654987335, 0.18799550831317902, 0.13096150755882263, 0.17884370684623718, 0.21474038064479828, 0.14727744460105896, 0.12332707643508911, 0.1018277257680893, 0.2069886326789856, 0.17102482914924622, 0.1543659269809723, 0.21035654842853546, 0.12014677375555038, 0.16767723858356476, 0.205994114279747, 0.19822418689727783, 0.16784365475177765, 0.15443992614746094, 0.2014029175043106, 0.13948367536067963, 0.1743355542421341, 0.1678028106689453, 0.18307098746299744, 0.1418365240097046, 0.1853872388601303, 0.16704030334949493, 0.10771522670984268, 0.14908206462860107, 0.11993622034788132, 0.1249409019947052, 0.3048035800457001, 0.2046525776386261, 0.291472852230072, 0.13581517338752747, 0.21466828882694244, 0.12635868787765503, 0.1337781697511673, 0.15733295679092407, 0.1295500546693802, 0.21867994964122772, 0.09513957798480988, 0.1352069228887558, 0.10912521183490753, 0.28201255202293396, 0.15379761159420013, 0.11194567382335663, 0.15490803122520447, 0.1751803159713745, 0.28112998604774475, 0.14894706010818481, 0.07575894892215729, 0.22687992453575134, 0.15315695106983185, 0.16693726181983948, 0.2839556336402893, 0.10963167250156403, 0.06521200388669968, 0.11308949440717697, 0.08847156167030334, 0.26146063208580017, 0.13657799363136292, 0.11157406866550446, 0.24136953055858612, 0.2197922319173813, 0.14182980358600616, 0.188917338848114, 0.10306845605373383, 0.10380657017230988, 0.11244731396436691, 0.17645594477653503, 0.09952878952026367, 0.21894681453704834, 0.18745790421962738, 0.17113324999809265, 0.2639714479446411, 0.22856512665748596, 0.15942448377609253, 0.1489425152540207, 0.14480268955230713, 0.13823039829730988, 0.16724665462970734, 0.09526713192462921, 0.10091405361890793, 0.17432937026023865, 0.15752197802066803, 0.16971202194690704, 0.20158587396144867, 0.28222784399986267, 0.22807949781417847, 0.20335642993450165, 0.1493905484676361, 0.10035622864961624, 0.12842825055122375, 0.16202320158481598, 0.17214539647102356, 0.09817949682474136, 0.268734872341156, 0.13642601668834686, 0.09383518993854523, 0.1085989773273468, 0.17742455005645752, 0.13963167369365692, 0.12993121147155762, 0.23391209542751312, 0.08425463736057281, 0.0828436091542244, 0.18073727190494537, 0.17389769852161407, 0.3631683886051178, 0.09378191828727722, 0.0717577189207077, 0.27866342663764954, 0.12759695947170258, 0.09896473586559296, 0.15359073877334595, 0.20984825491905212, 0.37325605750083923, 0.19350840151309967, 0.21452949941158295, 0.07912059873342514, 0.06343432515859604, 0.10796722769737244, 0.12474800646305084, 0.22496438026428223, 0.26911234855651855, 0.29925236105918884, 0.15807366371154785, 0.22286655008792877, 0.08637014031410217, 0.29677265882492065, 0.1347322016954422, 0.20672710239887238, 0.1011086031794548, 0.11407553404569626, 0.08942032605409622, 0.3292989432811737, 0.17231842875480652, 0.19871032238006592, 0.17919310927391052, 0.20978575944900513, 0.0989239513874054, 0.2592344582080841, 0.2225916087627411, 0.10612786561250687, 0.12591956555843353, 0.1150846779346466, 0.1711442470550537, 0.14510200917720795, 0.10660374164581299, 0.17546765506267548, 0.2567904591560364, 0.17793072760105133, 0.27810001373291016, 0.2652616500854492, 0.33976855874061584, 0.21504832804203033, 0.18785427510738373, 0.17125752568244934, 0.1882171928882599, 0.1721823811531067, 0.1902862936258316, 0.20692934095859528, 0.15708734095096588, 0.09657267481088638, 0.1609058678150177, 0.15678806602954865, 0.08881635218858719, 0.11937699466943741, 0.1606692522764206, 0.19971977174282074, 0.22825764119625092, 0.20802591741085052, 0.2710256576538086, 0.08090179413557053, 0.20550818741321564, 0.1103849858045578, 0.11560115218162537, 0.2412266880273819, 0.18215709924697876, 0.08081860840320587, 0.1275143325328827, 0.15151220560073853, 0.16072490811347961, 0.09541955590248108, 0.2051050066947937, 0.12318875640630722, 0.1309189647436142, 0.2512747645378113, 0.12283960729837418, 0.30342531204223633, 0.15447935461997986, 0.12072057276964188, 0.25414326786994934, 0.1252310574054718, 0.17681998014450073, 0.1919877827167511, 0.23781654238700867, 0.09765642136335373, 0.09496942162513733, 0.19197314977645874, 0.17842359840869904, 0.2775334417819977, 0.17981413006782532, 0.09659789502620697, 0.16713888943195343, 0.12462148815393448, 0.2731342911720276, 0.17940475046634674, 0.22684220969676971, 0.17879313230514526, 0.17307353019714355, 0.20408222079277039, 0.1757693886756897, 0.18473395705223083, 0.17293235659599304, 0.1591046303510666, 0.1483321338891983, 0.21902833878993988, 0.21803241968154907, 0.21705637872219086, 0.21983474493026733, 0.12739592790603638, 0.2023983895778656, 0.13400156795978546, 0.171844482421875, 0.15920209884643555, 0.19514499604701996, 0.10956694185733795, 0.13168001174926758, 0.17660540342330933, 0.2191818207502365, 0.2091134935617447, 0.12864233553409576, 0.18559345602989197, 0.12924423813819885, 0.2118629366159439, 0.11646236479282379, 0.12565629184246063, 0.21835742890834808, 0.24819165468215942, 0.18557725846767426, 0.12322035431861877, 0.24720709025859833, 0.14966847002506256, 0.17223255336284637, 0.12646520137786865, 0.25533798336982727, 0.18645019829273224, 0.17907656729221344, 0.1523270308971405, 0.10340980440378189, 0.2529837191104889, 0.11309736222028732, 0.275308221578598, 0.13015146553516388, 0.18453212082386017, 0.1516546905040741, 0.14857232570648193, 0.12880456447601318, 0.21682491898536682, 0.19045503437519073, 0.14296798408031464, 0.223579540848732, 0.18090254068374634, 0.15605290234088898, 0.13221539556980133, 0.12849576771259308, 0.2667141854763031, 0.12688735127449036, 0.2199147492647171, 0.14049650728702545, 0.11352689564228058, 0.1867436319589615, 0.14909163117408752, 0.1917446404695511, 0.19021324813365936, 0.15713585913181305, 0.17069590091705322, 0.13458329439163208, 0.1980946660041809, 0.10394508391618729, 0.09979643672704697, 0.2601114809513092, 0.16440549492835999, 0.14080025255680084, 0.13193118572235107, 0.13907784223556519, 0.16258472204208374, 0.24783450365066528, 0.32785525918006897, 0.17701780796051025, 0.19054828584194183, 0.21502873301506042, 0.12930285930633545, 0.1305728554725647, 0.26520296931266785, 0.26207295060157776, 0.23310916125774384, 0.11458206921815872, 0.27927166223526, 0.12456077337265015, 0.16349126398563385, 0.2533693015575409, 0.1088423877954483, 0.2694988250732422, 0.15267202258110046, 0.17224134504795074, 0.21002788841724396, 0.17771363258361816, 0.19404321908950806, 0.17388226091861725, 0.17777270078659058, 0.1692594587802887, 0.2045513540506363, 0.21856695413589478, 0.12880371510982513, 0.1131589263677597, 0.21552444994449615, 0.19852733612060547, 0.20581583678722382, 0.23246097564697266, 0.14910182356834412, 0.18589343130588531, 0.1694692075252533, 0.15217700600624084, 0.17477639019489288, 0.190986767411232, 0.2216503769159317, 0.22191044688224792, 0.16548311710357666, 0.11392724514007568, 0.1418938785791397, 0.15833638608455658, 0.14231353998184204, 0.13835252821445465, 0.22475330531597137, 0.1345699429512024, 0.1816445291042328, 0.15148046612739563, 0.1306578814983368, 0.11851292103528976, 0.26695555448532104, 0.13535720109939575, 0.23997922241687775, 0.17882046103477478, 0.2396102249622345, 0.18239907920360565, 0.2352401316165924, 0.1452435851097107, 0.1321806162595749, 0.11572486162185669, 0.11797740310430527, 0.22964593768119812, 0.16807593405246735, 0.22909937798976898, 0.17264069616794586, 0.20628538727760315, 0.2655593156814575, 0.17657069861888885, 0.16708871722221375, 0.2060709148645401, 0.15969985723495483, 0.1573600172996521, 0.16498805582523346, 0.17745094001293182, 0.17015591263771057, 0.18896032869815826, 0.14073467254638672, 0.16548624634742737, 0.16263921558856964, 0.19721627235412598, 0.14791172742843628, 0.1812264621257782, 0.1815597116947174, 0.15718676149845123, 0.18386805057525635, 0.12493542581796646, 0.1648000180721283, 0.2317744642496109, 0.20698687434196472, 0.1445043832063675, 0.16599783301353455, 0.17186856269836426, 0.198201984167099, 0.2560313940048218, 0.11866994947195053, 0.18357490003108978, 0.13856790959835052, 0.2205793708562851, 0.24209335446357727, 0.18956048786640167, 0.15399575233459473, 0.13451720774173737, 0.16777217388153076, 0.12115749716758728, 0.15016593039035797, 0.12093637138605118, 0.2515665590763092, 0.1872612088918686, 0.12179415673017502, 0.19705310463905334, 0.17511440813541412, 0.11953330039978027, 0.24672886729240417, 0.16889427602291107, 0.18339554965496063, 0.11803276091814041, 0.24943792819976807, 0.14907054603099823, 0.13901357352733612, 0.16991138458251953, 0.13223670423030853, 0.12963920831680298, 0.17092613875865936, 0.1600969135761261, 0.12116344273090363, 0.18606117367744446, 0.14521236717700958, 0.2387634515762329, 0.18511316180229187, 0.13000257313251495, 0.13673022389411926, 0.1259942203760147, 0.12919433414936066, 0.2198907881975174, 0.17886142432689667, 0.21832233667373657, 0.15421345829963684, 0.1657329499721527, 0.12114469707012177, 0.1973867267370224, 0.11025386303663254, 0.15735861659049988, 0.20002038776874542, 0.15631847083568573, 0.23657773435115814, 0.13931283354759216, 0.25906890630722046, 0.2545991539955139, 0.11724243313074112, 0.18585744500160217, 0.11632107198238373, 0.20213910937309265, 0.1311263144016266, 0.2336408793926239, 0.18603619933128357, 0.13231463730335236, 0.22464069724082947, 0.1932050585746765, 0.1661679446697235, 0.16806796193122864, 0.17789097130298615, 0.13122901320457458, 0.18248213827610016, 0.1291218250989914, 0.16307277977466583, 0.25459399819374084, 0.1227133721113205, 0.1212579756975174, 0.23790661990642548, 0.13923640549182892, 0.20886512100696564, 0.1540430337190628, 0.12813493609428406, 0.1843148022890091, 0.19075101613998413, 0.16354645788669586, 0.14850932359695435, 0.11613016575574875, 0.12182655185461044, 0.12632466852664948, 0.21495597064495087, 0.13548937439918518, 0.19362527132034302, 0.1308089941740036, 0.13675568997859955, 0.17456792294979095, 0.2531526982784271, 0.22907206416130066, 0.1289796233177185, 0.24035143852233887, 0.15255038440227509, 0.1593523472547531, 0.18681076169013977, 0.26151207089424133, 0.12445902824401855, 0.10755239427089691, 0.13002155721187592, 0.22200758755207062, 0.2788645029067993, 0.11834120750427246, 0.1434532254934311, 0.20372523367404938, 0.2114810198545456, 0.2311100959777832, 0.1364995539188385, 0.17358556389808655, 0.23331178724765778, 0.2301982343196869, 0.16626639664173126, 0.1342003047466278, 0.248854398727417, 0.13640737533569336, 0.21313095092773438, 0.17194095253944397, 0.2317315638065338, 0.15004907548427582, 0.13960513472557068, 0.15584906935691833, 0.13642288744449615, 0.11226289719343185, 0.19740203022956848, 0.18406444787979126, 0.16021955013275146, 0.21631477773189545, 0.2225845605134964, 0.19130778312683105, 0.1901220828294754, 0.1760021597146988, 0.13518200814723969, 0.14953094720840454, 0.1717255860567093, 0.19627635180950165, 0.2076186090707779, 0.1695423126220703, 0.20910443365573883, 0.1933220624923706, 0.1464225798845291, 0.13664038479328156, 0.22207076847553253, 0.17861764132976532, 0.12948524951934814, 0.2035759687423706, 0.1856333613395691, 0.15692657232284546, 0.19902166724205017, 0.20108209550380707, 0.17201875150203705, 0.20191265642642975, 0.13819368183612823, 0.18317259848117828, 0.16259028017520905, 0.13936719298362732, 0.13948999345302582, 0.18939991295337677, 0.17991597950458527, 0.1561305671930313, 0.1895424872636795, 0.12583118677139282, 0.1871524453163147, 0.20614975690841675, 0.12683051824569702, 0.16413907706737518, 0.20850369334220886, 0.12407758831977844, 0.22870847582817078, 0.11689900606870651, 0.12342110276222229, 0.17458878457546234, 0.19223926961421967, 0.17787104845046997, 0.18756164610385895, 0.17434826493263245, 0.1359541267156601, 0.2205243855714798, 0.2020927369594574, 0.15955457091331482, 0.18207767605781555, 0.2353971302509308, 0.21743227541446686, 0.11949323117733002, 0.13140852749347687, 0.16480956971645355, 0.17053674161434174, 0.1449093371629715, 0.1611095517873764, 0.12107844650745392, 0.2604021430015564, 0.17676502466201782, 0.11480483412742615, 0.11842901259660721, 0.2366715669631958, 0.24796226620674133, 0.1932615041732788, 0.11973286420106888, 0.13291829824447632, 0.14579325914382935, 0.1775629073381424, 0.17843112349510193, 0.1524944007396698, 0.24765408039093018, 0.18187712132930756, 0.20203085243701935, 0.16517283022403717, 0.21878628432750702, 0.12583477795124054, 0.13538242876529694, 0.12225350737571716, 0.1488666534423828, 0.2845899760723114, 0.1716407835483551, 0.12619322538375854, 0.22085580229759216, 0.16489310562610626, 0.1435462385416031, 0.158741295337677, 0.1964486986398697, 0.23754465579986572, 0.20354853570461273, 0.1954769790172577, 0.19708821177482605, 0.17071545124053955, 0.16076035797595978, 0.16302910447120667, 0.17269305884838104, 0.11719904094934464, 0.16806773841381073, 0.14880287647247314, 0.18201206624507904, 0.22990277409553528, 0.14754396677017212, 0.15618707239627838, 0.16414690017700195, 0.1650179773569107, 0.174386128783226, 0.18365511298179626, 0.15921151638031006, 0.13885186612606049, 0.15300753712654114, 0.1387205421924591, 0.1456926465034485, 0.11687134951353073, 0.22205646336078644, 0.1460779756307602, 0.17731045186519623, 0.2064029723405838, 0.1342829316854477, 0.147170290350914, 0.15871985256671906, 0.13400524854660034, 0.2035200595855713, 0.1332392692565918, 0.13121631741523743, 0.17519044876098633, 0.23978601396083832, 0.15486206114292145, 0.20488491654396057, 0.21048924326896667, 0.12862174212932587, 0.13346868753433228, 0.23313799500465393, 0.17330071330070496, 0.22927340865135193, 0.20903471112251282, 0.1714862585067749, 0.26188915967941284, 0.14430932700634003, 0.2210145890712738, 0.1957460343837738, 0.1489274501800537, 0.10284057259559631, 0.18014496564865112, 0.09754341840744019, 0.2128026932477951, 0.16996826231479645, 0.14833566546440125, 0.1399029642343521, 0.18179523944854736, 0.13073258101940155, 0.1963086575269699, 0.13807542622089386, 0.19571129977703094, 0.12892834842205048, 0.19578643143177032, 0.1960771530866623, 0.16350118815898895, 0.2367202490568161, 0.1954595297574997, 0.12226709723472595, 0.2128663957118988, 0.182808056473732, 0.20327778160572052, 0.13695670664310455, 0.14410893619060516, 0.13229990005493164, 0.17122545838356018, 0.17003986239433289, 0.21330347657203674, 0.15086007118225098, 0.2661795914173126, 0.1186550036072731, 0.18289031088352203, 0.1280033439397812, 0.12776534259319305, 0.2358628362417221, 0.12167876213788986, 0.20786790549755096, 0.1256171017885208, 0.1500915139913559, 0.14109553396701813, 0.14470158517360687, 0.1914817839860916, 0.2575663626194, 0.172978937625885, 0.21967850625514984, 0.18558382987976074, 0.2701638340950012, 0.22287172079086304, 0.18111251294612885, 0.1348622739315033, 0.11408569663763046, 0.1827886402606964, 0.17351260781288147, 0.12300234287977219, 0.11810080707073212, 0.2330595701932907, 0.11884599179029465, 0.14354291558265686, 0.21141350269317627, 0.12802653014659882, 0.14780931174755096, 0.12949661910533905, 0.23721864819526672, 0.12136269360780716, 0.14105269312858582, 0.10832103341817856, 0.16511638462543488, 0.1987895369529724, 0.22307559847831726, 0.2252957671880722, 0.27517634630203247, 0.24551628530025482, 0.10893911868333817, 0.25844234228134155, 0.25694555044174194, 0.1914653480052948, 0.17683622241020203, 0.1160360798239708, 0.16765974462032318, 0.10956412553787231, 0.17946290969848633, 0.18367156386375427, 0.17486752569675446, 0.21606063842773438, 0.16878722608089447, 0.176692932844162, 0.14238081872463226, 0.25267431139945984, 0.1279822289943695, 0.2585678994655609, 0.17148292064666748, 0.15778815746307373, 0.1689794808626175, 0.17353390157222748, 0.19677835702896118, 0.1735379993915558, 0.15304270386695862, 0.1276237517595291, 0.14579668641090393, 0.24865029752254486, 0.26529625058174133, 0.17261281609535217, 0.21546663343906403, 0.19794847071170807, 0.17704375088214874, 0.1304958611726761, 0.15910343825817108, 0.18174006044864655, 0.1735953837633133, 0.15272103250026703, 0.1757124662399292, 0.17714624106884003, 0.1079811155796051, 0.12874571979045868, 0.1463446021080017, 0.1747334897518158, 0.15677227079868317, 0.17085261642932892, 0.2061048150062561, 0.19537946581840515, 0.16005131602287292, 0.24080541729927063, 0.1668788343667984, 0.16576477885246277, 0.15633094310760498, 0.1544356495141983, 0.14737798273563385, 0.21097901463508606, 0.24064341187477112, 0.14425650238990784, 0.20364807546138763, 0.18397410213947296, 0.13353954255580902, 0.17453640699386597, 0.176529660820961, 0.14370866119861603, 0.13468657433986664, 0.17025451362133026, 0.15357713401317596, 0.22999735176563263, 0.1399550586938858, 0.12138884514570236, 0.14292888343334198, 0.18726515769958496, 0.13274431228637695, 0.19900298118591309, 0.1413348913192749, 0.2292845994234085, 0.21412670612335205, 0.19644440710544586, 0.12966224551200867, 0.2183489352464676, 0.1594383418560028, 0.20409001410007477, 0.1461944878101349, 0.14908598363399506, 0.163965106010437, 0.20157568156719208, 0.12960107624530792, 0.12912176549434662, 0.1602618545293808, 0.16971641778945923, 0.12807680666446686, 0.18888302147388458, 0.23682381212711334, 0.1503305733203888, 0.16236984729766846, 0.18523824214935303, 0.1622021645307541, 0.1255713403224945, 0.2105056196451187, 0.17387616634368896, 0.12594656646251678, 0.1860925704240799, 0.15498727560043335, 0.15863032639026642, 0.20727413892745972, 0.13520033657550812, 0.18716180324554443, 0.18661265075206757, 0.24948318302631378, 0.16928257048130035, 0.16502049565315247, 0.12091625481843948, 0.1987970769405365, 0.11979275941848755, 0.14602641761302948, 0.17095230519771576, 0.18865373730659485, 0.13802745938301086, 0.23379619419574738, 0.14893320202827454, 0.17717930674552917, 0.21919535100460052, 0.1516488492488861, 0.19282302260398865, 0.11498546600341797, 0.1508650779724121, 0.1458183377981186, 0.1914217323064804, 0.2150607854127884, 0.17792579531669617, 0.15978319942951202, 0.12373184412717819, 0.1644584834575653, 0.1738259643316269, 0.1499953716993332, 0.15253235399723053, 0.2685886323451996, 0.1989029198884964, 0.17720825970172882, 0.16056424379348755, 0.1702495962381363, 0.19356420636177063, 0.18931959569454193, 0.14794611930847168, 0.1934843510389328, 0.18691304326057434, 0.1708628237247467, 0.14838166534900665, 0.14351500570774078, 0.2589365243911743, 0.1464531123638153, 0.1972847282886505, 0.10554491728544235, 0.23812931776046753, 0.11924523860216141, 0.14403896033763885, 0.1854214072227478, 0.22491714358329773, 0.16828952729701996, 0.14767949283123016, 0.1909923106431961, 0.13290873169898987, 0.1326756775379181, 0.17936404049396515, 0.14036715030670166, 0.16471076011657715, 0.14332447946071625, 0.20090757310390472, 0.21783387660980225, 0.2145063728094101, 0.19714875519275665, 0.14545053243637085, 0.15780757367610931, 0.17000442743301392, 0.18730278313159943, 0.16662785410881042, 0.23773163557052612, 0.20432859659194946, 0.1500602811574936, 0.1702342927455902, 0.1722487062215805, 0.26064857840538025, 0.1535322666168213, 0.1542387455701828, 0.19817997515201569, 0.1304512470960617, 0.2174316644668579, 0.21297986805438995, 0.14843779802322388, 0.14851093292236328, 0.14485245943069458, 0.20556245744228363, 0.13079141080379486, 0.18203143775463104, 0.14477837085723877, 0.15093165636062622, 0.1599833220243454, 0.13586212694644928, 0.1391061395406723, 0.21215848624706268, 0.2080685794353485, 0.12038362771272659, 0.1959066390991211, 0.187367781996727, 0.13988788425922394, 0.21868149936199188, 0.1730908751487732, 0.16070955991744995, 0.21276827156543732, 0.14592856168746948, 0.16706520318984985, 0.1529289036989212, 0.12237570434808731, 0.24539850652217865, 0.15101398527622223, 0.155964195728302, 0.1392507702112198, 0.16973698139190674, 0.18437744677066803, 0.1787862330675125, 0.18201564252376556, 0.17664961516857147, 0.13374432921409607, 0.13954198360443115, 0.1742953658103943, 0.2675293982028961, 0.16284702718257904, 0.14557094871997833, 0.13352172076702118, 0.16232016682624817, 0.17395813763141632, 0.15499670803546906, 0.1480785310268402, 0.18823738396167755, 0.14661674201488495, 0.23010721802711487, 0.2244832068681717, 0.1477503925561905, 0.20984944701194763, 0.17600034177303314, 0.21464353799819946, 0.13619907200336456, 0.17252203822135925, 0.14647264778614044, 0.15047374367713928, 0.200058251619339, 0.15912587940692902, 0.147728830575943, 0.23453979194164276, 0.21044707298278809, 0.15078113973140717, 0.16036628186702728, 0.21783624589443207, 0.18568560481071472, 0.19139021635055542, 0.21724945306777954, 0.17595292627811432, 0.18044526875019073, 0.16339881718158722, 0.14948970079421997, 0.2129569947719574, 0.17200547456741333, 0.2216363251209259, 0.21373586356639862, 0.1612243950366974, 0.17471888661384583, 0.1697612702846527, 0.19818861782550812, 0.18543338775634766, 0.17564035952091217, 0.19207818806171417, 0.18201680481433868, 0.15116046369075775, 0.15153399109840393, 0.19539211690425873, 0.18874815106391907, 0.1641882061958313, 0.151716411113739, 0.16753841936588287, 0.14129340648651123, 0.173184335231781, 0.20349256694316864, 0.186958447098732, 0.10922371596097946, 0.1724245399236679, 0.17661042511463165, 0.1584395319223404, 0.18253657221794128, 0.14108021557331085, 0.1786106824874878, 0.18886145949363708, 0.16771604120731354, 0.20174521207809448, 0.15794052183628082, 0.20906290411949158, 0.21662744879722595, 0.14903680980205536, 0.13820137083530426, 0.16548578441143036, 0.1729874461889267, 0.14035868644714355, 0.20148099958896637, 0.23780202865600586, 0.12727509438991547, 0.16952016949653625, 0.13127319514751434, 0.16445867717266083, 0.15181636810302734, 0.18197201192378998, 0.2061702311038971, 0.18749605119228363, 0.16638846695423126, 0.17275908589363098, 0.21843019127845764, 0.1393888145685196, 0.2116605043411255, 0.17793771624565125, 0.18394699692726135, 0.18030965328216553, 0.16897328197956085, 0.1752849817276001, 0.195451021194458, 0.12831468880176544, 0.1616424024105072, 0.21458446979522705, 0.1290479302406311, 0.12247464060783386, 0.20186778903007507, 0.12763966619968414, 0.23473814129829407, 0.1971341073513031, 0.17087213695049286, 0.15578743815422058, 0.13621893525123596, 0.17119185626506805, 0.20253679156303406, 0.19580239057540894, 0.12846532464027405, 0.18973231315612793, 0.2351146936416626, 0.17899268865585327, 0.1798376441001892, 0.16198059916496277, 0.2069541960954666, 0.18004202842712402, 0.18055208027362823, 0.16436024010181427, 0.20148466527462006, 0.16971635818481445, 0.16498243808746338, 0.13527236878871918, 0.16672131419181824, 0.19549500942230225, 0.15433239936828613, 0.13614703714847565, 0.22422146797180176, 0.20131847262382507, 0.20679736137390137, 0.22138121724128723, 0.1912573277950287, 0.21746720373630524, 0.18599426746368408, 0.12615273892879486, 0.1996590942144394, 0.17300298810005188, 0.15631040930747986, 0.14912371337413788, 0.15409627556800842, 0.17391392588615417, 0.22532661259174347, 0.17751096189022064, 0.16087087988853455, 0.14694450795650482, 0.1982390433549881, 0.18452203273773193, 0.1337701976299286, 0.14520759880542755, 0.18660947680473328, 0.20406965911388397, 0.1743205040693283, 0.14427295327186584, 0.15392106771469116, 0.13692782819271088, 0.19484907388687134, 0.1400596797466278, 0.13396033644676208, 0.1815977543592453, 0.15627972781658173, 0.18573246896266937, 0.19191168248653412, 0.1670674830675125, 0.20384827256202698, 0.19009871780872345, 0.17138807475566864, 0.11782202124595642, 0.118434838950634, 0.17317794263362885, 0.13362829387187958, 0.16812843084335327, 0.16268038749694824, 0.2578059434890747, 0.18131442368030548, 0.16750356554985046, 0.18422892689704895, 0.1508994698524475, 0.14983102679252625, 0.2043994814157486, 0.16360147297382355, 0.14015471935272217, 0.191427081823349, 0.11392775923013687, 0.11317439377307892, 0.12287144362926483, 0.16038042306900024, 0.13870365917682648, 0.19254684448242188, 0.14086930453777313, 0.19961388409137726, 0.17281357944011688, 0.24762311577796936, 0.2721118927001953, 0.15855459868907928, 0.10780485719442368, 0.14288495481014252, 0.17954793572425842, 0.14012591540813446, 0.15108971297740936, 0.15423379838466644, 0.11405298113822937, 0.16988317668437958, 0.1782471090555191, 0.15754778683185577, 0.24403128027915955, 0.2024274617433548, 0.2306315004825592, 0.1809028834104538, 0.14678753912448883, 0.14752018451690674, 0.16849581897258759, 0.1673995554447174, 0.19454072415828705, 0.20612746477127075, 0.191585972905159, 0.14142675697803497, 0.1651155650615692, 0.1347517967224121, 0.12500731647014618, 0.14861026406288147, 0.10530395805835724, 0.1522175818681717, 0.10424775630235672, 0.1353439837694168, 0.10071983188390732, 0.2519848942756653, 0.17944253981113434, 0.24085059762001038, 0.21706809103488922, 0.1559898406267166, 0.25964903831481934, 0.22266772389411926, 0.14537207782268524, 0.15363764762878418, 0.13209573924541473, 0.19605979323387146, 0.22212786972522736, 0.14048558473587036, 0.1663588285446167, 0.1370197832584381, 0.10136187076568604, 0.1589447408914566, 0.15756380558013916, 0.2836104929447174, 0.1956550031900406, 0.17924824357032776, 0.14609897136688232, 0.1311008632183075, 0.14622880518436432, 0.1593332588672638, 0.15291684865951538, 0.14760351181030273, 0.14339099824428558, 0.14790412783622742, 0.16324496269226074, 0.23084641993045807, 0.14421753585338593, 0.15170815587043762, 0.18187572062015533, 0.1841883510351181, 0.19038601219654083, 0.1905432492494583, 0.11050767451524734, 0.1433051973581314, 0.12873342633247375, 0.2408359944820404, 0.20688915252685547, 0.15640780329704285, 0.24997395277023315, 0.2829805612564087, 0.15889300405979156, 0.10335727781057358, 0.10371332615613937, 0.18778692185878754, 0.1422189325094223, 0.18159931898117065, 0.22449353337287903, 0.15977932512760162, 0.13338986039161682, 0.13582687079906464, 0.14489607512950897, 0.23493948578834534, 0.1054752767086029, 0.19370396435260773, 0.17035377025604248, 0.105173759162426, 0.15300242602825165, 0.17439286410808563, 0.2274320125579834, 0.21761727333068848, 0.154829204082489, 0.21823203563690186, 0.1951880156993866, 0.18346576392650604, 0.268942266702652, 0.12720587849617004, 0.15313361585140228, 0.1106300801038742, 0.13520491123199463, 0.14342842996120453, 0.11165033280849457, 0.15961027145385742, 0.23768168687820435, 0.13333724439144135, 0.143757164478302, 0.18644410371780396, 0.22844485938549042, 0.18078148365020752, 0.188113734126091, 0.20628197491168976, 0.14273688197135925, 0.1593475490808487, 0.20959517359733582, 0.1266736537218094, 0.18111075460910797, 0.15367265045642853, 0.20194680988788605, 0.22090880572795868, 0.11439336091279984, 0.1691141128540039, 0.22191601991653442, 0.1548434942960739, 0.19554142653942108, 0.16456004977226257, 0.12641289830207825, 0.24906770884990692, 0.17047417163848877, 0.17424894869327545, 0.1898214966058731, 0.2013690024614334, 0.14518524706363678, 0.21937474608421326, 0.211000457406044, 0.1552266925573349, 0.21053293347358704, 0.15975221991539001, 0.19179941713809967, 0.13904689252376556, 0.12636351585388184, 0.17231537401676178, 0.12005678564310074, 0.1788860559463501, 0.17915070056915283, 0.17430409789085388, 0.17618395388126373, 0.17352530360221863, 0.19583934545516968, 0.1728275716304779, 0.19714169204235077, 0.20565283298492432, 0.14699606597423553, 0.17383548617362976, 0.19445280730724335, 0.1437080353498459, 0.16676710546016693, 0.1733188033103943, 0.12960654497146606, 0.14682753384113312, 0.20852941274642944, 0.2009483426809311, 0.193076953291893, 0.18052838742733002, 0.16280724108219147, 0.14081346988677979, 0.2037726491689682, 0.15464653074741364, 0.14288368821144104, 0.13220569491386414, 0.17284588515758514, 0.13054470717906952, 0.21425919234752655, 0.13881799578666687, 0.1281013786792755, 0.1282169371843338, 0.20435820519924164, 0.15520676970481873, 0.18929260969161987, 0.175047367811203, 0.18797704577445984, 0.1954479068517685, 0.17717336118221283, 0.1445847749710083, 0.15385738015174866, 0.13530029356479645, 0.1992131471633911, 0.13784442842006683, 0.19462274014949799, 0.1706826090812683, 0.11697382479906082, 0.16365662217140198, 0.1855604350566864, 0.16230258345603943, 0.15170910954475403, 0.20133398473262787, 0.20858706533908844, 0.1417025625705719, 0.12844154238700867, 0.21551372110843658, 0.15517528355121613, 0.13919176161289215, 0.20041653513908386, 0.18208760023117065, 0.14841438829898834, 0.18421028554439545, 0.18354135751724243, 0.17953066527843475, 0.13900379836559296, 0.1602705419063568, 0.1540183573961258, 0.21870917081832886, 0.1311907172203064, 0.2419222742319107, 0.1777634173631668, 0.19880689680576324, 0.20752352476119995, 0.11462472379207611, 0.15592855215072632, 0.1871824413537979, 0.20750801265239716, 0.15088939666748047, 0.13138574361801147, 0.2145799696445465, 0.16296297311782837, 0.14649273455142975, 0.2372649759054184, 0.21349167823791504, 0.15860706567764282, 0.17290031909942627, 0.1682075560092926, 0.13315661251544952, 0.2202751785516739, 0.19324511289596558, 0.17042289674282074, 0.12261293828487396, 0.23284177482128143, 0.19038598239421844, 0.15634138882160187, 0.17723920941352844, 0.15290991961956024, 0.1606469601392746, 0.15249338746070862, 0.1400994062423706, 0.1274469643831253, 0.14198894798755646, 0.1633310616016388, 0.18213242292404175, 0.19082510471343994, 0.1986476629972458, 0.17038923501968384, 0.16292399168014526, 0.16978499293327332, 0.16214413940906525, 0.19758792221546173, 0.19785788655281067, 0.20178093016147614, 0.23233027756214142, 0.1275305598974228, 0.20815180242061615, 0.16189336776733398, 0.18078680336475372, 0.18135300278663635, 0.16819408535957336, 0.131479412317276, 0.14731886982917786, 0.19165919721126556, 0.18057049810886383, 0.15672357380390167, 0.13155107200145721, 0.15041089057922363, 0.1804177314043045, 0.1667289435863495, 0.14018593728542328, 0.19129496812820435, 0.16257363557815552, 0.13159595429897308, 0.16642430424690247, 0.19029279053211212, 0.19247859716415405, 0.1807904988527298, 0.1738211214542389, 0.23812821507453918, 0.17995035648345947, 0.20033234357833862, 0.12949474155902863, 0.1262022852897644, 0.1883244663476944, 0.2291567474603653, 0.13180597126483917, 0.17561140656471252, 0.21622219681739807, 0.15869826078414917, 0.19288690388202667, 0.21702736616134644, 0.2169700711965561, 0.22537972033023834, 0.21203815937042236, 0.1531437635421753, 0.1859218180179596, 0.15170134603977203, 0.13675497472286224, 0.21803748607635498, 0.1328745186328888, 0.15923622250556946, 0.16426606476306915, 0.15656395256519318, 0.15591660141944885, 0.1809965819120407, 0.15400920808315277, 0.1698170155286789, 0.13387969136238098, 0.16277381777763367, 0.20251277089118958, 0.1497986614704132, 0.18002566695213318, 0.13088034093379974, 0.13047538697719574, 0.20273733139038086, 0.19935409724712372, 0.219801664352417, 0.22741927206516266, 0.15774628520011902, 0.168554425239563, 0.1855194866657257, 0.14598585665225983, 0.12937414646148682, 0.20084984600543976, 0.214466854929924, 0.1285906583070755, 0.15810714662075043, 0.16733519732952118, 0.2329176962375641, 0.1302414834499359, 0.12767307460308075, 0.18551424145698547, 0.15598243474960327, 0.17617793381214142, 0.23567983508110046, 0.15292946994304657, 0.18979088962078094, 0.18905067443847656, 0.13091105222702026, 0.1633342057466507, 0.21134448051452637, 0.19137462973594666, 0.12703843414783478, 0.1272144466638565, 0.144145205616951, 0.21223029494285583, 0.14997883141040802, 0.12557895481586456, 0.16788001358509064, 0.18034504354000092, 0.14673903584480286, 0.22820580005645752, 0.24059641361236572, 0.18425369262695312, 0.2087271511554718, 0.19029158353805542, 0.156170055270195, 0.14655044674873352, 0.13496504724025726, 0.19947324693202972, 0.16404938697814941, 0.1434953808784485, 0.23015965521335602, 0.12447104603052139, 0.12330829352140427, 0.1710471510887146, 0.17381440103054047, 0.12185212224721909, 0.17832456529140472, 0.1331246793270111, 0.15481650829315186, 0.1599438637495041, 0.18693189322948456, 0.18864841759204865, 0.16137418150901794, 0.19562731683254242, 0.21166269481182098, 0.1818246990442276, 0.14509831368923187, 0.16756966710090637, 0.15560518205165863, 0.15900899469852448, 0.17450836300849915, 0.16327714920043945, 0.17176462709903717, 0.2288556694984436, 0.14684158563613892, 0.13930773735046387, 0.24289977550506592, 0.1754622608423233, 0.15078873932361603, 0.12937778234481812, 0.1389206349849701, 0.1319320946931839, 0.18662022054195404, 0.16423365473747253, 0.16139990091323853, 0.12186363339424133, 0.24117721617221832, 0.15333619713783264, 0.18389476835727692, 0.1882314682006836, 0.16068117320537567, 0.12246298044919968, 0.1581725776195526, 0.18063689768314362, 0.17987731099128723, 0.12065349519252777, 0.16397719085216522, 0.18784259259700775, 0.14163963496685028, 0.15471400320529938, 0.181919127702713, 0.1609887182712555, 0.24979691207408905, 0.19050098955631256, 0.18944121897220612, 0.17358539998531342, 0.20505569875240326, 0.21108433604240417, 0.1603674441576004, 0.2249809354543686, 0.12155428528785706, 0.16185122728347778, 0.12776415050029755, 0.185053750872612, 0.23972339928150177, 0.20906324684619904, 0.2119261771440506, 0.19032886624336243, 0.16243717074394226, 0.17300648987293243, 0.17006069421768188, 0.2034056931734085, 0.14308464527130127, 0.17518797516822815, 0.2222539633512497, 0.18908335268497467, 0.17846474051475525, 0.16731710731983185, 0.197633296251297, 0.14146888256072998, 0.16163788735866547, 0.16012559831142426, 0.1233699694275856, 0.16211570799350739, 0.19718901813030243, 0.1596280187368393, 0.14315417408943176, 0.17424917221069336, 0.17941688001155853, 0.14444895088672638, 0.18824884295463562, 0.21143782138824463, 0.1815202534198761, 0.17524898052215576, 0.14279486238956451, 0.1890794187784195, 0.22503186762332916, 0.14375808835029602, 0.1773652583360672, 0.16196729242801666, 0.1834539920091629, 0.1446436494588852, 0.20496925711631775, 0.19506637752056122, 0.20554544031620026, 0.17435303330421448, 0.18877731263637543, 0.15778425335884094, 0.20573866367340088, 0.1681557297706604, 0.17427079379558563, 0.15234597027301788, 0.18293531239032745, 0.21753613650798798, 0.15320627391338348, 0.16559231281280518, 0.16391055285930634, 0.16995160281658173, 0.1519239991903305, 0.17402324080467224, 0.16711892187595367, 0.13609394431114197, 0.1662115603685379, 0.15761591494083405, 0.17431840300559998, 0.1786298304796219, 0.1840110719203949, 0.21863849461078644, 0.18180283904075623, 0.1414409726858139, 0.15000410377979279, 0.22247005999088287, 0.1431088000535965, 0.15370850265026093, 0.18997563421726227, 0.17239689826965332, 0.1487804353237152, 0.13704192638397217, 0.1431802362203598, 0.1533297449350357, 0.20259438455104828, 0.1402793824672699, 0.13154776394367218, 0.1688479334115982, 0.1411220133304596, 0.16015611588954926, 0.20550525188446045, 0.1732180118560791, 0.21058978140354156, 0.14439715445041656, 0.15537258982658386, 0.16988590359687805, 0.17935672402381897, 0.17404983937740326, 0.14432989060878754, 0.21635235846042633, 0.2341989278793335, 0.12431292980909348, 0.16717776656150818, 0.13839229941368103, 0.16690392792224884, 0.14960183203220367, 0.18696078658103943, 0.20290055871009827, 0.16613729298114777, 0.1765662431716919, 0.17012351751327515, 0.21906143426895142, 0.21731051802635193, 0.12862807512283325, 0.1258145421743393, 0.1784692406654358, 0.17021425068378448, 0.15665389597415924, 0.19058407843112946, 0.16480369865894318, 0.1922372430562973, 0.1770123988389969, 0.18382936716079712, 0.13189294934272766, 0.1420973241329193, 0.17635101079940796, 0.1644875705242157, 0.14512355625629425, 0.15917707979679108, 0.16009148955345154, 0.16305024921894073, 0.2073143720626831, 0.16431553661823273, 0.12541049718856812, 0.13519993424415588, 0.1797839254140854, 0.1605706363916397, 0.16445745527744293, 0.23810793459415436, 0.18567079305648804, 0.20211032032966614, 0.18460369110107422, 0.17498576641082764, 0.17383265495300293, 0.12454647570848465, 0.23916497826576233, 0.1451466828584671, 0.1900777667760849, 0.1912209540605545, 0.1903482973575592, 0.2246745377779007, 0.1716097891330719, 0.13774894177913666, 0.18196797370910645, 0.1657232642173767, 0.19900920987129211, 0.22717978060245514, 0.15467895567417145, 0.23811250925064087, 0.17705756425857544, 0.14102612435817719, 0.18831971287727356, 0.1870502084493637, 0.1618180125951767, 0.16928265988826752, 0.20232771337032318, 0.20302845537662506, 0.19946198165416718, 0.1488841325044632, 0.15420450270175934, 0.18750523030757904, 0.15087534487247467, 0.16578271985054016, 0.23538000881671906, 0.142739936709404, 0.14451438188552856, 0.19145657122135162, 0.13345731794834137, 0.1330968737602234, 0.17130613327026367, 0.15165136754512787, 0.12474124878644943, 0.1933925747871399, 0.18789580464363098, 0.15063706040382385, 0.2159510850906372, 0.16961830854415894, 0.1866006851196289, 0.2111027091741562, 0.15523111820220947, 0.168254554271698, 0.1298931986093521, 0.13013948500156403, 0.1555420607328415]\n",
            "Val loss 0.16846954894690175\n",
            "Val auc roc 0.5119657052066731\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b6a48a4b8cd4ee6b1338dab4119f35e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2036.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1708\n",
            "Train Losses : [0.20581626892089844, 0.12969917058944702, 0.15764647722244263, 0.14594045281410217, 0.1278967708349228, 0.1885310411453247, 0.22729207575321198, 0.2194555550813675, 0.16865015029907227, 0.14600785076618195, 0.18482033908367157, 0.1563921719789505, 0.15492890775203705, 0.16297122836112976, 0.13307957351207733, 0.21414270997047424, 0.15096694231033325, 0.16984176635742188, 0.13599473237991333, 0.16865378618240356, 0.1646115779876709, 0.20961037278175354, 0.23184318840503693, 0.1877054125070572, 0.14483779668807983, 0.20346158742904663, 0.12464277446269989, 0.12576116621494293, 0.1569482535123825, 0.20249848067760468, 0.1523679792881012, 0.24101677536964417, 0.1704675853252411, 0.17382308840751648, 0.12485247105360031, 0.18907280266284943, 0.14782041311264038, 0.14428137242794037, 0.173372283577919, 0.1287030428647995, 0.16463357210159302, 0.12220882624387741, 0.14988848567008972, 0.1627936065196991, 0.14145056903362274, 0.24616612493991852, 0.18308179080486298, 0.1820986270904541, 0.16793116927146912, 0.23118387162685394, 0.18852265179157257, 0.20123817026615143, 0.12316861748695374, 0.21046613156795502, 0.14805051684379578, 0.22068840265274048, 0.12234952300786972, 0.1543215662240982, 0.14864639937877655, 0.1984226256608963, 0.195936381816864, 0.12275733053684235, 0.1845322549343109, 0.1735367625951767, 0.12273269891738892, 0.1573014259338379, 0.24068373441696167, 0.14053873717784882, 0.16775070130825043, 0.2400074452161789, 0.14821922779083252, 0.16475321352481842, 0.12413240969181061, 0.17688602209091187, 0.23621073365211487, 0.23893484473228455, 0.17043437063694, 0.13964836299419403, 0.130402073264122, 0.16843535006046295, 0.20108672976493835, 0.1777854710817337, 0.19666153192520142, 0.1252761334180832, 0.1465853899717331, 0.17598429322242737, 0.1877918243408203, 0.16993428766727448, 0.152132049202919, 0.1253541260957718, 0.19183963537216187, 0.142194464802742, 0.1707688570022583, 0.17362841963768005, 0.1520392745733261, 0.1519196629524231, 0.20691566169261932, 0.16276615858078003, 0.14818525314331055, 0.1598726361989975, 0.19477643072605133, 0.16264089941978455, 0.16027644276618958, 0.15018770098686218, 0.11864813417196274, 0.11845412850379944, 0.19714248180389404, 0.14884941279888153, 0.15647244453430176, 0.2335616946220398, 0.13541707396507263, 0.2241245061159134, 0.16581156849861145, 0.24595791101455688, 0.14325833320617676, 0.1495157927274704, 0.1909516453742981, 0.22565923631191254, 0.16911740601062775, 0.15742720663547516, 0.19963812828063965, 0.17300604283809662, 0.17000751197338104, 0.11643792688846588, 0.11563362181186676, 0.2328299582004547, 0.152883842587471, 0.25626808404922485, 0.13233035802841187, 0.17615284025669098, 0.16092601418495178, 0.11615560948848724, 0.11485709249973297, 0.16153672337532043, 0.15179818868637085, 0.13239191472530365, 0.15130329132080078, 0.20396672189235687, 0.2083291858434677, 0.11187677085399628, 0.1880379617214203, 0.2029045671224594, 0.11080601066350937, 0.1108175590634346, 0.13837862014770508, 0.151046484708786, 0.26142507791519165, 0.17700114846229553, 0.10724668204784393, 0.1452571600675583, 0.17427343130111694, 0.16158191859722137, 0.19568616151809692, 0.10460886359214783, 0.20059385895729065, 0.1401146650314331, 0.16637647151947021, 0.10265978425741196, 0.1470344364643097, 0.17385780811309814, 0.22149619460105896, 0.21879585087299347, 0.10035796463489532, 0.22837884724140167, 0.16466590762138367, 0.24649611115455627, 0.16556712985038757, 0.18190406262874603, 0.13395358622074127, 0.13110198080539703, 0.27986615896224976, 0.1745571494102478, 0.23528189957141876, 0.20197917520999908, 0.18865177035331726, 0.19045390188694, 0.13928759098052979, 0.10779090970754623, 0.1874985694885254, 0.16062268614768982, 0.14622874557971954, 0.15213605761528015, 0.14316697418689728, 0.18289682269096375, 0.2116524875164032, 0.13288553059101105, 0.17429737746715546, 0.21672303974628448, 0.14686250686645508, 0.10870058834552765, 0.14638948440551758, 0.17557431757450104, 0.13859616219997406, 0.20390118658542633, 0.1267910897731781, 0.18982447683811188, 0.20012713968753815, 0.2696250081062317, 0.21190157532691956, 0.19036118686199188, 0.1627943217754364, 0.126799538731575, 0.2452593743801117, 0.18140913546085358, 0.14472593367099762, 0.16685797274112701, 0.19189906120300293, 0.18999965488910675, 0.258339524269104, 0.16974219679832458, 0.1487656831741333, 0.21609379351139069, 0.21297752857208252, 0.2017911970615387, 0.17673373222351074, 0.2063864916563034, 0.11944717168807983, 0.12027493119239807, 0.11989886313676834, 0.1590198427438736, 0.19253388047218323, 0.19776873290538788, 0.19190692901611328, 0.18536457419395447, 0.17744821310043335, 0.1714508980512619, 0.23017342388629913, 0.22021755576133728, 0.16294988989830017, 0.15219727158546448, 0.16510219871997833, 0.200003981590271, 0.18081574141979218, 0.17097961902618408, 0.23628877103328705, 0.15979135036468506, 0.20015676319599152, 0.18275630474090576, 0.15631802380084991, 0.12428756803274155, 0.1999000757932663, 0.20400966703891754, 0.18136024475097656, 0.23656702041625977, 0.1507120281457901, 0.20706012845039368, 0.13911044597625732, 0.18528522551059723, 0.15862128138542175, 0.1766831874847412, 0.15075404942035675, 0.21221192181110382, 0.18771754205226898, 0.17710188031196594, 0.12862004339694977, 0.18959972262382507, 0.14210253953933716, 0.16778427362442017, 0.17593228816986084, 0.15283717215061188, 0.14824619889259338, 0.12984110414981842, 0.149378702044487, 0.170263409614563, 0.14670810103416443, 0.17297330498695374, 0.19056056439876556, 0.20776662230491638, 0.15277284383773804, 0.19834254682064056, 0.23171941936016083, 0.2014787644147873, 0.1760111302137375, 0.1892353892326355, 0.1617116928100586, 0.14260299503803253, 0.22692839801311493, 0.14465977251529694, 0.18517248332500458, 0.17204423248767853, 0.1566346436738968, 0.1578129529953003, 0.1539257913827896, 0.22513702511787415, 0.16498573124408722, 0.14817459881305695, 0.19057634472846985, 0.13217675685882568, 0.20051440596580505, 0.1810365617275238, 0.14849311113357544, 0.16693320870399475, 0.18792778253555298, 0.19696925580501556, 0.1866271048784256, 0.13166388869285583, 0.16618452966213226, 0.17994804680347443, 0.1808525174856186, 0.1468011736869812, 0.14620134234428406, 0.2051907479763031, 0.175101637840271, 0.1599506139755249, 0.19012902677059174, 0.17084264755249023, 0.18603092432022095, 0.14313054084777832, 0.20637883245944977, 0.14438937604427338, 0.15654973685741425, 0.14371302723884583, 0.13924036920070648, 0.15264829993247986, 0.1428280770778656, 0.19933655858039856, 0.23098690807819366, 0.1610078066587448, 0.15666574239730835, 0.15565019845962524, 0.23212382197380066, 0.2348324954509735, 0.1283472180366516, 0.15337100625038147, 0.1283528059720993, 0.165043443441391, 0.15585601329803467, 0.20865854620933533, 0.12698814272880554, 0.2254774272441864, 0.16330133378505707, 0.1511755734682083, 0.18510660529136658, 0.18763263523578644, 0.16574092209339142, 0.144145205616951, 0.13147825002670288, 0.1671547293663025, 0.1963314563035965, 0.23715870082378387, 0.17476466298103333, 0.1574012190103531, 0.15030042827129364, 0.1581169217824936, 0.1891273409128189, 0.14307089149951935, 0.12615966796875, 0.1895395964384079, 0.14417316019535065, 0.1624678075313568, 0.18320655822753906, 0.20913226902484894, 0.1482868641614914, 0.14116735756397247, 0.20473149418830872, 0.20603661239147186, 0.19622200727462769, 0.15787175297737122, 0.15126794576644897, 0.1550583690404892, 0.1622592806816101, 0.14698031544685364, 0.21856525540351868, 0.19232888519763947, 0.12628941237926483, 0.13426855206489563, 0.1754598170518875, 0.16522978246212006, 0.21483103930950165, 0.12542711198329926, 0.12507189810276031, 0.18706351518630981, 0.19976159930229187, 0.1347198784351349, 0.15007948875427246, 0.16866588592529297, 0.18591973185539246, 0.16744309663772583, 0.1463221162557602, 0.20640499889850616, 0.15249145030975342, 0.19149969518184662, 0.176529660820961, 0.15069851279258728, 0.19502392411231995, 0.12603285908699036, 0.14863361418247223, 0.143586203455925, 0.20341721177101135, 0.16713136434555054, 0.1821059137582779, 0.145150288939476, 0.1707281470298767, 0.19263768196105957, 0.1378229707479477, 0.21494267880916595, 0.2073356658220291, 0.14289559423923492, 0.16518741846084595, 0.12123864889144897, 0.14654721319675446, 0.19235555827617645, 0.16451160609722137, 0.18872641026973724, 0.15612748265266418, 0.1318010836839676, 0.15218721330165863, 0.12006387859582901, 0.1715994030237198, 0.17564326524734497, 0.11840222775936127, 0.13861969113349915, 0.1854517161846161, 0.16798773407936096, 0.17289137840270996, 0.11485118418931961, 0.11417460441589355, 0.17739254236221313, 0.144830122590065, 0.2219352126121521, 0.13695617020130157, 0.11070261895656586, 0.17644084990024567, 0.1359490603208542, 0.21411879360675812, 0.12520456314086914, 0.21350903809070587, 0.10766009241342545, 0.22502166032791138, 0.19603346288204193, 0.17905838787555695, 0.19886110723018646, 0.14703792333602905, 0.17328956723213196, 0.13448281586170197, 0.1530120074748993, 0.16841886937618256, 0.1675720363855362, 0.1489124745130539, 0.10649247467517853, 0.21236833930015564, 0.21202945709228516, 0.18624071776866913, 0.20916078984737396, 0.17805403470993042, 0.12470407038927078, 0.12597602605819702, 0.17830674350261688, 0.22971577942371368, 0.12902168929576874, 0.169647216796875, 0.10907087475061417, 0.14086809754371643, 0.17415711283683777, 0.15680377185344696, 0.10885872691869736, 0.1682431548833847, 0.18207105994224548, 0.1589495986700058, 0.14662139117717743, 0.13939164578914642, 0.12741973996162415, 0.16430741548538208, 0.1882707178592682, 0.1566527634859085, 0.18489013612270355, 0.2749299705028534, 0.14797163009643555, 0.2224905788898468, 0.2625754773616791, 0.1969839483499527, 0.15519291162490845, 0.1325129121541977, 0.2653883397579193, 0.14073936641216278, 0.13646867871284485, 0.2188750058412552, 0.1549864411354065, 0.14469394087791443, 0.14726893603801727, 0.21493104100227356, 0.16833247244358063, 0.15434607863426208, 0.2559854984283447, 0.1510571539402008, 0.2287510186433792, 0.25573912262916565, 0.1511446237564087, 0.25200754404067993, 0.20121979713439941, 0.18292345106601715, 0.1883659064769745, 0.15042456984519958, 0.14855755865573883, 0.12029646337032318, 0.13587674498558044, 0.15169139206409454, 0.14708437025547028, 0.12325137108564377, 0.2091570347547531, 0.23740021884441376, 0.144281804561615, 0.14948290586471558, 0.2148638367652893, 0.15667448937892914, 0.15306544303894043, 0.1637095808982849, 0.1505764275789261, 0.17445503175258636, 0.18496593832969666, 0.191310316324234, 0.18712535500526428, 0.17946802079677582, 0.18252424895763397, 0.21811428666114807, 0.13018594682216644, 0.12506569921970367, 0.16462571918964386, 0.1662687212228775, 0.16446906328201294, 0.1566666066646576, 0.1854347288608551, 0.12427140772342682, 0.1969016045331955, 0.1640336662530899, 0.1657659411430359, 0.1342010647058487, 0.15899981558322906, 0.17946650087833405, 0.13391470909118652, 0.15155336260795593, 0.15049344301223755, 0.14196684956550598, 0.14469395577907562, 0.18654051423072815, 0.18962611258029938, 0.14278428256511688, 0.22024597227573395, 0.22283847630023956, 0.1194329708814621, 0.2163347452878952, 0.22269324958324432, 0.17175079882144928, 0.14923489093780518, 0.15792952477931976, 0.11899051070213318, 0.17174428701400757, 0.18687865138053894, 0.17935790121555328, 0.1687907874584198, 0.16757133603096008, 0.16438698768615723, 0.16395094990730286, 0.21231287717819214, 0.2015504390001297, 0.1821231096982956, 0.15131136775016785, 0.19739606976509094, 0.1795763224363327, 0.15708056092262268, 0.15333928167819977, 0.12600381672382355, 0.1322660595178604, 0.13824093341827393, 0.2538242042064667, 0.11475459486246109, 0.14573553204536438, 0.18408003449440002, 0.12270289659500122, 0.17250211536884308, 0.14783920347690582, 0.11284537613391876, 0.14402852952480316, 0.147428497672081, 0.20794861018657684, 0.13921956717967987, 0.1658371239900589, 0.18965230882167816, 0.10921493917703629, 0.210317462682724, 0.26523256301879883, 0.17877531051635742, 0.1707080453634262, 0.20086832344532013, 0.13650889694690704, 0.18800686299800873, 0.18745283782482147, 0.20755888521671295, 0.1807568073272705, 0.2022944688796997, 0.16321852803230286, 0.22107459604740143, 0.20013239979743958, 0.25351154804229736, 0.1611698567867279, 0.16231867671012878, 0.18532393872737885, 0.11981874704360962, 0.20628440380096436, 0.13142317533493042, 0.16198945045471191, 0.18594811856746674, 0.20314469933509827, 0.2141294628381729, 0.18075944483280182, 0.1683107167482376, 0.14174696803092957, 0.21624037623405457, 0.12658224999904633, 0.19071969389915466, 0.23153027892112732, 0.16527508199214935, 0.1293153613805771, 0.1876540333032608, 0.14521223306655884, 0.15798115730285645, 0.17369858920574188, 0.15921923518180847, 0.20647817850112915, 0.16667304933071136, 0.16684934496879578, 0.208830326795578, 0.1994699090719223, 0.17709916830062866, 0.16698987782001495, 0.2035258710384369, 0.16665902733802795, 0.1772851049900055, 0.17328500747680664, 0.1928829401731491, 0.15291999280452728, 0.14914479851722717, 0.17012253403663635, 0.21635523438453674, 0.16839812695980072, 0.15403439104557037, 0.1635451316833496, 0.13796699047088623, 0.1772603839635849, 0.2118164300918579, 0.16085806488990784, 0.15489999949932098, 0.1707276552915573, 0.1621788591146469, 0.1357554793357849, 0.13980259001255035, 0.2203953117132187, 0.165241539478302, 0.18495464324951172, 0.14923489093780518, 0.1335933804512024, 0.18242062628269196, 0.1487882137298584, 0.17536404728889465, 0.15225766599178314, 0.20458248257637024, 0.19330456852912903, 0.14149850606918335, 0.1667584776878357, 0.20531220734119415, 0.1691741645336151, 0.16682158410549164, 0.13007883727550507, 0.20537926256656647, 0.17539744079113007, 0.17075565457344055, 0.151759535074234, 0.14672815799713135, 0.19487158954143524, 0.17556357383728027, 0.1612267941236496, 0.1297936737537384, 0.141580268740654, 0.15080399811267853, 0.1425539255142212, 0.1457841545343399, 0.23233242332935333, 0.14576831459999084, 0.17477239668369293, 0.15448695421218872, 0.16289760172367096, 0.1958889663219452, 0.14227555692195892, 0.1612568497657776, 0.20947399735450745, 0.1660241037607193, 0.15482620894908905, 0.20124348998069763, 0.18787412345409393, 0.214941143989563, 0.23064298927783966, 0.1610563099384308, 0.1700514405965805, 0.19922930002212524, 0.12404083460569382, 0.1682216078042984, 0.12365108728408813, 0.1459500938653946, 0.14675338566303253, 0.19287602603435516, 0.20977699756622314, 0.2213405817747116, 0.1500326693058014, 0.14665931463241577, 0.18942832946777344, 0.15635304152965546, 0.14011861383914948, 0.1462363302707672, 0.14884346723556519, 0.1675156056880951, 0.14237092435359955, 0.15075987577438354, 0.11868447810411453, 0.18071918189525604, 0.18233785033226013, 0.14378976821899414, 0.2111481875181198, 0.21097472310066223, 0.11615880578756332, 0.22072987258434296, 0.18630726635456085, 0.21103529632091522, 0.15509088337421417, 0.1444074660539627, 0.22402162849903107, 0.13735738396644592, 0.2002604603767395, 0.21391300857067108, 0.16879118978977203, 0.19542114436626434, 0.2144869565963745, 0.17991554737091064, 0.21230138838291168, 0.20387086272239685, 0.1579018235206604, 0.13789816200733185, 0.19281284511089325, 0.18086932599544525, 0.12311931699514389, 0.21700666844844818, 0.19984392821788788, 0.15199388563632965, 0.21271564066410065, 0.17145012319087982, 0.1275571584701538, 0.2344428449869156, 0.15658867359161377, 0.21576295793056488, 0.19189374148845673, 0.14756172895431519, 0.1598852574825287, 0.19171331822872162, 0.1966913640499115, 0.17493747174739838, 0.16991785168647766, 0.18143723905086517, 0.17623141407966614, 0.16008928418159485, 0.1759483963251114, 0.14681462943553925, 0.21834348142147064, 0.15778464078903198, 0.16761893033981323, 0.13134637475013733, 0.13125042617321014, 0.18377536535263062, 0.130748450756073, 0.16844376921653748, 0.18029849231243134, 0.16992253065109253, 0.17772051692008972, 0.1292731612920761, 0.2166694551706314, 0.16978886723518372, 0.163421168923378, 0.1717527210712433, 0.13999637961387634, 0.15034262835979462, 0.17299611866474152, 0.1546284705400467, 0.23315124213695526, 0.20614391565322876, 0.12711717188358307, 0.23209315538406372, 0.14644424617290497, 0.14441712200641632, 0.18120822310447693, 0.1503974348306656, 0.1595691740512848, 0.1384606659412384, 0.1658988744020462, 0.16085149347782135, 0.18534256517887115, 0.17660845816135406, 0.12680551409721375, 0.18632908165454865, 0.21529017388820648, 0.175215944647789, 0.23440304398536682, 0.22650586068630219, 0.13825580477714539, 0.1411505490541458, 0.21030275523662567, 0.16132520139217377, 0.15472225844860077, 0.17885634303092957, 0.1968894600868225, 0.23013032972812653, 0.15532229840755463, 0.20207145810127258, 0.21675856411457062, 0.130868598818779, 0.17146466672420502, 0.18171638250350952, 0.17078910768032074, 0.20077428221702576, 0.13237346708774567, 0.14514820277690887, 0.17393583059310913, 0.16618143022060394, 0.1709452122449875, 0.1527949720621109, 0.184148371219635, 0.20366857945919037, 0.1729111224412918, 0.16323599219322205, 0.13262026011943817, 0.13250480592250824, 0.13214293122291565, 0.13763795793056488, 0.15818561613559723, 0.1918797343969345, 0.15046857297420502, 0.13012444972991943, 0.16220013797283173, 0.1862882673740387, 0.20362314581871033, 0.12773333489894867, 0.22979161143302917, 0.18386849761009216, 0.12695473432540894, 0.1637963503599167, 0.20169153809547424, 0.18081457912921906, 0.1253141164779663, 0.1860770881175995, 0.14763054251670837, 0.1851852387189865, 0.16705945134162903, 0.14778001606464386, 0.180709108710289, 0.1223699301481247, 0.20374737679958344, 0.1710462123155594, 0.18164955079555511, 0.21132919192314148, 0.22027909755706787, 0.14720214903354645, 0.19878268241882324, 0.18739452958106995, 0.22720544040203094, 0.21205595135688782, 0.21007488667964935, 0.1602615863084793, 0.17645084857940674, 0.18609727919101715, 0.205226331949234, 0.15805041790008545, 0.15968984365463257, 0.1798531413078308, 0.1529710739850998, 0.15982139110565186, 0.1852051317691803, 0.16955533623695374, 0.18369081616401672, 0.21737268567085266, 0.14988455176353455, 0.17129841446876526, 0.1520436853170395, 0.18433696031570435, 0.14873169362545013, 0.1411937177181244, 0.18465018272399902, 0.1545105278491974, 0.12804847955703735, 0.20290835201740265, 0.14518506824970245, 0.14614510536193848, 0.12697938084602356, 0.12663407623767853, 0.1746966540813446, 0.2102193385362625, 0.17204387485980988, 0.1481412798166275, 0.17204055190086365, 0.15244905650615692, 0.12371394783258438, 0.17343498766422272, 0.18414273858070374, 0.15386748313903809, 0.12197814881801605, 0.18019740283489227, 0.1509045511484146, 0.17507649958133698, 0.15250523388385773, 0.1786726415157318, 0.1762339025735855, 0.20531174540519714, 0.18804219365119934, 0.14183013141155243, 0.19663701951503754, 0.14646154642105103, 0.20343884825706482, 0.18187741935253143, 0.1500648558139801, 0.1513284295797348, 0.11776486784219742, 0.14163419604301453, 0.2175436168909073, 0.1165795549750328, 0.17716051638126373, 0.14479030668735504, 0.11555849760770798, 0.11494781821966171, 0.1142304316163063, 0.11330556124448776, 0.13871581852436066, 0.2111646682024002, 0.13837404549121857, 0.10979320108890533, 0.18079981207847595, 0.20992489159107208, 0.18701927363872528, 0.10732997953891754, 0.1866195797920227, 0.17084644734859467, 0.225694939494133, 0.23189659416675568, 0.18003933131694794, 0.17146627604961395, 0.13916347920894623, 0.1784617304801941, 0.17393876612186432, 0.18823054432868958, 0.21961982548236847, 0.17358148097991943, 0.1684638112783432, 0.14080627262592316, 0.1892203986644745, 0.10853154212236404, 0.22383230924606323, 0.1904420256614685, 0.14199838042259216, 0.18448178470134735, 0.17494668066501617, 0.13150660693645477, 0.2629547417163849, 0.17735405266284943, 0.2206810861825943, 0.13622210919857025, 0.17310763895511627, 0.18981826305389404, 0.18204541504383087, 0.16653376817703247, 0.13106438517570496, 0.1427837461233139, 0.11418700218200684, 0.20959748327732086, 0.11442653834819794, 0.1383773535490036, 0.17408959567546844, 0.1705946922302246, 0.14554838836193085, 0.1810678243637085, 0.15343672037124634, 0.15190741419792175, 0.17822742462158203, 0.18814194202423096, 0.11266929656267166, 0.14533749222755432, 0.11190378665924072, 0.21429970860481262, 0.16960670053958893, 0.13729916512966156, 0.1535966396331787, 0.1315234750509262, 0.17185179889202118, 0.1375366747379303, 0.13938486576080322, 0.18807722628116608, 0.18092738091945648, 0.10745640099048615, 0.19025589525699615, 0.13717259466648102, 0.10578807443380356, 0.16546595096588135, 0.166859433054924, 0.21915659308433533, 0.2206154465675354, 0.1679249256849289, 0.17437127232551575, 0.18531285226345062, 0.13298006355762482, 0.16791652143001556, 0.1556294709444046, 0.20222900807857513, 0.19238613545894623, 0.21050365269184113, 0.21749959886074066, 0.13619598746299744, 0.19309906661510468, 0.14291520416736603, 0.10782502591609955, 0.20418475568294525, 0.14694984257221222, 0.14117646217346191, 0.12975791096687317, 0.18917545676231384, 0.18551558256149292, 0.13572390377521515, 0.1592816412448883, 0.23708312213420868, 0.181040957570076, 0.1286853849887848, 0.10687605291604996, 0.10703624039888382, 0.14067649841308594, 0.10576566308736801, 0.1791544407606125, 0.10501068085432053, 0.18163098394870758, 0.19304554164409637, 0.14989304542541504, 0.14513099193572998, 0.15558144450187683, 0.14178356528282166, 0.1696528047323227, 0.13867627084255219, 0.14586246013641357, 0.15010260045528412, 0.17541955411434174, 0.11679357290267944, 0.24038127064704895, 0.16837267577648163, 0.09719487279653549, 0.09687814861536026, 0.136665478348732, 0.23311471939086914, 0.2279297560453415, 0.2459777593612671, 0.13483160734176636, 0.13061073422431946, 0.2303617298603058, 0.09667284041643143, 0.18947602808475494, 0.22206410765647888, 0.21922920644283295, 0.1469266414642334, 0.17598581314086914, 0.1519186645746231, 0.14514458179473877, 0.0988135114312172, 0.23005259037017822, 0.2189837098121643, 0.1652936488389969, 0.1353403776884079, 0.19251875579357147, 0.18584954738616943, 0.1813589185476303, 0.18456187844276428, 0.1837375909090042, 0.1800108700990677, 0.15588076412677765, 0.19347986578941345, 0.19911101460456848, 0.1376066505908966, 0.10381023585796356, 0.2762444317340851, 0.18602922558784485, 0.20149393379688263, 0.17403888702392578, 0.1668606996536255, 0.222529336810112, 0.2676757276058197, 0.14427772164344788, 0.18366776406764984, 0.1737005114555359, 0.26090240478515625, 0.15556971728801727, 0.13923616707324982, 0.1389850676059723, 0.19247937202453613, 0.114568792283535, 0.1381191462278366, 0.19790181517601013, 0.14774663746356964, 0.13832862675189972, 0.16996492445468903, 0.1438085287809372, 0.1582186371088028, 0.16633003950119019, 0.2008952796459198, 0.23028063774108887, 0.13655851781368256, 0.17581744492053986, 0.21286050975322723, 0.1963772475719452, 0.16860821843147278, 0.15411855280399323, 0.1300884634256363, 0.15001611411571503, 0.17978988587856293, 0.25018301606178284, 0.15169796347618103, 0.17323152720928192, 0.2225722074508667, 0.14332595467567444, 0.24734842777252197, 0.1797550469636917, 0.1968444436788559, 0.21064971387386322, 0.15191102027893066, 0.19202101230621338, 0.21100600063800812, 0.12248019129037857, 0.15254710614681244, 0.13718223571777344, 0.12397289276123047, 0.1569271683692932, 0.17891083657741547, 0.14566946029663086, 0.12293073534965515, 0.16040261089801788, 0.12232072651386261, 0.1408643126487732, 0.12125266343355179, 0.18267865478992462, 0.24375803768634796, 0.2256966084241867, 0.18363161385059357, 0.1954919546842575, 0.24395418167114258, 0.13342852890491486, 0.24174432456493378, 0.17620758712291718, 0.18780353665351868, 0.19261620938777924, 0.16255374252796173, 0.12418169528245926, 0.16982370615005493, 0.14912405610084534, 0.14025817811489105, 0.13578706979751587, 0.21815252304077148, 0.17267632484436035, 0.13614396750926971, 0.15763144195079803, 0.2367643266916275, 0.16181263327598572, 0.16062089800834656, 0.16226084530353546, 0.18325693905353546, 0.18924444913864136, 0.1531011164188385, 0.14875906705856323, 0.16764144599437714, 0.23605066537857056, 0.1394849419593811, 0.15670305490493774, 0.1907990425825119, 0.1931980401277542, 0.1254388391971588, 0.16523243486881256, 0.15644584596157074, 0.12493719905614853, 0.1977233588695526, 0.12403549998998642, 0.2141132652759552, 0.14205576479434967, 0.1570052206516266, 0.12273942679166794, 0.19694675505161285, 0.12195143848657608, 0.19114616513252258, 0.24237284064292908, 0.12069308757781982, 0.19401568174362183, 0.1542607694864273, 0.16122181713581085, 0.15848751366138458, 0.216798335313797, 0.17840507626533508, 0.13954132795333862, 0.16527940332889557, 0.17433325946331024, 0.15867966413497925, 0.16371957957744598, 0.1800067126750946, 0.17768274247646332, 0.11957597732543945, 0.14091765880584717, 0.11896482855081558, 0.1655448079109192, 0.19778096675872803, 0.13142536580562592, 0.1964688003063202, 0.14830079674720764, 0.11750946938991547, 0.158948615193367, 0.15203124284744263, 0.18200160562992096, 0.14269793033599854, 0.2514439821243286, 0.18003301322460175, 0.15622185170650482, 0.15758293867111206, 0.11561629176139832, 0.13994210958480835, 0.11457101255655289, 0.14712202548980713, 0.20171430706977844, 0.18441876769065857, 0.1662716269493103, 0.1784953773021698, 0.14352397620677948, 0.15471816062927246, 0.18909695744514465, 0.14704583585262299, 0.172440767288208, 0.22414524853229523, 0.1106678694486618, 0.22113877534866333, 0.13568755984306335, 0.15899211168289185, 0.22454330325126648, 0.2121976613998413, 0.2123148888349533, 0.19440817832946777, 0.260678231716156, 0.16812439262866974, 0.1330302506685257, 0.18287302553653717, 0.1746906340122223, 0.18670906126499176, 0.13910174369812012, 0.21652747690677643, 0.23498420417308807, 0.22489619255065918, 0.13393640518188477, 0.19588282704353333, 0.1529368907213211, 0.16110867261886597, 0.1183381900191307, 0.17246267199516296, 0.17093037068843842, 0.1616373509168625, 0.21441198885440826, 0.15129368007183075, 0.21837465465068817, 0.221001535654068, 0.1588922142982483, 0.12127812206745148, 0.24227555096149445, 0.20588134229183197, 0.15908095240592957, 0.17389298975467682, 0.15079741179943085, 0.13895398378372192, 0.1563027799129486, 0.17807655036449432, 0.12419351935386658, 0.17798374593257904, 0.20422393083572388, 0.14865808188915253, 0.12413246929645538, 0.13834303617477417, 0.15325555205345154, 0.1233503520488739, 0.15130220353603363, 0.2062499225139618, 0.2102673053741455, 0.18296630680561066, 0.1412285566329956, 0.2223561704158783, 0.18011124432086945, 0.15547378361225128, 0.24138855934143066, 0.14094345271587372, 0.16314257681369781, 0.14497749507427216, 0.15317648649215698, 0.12291263788938522, 0.20567812025547028, 0.12288801372051239, 0.21137672662734985, 0.14897817373275757, 0.16192060708999634, 0.1783878356218338, 0.24066512286663055, 0.2098907083272934, 0.1826346069574356, 0.19536340236663818, 0.2013457864522934, 0.12316616624593735, 0.14255382120609283, 0.12355978041887283, 0.16595366597175598, 0.1630745530128479, 0.15116740763187408, 0.17695504426956177, 0.16935963928699493, 0.1905684620141983, 0.17773793637752533, 0.1834336221218109, 0.17619629204273224, 0.16953475773334503, 0.12392004579305649, 0.14791111648082733, 0.12369457632303238, 0.1274203509092331, 0.207427516579628, 0.18603409826755524, 0.21012437343597412, 0.14546333253383636, 0.12287785112857819, 0.1859932243824005, 0.12354876101016998, 0.12248886376619339, 0.2055307775735855, 0.1216621994972229, 0.16583432257175446, 0.17674195766448975, 0.1871454417705536, 0.2261955887079239, 0.18725918233394623, 0.12043397128582001, 0.14850065112113953, 0.2088272124528885, 0.12042994052171707, 0.141102597117424, 0.12000725418329239, 0.22643476724624634, 0.11937699466943741, 0.14141641557216644, 0.19684617221355438, 0.20177902281284332, 0.11802348494529724, 0.11801549792289734, 0.1510516107082367, 0.13552948832511902, 0.20793092250823975, 0.21408402919769287, 0.14860880374908447, 0.18733303248882294, 0.1481124758720398, 0.17609840631484985, 0.20196186006069183, 0.1445978432893753, 0.14427326619625092, 0.1405903548002243, 0.17447762191295624, 0.14098887145519257, 0.13255740702152252, 0.19330640137195587, 0.11441990733146667, 0.19410477578639984, 0.2104000449180603, 0.17721910774707794, 0.1746511161327362, 0.2029934674501419, 0.2051847130060196, 0.15910643339157104, 0.16630399227142334, 0.14944016933441162, 0.1978885680437088, 0.13240405917167664, 0.11349557340145111, 0.11320594698190689, 0.25780028104782104, 0.18093207478523254, 0.19041307270526886, 0.189256489276886, 0.13643021881580353, 0.25692012906074524, 0.13422894477844238, 0.13689561188220978, 0.2210051417350769, 0.16877903044223785, 0.15550601482391357, 0.13969522714614868, 0.16185538470745087, 0.13702388107776642, 0.20032672584056854, 0.12791390717029572, 0.20859967172145844, 0.19797135889530182, 0.17179375886917114, 0.1545594334602356, 0.13829633593559265, 0.20766162872314453, 0.2200644314289093, 0.11728481948375702, 0.1799958050251007, 0.1787494271993637, 0.11834642291069031, 0.15186162292957306, 0.11780528724193573, 0.13186371326446533, 0.17265497148036957, 0.1947120726108551, 0.2174840271472931, 0.15124833583831787, 0.15760883688926697, 0.2187725454568863, 0.24879920482635498, 0.25992101430892944, 0.14274942874908447, 0.1733129769563675, 0.2270725965499878, 0.11972273886203766, 0.14008517563343048, 0.15208421647548676, 0.20431293547153473, 0.2356746345758438, 0.22269144654273987, 0.19806180894374847, 0.18830381333827972, 0.18435733020305634, 0.16171951591968536, 0.20377930998802185, 0.23533689975738525, 0.18402457237243652, 0.20062430202960968, 0.16155052185058594, 0.20265445113182068, 0.1264212429523468, 0.15863946080207825, 0.13278426229953766, 0.16056329011917114, 0.12758244574069977, 0.20065955817699432, 0.16104213893413544, 0.17284812033176422, 0.18929433822631836, 0.15229062736034393, 0.18617096543312073, 0.17320707440376282, 0.15699967741966248, 0.21170711517333984, 0.12703940272331238, 0.13304412364959717, 0.20984210073947906, 0.15464867651462555, 0.18874867260456085, 0.1663086712360382, 0.13786451518535614, 0.18990491330623627, 0.16728347539901733, 0.19110356271266937, 0.22139601409435272, 0.144195094704628, 0.23554392158985138, 0.186116561293602, 0.21075886487960815, 0.18214571475982666, 0.20925068855285645, 0.15723194181919098, 0.1279401332139969, 0.2076939195394516, 0.23073020577430725, 0.1795419156551361, 0.14517781138420105, 0.1951173096895218, 0.15859828889369965, 0.1698952317237854, 0.130477637052536, 0.2035561054944992, 0.14759132266044617, 0.14507906138896942, 0.15504886209964752, 0.22629672288894653, 0.15223146975040436, 0.15589213371276855, 0.16052621603012085, 0.18809814751148224, 0.18366669118404388, 0.19093845784664154, 0.14292672276496887, 0.15505164861679077, 0.13194887340068817, 0.16529813408851624, 0.13168968260288239, 0.17709453403949738, 0.2112623155117035, 0.20150534808635712, 0.1437450498342514, 0.20714208483695984, 0.1892426311969757, 0.17576509714126587, 0.15847495198249817, 0.14605234563350677, 0.20766307413578033, 0.17433208227157593, 0.18686775863170624, 0.20778848230838776, 0.1549893617630005, 0.13220913708209991, 0.16835904121398926, 0.22465266287326813, 0.18503205478191376, 0.1610567420721054, 0.1594095379114151, 0.13304661214351654, 0.1581491231918335, 0.14826105535030365, 0.18736708164215088, 0.18963508307933807, 0.14843030273914337, 0.20246084034442902, 0.1881992667913437, 0.1610204428434372, 0.20016805827617645, 0.17752905189990997, 0.17284570634365082, 0.16553127765655518, 0.18559794127941132, 0.19554227590560913, 0.19174091517925262, 0.16901527345180511, 0.17421124875545502, 0.16940756142139435, 0.16550232470035553, 0.206682950258255, 0.16073167324066162, 0.18328364193439484, 0.1660403311252594, 0.1453067511320114, 0.18348009884357452, 0.17786861956119537, 0.13396506011486053, 0.18054407835006714, 0.15162891149520874, 0.18191184103488922, 0.18710489571094513, 0.14933565258979797, 0.1915799081325531, 0.2029896378517151, 0.14268562197685242, 0.1572227030992508, 0.1582116037607193, 0.18562644720077515, 0.18687108159065247, 0.13598044216632843, 0.1706010103225708, 0.13611602783203125, 0.19742941856384277, 0.13348864018917084, 0.15187840163707733, 0.209004744887352, 0.1326914131641388, 0.16116738319396973, 0.16470462083816528, 0.15222550928592682, 0.1615106463432312, 0.16284610331058502, 0.15290938317775726, 0.17814253270626068, 0.1962898075580597, 0.18251892924308777, 0.1617075651884079, 0.18136201798915863, 0.21191370487213135, 0.1736282855272293, 0.14761251211166382, 0.14909157156944275, 0.16747234761714935, 0.2083512246608734, 0.18295030295848846, 0.14224153757095337, 0.12826895713806152, 0.15843155980110168, 0.15579994022846222, 0.18909002840518951, 0.12694165110588074, 0.1956384778022766, 0.1789436638355255, 0.16436132788658142, 0.13751430809497833, 0.166019007563591, 0.14981696009635925, 0.20744535326957703, 0.1490330696105957, 0.17172282934188843, 0.1804218590259552, 0.143202543258667, 0.20394602417945862, 0.17564435303211212, 0.21097637712955475, 0.15559636056423187, 0.17881764471530914, 0.18301916122436523, 0.17877309024333954, 0.15812434256076813, 0.19143341481685638, 0.2358706295490265, 0.22287558019161224, 0.14812494814395905, 0.20756839215755463, 0.15052278339862823, 0.16631321609020233, 0.17881785333156586, 0.1489609181880951, 0.19036166369915009, 0.1514071822166443, 0.13698796927928925, 0.17675453424453735, 0.1487244814634323, 0.16063033044338226, 0.1537662297487259, 0.1562040001153946, 0.17355962097644806, 0.19156311452388763, 0.181890606880188, 0.17693601548671722, 0.20596177875995636, 0.16774213314056396, 0.1274457424879074, 0.1501699835062027, 0.16564670205116272, 0.16632947325706482, 0.23179112374782562, 0.20612871646881104, 0.13626903295516968, 0.15444089472293854, 0.1734423041343689, 0.16667133569717407, 0.1622214913368225, 0.229507714509964, 0.12898720800876617, 0.20534229278564453, 0.16359560191631317, 0.15092329680919647, 0.18963250517845154, 0.20358867943286896, 0.13022474944591522, 0.16580429673194885, 0.18584993481636047, 0.1300501823425293, 0.2026078850030899, 0.18997028470039368, 0.19406761229038239, 0.1821807622909546, 0.20347706973552704, 0.17040064930915833, 0.13034914433956146, 0.18717466294765472, 0.180678591132164, 0.17678587138652802, 0.18117403984069824, 0.14235156774520874, 0.195807546377182, 0.14870741963386536, 0.18713818490505219, 0.1408105492591858, 0.16874000430107117, 0.17685392498970032, 0.13104385137557983, 0.1993098258972168, 0.14857301115989685, 0.15663672983646393, 0.1500852108001709, 0.15978166460990906, 0.18060728907585144, 0.22662116587162018, 0.17584672570228577, 0.15783274173736572, 0.19079655408859253, 0.1548144817352295, 0.17755338549613953, 0.18232840299606323, 0.22601139545440674, 0.15305162966251373, 0.17169803380966187, 0.21177732944488525, 0.18949246406555176, 0.1581333428621292, 0.13264428079128265, 0.20937921106815338, 0.189736008644104, 0.17265532910823822, 0.16569559276103973, 0.17256875336170197, 0.17092938721179962, 0.16092130541801453, 0.1857099086046219, 0.17684400081634521, 0.18171633780002594, 0.18824100494384766, 0.16077227890491486, 0.1618645340204239, 0.16715149581432343, 0.21990710496902466, 0.15027539432048798, 0.17866113781929016, 0.16500139236450195, 0.13960054516792297, 0.19159123301506042, 0.13619329035282135, 0.17323310673236847, 0.1775958091020584, 0.1838669329881668, 0.21807946264743805, 0.16846053302288055, 0.19189581274986267, 0.19965051114559174, 0.16121146082878113, 0.14383797347545624, 0.21326903998851776, 0.18544022738933563, 0.19524243474006653, 0.16095419228076935, 0.18686702847480774, 0.14029920101165771, 0.20370697975158691, 0.19443273544311523, 0.16480711102485657, 0.16276051104068756, 0.15164098143577576, 0.16570653021335602, 0.1824173629283905, 0.16050657629966736, 0.17558078467845917, 0.18537791073322296, 0.14740343391895294, 0.14079377055168152, 0.19327889382839203, 0.20124934613704681, 0.16535058617591858, 0.20631492137908936, 0.141167551279068, 0.18172970414161682, 0.18294689059257507, 0.2031191736459732, 0.16945317387580872, 0.16004863381385803, 0.1646658331155777, 0.14090918004512787, 0.16363166272640228, 0.1522722691297531, 0.21285384893417358, 0.16169309616088867, 0.14012902975082397, 0.18564726412296295, 0.18683812022209167, 0.19734977185726166, 0.14504772424697876, 0.19800636172294617, 0.20697319507598877, 0.1880728155374527, 0.1460144817829132, 0.1664159595966339, 0.17083223164081573, 0.1648237407207489, 0.20136523246765137, 0.17226970195770264, 0.1413951963186264, 0.18308819830417633, 0.17613959312438965, 0.1696079969406128, 0.2113579362630844, 0.16175565123558044, 0.18872131407260895, 0.14207811653614044, 0.1681520789861679, 0.17425893247127533, 0.16430127620697021, 0.19724927842617035, 0.16472956538200378, 0.1968214064836502, 0.14256958663463593, 0.1528710573911667, 0.15399602055549622, 0.16168783605098724, 0.17181196808815002, 0.17242461442947388, 0.17779138684272766, 0.16926178336143494, 0.1516096591949463, 0.2017372101545334, 0.18131713569164276, 0.16937388479709625, 0.16894705593585968, 0.17404383420944214, 0.18135938048362732, 0.16392144560813904, 0.17057806253433228, 0.16421367228031158, 0.17725686728954315, 0.15851792693138123, 0.21213047206401825, 0.14087699353694916, 0.1480722278356552, 0.1430455595254898, 0.18111801147460938, 0.17812813818454742, 0.16812653839588165, 0.19377703964710236, 0.17786963284015656, 0.1584140658378601, 0.1729196161031723, 0.181771919131279, 0.18144601583480835, 0.2117425501346588, 0.14126944541931152, 0.18557502329349518, 0.17849421501159668, 0.15574124455451965, 0.163209468126297, 0.17341278493404388, 0.1448044627904892, 0.14178887009620667, 0.16753986477851868, 0.14163164794445038, 0.21093370020389557, 0.20156440138816833, 0.14684917032718658, 0.1968250423669815, 0.18920066952705383, 0.16165770590305328, 0.17012539505958557, 0.21081143617630005, 0.17492470145225525, 0.15762855112552643, 0.17207734286785126, 0.15538564324378967, 0.17896217107772827, 0.1858745962381363, 0.15997569262981415, 0.13919037580490112, 0.19122375547885895, 0.19375011324882507, 0.18169087171554565, 0.17055143415927887, 0.18425562977790833, 0.17747725546360016, 0.1633276343345642, 0.21445442736148834, 0.20056688785552979, 0.15229247510433197, 0.16479581594467163, 0.17856290936470032, 0.14442715048789978, 0.1599225550889969, 0.16609151661396027, 0.1768808215856552, 0.16847258806228638, 0.19056415557861328, 0.16000114381313324, 0.20432518422603607, 0.1913875937461853, 0.170993372797966, 0.2093585878610611, 0.14356787502765656, 0.16858233511447906, 0.17097507417201996, 0.14328843355178833, 0.20869556069374084, 0.17446932196617126, 0.16316941380500793, 0.18413883447647095, 0.17525240778923035, 0.1629236936569214, 0.1421981006860733, 0.18475660681724548, 0.1792203038930893, 0.15548376739025116, 0.16044870018959045, 0.16571955382823944, 0.14171382784843445, 0.1746203899383545, 0.2112591564655304, 0.14089104533195496, 0.17267432808876038, 0.19625519216060638, 0.16116642951965332, 0.1585250198841095, 0.1477992683649063, 0.15984556078910828, 0.18067364394664764, 0.14321516454219818, 0.15456417202949524, 0.160744771361351, 0.17755720019340515, 0.18390744924545288, 0.13869059085845947, 0.17511890828609467, 0.13547348976135254, 0.18024155497550964, 0.21692945063114166, 0.18521156907081604, 0.17951592803001404, 0.13782118260860443, 0.22038964927196503, 0.13808457553386688, 0.1377815455198288, 0.18829064071178436, 0.17863623797893524, 0.18463045358657837, 0.16558969020843506, 0.16557233035564423, 0.15763384103775024, 0.21727846562862396, 0.20286248624324799, 0.20769809186458588, 0.19176636636257172, 0.17711564898490906, 0.14579786360263824, 0.16707025468349457, 0.20465843379497528, 0.16412119567394257, 0.1958836317062378, 0.14545747637748718, 0.1718834489583969, 0.19208025932312012, 0.194767564535141, 0.20963643491268158, 0.174953430891037, 0.18238182365894318, 0.13990294933319092, 0.16192364692687988, 0.17046329379081726, 0.16517865657806396, 0.1994892656803131, 0.15759943425655365, 0.16263434290885925, 0.1536758840084076, 0.16563645005226135, 0.159433513879776, 0.15416987240314484, 0.15141113102436066, 0.17646080255508423, 0.1491064876317978, 0.205964133143425, 0.14090056717395782, 0.19217263162136078, 0.15515963733196259, 0.15694323182106018, 0.16708117723464966, 0.1646246314048767, 0.18241925537586212, 0.18110349774360657, 0.20294339954853058, 0.14033369719982147, 0.19223885238170624, 0.142722487449646, 0.17221961915493011, 0.1813787966966629, 0.1683131903409958, 0.1625971794128418, 0.16371971368789673, 0.18460795283317566, 0.1955190747976303, 0.16033615171909332, 0.18919683992862701, 0.18124082684516907, 0.1952042430639267, 0.16344644129276276, 0.16517184674739838, 0.17553138732910156, 0.1574033796787262, 0.16984842717647552, 0.18268513679504395, 0.1549787074327469, 0.1540556401014328, 0.17963463068008423, 0.1706775575876236, 0.20199620723724365, 0.1564934104681015, 0.2000499814748764, 0.18084199726581573, 0.15528076887130737, 0.1656053066253662, 0.2163691222667694, 0.1566454917192459, 0.17798244953155518, 0.15427887439727783, 0.18371765315532684, 0.17989742755889893, 0.15936273336410522, 0.1643909364938736, 0.15486577153205872, 0.1713237762451172, 0.15865440666675568, 0.18504971265792847, 0.20042185485363007, 0.17106027901172638, 0.1381521075963974, 0.1852153092622757, 0.17774026095867157, 0.15455715358257294, 0.1609414517879486, 0.1373698115348816, 0.17508918046951294, 0.19567236304283142, 0.16950008273124695, 0.18077048659324646, 0.16314132511615753, 0.14352422952651978, 0.13569438457489014, 0.17023415863513947, 0.15511472523212433, 0.15155428647994995, 0.17090271413326263, 0.18604160845279694, 0.17487862706184387, 0.17240406572818756, 0.22112828493118286, 0.17926247417926788, 0.17236892879009247, 0.18749018013477325, 0.19777798652648926, 0.15024279057979584, 0.20193681120872498, 0.1904434710741043, 0.1546621024608612, 0.1867285817861557, 0.14764095842838287, 0.14474233984947205, 0.15346930921077728, 0.18282140791416168, 0.1700577735900879, 0.1975196748971939, 0.13582557439804077, 0.16251346468925476, 0.1557779312133789, 0.20813612639904022]\n",
            "Val loss 0.16916074915954718\n",
            "Val auc roc 0.503868156904675\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d953dce3659f42739ca836c2b8874206",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2036.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1698\n",
            "Train Losses : [0.1684037744998932, 0.16653867065906525, 0.15032222867012024, 0.1352839469909668, 0.19132493436336517, 0.15540944039821625, 0.19488047063350677, 0.19816823303699493, 0.16736099123954773, 0.14942322671413422, 0.1736292690038681, 0.1942155808210373, 0.18423661589622498, 0.1706831157207489, 0.13535632193088531, 0.1668609380722046, 0.16544415056705475, 0.16071216762065887, 0.1547345072031021, 0.14666011929512024, 0.17946672439575195, 0.20212098956108093, 0.1556592434644699, 0.15575875341892242, 0.15540654957294464, 0.1354665458202362, 0.16480490565299988, 0.17812436819076538, 0.19411307573318481, 0.1857624053955078, 0.15455152094364166, 0.19112859666347504, 0.20415277779102325, 0.14509031176567078, 0.1625666469335556, 0.17058402299880981, 0.19023436307907104, 0.17772604525089264, 0.16173455119132996, 0.2120802402496338, 0.15177135169506073, 0.15733353793621063, 0.1913696676492691, 0.19475050270557404, 0.20205748081207275, 0.20880217850208282, 0.1552981287240982, 0.18699437379837036, 0.15911315381526947, 0.18218141794204712, 0.14773695170879364, 0.17884515225887299, 0.17851614952087402, 0.19320087134838104, 0.16581754386425018, 0.155211940407753, 0.18667790293693542, 0.18346571922302246, 0.16364561021327972, 0.16480596363544464, 0.1958976835012436, 0.1531173288822174, 0.14712762832641602, 0.16841936111450195, 0.13519859313964844, 0.15455934405326843, 0.165603905916214, 0.2059938609600067, 0.17463253438472748, 0.14836575090885162, 0.17570731043815613, 0.19382131099700928, 0.1473609209060669, 0.19642390310764313, 0.1548074334859848, 0.18813197314739227, 0.20164300501346588, 0.1453903764486313, 0.13514429330825806, 0.19709669053554535, 0.135074645280838, 0.16786789894104004, 0.13501761853694916, 0.18186022341251373, 0.15792159736156464, 0.16853047907352448, 0.1350587010383606, 0.19713981449604034, 0.14350977540016174, 0.1589537262916565, 0.18299436569213867, 0.1350332349538803, 0.17902810871601105, 0.1999691277742386, 0.21259722113609314, 0.1649947613477707, 0.15967582166194916, 0.15481983125209808, 0.16717429459095, 0.18786293268203735, 0.19594192504882812, 0.1904629021883011, 0.1514674723148346, 0.13470607995986938, 0.1942388266324997, 0.17048493027687073, 0.15647614002227783, 0.15535159409046173, 0.1617295742034912, 0.18794064223766327, 0.17189598083496094, 0.1992056518793106, 0.19299302995204926, 0.13741590082645416, 0.13464152812957764, 0.17013365030288696, 0.1346110999584198, 0.1743609756231308, 0.19894282519817352, 0.22086389362812042, 0.16509638726711273, 0.1902329921722412, 0.19840706884860992, 0.17380818724632263, 0.17192594707012177, 0.15498872101306915, 0.1569540649652481, 0.19368457794189453, 0.1481296271085739, 0.17779721319675446, 0.17563706636428833, 0.13477173447608948, 0.1714554727077484, 0.15296858549118042, 0.22045397758483887, 0.2086697518825531, 0.1806889921426773, 0.17744819819927216, 0.1586301624774933, 0.18051820993423462, 0.17533613741397858, 0.175503209233284, 0.1348469853401184, 0.13472630083560944, 0.1544218361377716, 0.17912422120571136, 0.18960228562355042, 0.17266283929347992, 0.196080282330513, 0.16425582766532898, 0.1790819615125656, 0.15939398109912872, 0.16634272038936615, 0.15409055352210999, 0.15412913262844086, 0.17944690585136414, 0.18833677470684052, 0.1662789136171341, 0.22037091851234436, 0.13486872613430023, 0.1969575136899948, 0.13462677597999573, 0.20403018593788147, 0.17736345529556274, 0.17210683226585388, 0.13471582531929016, 0.14741197228431702, 0.16014131903648376, 0.15535856783390045, 0.1995520144701004, 0.19609375298023224, 0.1875876635313034, 0.14925067126750946, 0.14328911900520325, 0.19413802027702332, 0.15621095895767212, 0.13449496030807495, 0.22052764892578125, 0.17166449129581451, 0.14699311554431915, 0.16283424198627472, 0.16650062799453735, 0.15354223549365997, 0.1739928275346756, 0.134518563747406, 0.20868271589279175, 0.15602876245975494, 0.1671699732542038, 0.22105994820594788, 0.1563924252986908, 0.22048485279083252, 0.17502422630786896, 0.1646285206079483, 0.19388771057128906, 0.13447564840316772, 0.17424167692661285, 0.16112789511680603, 0.14266811311244965, 0.20197945833206177, 0.15948782861232758, 0.18812398612499237, 0.15041328966617584, 0.1718732863664627, 0.15877333283424377, 0.1701572835445404, 0.13472677767276764, 0.14648298919200897, 0.1345406025648117, 0.16273599863052368, 0.18452584743499756, 0.16161686182022095, 0.145663782954216, 0.1419975906610489, 0.1789560765028, 0.20538924634456635, 0.13439878821372986, 0.13438710570335388, 0.15737229585647583, 0.150643989443779, 0.16658632457256317, 0.13474148511886597, 0.16100135445594788, 0.13421644270420074, 0.1623300313949585, 0.15394923090934753, 0.22132979333400726, 0.18570232391357422, 0.16043172776699066, 0.17646662890911102, 0.15050600469112396, 0.16896750032901764, 0.1918972134590149, 0.1488272100687027, 0.16591531038284302, 0.13403354585170746, 0.19812369346618652, 0.14893676340579987, 0.13414056599140167, 0.1418716311454773, 0.15420955419540405, 0.1666208803653717, 0.18496814370155334, 0.13388729095458984, 0.15428651869297028, 0.1529378890991211, 0.1802801936864853, 0.20073741674423218, 0.177244171500206, 0.13344796001911163, 0.19167913496494293, 0.15960046648979187, 0.1803928017616272, 0.2047230750322342, 0.16245515644550323, 0.1466885358095169, 0.15960343182086945, 0.15704317390918732, 0.1584625393152237, 0.17363540828227997, 0.151105135679245, 0.20320379734039307, 0.18227387964725494, 0.13331560790538788, 0.14912967383861542, 0.17998237907886505, 0.18144027888774872, 0.16762003302574158, 0.13316144049167633, 0.15919263660907745, 0.18750931322574615, 0.17954501509666443, 0.1898367702960968, 0.17056702077388763, 0.15967628359794617, 0.20633886754512787, 0.16260619461536407, 0.15189465880393982, 0.1900668740272522, 0.19627617299556732, 0.15576070547103882, 0.16525548696517944, 0.15650241076946259, 0.1801365464925766, 0.17331622540950775, 0.1330765038728714, 0.19970427453517914, 0.184226855635643, 0.2231692671775818, 0.13309532403945923, 0.1872745305299759, 0.15062598884105682, 0.1519184559583664, 0.150386780500412, 0.19905120134353638, 0.1765778660774231, 0.17422154545783997, 0.1893971711397171, 0.16105377674102783, 0.19170375168323517, 0.1557324081659317, 0.18208733201026917, 0.18026362359523773, 0.22280417382717133, 0.1333191841840744, 0.18214526772499084, 0.14227914810180664, 0.16483823955059052, 0.17118267714977264, 0.17378345131874084, 0.17149759829044342, 0.17213180661201477, 0.13325592875480652, 0.15281035006046295, 0.19443488121032715, 0.15334552526474, 0.22249409556388855, 0.18585871160030365, 0.13308916985988617, 0.1990780085325241, 0.19540025293827057, 0.2225557267665863, 0.1832708716392517, 0.15401123464107513, 0.1475898176431656, 0.16236181557178497, 0.14956603944301605, 0.16513514518737793, 0.17715586721897125, 0.16257239878177643, 0.1333158016204834, 0.16996942460536957, 0.17934484779834747, 0.17901375889778137, 0.1567167341709137, 0.1575499325990677, 0.2223028987646103, 0.16586977243423462, 0.19441325962543488, 0.14920087158679962, 0.19532088935375214, 0.14043112099170685, 0.19061604142189026, 0.21038518846035004, 0.15702170133590698, 0.20299631357192993, 0.1786537617444992, 0.13330823183059692, 0.17608149349689484, 0.1680385023355484, 0.149104043841362, 0.14403396844863892, 0.133540540933609, 0.18315619230270386, 0.15086016058921814, 0.20603014528751373, 0.19135752320289612, 0.18707285821437836, 0.13321125507354736, 0.17755047976970673, 0.15369272232055664, 0.1799221783876419, 0.15099366009235382, 0.13325235247612, 0.2068730890750885, 0.20133249461650848, 0.1689264327287674, 0.18917421996593475, 0.22276882827281952, 0.2000804841518402, 0.16728325188159943, 0.13382072746753693, 0.17746789753437042, 0.18424244225025177, 0.20676809549331665, 0.15033988654613495, 0.17553071677684784, 0.2222677767276764, 0.17163529992103577, 0.15633462369441986, 0.19596125185489655, 0.18430688977241516, 0.1774345487356186, 0.18360547721385956, 0.16624146699905396, 0.22183586657047272, 0.15870903432369232, 0.1577032506465912, 0.15122146904468536, 0.18873246014118195, 0.13365623354911804, 0.19377535581588745, 0.13828378915786743, 0.1653217226266861, 0.19171832501888275, 0.17833645641803741, 0.1857500672340393, 0.16069036722183228, 0.18432773649692535, 0.13397163152694702, 0.1956256479024887, 0.17960810661315918, 0.13054178655147552, 0.17249836027622223, 0.22175732254981995, 0.16350381076335907, 0.22183723747730255, 0.17783638834953308, 0.13375575840473175, 0.1337822675704956, 0.18664129078388214, 0.1337057501077652, 0.16465231776237488, 0.16736125946044922, 0.15696924924850464, 0.16114260256290436, 0.16897250711917877, 0.1820816993713379, 0.16752053797245026, 0.16054438054561615, 0.1597098857164383, 0.16324523091316223, 0.17859944701194763, 0.17379987239837646, 0.16903159022331238, 0.15104392170906067, 0.13142277300357819, 0.1550523042678833, 0.15273992717266083, 0.16881878674030304, 0.1954914927482605, 0.21636097133159637, 0.17064647376537323, 0.14276748895645142, 0.18906238675117493, 0.16483597457408905, 0.22220100462436676, 0.1596667319536209, 0.16842150688171387, 0.14597682654857635, 0.1335940659046173, 0.1415066421031952, 0.13400442898273468, 0.16610372066497803, 0.17844194173812866, 0.17331793904304504, 0.13389091193675995, 0.20829668641090393, 0.20075927674770355, 0.17579786479473114, 0.1748121678829193, 0.16399052739143372, 0.17997556924819946, 0.13399387896060944, 0.15912306308746338, 0.17872574925422668, 0.1630769819021225, 0.13351306319236755, 0.17114472389221191, 0.17179131507873535, 0.1822468787431717, 0.14702241122722626, 0.16971662640571594, 0.17635038495063782, 0.1333182454109192, 0.1732437014579773, 0.20038296282291412, 0.14195722341537476, 0.18993636965751648, 0.2183581292629242, 0.1503993272781372, 0.15192687511444092, 0.1927754282951355, 0.16302691400051117, 0.17498697340488434, 0.18345887959003448, 0.16114556789398193, 0.1648644357919693, 0.18446595966815948, 0.19826382398605347, 0.15352357923984528, 0.16241449117660522, 0.15183080732822418, 0.16757431626319885, 0.13338607549667358, 0.16476380825042725, 0.19645294547080994, 0.13326872885227203, 0.151590496301651, 0.1750521957874298, 0.20461405813694, 0.16936828196048737, 0.19811896979808807, 0.1955781877040863, 0.17227362096309662, 0.16133980453014374, 0.19849039614200592, 0.17822684347629547, 0.1489066630601883, 0.2010824978351593, 0.15253093838691711, 0.18423773348331451, 0.1861112266778946, 0.17818337678909302, 0.17594730854034424, 0.18689899146556854, 0.18653780221939087, 0.13322852551937103, 0.1813020408153534, 0.14206446707248688, 0.19492536783218384, 0.182482048869133, 0.1576981246471405, 0.14848682284355164, 0.13700221478939056, 0.15129932761192322, 0.1931360960006714, 0.16021691262722015, 0.1601448655128479, 0.144504576921463, 0.16109918057918549, 0.13322794437408447, 0.19188718497753143, 0.1488076150417328, 0.13330544531345367, 0.21220789849758148, 0.14497718214988708, 0.17045141756534576, 0.1964753121137619, 0.143535777926445, 0.19815167784690857, 0.15517528355121613, 0.13309989869594574, 0.15714026987552643, 0.1527024656534195, 0.15155240893363953, 0.19509468972682953, 0.1775706559419632, 0.17462873458862305, 0.19404572248458862, 0.15827910602092743, 0.15932968258857727, 0.19295431673526764, 0.15590934455394745, 0.1801350712776184, 0.1557254195213318, 0.14409196376800537, 0.1697738915681839, 0.19203589856624603, 0.20664654672145844, 0.20424166321754456, 0.17892026901245117, 0.1498102992773056, 0.14367401599884033, 0.15348532795906067, 0.1329091340303421, 0.18973509967327118, 0.1556987762451172, 0.1880638301372528, 0.2228904664516449, 0.2230272889137268, 0.1858453005552292, 0.17712698876857758, 0.15279601514339447, 0.15533284842967987, 0.15883629024028778, 0.20319563150405884, 0.16341443359851837, 0.22270748019218445, 0.18651294708251953, 0.1509055644273758, 0.16702565550804138, 0.1826464980840683, 0.15482749044895172, 0.15283267199993134, 0.13329210877418518, 0.19640114903450012, 0.18026359379291534, 0.20271481573581696, 0.19523018598556519, 0.148769810795784, 0.18035750091075897, 0.2041611671447754, 0.14867620170116425, 0.14544937014579773, 0.22249636054039001, 0.1441197395324707, 0.22239889204502106, 0.1726127564907074, 0.19877779483795166, 0.18689054250717163, 0.167339488863945, 0.1972087323665619, 0.15786951780319214, 0.1786881387233734, 0.17091742157936096, 0.13358715176582336, 0.15851038694381714, 0.1568436175584793, 0.1493726223707199, 0.22267360985279083, 0.17559100687503815, 0.1967833936214447, 0.13359364867210388, 0.1605154573917389, 0.1564294844865799, 0.1709459125995636, 0.18328483402729034, 0.17956921458244324, 0.22201605141162872, 0.17774294316768646, 0.14765730500221252, 0.1576022505760193, 0.17809051275253296, 0.1335037350654602, 0.2030850201845169, 0.17089074850082397, 0.15899059176445007, 0.14888310432434082, 0.14919599890708923, 0.16822120547294617, 0.2010120451450348, 0.18019738793373108, 0.15341489017009735, 0.15725746750831604, 0.18090751767158508, 0.19376888871192932, 0.14485310018062592, 0.1678542196750641, 0.16020536422729492, 0.13346396386623383, 0.194280743598938, 0.15972642600536346, 0.20439577102661133, 0.15296880900859833, 0.13339504599571228, 0.18190883100032806, 0.15721023082733154, 0.15929782390594482, 0.17581434547901154, 0.1442655622959137, 0.14204637706279755, 0.19076290726661682, 0.13378559052944183, 0.15066397190093994, 0.19844327867031097, 0.1687411665916443, 0.18233223259449005, 0.16239893436431885, 0.16249974071979523, 0.1504078060388565, 0.14923420548439026, 0.18546505272388458, 0.15517504513263702, 0.17007365822792053, 0.1331382691860199, 0.22286257147789001, 0.13314580917358398, 0.16289874911308289, 0.1335662454366684, 0.14711414277553558, 0.15831157565116882, 0.13313451409339905, 0.18417908251285553, 0.15646234154701233, 0.18762125074863434, 0.13288737833499908, 0.14822161197662354, 0.15030820667743683, 0.15578415989875793, 0.15497037768363953, 0.19682487845420837, 0.16619595885276794, 0.18019211292266846, 0.18743444979190826, 0.15874457359313965, 0.13249751925468445, 0.20051108300685883, 0.17207591235637665, 0.1325262039899826, 0.15812674164772034, 0.1965082436800003, 0.17339569330215454, 0.13246747851371765, 0.18670745193958282, 0.19468528032302856, 0.15673497319221497, 0.14377300441265106, 0.15845683217048645, 0.15986908972263336, 0.13243544101715088, 0.18479220569133759, 0.153453528881073, 0.17061853408813477, 0.19400715827941895, 0.17393019795417786, 0.17374823987483978, 0.15156574547290802, 0.18905319273471832, 0.15812912583351135, 0.16812078654766083, 0.17014622688293457, 0.13214892148971558, 0.14144740998744965, 0.17977102100849152, 0.19154773652553558, 0.1424669623374939, 0.20603179931640625, 0.20065376162528992, 0.19862642884254456, 0.20861296355724335, 0.15521609783172607, 0.13210153579711914, 0.1680070161819458, 0.2119745910167694, 0.1324164867401123, 0.19499540328979492, 0.17772164940834045, 0.1726568639278412, 0.20562298595905304, 0.17450553178787231, 0.21432062983512878, 0.16527877748012543, 0.1321876496076584, 0.2239210158586502, 0.17553868889808655, 0.16214926540851593, 0.17533963918685913, 0.1839897632598877, 0.16438618302345276, 0.16915172338485718, 0.1324533224105835, 0.19442372024059296, 0.17035141587257385, 0.19274799525737762, 0.14980731904506683, 0.22383075952529907, 0.1547584980726242, 0.17646270990371704, 0.2082621455192566, 0.16107352077960968, 0.1558871567249298, 0.2134181708097458, 0.16147933900356293, 0.18710345029830933, 0.22376562654972076, 0.15429262816905975, 0.17356055974960327, 0.16001036763191223, 0.13235197961330414, 0.19419105350971222, 0.19533446431159973, 0.19514012336730957, 0.17425793409347534, 0.17142939567565918, 0.17465950548648834, 0.1327584981918335, 0.14847508072853088, 0.2050030380487442, 0.1541132926940918, 0.16579240560531616, 0.1589813232421875, 0.1326664686203003, 0.16777481138706207, 0.2201102375984192, 0.17179925739765167, 0.15349268913269043, 0.1323564201593399, 0.17558273673057556, 0.182926207780838, 0.17382468283176422, 0.15707454085350037, 0.20164494216442108, 0.15571029484272003, 0.16577591001987457, 0.1325700730085373, 0.13239149749279022, 0.1628645807504654, 0.1800803542137146, 0.177249938249588, 0.14441607892513275, 0.2002338320016861, 0.17745836079120636, 0.18582268059253693, 0.15334132313728333, 0.14047497510910034, 0.175509974360466, 0.1572093367576599, 0.14602704346179962, 0.14703066647052765, 0.17977307736873627, 0.1851615309715271, 0.18825127184391022, 0.18347734212875366, 0.17153388261795044, 0.13243810832500458, 0.15954898297786713, 0.20744863152503967, 0.16241076588630676, 0.17922018468379974, 0.17211072146892548, 0.1814725697040558, 0.13742701709270477, 0.19607529044151306, 0.1325434148311615, 0.1711345762014389, 0.20217536389827728, 0.17193886637687683, 0.21544446051120758, 0.20633022487163544, 0.1322605013847351, 0.1604195386171341, 0.21126049757003784, 0.1721779853105545, 0.16364629566669464, 0.14700983464717865, 0.19085852801799774, 0.15926040709018707, 0.16494999825954437, 0.15583544969558716, 0.13210786879062653, 0.16354691982269287, 0.15207931399345398, 0.13199734687805176, 0.22413793206214905, 0.19340437650680542, 0.19689823687076569, 0.1765587478876114, 0.17992673814296722, 0.19496163725852966, 0.185016930103302, 0.20116417109966278, 0.13221584260463715, 0.1995745599269867, 0.19234615564346313, 0.17772085964679718, 0.15239903330802917, 0.2010999172925949, 0.13231723010540009, 0.16600333154201508, 0.16465981304645538, 0.19261790812015533, 0.19067837297916412, 0.17292295396327972, 0.19770748913288116, 0.2237994223833084, 0.17308370769023895, 0.20726440846920013, 0.13230746984481812, 0.14684203267097473, 0.1325467973947525, 0.22385647892951965, 0.18640674650669098, 0.1842455118894577, 0.19419699907302856, 0.15090475976467133, 0.16522005200386047, 0.1630934178829193, 0.19552433490753174, 0.20348723232746124, 0.15470129251480103, 0.13262204825878143, 0.1486080437898636, 0.1604163944721222, 0.15559007227420807, 0.157340407371521, 0.13251841068267822, 0.16902831196784973, 0.19046182930469513, 0.18562451004981995, 0.17650514841079712, 0.1677679866552353, 0.17620502412319183, 0.2026873230934143, 0.20379303395748138, 0.1402234584093094, 0.16004455089569092, 0.19660256803035736, 0.14373715221881866, 0.1554384082555771, 0.15361064672470093, 0.17164382338523865, 0.1832849234342575, 0.14768534898757935, 0.20789241790771484, 0.14742113649845123, 0.1779138147830963, 0.20752745866775513, 0.2045671045780182, 0.18432950973510742, 0.19276222586631775, 0.16155441105365753, 0.15708424150943756, 0.1331007182598114, 0.17038698494434357, 0.17010974884033203, 0.13268132507801056, 0.16308364272117615, 0.1903051882982254, 0.13266077637672424, 0.22387820482254028, 0.14798329770565033, 0.14777913689613342, 0.1901470124721527, 0.16453464329242706, 0.1520807296037674, 0.16480273008346558, 0.15628361701965332, 0.13246767222881317, 0.16975589096546173, 0.1877543032169342, 0.22361338138580322, 0.1519310623407364, 0.1802125871181488, 0.17095254361629486, 0.18646939098834991, 0.14917704463005066, 0.19123327732086182, 0.15030474960803986, 0.17761357128620148, 0.1512664556503296, 0.1605827957391739, 0.16764406859874725, 0.13231393694877625, 0.18188869953155518, 0.19998066127300262, 0.2082226723432541, 0.17793919146060944, 0.1545698195695877, 0.2141866385936737, 0.1529230922460556, 0.22374556958675385, 0.17491962015628815, 0.17107081413269043, 0.17728735506534576, 0.22370900213718414, 0.20013606548309326, 0.19597889482975006, 0.16047832369804382, 0.1447514444589615, 0.16981244087219238, 0.14078661799430847, 0.1497458964586258, 0.13257810473442078, 0.1452178955078125, 0.18478217720985413, 0.19531509280204773, 0.18331176042556763, 0.1328291893005371, 0.20322494208812714, 0.1653376966714859, 0.17782869935035706, 0.1421126425266266, 0.14175087213516235, 0.14644469320774078, 0.19872647523880005, 0.15583424270153046, 0.13800200819969177, 0.13594216108322144, 0.15114711225032806, 0.1483483463525772, 0.1484818458557129, 0.20735859870910645, 0.21416817605495453, 0.1571740359067917, 0.13248276710510254, 0.18596333265304565, 0.1518924981355667, 0.16813072562217712, 0.1396215260028839, 0.16352930665016174, 0.16649211943149567, 0.20002323389053345, 0.15988986194133759, 0.1924961656332016, 0.14944130182266235, 0.13234491646289825, 0.1987915337085724, 0.1656964272260666, 0.17192602157592773, 0.1749967336654663, 0.18316496908664703, 0.14392946660518646, 0.13234283030033112, 0.15166375041007996, 0.20356619358062744, 0.18990680575370789, 0.1461702436208725, 0.17490386962890625, 0.13230636715888977, 0.19606606662273407, 0.18238522112369537, 0.13234515488147736, 0.1712147444486618, 0.13227926194667816, 0.19831979274749756, 0.1519530862569809, 0.17686961591243744, 0.1323213428258896, 0.14378900825977325, 0.1505042165517807, 0.13230080902576447, 0.19582271575927734, 0.16161295771598816, 0.14634722471237183, 0.19357791543006897, 0.18150268495082855, 0.13199041783809662, 0.1646512895822525, 0.19105125963687897, 0.20374846458435059, 0.16086269915103912, 0.20068477094173431, 0.148255854845047, 0.17941094934940338, 0.20707112550735474, 0.16505424678325653, 0.14109529554843903, 0.16219966113567352, 0.20006194710731506, 0.22323252260684967, 0.18472863733768463, 0.15278711915016174, 0.17076128721237183, 0.1320592164993286, 0.18855589628219604, 0.19522298872470856, 0.1533644199371338, 0.1545068770647049, 0.17061808705329895, 0.18160100281238556, 0.15607935190200806, 0.15796959400177002, 0.18553951382637024, 0.18293805420398712, 0.13191264867782593, 0.14558719098567963, 0.16383622586727142, 0.15714238584041595, 0.16574235260486603, 0.17569464445114136, 0.15952211618423462, 0.17010878026485443, 0.15640570223331451, 0.18007831275463104, 0.2094004899263382, 0.1323665976524353, 0.13190920650959015, 0.19981476664543152, 0.180677592754364, 0.145835280418396, 0.1737573891878128, 0.2002619504928589, 0.16314321756362915, 0.1607329398393631, 0.15367116034030914, 0.1658160239458084, 0.16893123090267181, 0.20118877291679382, 0.13171523809432983, 0.16032250225543976, 0.16315379738807678, 0.1926489621400833, 0.15885962545871735, 0.14476945996284485, 0.1316143125295639, 0.19355735182762146, 0.2071860134601593, 0.17162370681762695, 0.14235204458236694, 0.17104481160640717, 0.17503973841667175, 0.18613843619823456, 0.2078980952501297, 0.16142068803310394, 0.1314953863620758, 0.13173672556877136, 0.2090647965669632, 0.18164820969104767, 0.14811202883720398, 0.13153865933418274, 0.18324154615402222, 0.16021466255187988, 0.14801038801670074, 0.19389277696609497, 0.15761618316173553, 0.1544465869665146, 0.18350887298583984, 0.1780698597431183, 0.16843055188655853, 0.15987971425056458, 0.1466197371482849, 0.16517160832881927, 0.13155768811702728, 0.17246854305267334, 0.1493477076292038, 0.20663337409496307, 0.14117993414402008, 0.14726483821868896, 0.20594820380210876, 0.22525736689567566, 0.15075381100177765, 0.1827709823846817, 0.18807809054851532, 0.2003612518310547, 0.1891392171382904, 0.13159960508346558, 0.15495158731937408, 0.16427350044250488, 0.14438103139400482, 0.16041205823421478, 0.14950120449066162, 0.17269349098205566, 0.14164383709430695, 0.19354403018951416, 0.18121197819709778, 0.14813748002052307, 0.1312083899974823, 0.14898541569709778, 0.18412192165851593, 0.16279324889183044, 0.14308807253837585, 0.1361706703901291, 0.20700505375862122, 0.15678521990776062, 0.13131000101566315, 0.16324542462825775, 0.18686625361442566, 0.20596002042293549, 0.16969339549541473, 0.14885850250720978, 0.15706494450569153, 0.16176548600196838, 0.13116022944450378, 0.1623019129037857, 0.15172706544399261, 0.15992674231529236, 0.19249293208122253, 0.20641086995601654, 0.13105040788650513, 0.17785489559173584, 0.22565552592277527, 0.16857801377773285, 0.14192765951156616, 0.16864120960235596, 0.17028267681598663, 0.16005557775497437, 0.1672936975955963, 0.15202312171459198, 0.15102504193782806, 0.1512378603219986, 0.15149925649166107, 0.19674132764339447, 0.1805291473865509, 0.15878860652446747, 0.13092797994613647, 0.20903564989566803, 0.1973314881324768, 0.1690305918455124, 0.1689610779285431, 0.14465662837028503, 0.16147878766059875, 0.17936432361602783, 0.2071462720632553, 0.1668010950088501, 0.1309724599123001, 0.16463828086853027, 0.13104230165481567, 0.14326415956020355, 0.1670260727405548, 0.18749238550662994, 0.13079307973384857, 0.2008415013551712, 0.15862545371055603, 0.16367056965827942, 0.17365489900112152, 0.19698449969291687, 0.15326862037181854, 0.17874102294445038, 0.17153097689151764, 0.17789679765701294, 0.13355779647827148, 0.13117390871047974, 0.21408210694789886, 0.16733315587043762, 0.22635523974895477, 0.1736181676387787, 0.13323484361171722, 0.16773614287376404, 0.15939439833164215, 0.17797864973545074, 0.16130481660366058, 0.2071554958820343, 0.18745124340057373, 0.15068389475345612, 0.15577276051044464, 0.13072548806667328, 0.20208969712257385, 0.1545291393995285, 0.1412767916917801, 0.2091626077890396, 0.14739428460597992, 0.17801856994628906, 0.19770029187202454, 0.17573979496955872, 0.15552927553653717, 0.2265358418226242, 0.1655457615852356, 0.20265421271324158, 0.17994511127471924, 0.14973385632038116, 0.16135145723819733, 0.1813688725233078, 0.1682889759540558, 0.16303445398807526, 0.19643670320510864, 0.18876014649868011, 0.2040197253227234, 0.13108064234256744, 0.17089806497097015, 0.16253051161766052, 0.1494903564453125, 0.1587790548801422, 0.2018984854221344, 0.22973434627056122, 0.22601869702339172, 0.1692565232515335, 0.16400443017482758, 0.15882299840450287, 0.2076617032289505, 0.17149639129638672, 0.16506952047348022, 0.2033924162387848, 0.1556152105331421, 0.1594134420156479, 0.13238364458084106, 0.1531473696231842, 0.1309330016374588, 0.16126030683517456, 0.1806136667728424, 0.14506451785564423, 0.16896992921829224, 0.19309908151626587, 0.193791463971138, 0.15751470625400543, 0.15107621252536774, 0.147474005818367, 0.17798127233982086, 0.16243331134319305, 0.15964537858963013, 0.1560819000005722, 0.1799800992012024, 0.17207415401935577, 0.1790628433227539, 0.17655718326568604, 0.17864125967025757, 0.18502850830554962, 0.20198827981948853, 0.17405669391155243, 0.16570580005645752, 0.18892569839954376, 0.17240257561206818, 0.1587190479040146, 0.1669793576002121, 0.17422588169574738, 0.19357247650623322, 0.14890514314174652, 0.1760150045156479, 0.16389352083206177, 0.14436733722686768, 0.169244647026062, 0.1800212860107422, 0.152669757604599, 0.19957567751407623, 0.2285882830619812, 0.14819085597991943, 0.14632150530815125, 0.13910894095897675, 0.181844100356102, 0.18204310536384583, 0.18188326060771942, 0.20754298567771912, 0.1311439871788025, 0.13124807178974152, 0.15674495697021484, 0.16175314784049988, 0.17327535152435303, 0.16790451109409332, 0.15905670821666718, 0.13122481107711792, 0.2055179625749588, 0.16735821962356567, 0.15207765996456146, 0.17788903415203094, 0.16687580943107605, 0.22587236762046814, 0.154667928814888, 0.17443498969078064, 0.1583469957113266, 0.17264467477798462, 0.17231078445911407, 0.16815371811389923, 0.1763409674167633, 0.14303329586982727, 0.2104588896036148, 0.18259462714195251, 0.13099956512451172, 0.18487241864204407, 0.1538250744342804, 0.15165336430072784, 0.13098639249801636, 0.14661674201488495, 0.16721871495246887, 0.19914573431015015, 0.17271986603736877, 0.17433646321296692, 0.13105332851409912, 0.22581253945827484, 0.1707153022289276, 0.16857939958572388, 0.18026359379291534, 0.17947395145893097, 0.18852835893630981, 0.18196849524974823, 0.1366605907678604, 0.13636407256126404, 0.17635852098464966, 0.18185164034366608, 0.17316772043704987, 0.1555958241224289, 0.1474289447069168, 0.18003299832344055, 0.16887293756008148, 0.1831168383359909, 0.1799241602420807, 0.16857154667377472, 0.14734122157096863, 0.18364179134368896, 0.1628415584564209, 0.1582358181476593, 0.20214428007602692, 0.16549353301525116, 0.17036744952201843, 0.22585773468017578, 0.1487642526626587, 0.18346725404262543, 0.14368174970149994, 0.1519848257303238, 0.22613203525543213, 0.1459890455007553, 0.13961674273014069, 0.13097722828388214, 0.19452854990959167, 0.14537134766578674, 0.14802919328212738, 0.19461672008037567, 0.16907314956188202, 0.2259851098060608, 0.22596020996570587, 0.18783557415008545, 0.15267616510391235, 0.18486234545707703, 0.16012553870677948, 0.18118256330490112, 0.14457233250141144, 0.2261800915002823, 0.16089673340320587, 0.16195368766784668, 0.1722574681043625, 0.1850736439228058, 0.13092446327209473, 0.14886488020420074, 0.1488974690437317, 0.1647932231426239, 0.20081204175949097, 0.1921527087688446, 0.14645710587501526, 0.20227490365505219, 0.17708759009838104, 0.20414094626903534, 0.1603507697582245, 0.2070954591035843, 0.1977747678756714, 0.1509849578142166, 0.17028416693210602, 0.1416867971420288, 0.18938389420509338, 0.16921569406986237, 0.18039359152317047, 0.16445431113243103, 0.15838393568992615, 0.17149119079113007, 0.2068779021501541, 0.16504259407520294, 0.17787182331085205, 0.21225382387638092, 0.18278345465660095, 0.14469677209854126, 0.15718011558055878, 0.15047872066497803, 0.14933831989765167, 0.13969793915748596, 0.1559840738773346, 0.1774238646030426, 0.15654690563678741, 0.18384374678134918, 0.20219098031520844, 0.22563664615154266, 0.1522001475095749, 0.18678633868694305, 0.1837337762117386, 0.13110679388046265, 0.1834595799446106, 0.15024840831756592, 0.1700000911951065, 0.1516314148902893, 0.16426175832748413, 0.1744096577167511, 0.20328198373317719, 0.13974551856517792, 0.15908899903297424, 0.14643588662147522, 0.14795593917369843, 0.15742093324661255, 0.15849806368350983, 0.20494715869426727, 0.19414570927619934, 0.16903159022331238, 0.13096913695335388, 0.15265293419361115, 0.18688179552555084, 0.17559927701950073, 0.18511345982551575, 0.17894776165485382, 0.15639899671077728, 0.1958896815776825, 0.18976806104183197, 0.162079319357872, 0.17635507881641388, 0.16965577006340027, 0.17539921402931213, 0.16806389391422272, 0.14145688712596893, 0.16908788681030273, 0.1635081022977829, 0.14789193868637085, 0.1523168683052063, 0.1955946534872055, 0.16601143777370453, 0.17353813350200653, 0.16238108277320862, 0.171064093708992, 0.18768613040447235, 0.18040971457958221, 0.1797901839017868, 0.16252583265304565, 0.17537915706634521, 0.18444745242595673, 0.18452854454517365, 0.19717533886432648, 0.1691959947347641, 0.2259359061717987, 0.15522275865077972, 0.13095037639141083, 0.13552014529705048, 0.18171702325344086, 0.1790555715560913, 0.14715638756752014, 0.16119275987148285, 0.14285583794116974, 0.2256772667169571, 0.13112373650074005, 0.17811724543571472, 0.1745242327451706, 0.15183965861797333, 0.131290003657341, 0.1404097080230713, 0.17445296049118042, 0.14386700093746185, 0.13095594942569733, 0.18331082165241241, 0.20357263088226318, 0.18556351959705353, 0.20543943345546722, 0.19234433770179749, 0.14726108312606812, 0.14641566574573517, 0.1475740522146225, 0.23323391377925873, 0.1814478635787964, 0.16826637089252472, 0.20502826571464539, 0.1597110778093338, 0.18535956740379333, 0.191290482878685, 0.13111552596092224, 0.17092683911323547, 0.14775046706199646, 0.15482230484485626, 0.15340304374694824, 0.18827326595783234, 0.14070872962474823, 0.1671268194913864, 0.17998342216014862, 0.13112974166870117, 0.1851247251033783, 0.18859004974365234, 0.14402779936790466, 0.1808353215456009, 0.14289239048957825, 0.19544588029384613, 0.15499262511730194, 0.20197856426239014, 0.1850082278251648, 0.14185360074043274, 0.15181943774223328, 0.1733124703168869, 0.17390544712543488, 0.2046300768852234, 0.1310393065214157, 0.15990249812602997, 0.18576624989509583, 0.14883114397525787, 0.17277023196220398, 0.1380680501461029, 0.1607256382703781, 0.15259790420532227, 0.17973341047763824, 0.1314212679862976, 0.15609358251094818, 0.22599229216575623, 0.15688496828079224, 0.17449264228343964, 0.14940686523914337, 0.1556747406721115, 0.13096243143081665, 0.13104860484600067, 0.16864198446273804, 0.18084920942783356, 0.19633163511753082, 0.18196457624435425, 0.16237087547779083, 0.1640995889902115, 0.1887396275997162, 0.1487634778022766, 0.16041526198387146, 0.22187267243862152, 0.1687912493944168, 0.1885915845632553, 0.17914414405822754, 0.1310499906539917, 0.1661653071641922, 0.22619648277759552, 0.15628774464130402, 0.18485549092292786, 0.15954743325710297, 0.16696542501449585, 0.14924535155296326, 0.1915893703699112, 0.14870071411132812, 0.1759619116783142, 0.18318286538124084, 0.15388837456703186, 0.17229565978050232, 0.1468651294708252, 0.16410702466964722, 0.17744038999080658, 0.13080814480781555, 0.16024249792099, 0.13700386881828308, 0.1834181696176529, 0.1756124198436737, 0.15429940819740295, 0.21944977343082428, 0.17001430690288544, 0.13087768852710724, 0.17624962329864502, 0.16956190764904022, 0.17849507927894592, 0.16538816690444946, 0.1755029261112213, 0.1895146518945694, 0.13068266212940216, 0.16643857955932617, 0.1780131608247757, 0.15238459408283234, 0.1568802297115326, 0.14966076612472534, 0.19688521325588226, 0.16454850137233734, 0.13094983994960785, 0.20252899825572968, 0.14630727469921112, 0.16080288589000702, 0.16498638689517975, 0.15408764779567719, 0.16480109095573425, 0.17662174999713898, 0.14431537687778473, 0.19903407990932465, 0.1562676727771759, 0.18002407252788544, 0.19109874963760376, 0.15914976596832275, 0.16017630696296692, 0.1578887552022934, 0.2010885626077652, 0.13736088573932648, 0.19183407723903656, 0.14407533407211304, 0.16226454079151154, 0.16471794247627258, 0.182189479470253, 0.17465336620807648, 0.15376786887645721, 0.18308129906654358, 0.17264892160892487, 0.20363499224185944, 0.14386476576328278, 0.2060687243938446, 0.1499863564968109, 0.16023565828800201, 0.16921266913414001, 0.20430602133274078, 0.2262977808713913, 0.17261788249015808, 0.15804465115070343, 0.15359963476657867, 0.19688963890075684, 0.17577306926250458, 0.16968940198421478, 0.13054940104484558, 0.21917404234409332, 0.18071940541267395, 0.17966455221176147, 0.14668138325214386, 0.17685024440288544, 0.15428034961223602, 0.13079386949539185, 0.13058412075042725, 0.20133228600025177, 0.1882692575454712, 0.22628936171531677, 0.16522273421287537, 0.15033771097660065, 0.2011059820652008, 0.1689879149198532, 0.1556364744901657, 0.2188655436038971, 0.19314074516296387, 0.1895178109407425, 0.1656264364719391, 0.15676777064800262, 0.15272188186645508, 0.1386348456144333, 0.15676231682300568, 0.22656460106372833, 0.13073350489139557, 0.16496062278747559, 0.13065478205680847, 0.21197691559791565, 0.1745564341545105, 0.14345978200435638, 0.2027054876089096, 0.13876736164093018, 0.18449613451957703, 0.17459431290626526, 0.16475553810596466, 0.2264883816242218, 0.15702316164970398, 0.16864222288131714, 0.17137488722801208, 0.19363939762115479, 0.153579980134964, 0.1495736688375473, 0.18943482637405396, 0.13063734769821167, 0.16116639971733093, 0.1630147099494934, 0.1420878916978836, 0.1465553194284439, 0.1451382339000702, 0.2122911959886551, 0.1852879673242569, 0.156594917178154, 0.2001427859067917, 0.13055700063705444, 0.1769273579120636, 0.15171557664871216, 0.15335915982723236, 0.19138272106647491, 0.16333889961242676, 0.16684597730636597, 0.15240417420864105, 0.1717957854270935, 0.19273127615451813, 0.20580196380615234, 0.18303120136260986, 0.2265472114086151, 0.20402051508426666, 0.13062697649002075, 0.15026850998401642, 0.1634022742509842, 0.1585463285446167, 0.13060888648033142, 0.17247502505779266, 0.1572105884552002, 0.17769378423690796, 0.13068360090255737, 0.19108931720256805, 0.17185308039188385, 0.13078299164772034, 0.1532888263463974, 0.1736350953578949, 0.18284809589385986, 0.18861821293830872, 0.15164577960968018, 0.1559632122516632, 0.1306474804878235, 0.177326038479805, 0.17899397015571594, 0.19748403131961823, 0.20906025171279907, 0.16887342929840088, 0.15296906232833862, 0.16880033910274506, 0.15187735855579376, 0.1587059497833252, 0.17137134075164795, 0.19457174837589264, 0.22649595141410828, 0.1307455450296402, 0.15221217274665833, 0.13058717548847198, 0.14562995731830597, 0.1878189891576767, 0.1495862901210785, 0.15849024057388306, 0.16151177883148193, 0.17869527637958527, 0.15221340954303741, 0.20146092772483826, 0.15283937752246857, 0.17440326511859894, 0.1805521696805954, 0.1913623958826065, 0.22355355322360992, 0.1442805677652359, 0.18122588098049164, 0.1534091830253601, 0.14687056839466095, 0.1928313970565796, 0.1617453545331955, 0.15493397414684296, 0.17756709456443787, 0.21210962533950806, 0.21261493861675262, 0.18528181314468384, 0.16801995038986206, 0.1820472627878189, 0.1764862984418869, 0.14964020252227783, 0.15109142661094666, 0.14016249775886536, 0.15842387080192566, 0.19570274651050568, 0.1304691731929779, 0.14772474765777588, 0.17302928864955902, 0.16690966486930847, 0.1603117287158966, 0.15299998223781586, 0.1454973667860031, 0.2052350789308548, 0.1376848816871643, 0.17363695800304413, 0.173607736825943, 0.15301425755023956, 0.14353284239768982, 0.13075144588947296, 0.18552902340888977, 0.19894783198833466, 0.1673964411020279, 0.15214921534061432, 0.15984304249286652, 0.14533482491970062, 0.15694621205329895, 0.15678569674491882, 0.17524003982543945, 0.1828497350215912, 0.15545304119586945, 0.1670997589826584, 0.19879120588302612, 0.15505950152873993, 0.1996893584728241, 0.16972406208515167, 0.1907079517841339, 0.18087223172187805, 0.15997080504894257, 0.1499585658311844, 0.1575017124414444, 0.15405778586864471, 0.16050183773040771, 0.1523381769657135, 0.1626412570476532, 0.1458864063024521, 0.17574352025985718, 0.16260285675525665, 0.1790064573287964, 0.18988734483718872, 0.18452081084251404, 0.18187697231769562, 0.15377752482891083, 0.14671579003334045, 0.17260897159576416, 0.19508489966392517, 0.15219905972480774, 0.21057221293449402, 0.17215727269649506, 0.1841627061367035, 0.1700066775083542, 0.156921848654747, 0.18213129043579102, 0.17253626883029938, 0.13040278851985931, 0.16669417917728424, 0.19310280680656433, 0.17953592538833618, 0.15042312443256378, 0.13070368766784668, 0.20317430794239044, 0.14390702545642853, 0.14686542749404907, 0.1463809311389923, 0.18244785070419312, 0.1532847136259079, 0.18715542554855347, 0.14462080597877502, 0.14254865050315857, 0.18039685487747192, 0.20303453505039215, 0.15735524892807007, 0.15469655394554138, 0.1611350029706955, 0.20207953453063965, 0.18911637365818024, 0.22671936452388763, 0.1835424304008484, 0.20131300389766693, 0.17505289614200592, 0.15108822286128998, 0.22665971517562866, 0.18783921003341675, 0.1696074903011322, 0.2266245037317276, 0.14532570540905, 0.17094464600086212, 0.1304357796907425, 0.17202483117580414, 0.17912036180496216, 0.15911462903022766, 0.1811145693063736, 0.13044583797454834, 0.15076860785484314, 0.17294901609420776, 0.13039635121822357, 0.22689899802207947, 0.22673210501670837, 0.13036565482616425, 0.15821473300457, 0.2270580232143402, 0.15690740942955017, 0.13033251464366913, 0.19281570613384247, 0.1927005648612976, 0.22686685621738434, 0.13074392080307007, 0.17288915812969208, 0.20261546969413757, 0.18309280276298523, 0.16816900670528412, 0.16529996693134308, 0.17677561938762665, 0.19711902737617493, 0.19940955936908722, 0.13053035736083984, 0.13049381971359253, 0.1304495483636856, 0.1499781310558319, 0.157232403755188, 0.1594540774822235, 0.14788898825645447, 0.1813099980354309, 0.1499462127685547, 0.19179373979568481, 0.2028185874223709, 0.1679506003856659, 0.13851891458034515, 0.13046608865261078, 0.13030539453029633, 0.2039155811071396, 0.15087242424488068, 0.16151967644691467, 0.13031326234340668, 0.15427443385124207, 0.20766574144363403, 0.16527359187602997, 0.17136169970035553, 0.18487738072872162, 0.17726671695709229, 0.16391803324222565, 0.16799814999103546, 0.14236976206302643, 0.1617862582206726, 0.1304292529821396, 0.14518997073173523, 0.15062707662582397, 0.16517303884029388, 0.16912367939949036, 0.1953703910112381, 0.15444935858249664, 0.19341140985488892, 0.16918587684631348, 0.155763640999794, 0.17736415565013885, 0.1668708175420761, 0.20875334739685059, 0.13072794675827026, 0.18907730281352997, 0.13038983941078186, 0.15190188586711884, 0.2129109799861908, 0.19493140280246735, 0.1304134726524353, 0.20924316346645355, 0.19373559951782227, 0.19285838305950165, 0.13036224246025085, 0.15293475985527039, 0.14026184380054474, 0.2047295868396759, 0.15566584467887878, 0.15482588112354279, 0.22672028839588165, 0.13027118146419525, 0.1649269014596939, 0.16162759065628052, 0.19633382558822632, 0.14247220754623413, 0.14476658403873444, 0.22684705257415771, 0.1496955007314682, 0.14730672538280487, 0.2146029770374298, 0.1498614102602005, 0.17864534258842468, 0.19543422758579254, 0.16436903178691864, 0.20109103620052338, 0.18156513571739197, 0.15842925012111664, 0.20502455532550812, 0.13032902777194977, 0.18043676018714905, 0.15410380065441132, 0.2025289088487625, 0.14732733368873596, 0.19960689544677734, 0.22720520198345184, 0.13035275042057037, 0.18901991844177246, 0.13084478676319122, 0.14662018418312073, 0.19383838772773743, 0.21038153767585754, 0.15579862892627716, 0.16588221490383148, 0.15133292973041534, 0.17823728919029236, 0.18797633051872253, 0.18533609807491302, 0.19879433512687683, 0.18902134895324707, 0.19115012884140015, 0.17758992314338684, 0.1847369372844696, 0.15966308116912842, 0.19619403779506683, 0.19329576194286346, 0.1582634449005127, 0.1868184208869934, 0.22670787572860718, 0.19330967962741852, 0.20217043161392212, 0.1436339020729065, 0.18089163303375244, 0.18477079272270203, 0.14777407050132751, 0.18108759820461273, 0.13038043677806854, 0.2268761694431305, 0.1522487848997116, 0.1846240907907486, 0.1557728499174118, 0.18745803833007812, 0.17042721807956696, 0.2037043571472168, 0.18579353392124176, 0.18519002199172974, 0.226827472448349, 0.15293219685554504, 0.16201482713222504, 0.1524248868227005, 0.17281365394592285, 0.20120324194431305, 0.15263013541698456, 0.1569216400384903, 0.13066521286964417, 0.16450704634189606, 0.1752154529094696, 0.13045087456703186, 0.15461356937885284, 0.14940033853054047, 0.16914191842079163, 0.1770744025707245, 0.21274355053901672, 0.21142445504665375, 0.19623826444149017, 0.15698644518852234, 0.19255365431308746, 0.18361586332321167, 0.16675874590873718, 0.15895725786685944, 0.19726911187171936, 0.13047002255916595, 0.14639724791049957, 0.22751587629318237]\n",
            "Val loss 0.16885551621791403\n",
            "Val auc roc 0.49941792782305006\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}