{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Only_Informative_No_Dup_New.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b51ad2b845e346a9ba689037736d3398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc7d9370d3674700ac199e0019af7884",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_69027da5a0954baebafdbfce5d15472b",
              "IPY_MODEL_add0f987165a462eaf9d54edf3e91350"
            ]
          }
        },
        "fc7d9370d3674700ac199e0019af7884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69027da5a0954baebafdbfce5d15472b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_337cdb708a47496cac34d9fd861226b3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c55ac36fcd64b17b069f701427fa549"
          }
        },
        "add0f987165a462eaf9d54edf3e91350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0731edef245a4ca5a51559291f87992f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:15&lt;00:00, 15.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fadaae75941c496085528211e568bdf4"
          }
        },
        "337cdb708a47496cac34d9fd861226b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c55ac36fcd64b17b069f701427fa549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0731edef245a4ca5a51559291f87992f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fadaae75941c496085528211e568bdf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89a84be592684759ad2dedfda2163474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25c58a8b1569468e97d6f8abfc1bac80",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_615bd7283f0d4eaa8f34d8c81ccc0a8b",
              "IPY_MODEL_3e34230f1d8d4969979f858fbef95a88"
            ]
          }
        },
        "25c58a8b1569468e97d6f8abfc1bac80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "615bd7283f0d4eaa8f34d8c81ccc0a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81cb956ccbac4ef49d6485c1663bcb6b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e95ebfe730840b48909508d6014cf04"
          }
        },
        "3e34230f1d8d4969979f858fbef95a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_85d6d53e9c524cefa1272f1b1f90b7c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [45:47&lt;00:00,  1.72s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_550ec52ba6984fcfa6db39c72826c781"
          }
        },
        "81cb956ccbac4ef49d6485c1663bcb6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e95ebfe730840b48909508d6014cf04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85d6d53e9c524cefa1272f1b1f90b7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "550ec52ba6984fcfa6db39c72826c781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff39186733614994b7eb0e5ca881d73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cddf4e813db04e7b91cc5ed33f40959b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0efc3af3304e4d6f945bc241363fff87",
              "IPY_MODEL_c9c9a537209141d38d35864b752ff3e4"
            ]
          }
        },
        "cddf4e813db04e7b91cc5ed33f40959b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0efc3af3304e4d6f945bc241363fff87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1df8f1bea6e4d60b32179a2979e7ea4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f328485b19a4a5495a707db21eb0d3b"
          }
        },
        "c9c9a537209141d38d35864b752ff3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d6e3cd730864e8a8b9757b2adfb379c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [14:57&lt;00:00,  1.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d58d57500ee4a32989f6812d49f200f"
          }
        },
        "c1df8f1bea6e4d60b32179a2979e7ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f328485b19a4a5495a707db21eb0d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d6e3cd730864e8a8b9757b2adfb379c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d58d57500ee4a32989f6812d49f200f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89c529dc268f435682f7911b40336438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e63126fc6ba42308386c551ee69bb9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a95f602d274f4ae68dbbf9fb273ef570",
              "IPY_MODEL_8da53bb6e7c34550927a584d32758768"
            ]
          }
        },
        "2e63126fc6ba42308386c551ee69bb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a95f602d274f4ae68dbbf9fb273ef570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a5ad2edf7f34d6fafb0d0077decaf92",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d21f12ee27a647819ec85aff825a3d9e"
          }
        },
        "8da53bb6e7c34550927a584d32758768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ddfb68f706514bfda2eed88f147bef05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:01&lt;00:00,  1.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d89101a5c2946129bcaad44eb69c8b7"
          }
        },
        "0a5ad2edf7f34d6fafb0d0077decaf92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d21f12ee27a647819ec85aff825a3d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddfb68f706514bfda2eed88f147bef05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d89101a5c2946129bcaad44eb69c8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0af3e63c-4d99-48a1-e175-66de5087ebc1"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 20.06 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03bfe789-b096-484a-94f4-4a3463c3d7cf"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0e6039d-c407-4398-d39f-703bf15b656a"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b03738eb-9a4b-4c9b-8a3e-99fe2c1e9dfb"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c244cb6f-b226-4239-d7fb-6088e290e3d9"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "ba2878cb-768f-447b-b90f-25b443abfc5c"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 66.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6a187e79-8a9d-4df7-cc76-7d69b372ff69"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "143acafd-3ffb-4414-b4b5-0bf3a796b00f"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c7383399-0443-4eb1-baf5-670ad990e2a4"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ab533934-140c-441f-f5c0-440642f1310b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    As @CrispinSimon discusses issues that need to...\n",
              "1    Allegations baseless; I was shocked: Arjun on ...\n",
              "2                                        ption - no:2 \n",
              "3    #recoil The best feeling in the world! #womenr...\n",
              "4    Sanjjanaa Galrani's First Reaction Over  Ravi ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b32257bf-3360-46a2-b225-015c06301065"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:15<00:00, 26188431.01B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3fc3419-70d2-40ed-c893-0d51f60d5c11"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 676795.16B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a3e6eae0-5a41-4af6-fcf7-714749154405"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "269d2e31-10b9-4113-cc9a-427945944999"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8987f96-6eff-4888-9300-3e006c91b0e1"
      },
      "source": [
        "col_name = \"Text_Only_Informative\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 0 # or 0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760,
          "referenced_widgets": [
            "b51ad2b845e346a9ba689037736d3398",
            "fc7d9370d3674700ac199e0019af7884",
            "69027da5a0954baebafdbfce5d15472b",
            "add0f987165a462eaf9d54edf3e91350",
            "337cdb708a47496cac34d9fd861226b3",
            "7c55ac36fcd64b17b069f701427fa549",
            "0731edef245a4ca5a51559291f87992f",
            "fadaae75941c496085528211e568bdf4",
            "89a84be592684759ad2dedfda2163474",
            "25c58a8b1569468e97d6f8abfc1bac80",
            "615bd7283f0d4eaa8f34d8c81ccc0a8b",
            "3e34230f1d8d4969979f858fbef95a88",
            "81cb956ccbac4ef49d6485c1663bcb6b",
            "1e95ebfe730840b48909508d6014cf04",
            "85d6d53e9c524cefa1272f1b1f90b7c6",
            "550ec52ba6984fcfa6db39c72826c781",
            "ff39186733614994b7eb0e5ca881d73e",
            "cddf4e813db04e7b91cc5ed33f40959b",
            "0efc3af3304e4d6f945bc241363fff87",
            "c9c9a537209141d38d35864b752ff3e4",
            "c1df8f1bea6e4d60b32179a2979e7ea4",
            "2f328485b19a4a5495a707db21eb0d3b",
            "5d6e3cd730864e8a8b9757b2adfb379c",
            "7d58d57500ee4a32989f6812d49f200f",
            "89c529dc268f435682f7911b40336438",
            "2e63126fc6ba42308386c551ee69bb9a",
            "a95f602d274f4ae68dbbf9fb273ef570",
            "8da53bb6e7c34550927a584d32758768",
            "0a5ad2edf7f34d6fafb0d0077decaf92",
            "d21f12ee27a647819ec85aff825a3d9e",
            "ddfb68f706514bfda2eed88f147bef05",
            "4d89101a5c2946129bcaad44eb69c8b7"
          ]
        },
        "outputId": "49c39f86-91d2-4ffd-a388-dbadf727c782"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 6382\n",
            "Old data length : 1596\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 1596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b51ad2b845e346a9ba689037736d3398",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89a84be592684759ad2dedfda2163474",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1448\n",
            "Train Losses : [0.18739363551139832, 2.892298936843872, 0.5352752208709717, 0.2443910539150238, 1.3579370975494385, 2.7866549491882324, 1.5849453210830688, 0.5379976630210876, 0.4573802947998047, 0.15415368974208832, 0.5030826926231384, 0.42464905977249146, 0.3285589814186096, 0.5279983878135681, 0.09156995266675949, 0.3913363814353943, 0.12542229890823364, 0.1789914220571518, 0.10853172838687897, 0.3032393455505371, 0.16516897082328796, 0.06887948513031006, 0.0250149704515934, 0.12445300817489624, 0.7845592498779297, 0.11005718261003494, 0.21161358058452606, 0.38323700428009033, 0.5512430667877197, 0.516894519329071, 0.03767124563455582, 0.30363452434539795, 0.10496173053979874, 0.06163821741938591, 0.054970625787973404, 0.03311347961425781, 0.043079860508441925, 0.3200896382331848, 0.13134585320949554, 0.03967137262225151, 0.15955863893032074, 0.05649080127477646, 0.3394276201725006, 0.021689582616090775, 0.48843061923980713, 0.0912926197052002, 0.10175405442714691, 0.021514080464839935, 0.12683089077472687, 0.0253286212682724, 0.022406326606869698, 0.2992919385433197, 0.09913282841444016, 0.07481533288955688, 0.03220195695757866, 0.031038889661431313, 0.10179989039897919, 0.53230220079422, 0.03352582827210426, 0.030798284336924553, 0.1715724766254425, 0.35807064175605774, 0.12284157425165176, 0.3409622311592102, 0.20702354609966278, 0.0387725755572319, 0.030017012730240822, 0.3126647472381592, 0.17410369217395782, 0.0372314378619194, 0.10641168057918549, 0.12529462575912476, 0.06559859961271286, 0.03932952880859375, 0.13383670151233673, 0.08985529094934464, 0.3395257592201233, 0.048506639897823334, 0.23234079778194427, 0.06486539542675018, 0.025184595957398415, 0.04907059669494629, 0.05681255832314491, 0.11994502693414688, 0.5906758308410645, 0.035826075822114944, 0.3841472268104553, 0.20568574965000153, 0.1636810004711151, 0.056154463440179825, 0.051793403923511505, 0.04374845325946808, 0.3206622302532196, 0.20776449143886566, 0.42111286520957947, 0.06205461174249649, 0.047033704817295074, 0.04931073635816574, 0.19240759313106537, 0.26867905259132385, 0.04815492779016495, 0.07523136585950851, 0.04671214520931244, 0.1239570751786232, 0.08469443023204803, 0.0758691132068634, 0.15694186091423035, 0.046067994087934494, 0.09345740079879761, 0.10878060013055801, 0.208492249250412, 0.14043709635734558, 0.1807892918586731, 0.11665500700473785, 0.03836919367313385, 0.13442057371139526, 0.03353693708777428, 0.029450003057718277, 0.029240181669592857, 0.08749562501907349, 0.026097221300005913, 0.14092379808425903, 0.02147858589887619, 0.15792714059352875, 0.7040188908576965, 0.153711199760437, 0.018490228801965714, 0.102411650121212, 0.024981748312711716, 0.048299390822649, 0.3956141769886017, 0.02108657732605934, 0.11871114373207092, 0.18237903714179993, 0.07620881497859955, 0.031337231397628784, 0.4389077425003052, 0.39798101782798767, 0.022384144365787506, 0.06406411528587341, 0.4654323160648346, 0.02638176828622818, 0.08629204332828522, 0.20129865407943726, 0.031357232481241226, 0.08853761106729507, 0.049863383173942566, 0.2885199189186096, 0.12596192955970764, 0.11640306562185287, 0.30219390988349915, 0.17959319055080414, 0.33517593145370483, 0.05452251434326172, 0.05617576465010643, 0.31291720271110535, 0.1775035560131073, 0.06734414398670197, 0.07619588822126389, 0.07479602843523026, 0.2810448110103607, 0.1567312628030777, 0.07360027730464935, 0.112219899892807, 0.1589670181274414, 0.20235423743724823, 0.1101953387260437, 0.09674199670553207, 0.07017869502305984, 0.1930893361568451, 0.06709852814674377, 0.1255030632019043, 0.06437450647354126, 0.14817555248737335, 0.058523647487163544, 0.0545404851436615, 0.10486515611410141, 0.19026845693588257, 0.08490515500307083, 0.09601730108261108, 0.041038628667593, 0.04527515172958374, 0.2505752444267273, 0.03459246829152107, 0.18905559182167053, 0.07026723027229309, 0.07565795630216599, 0.033234406262636185, 0.029127568006515503, 0.39341339468955994, 0.11567416042089462, 0.657820999622345, 0.08552777022123337, 0.03241529315710068, 0.06286431849002838, 0.03746575862169266, 0.03346346318721771, 0.08645021915435791, 0.12944528460502625, 0.03789801150560379, 0.21326614916324615, 0.4270871877670288, 0.08749131113290787, 0.050309184938669205, 0.17183132469654083, 0.04648197814822197, 0.09320687502622604, 0.11042998731136322, 0.15420709550380707, 0.1014961376786232, 0.03516283258795738, 0.1448133885860443, 0.09906657040119171, 0.2740846574306488, 0.07772282510995865, 0.04097376763820648, 0.30884185433387756, 0.3698306679725647, 0.17761604487895966, 0.14446012675762177, 0.074734628200531, 0.09708130359649658, 0.0913376435637474, 0.39822739362716675, 0.30103230476379395, 0.20916064083576202, 0.07392174005508423, 0.08868276327848434, 0.10422172397375107, 0.16624236106872559, 0.15386228263378143, 0.11413875967264175, 0.22978082299232483, 0.12851902842521667, 0.16302645206451416, 0.27763932943344116, 0.25243517756462097, 0.07775460928678513, 0.10246604681015015, 0.1359255313873291, 0.07833929359912872, 0.2048291116952896, 0.12934337556362152, 0.21029552817344666, 0.2233317345380783, 0.1197500005364418, 0.18770281970500946, 0.16189280152320862, 0.11935043334960938, 0.14593565464019775, 0.1593504697084427, 0.08215771615505219, 0.26507049798965454, 0.10409954935312271, 0.07754841446876526, 0.16716456413269043, 0.10871875286102295, 0.22222714126110077, 0.07396209239959717, 0.16850803792476654, 0.0742347314953804, 0.0761980190873146, 0.07110198587179184, 0.14880338311195374, 0.07036441564559937, 0.13111862540245056, 0.05544330179691315, 0.1582029163837433, 0.04848290979862213, 0.3559316098690033, 0.041957296431064606, 0.10849188268184662, 0.07176663726568222, 0.03630860149860382, 0.12448439747095108, 0.27498793601989746, 0.12997932732105255, 0.07321532070636749, 0.027250491082668304, 0.21643921732902527, 0.09673888981342316, 0.02558027021586895, 0.29914429783821106, 0.2311115264892578, 0.02427167072892189, 0.12947431206703186, 0.056769732385873795, 0.09244287759065628, 0.29904425144195557, 0.20762178301811218, 0.02744101732969284, 0.15919451415538788, 0.09758708626031876, 0.10232039541006088, 0.031100628897547722, 0.13704326748847961, 0.2921990752220154, 0.09167689830064774, 0.03532572090625763, 0.15190666913986206, 0.4299007058143616, 0.27948224544525146, 0.14332722127437592, 0.25102272629737854, 0.45430660247802734, 0.3450390696525574, 0.058869946748018265, 0.10183114558458328, 0.099106565117836, 0.13063882291316986, 0.07313012331724167, 0.17509689927101135, 0.18204954266548157, 0.18007689714431763, 0.0829254761338234, 0.1430082768201828, 0.13813789188861847, 0.10436209291219711, 0.1338302344083786, 0.077826127409935, 0.10262810438871384, 0.3741762042045593, 0.12217525392770767, 0.07739609479904175, 0.10052399337291718, 0.1369779407978058, 0.18183143436908722, 0.36079081892967224, 0.2718554735183716, 0.08275160938501358, 0.11231308430433273, 0.08283986896276474, 0.08713467419147491, 0.09793010354042053, 0.07992130517959595, 0.11522354185581207, 0.09244073927402496, 0.3522936701774597, 0.0739583671092987, 0.08688821643590927, 0.07687276601791382, 0.07293617725372314, 0.06905274093151093, 0.06881534308195114, 0.22175514698028564, 0.30291542410850525, 0.09339238703250885, 0.431972473859787, 0.13519445061683655, 0.057610563933849335, 0.05884281545877457, 0.20526830852031708, 0.059815436601638794, 0.2082294076681137, 0.18795131146907806, 0.3211553692817688, 0.08565796166658401, 0.1002572700381279, 0.16165851056575775, 0.10841458290815353, 0.05333688110113144, 0.05360821262001991, 0.12132713943719864, 0.3435204029083252, 0.20497167110443115, 0.050675034523010254, 0.1106494665145874, 0.17688339948654175, 0.10686730593442917, 0.23041540384292603, 0.12171164900064468, 0.3353862762451172, 0.1836438626050949, 0.09810177981853485, 0.10267514735460281, 0.08317599445581436, 0.13587267696857452, 0.182620570063591, 0.09097925573587418, 0.05727706477046013, 0.2911340892314911, 0.15175846219062805, 0.24571962654590607, 0.05855178460478783, 0.13021384179592133, 0.2937946021556854, 0.06080431863665581, 0.20454593002796173, 0.06175113096833229, 0.1188066229224205, 0.15285219252109528, 0.12068431079387665, 0.05995151773095131, 0.1620590090751648, 0.14123986661434174, 0.12052682042121887, 0.0535072386264801, 0.09891790896654129, 0.25839611887931824, 0.2642230987548828, 0.1081620380282402, 0.2978699803352356, 0.05236285179853439, 0.05355384573340416, 0.3752933442592621, 0.22053329646587372, 0.21790339052677155, 0.09136998653411865, 0.05667954310774803, 0.11172198504209518, 0.05990726500749588, 0.16997908055782318, 0.05854771286249161, 0.21115902066230774, 0.12343963235616684, 0.11669499427080154, 0.1937297284603119, 0.12904827296733856, 0.10675667226314545, 0.0570000521838665, 0.22458139061927795, 0.10523892194032669, 0.158469095826149, 0.12529680132865906, 0.054580431431531906, 0.10738350450992584, 0.20912952721118927, 0.11660107970237732, 0.20251470804214478, 0.11484701931476593, 0.16561757028102875, 0.051460567861795425, 0.20417501032352448, 0.051271598786115646, 0.20722588896751404, 0.18883389234542847, 0.12394523620605469, 0.33718109130859375, 0.278859406709671, 0.14343656599521637, 0.14523130655288696, 0.32236629724502563, 0.16437602043151855, 0.154608815908432, 0.10655941814184189, 0.07209216803312302, 0.0728897675871849, 0.11524711549282074, 0.18832433223724365, 0.1242356151342392, 0.07399874925613403, 0.07548678666353226, 0.0718855932354927, 0.20545096695423126, 0.13732829689979553, 0.0678989589214325, 0.12942597270011902, 0.2897724211215973, 0.18952913582324982, 0.10145868360996246, 0.3957618176937103, 0.061794571578502655, 0.16976049542427063, 0.26140645146369934, 0.26541557908058167, 0.15497691929340363, 0.062222547829151154, 0.0622309073805809, 0.14148396253585815, 0.060730867087841034, 0.21950355172157288, 0.1134934201836586, 0.0581829808652401, 0.05716581642627716, 0.11464326083660126, 0.132744699716568, 0.22636868059635162, 0.18649275600910187, 0.04935624450445175, 0.19742675125598907, 0.09027405083179474, 0.20953643321990967, 0.10635837912559509, 0.11818991601467133, 0.25185856223106384, 0.11982651799917221, 0.23532801866531372, 0.09670804440975189, 0.04797101393342018, 0.12592880427837372, 0.04727298021316528, 0.09940063953399658, 0.045889247208833694, 0.25812309980392456, 0.1942673623561859, 0.23431208729743958, 0.38898563385009766, 0.19465428590774536, 0.12650902569293976, 0.10414660722017288, 0.14238132536411285, 0.12212240695953369, 0.05306611955165863, 0.053190749138593674, 0.30408212542533875, 0.19016842544078827, 0.05507699027657509, 0.05562920868396759, 0.05598347261548042, 0.19234660267829895, 0.05547473952174187, 0.2300412356853485, 0.05496291071176529, 0.12067440897226334, 0.12564851343631744, 0.1278182566165924, 0.1208571195602417, 0.12363358587026596, 0.3432275652885437, 0.17200221121311188, 0.051102135330438614, 0.1167159453034401, 0.3344157040119171, 0.13642561435699463, 0.3379454016685486, 0.12206166982650757, 0.12704572081565857, 0.058717526495456696, 0.13178160786628723, 0.0962015837430954, 0.060607925057411194, 0.06018790975213051, 0.19754695892333984, 0.11848011612892151, 0.13481730222702026, 0.12455256283283234, 0.1870107501745224, 0.17589499056339264, 0.10108784586191177, 0.055729400366544724, 0.21279276907444, 0.05513416603207588, 0.05472579970955849, 0.19313345849514008, 0.12427136301994324, 0.2183653861284256, 0.20767800509929657, 0.19840674102306366, 0.11537213623523712, 0.1374652087688446, 0.0901080071926117, 0.13378667831420898, 0.3023550510406494, 0.1192326620221138, 0.11392408609390259, 0.1339072436094284, 0.05469329655170441, 0.1277504712343216, 0.053873565047979355, 0.13144145905971527, 0.14099706709384918, 0.05096490681171417, 0.12643468379974365, 0.115010567009449, 0.04726052284240723, 0.1352439969778061, 0.1172962561249733, 0.3854384124279022, 0.041792791336774826, 0.041349027305841446, 0.04057030752301216, 0.039513882249593735, 0.03846454247832298, 0.03723869100213051, 0.035922229290008545, 0.03446507453918457, 0.09538325667381287, 0.031655244529247284, 0.03023338131606579, 0.11906160414218903, 0.027703728526830673, 0.10662729293107986, 0.025125257670879364, 0.023929402232170105, 0.22424310445785522, 0.4546125829219818, 0.02268575318157673, 0.10891707241535187, 0.022995250299572945, 0.29540854692459106, 0.08330497145652771, 0.09681685268878937, 0.2917165160179138, 0.1302584409713745, 0.09102991223335266, 0.026077862828969955, 0.231967031955719, 0.4103897213935852, 0.10715137422084808, 0.11534331738948822, 0.031499430537223816, 0.20749910175800323, 0.2326095998287201, 0.11628396809101105, 0.37011685967445374, 0.11592019349336624, 0.23363818228244781, 0.24595098197460175, 0.12587858736515045, 0.11985985934734344, 0.3083771765232086, 0.11399498581886292, 0.10310449451208115, 0.11242601275444031, 0.2046169489622116, 0.06228530779480934, 0.10296572744846344, 0.19260337948799133, 0.06583355367183685, 0.0919046625494957, 0.2614809274673462, 0.18613378703594208, 0.0697651356458664, 0.07019880414009094, 0.07082954794168472, 0.11677411198616028, 0.18796783685684204, 0.13177423179149628, 0.13798043131828308, 0.13890451192855835, 0.12854395806789398, 0.18570135533809662, 0.06502345949411392, 0.12142699956893921, 0.13679124414920807, 0.15601621568202972, 0.060423754155635834, 0.11581259965896606, 0.05754506215453148, 0.05615498498082161, 0.054320987313985825, 0.052488770335912704, 0.21304544806480408, 0.19338113069534302, 0.4737599492073059, 0.3435741364955902, 0.050807319581508636, 0.052182286977767944, 0.12463045120239258, 0.1153784766793251, 0.3129291534423828, 0.1977902054786682, 0.12453032284975052, 0.13973474502563477, 0.19691622257232666, 0.06109033152461052, 0.06195239722728729, 0.19216716289520264, 0.06217677518725395, 0.19511975347995758, 0.2964513897895813, 0.18685373663902283, 0.19088509678840637, 0.1267087608575821, 0.12202112376689911, 0.1267532855272293, 0.19961075484752655, 0.0699637308716774, 0.14900292456150055, 0.07002979516983032, 0.06939750164747238, 0.12780004739761353, 0.06718983501195908, 0.13087740540504456, 0.12240474671125412, 0.15270569920539856, 0.35298478603363037, 0.061013881117105484, 0.11307787150144577, 0.2752940356731415, 0.1622570902109146, 0.2959211766719818, 0.06243521720170975, 0.18583935499191284, 0.09090151637792587, 0.06624401360750198, 0.19583429396152496, 0.19779621064662933, 0.11818938702344894, 0.1327180117368698, 0.19579939544200897, 0.06931644678115845, 0.15247103571891785, 0.12382686883211136, 0.09832023829221725, 0.13654395937919617, 0.15616337954998016, 0.0678829774260521, 0.21397024393081665, 0.22551855444908142, 0.11238458007574081, 0.06581615656614304, 0.12371686100959778, 0.14237575232982635, 0.134150430560112, 0.11831460893154144, 0.1426354944705963, 0.1257176697254181, 0.055432651191949844, 0.24877281486988068, 0.1355762630701065, 0.30801260471343994, 0.13502471148967743, 0.10697495192289352, 0.11488880962133408, 0.12945556640625, 0.36244499683380127, 0.10277266055345535, 0.11383537948131561, 0.050551868975162506, 0.0829520970582962, 0.21250124275684357, 0.11233405023813248, 0.1296250820159912, 0.049857720732688904, 0.12829352915287018, 0.048648420721292496, 0.11992109566926956, 0.22242730855941772, 0.09606867283582687, 0.2451072633266449, 0.04571930319070816, 0.20682168006896973, 0.12308774143457413, 0.21136799454689026, 0.10399007052183151, 0.12577484548091888, 0.12929952144622803, 0.11325033754110336, 0.1705019772052765, 0.12779907882213593, 0.11524798721075058, 0.04441298916935921, 0.1159021258354187, 0.21420376002788544, 0.10880839824676514, 0.19789573550224304, 0.11055154353380203, 0.21548032760620117, 0.11313635110855103, 0.2008797973394394, 0.04405593127012253, 0.23065340518951416, 0.11354202777147293, 0.04492693021893501, 0.3428490161895752, 0.21989737451076508, 0.11187858879566193, 0.10501521080732346, 0.12384017556905746, 0.12294570356607437, 0.08941571414470673, 0.050405703485012054, 0.10869581252336502, 0.04994604364037514, 0.10324954241514206, 0.12526629865169525, 0.10709547996520996, 0.1312706470489502, 0.046265099197626114, 0.0453142412006855, 0.11916106194257736, 0.23539811372756958, 0.22344574332237244, 0.11435988545417786, 0.10929550975561142, 0.19771349430084229, 0.04110066220164299, 0.04090898483991623, 0.0403662845492363, 0.22129274904727936, 0.3728848695755005, 0.37451061606407166, 0.11125797778367996, 0.12693509459495544, 0.10763303935527802, 0.044946786016225815, 0.20809566974639893, 0.1166386678814888, 0.12571102380752563, 0.33393383026123047, 0.11671195924282074, 0.04974253848195076, 0.11715830862522125, 0.10084168612957001, 0.23036782443523407, 0.05221032723784447, 0.11917292326688766, 0.11543743312358856, 0.10801287740468979, 0.10963208228349686, 0.11232730746269226, 0.19764089584350586, 0.11795235425233841, 0.22098128497600555, 0.11124604195356369, 0.05036424100399017, 0.0504557304084301, 0.049288664013147354, 0.19099269807338715, 0.04824588820338249, 0.04717410355806351, 0.046229079365730286, 0.045014698058366776, 0.20747290551662445, 0.23699726164340973, 0.3321852385997772, 0.04310859367251396, 0.20544111728668213, 0.10324445366859436, 0.04439517855644226, 0.10831477493047714, 0.34389781951904297, 0.045212943106889725, 0.04581162706017494, 0.10989584028720856, 0.04622350260615349, 0.10615606606006622, 0.11291619390249252, 0.353778213262558, 0.046151671558618546, 0.11646080017089844, 0.10107993334531784, 0.14027969539165497, 0.19484874606132507, 0.19896747171878815, 0.23096047341823578, 0.09799370914697647, 0.11074306815862656, 0.04853513091802597, 0.1106172651052475, 0.04893043264746666, 0.3536098003387451, 0.048713769763708115, 0.10396593064069748, 0.049292776733636856, 0.1992976814508438, 0.10691794753074646, 0.11933204531669617, 0.048920392990112305, 0.11044143885374069, 0.048364005982875824, 0.047447118908166885, 0.21619747579097748, 0.11961109191179276, 0.1152053251862526, 0.1864418238401413, 0.22121469676494598, 0.11301112920045853, 0.20559509098529816, 0.04459923505783081, 0.23007261753082275, 0.22877152264118195, 0.13012269139289856, 0.09564489871263504, 0.21564066410064697, 0.04658839851617813, 0.17772725224494934, 0.11685425043106079, 0.1221759095788002, 0.047702088952064514, 0.12191998213529587, 0.047777701169252396, 0.11057973653078079, 0.19551610946655273, 0.04612930119037628, 0.1129206046462059, 0.04479646310210228, 0.21328209340572357, 0.19760635495185852, 0.23211896419525146, 0.2047581821680069, 0.04449894279241562, 0.12655435502529144, 0.1002146527171135, 0.18777157366275787, 0.11458127945661545, 0.1914704591035843, 0.04545833170413971, 0.08795591443777084, 0.3595406413078308, 0.04643053561449051, 0.20386072993278503, 0.04817536473274231, 0.048736874014139175, 0.09715791046619415, 0.09894018620252609, 0.048388928174972534, 0.21420107781887054, 0.10724387317895889, 0.12135479599237442, 0.0857180580496788, 0.04734443500638008, 0.10576660931110382, 0.30218973755836487, 0.04623778164386749, 0.27091526985168457, 0.04701242595911026, 0.04661746323108673, 0.18376244604587555, 0.10522083938121796, 0.11853263527154922, 0.045940544456243515, 0.10518632084131241, 0.3156442642211914, 0.21719880402088165, 0.046062495559453964, 0.20183303952217102, 0.17406117916107178, 0.2518014907836914, 0.0973849892616272, 0.18228194117546082, 0.09978679567575455, 0.13822486996650696, 0.10599132627248764, 0.2164987027645111, 0.09391797333955765, 0.19735869765281677, 0.13808129727840424, 0.21001246571540833, 0.3438836932182312, 0.21679086983203888, 0.05886199325323105, 0.11363442242145538, 0.20927326381206512, 0.11396299302577972, 0.06306584179401398, 0.12643277645111084, 0.06352649629116058, 0.06407835334539413, 0.1280168741941452, 0.15757568180561066, 0.11561691761016846, 0.14601345360279083, 0.21111047267913818, 0.06018131226301193, 0.20159608125686646, 0.16365765035152435, 0.1104719340801239, 0.1298150271177292, 0.058138247579336166, 0.19715425372123718, 0.11690811067819595, 0.23703943192958832, 0.056607671082019806, 0.08939334005117416, 0.10345672816038132, 0.13349634408950806, 0.19883127510547638, 0.1353975385427475, 0.11219041794538498, 0.25714707374572754, 0.10895708203315735, 0.18393322825431824, 0.10604973137378693, 0.11409279704093933, 0.12406879663467407, 0.3018401265144348, 0.05174364522099495, 0.10606058686971664, 0.12159735709428787, 0.19813553988933563, 0.052638940513134, 0.14926201105117798, 0.11597267538309097, 0.12372971326112747, 0.05059308558702469, 0.09682503342628479, 0.049397315829992294, 0.10481724143028259, 0.12367827445268631, 0.04588519409298897, 0.12255634367465973, 0.20879743993282318, 0.10684468597173691, 0.2584434151649475, 0.20258164405822754, 0.2357504963874817, 0.12210849672555923, 0.042396288365125656, 0.0420709028840065, 0.11048966646194458, 0.1928083598613739, 0.37856000661849976, 0.13167764246463776, 0.13163740932941437, 0.04364388808608055, 0.10025922954082489, 0.04401601478457451, 0.1113717257976532, 0.11371231079101562, 0.22674140334129333, 0.11934519559144974, 0.26027119159698486, 0.24676741659641266, 0.10581178218126297, 0.2289462834596634, 0.1281440556049347, 0.23390111327171326, 0.04612678289413452, 0.13007034361362457, 0.04669621214270592, 0.11464866250753403, 0.36850273609161377, 0.2104908525943756, 0.09939819574356079, 0.11774308979511261, 0.10520794987678528, 0.10475354641675949, 0.10950422286987305, 0.049972593784332275, 0.05005641281604767, 0.10869821161031723, 0.2127874791622162, 0.11512797325849533, 0.04827142879366875, 0.047531113028526306, 0.10178200900554657, 0.10837116092443466, 0.10088103264570236, 0.04424978792667389, 0.043406616896390915, 0.3414296507835388, 0.2248154580593109, 0.3600989282131195, 0.1951807588338852, 0.10857988148927689, 0.045813512057065964, 0.046469174325466156, 0.11859679967164993, 0.11424899101257324, 0.19488492608070374, 0.1308797001838684, 0.048097528517246246, 0.22559629380702972, 0.24066272377967834, 0.1091252937912941, 0.04857049137353897, 0.11984843760728836, 0.1215951144695282, 0.11433640867471695, 0.11581986397504807, 0.1099572479724884, 0.12062077969312668, 0.11681138724088669, 0.046131741255521774, 0.04553280398249626, 0.11852257698774338, 0.20121514797210693, 0.042824093252420425, 0.10536529123783112, 0.11508353054523468, 0.10633144527673721, 0.12060030549764633, 0.11061903089284897, 0.11070660501718521, 0.11002849787473679, 0.03705502301454544, 0.19396694004535675, 0.1033109501004219, 0.08803143352270126, 0.5682333707809448, 0.10810613632202148, 0.1147940382361412, 0.105473093688488, 0.24092483520507812, 0.03907495737075806, 0.10956508666276932, 0.20964515209197998, 0.10376486927270889, 0.12295089662075043, 0.041822098195552826, 0.0420357808470726, 0.22002530097961426, 0.10544518381357193, 0.0425809770822525, 0.13010752201080322, 0.31947261095046997, 0.04270389676094055, 0.04289115592837334, 0.11354944109916687, 0.19982850551605225, 0.09223470091819763, 0.12558889389038086, 0.043665558099746704, 0.1988980770111084, 0.1905960887670517, 0.18314801156520844, 0.11710146069526672, 0.12872834503650665, 0.24800051748752594, 0.04572600498795509, 0.045704931020736694, 0.04601637274026871, 0.21258129179477692, 0.09749513864517212, 0.04568861424922943, 0.04547921568155289, 0.0926394835114479, 0.08959373086690903, 0.11121568828821182, 0.09367197751998901, 0.17355434596538544, 0.22296100854873657, 0.4129805266857147, 0.20709414780139923, 0.13187876343727112, 0.12097378075122833, 0.11649741977453232, 0.35094431042671204, 0.10438662022352219, 0.18505440652370453, 0.20080684125423431, 0.2315746396780014, 0.0523659884929657, 0.05325351282954216, 0.11527115851640701, 0.1007838025689125, 0.05571351572871208, 0.21559526026248932, 0.17851532995700836, 0.09837295114994049, 0.12303169816732407, 0.056306853890419006, 0.0564628466963768, 0.3159829378128052, 0.18134886026382446, 0.23541678488254547, 0.190791517496109, 0.19892533123493195, 0.2626856565475464, 0.21020855009555817, 0.10830621421337128, 0.06436926126480103, 0.14116214215755463, 0.0671393945813179, 0.0678008645772934, 0.16926456987857819, 0.10777736455202103, 0.2780804932117462, 0.091873399913311, 0.1971362829208374, 0.10182606428861618, 0.06920026987791061, 0.11291280388832092, 0.06947159022092819, 0.13505423069000244, 0.21690262854099274, 0.1374068558216095, 0.06709190458059311, 0.20641343295574188, 0.12437900900840759, 0.14649178087711334, 0.06402792036533356, 0.06312472373247147, 0.19047585129737854, 0.061130769550800323, 0.1257653832435608, 0.12317753583192825, 0.1063561663031578, 0.05595200136303902, 0.11101440340280533, 0.18472711741924286, 0.052367065101861954, 0.12089667469263077, 0.3235889673233032, 0.2301548272371292, 0.050051718950271606, 0.21594732999801636, 0.05000286549329758, 0.09408675879240036, 0.3425515294075012, 0.21712973713874817, 0.12590818107128143, 0.13662980496883392, 0.12865059077739716, 0.23146171867847443, 0.19336198270320892, 0.11494743078947067, 0.2170555591583252, 0.134392648935318, 0.1272670179605484, 0.12464652210474014, 0.23799000680446625, 0.20709413290023804, 0.1231534481048584, 0.11301784217357635, 0.05658959969878197, 0.22409576177597046, 0.22808347642421722, 0.05709841102361679, 0.10795418918132782, 0.20635509490966797, 0.12137886881828308, 0.10591953247785568, 0.12037980556488037, 0.056875135749578476, 0.12145791947841644, 0.11295310407876968, 0.11553018540143967, 0.18739254772663116, 0.10992997884750366, 0.11799722164869308, 0.11463651806116104, 0.11400453746318817, 0.10723508894443512, 0.050804585218429565, 0.3343569338321686, 0.12459234148263931, 0.10456359386444092, 0.21012791991233826, 0.04948675259947777, 0.12357060611248016, 0.10472221672534943, 0.21589511632919312, 0.04848916828632355, 0.048234663903713226, 0.04775558412075043, 0.11727254092693329, 0.11421379446983337, 0.33728742599487305, 0.10723218321800232, 0.11838226765394211, 0.11085867136716843, 0.12401200830936432, 0.31989291310310364, 0.04558191075921059, 0.11618052423000336, 0.12477867305278778, 0.0459749735891819, 0.1072985976934433, 0.04541918262839317, 0.10851869732141495, 0.11255873739719391, 0.1093725860118866, 0.3373931050300598, 0.04364147037267685, 0.04379379376769066, 0.10884498059749603, 0.20997005701065063, 0.2132662981748581, 0.04343569278717041, 0.10778338462114334, 0.043332427740097046, 0.22546182572841644, 0.12030523270368576, 0.22820276021957397, 0.043367739766836166, 0.11388008296489716, 0.04314295947551727, 0.3612447679042816, 0.10788767784833908, 0.04376932978630066, 0.11350802332162857, 0.1166062280535698, 0.043781038373708725, 0.11002396792173386, 0.04312282055616379, 0.10393010079860687, 0.11961302906274796, 0.20248986780643463, 0.11195261031389236, 0.20438112318515778, 0.20671895146369934, 0.11878640204668045, 0.041300926357507706, 0.04124247282743454, 0.11396899074316025, 0.0407230369746685, 0.11436547338962555, 0.10372097790241241, 0.10343129187822342, 0.038560688495635986, 0.038139455020427704, 0.3611283004283905, 0.2347612977027893, 0.10729292035102844, 0.35741496086120605, 0.03895658627152443, 0.2295229732990265, 0.38129234313964844, 0.21533402800559998, 0.04381564259529114, 0.21698899567127228, 0.11352855712175369, 0.11812490969896317, 0.10919693112373352, 0.20917664468288422, 0.05096559226512909, 0.11855559051036835, 0.1104726642370224, 0.05247916653752327, 0.20719902217388153, 0.052919670939445496, 0.052875813096761703, 0.12396601587533951, 0.05216151103377342, 0.3151409327983856, 0.11416186392307281, 0.2078646719455719, 0.10876594483852386, 0.1144803836941719, 0.05248595029115677, 0.12179181724786758, 0.052073150873184204, 0.21955884993076324, 0.10576831549406052, 0.10609441995620728, 0.2036726176738739, 0.2028060257434845, 0.11444579809904099, 0.12266426533460617, 0.208533376455307, 0.31933343410491943, 0.05133337900042534, 0.11553268879652023, 0.3130137622356415, 0.19447748363018036, 0.2035531997680664, 0.11911020427942276, 0.05683668702840805, 0.11879704892635345, 0.10519228130578995, 0.058271992951631546, 0.11781640350818634, 0.19935211539268494, 0.19209320843219757, 0.12769873440265656, 0.12170154601335526, 0.05873577669262886, 0.2858911454677582, 0.11162237823009491, 0.2014872431755066, 0.11135631054639816, 0.05975988134741783, 0.12455731630325317, 0.20074908435344696, 0.1158595159649849, 0.11308538913726807, 0.05925695598125458, 0.2031717598438263, 0.05861875042319298, 0.11742707341909409, 0.11869367957115173, 0.05680355429649353, 0.11678526550531387, 0.05523871257901192, 0.21141529083251953, 0.22202330827713013, 0.21906614303588867, 0.05293040722608566, 0.19854286313056946, 0.10778185725212097, 0.12069358676671982, 0.05174611881375313, 0.05135885253548622, 0.12181965261697769, 0.049807071685791016, 0.10468213260173798, 0.11388879269361496, 0.047113414853811264, 0.11651987582445145, 0.04511861503124237, 0.0440034344792366, 0.22532208263874054, 0.11932490020990372, 0.041337862610816956, 0.11256849765777588, 0.11093879491090775, 0.12004760652780533, 0.11096446961164474, 0.11291686445474625, 0.036402057856321335, 0.23062069714069366, 0.2222648561000824, 0.10440030694007874, 0.2313688099384308, 0.21940942108631134, 0.03512745350599289, 0.035266876220703125, 0.035237230360507965, 0.111151322722435, 0.2169470638036728, 0.11088499426841736, 0.03497011587023735, 0.10344884544610977, 0.21426893770694733, 0.10421718657016754, 0.21057473123073578, 0.1037750169634819, 0.1197497546672821, 0.03526855632662773, 0.10940506309270859, 0.10290244221687317, 0.22900891304016113, 0.035133592784404755, 0.1050742045044899, 0.11459843069314957, 0.107334204018116, 0.21019300818443298, 0.22250346839427948, 0.10737968981266022, 0.035453591495752335, 0.0994013249874115, 0.19319875538349152, 0.0357196219265461, 0.10273364186286926, 0.20381073653697968, 0.03626464307308197, 0.036259837448596954, 0.036296334117650986, 0.12420587241649628, 0.5603444576263428, 0.20927128195762634, 0.11895117163658142, 0.23275764286518097, 0.040337059646844864, 0.21153458952903748, 0.24051377177238464, 0.10190688818693161, 0.11771959811449051, 0.18464305996894836, 0.10518162697553635, 0.047531381249427795, 0.12054313719272614, 0.23393386602401733, 0.09705331176519394, 0.04927370697259903, 0.049635149538517, 0.04928182438015938, 0.048983294516801834, 0.11020812392234802, 0.23716242611408234, 0.04814048856496811, 0.04747942090034485, 0.11893918365240097, 0.1064186841249466, 0.09808102995157242, 0.10159086436033249, 0.11659341305494308, 0.04356670379638672, 0.20861849188804626, 0.042489100247621536, 0.3276323080062866, 0.20006506145000458, 0.115608349442482, 0.10747440904378891, 0.04279550164937973, 0.3678221106529236, 0.2358464151620865, 0.04426303505897522, 0.31537890434265137, 0.04580036923289299, 0.11830335110425949, 0.12484654039144516, 0.11971133947372437, 0.04848296195268631, 0.1298869401216507, 0.04860628396272659, 0.04843318089842796, 0.1205424889922142, 0.22928021848201752, 0.10379911214113235, 0.12366262078285217, 0.04677688702940941, 0.10733141005039215, 0.045802559703588486, 0.20121754705905914, 0.11047333478927612, 0.1060953363776207, 0.11810877174139023, 0.1076226681470871, 0.35506391525268555, 0.10587982088327408, 0.04344027116894722, 0.11202115565538406, 0.20984162390232086, 0.12235665321350098, 0.12115432322025299, 0.10322646051645279, 0.22308103740215302, 0.04349631816148758, 0.11947070807218552, 0.21045032143592834, 0.33933112025260925, 0.21997211873531342, 0.2244490683078766, 0.10789521783590317, 0.04688050225377083, 0.11218874901533127, 0.04785120114684105, 0.04807797446846962, 0.2119257003068924, 0.0482032373547554, 0.04819954186677933, 0.047934480011463165, 0.047470949590206146, 0.20232611894607544, 0.1023918092250824, 0.10339119285345078, 0.1142236664891243, 0.04532938450574875, 0.044730257242918015, 0.22519540786743164, 0.11172930151224136, 0.215912863612175, 0.11170618236064911, 0.2063046395778656, 0.11108710616827011, 0.042661432176828384, 0.34063583612442017, 0.20178678631782532, 0.34330421686172485, 0.04479483142495155, 0.04583842679858208, 0.21335913240909576, 0.047256987541913986, 0.22884604334831238, 0.1117369756102562, 0.11290491372346878, 0.19965024292469025, 0.05024818703532219, 0.11552001535892487, 0.12410280853509903, 0.050836220383644104, 0.31746917963027954, 0.21319134533405304, 0.11140201985836029, 0.052168380469083786, 0.052449069917201996, 0.05238283425569534, 0.0520966500043869, 0.11135600507259369, 0.21616517007350922, 0.10863446444272995, 0.05061422288417816, 0.05014358088374138, 0.11344737559556961, 0.10520485043525696, 0.10990085452795029, 0.19702905416488647, 0.11869431287050247, 0.12146154791116714, 0.04589119553565979, 0.11620303243398666, 0.20696072280406952, 0.04424602910876274, 0.0437697134912014, 0.10353250056505203, 0.1088070422410965, 0.5111242532730103, 0.19854022562503815, 0.10926561802625656, 0.10266397893428802, 0.11423636227846146, 0.044735051691532135, 0.10848895460367203, 0.11030108481645584, 0.3458448648452759, 0.11344490945339203, 0.11615690588951111, 0.12207645177841187, 0.11633636802434921, 0.04696446657180786, 0.04689275845885277, 0.046630095690488815, 0.1183437630534172, 0.10646159201860428, 0.1182839423418045, 0.1071518212556839, 0.12401673942804337, 0.11796983331441879, 0.04306244105100632, 0.10919708758592606, 0.2195836901664734, 0.11210951209068298, 0.11174766719341278, 0.10753372311592102, 0.21958188712596893, 0.10701925307512283, 0.11650565266609192, 0.0392938069999218, 0.2163729965686798, 0.22450381517410278, 0.11434710025787354]\n",
            "Val loss 0.1327867587668854\n",
            "Val auc roc 0.5\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff39186733614994b7eb0e5ca881d73e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1329\n",
            "Train Losses : [0.21477800607681274, 0.1072985827922821, 0.10957749933004379, 0.1119387075304985, 0.03948567062616348, 0.10542070120573044, 0.21168315410614014, 0.22716198861598969, 0.11330626159906387, 0.2185228317975998, 0.03996890410780907, 0.04018814116716385, 0.10772882401943207, 0.10773471742868423, 0.21538770198822021, 0.11006499081850052, 0.2218809723854065, 0.10755175352096558, 0.04053906723856926, 0.20766842365264893, 0.04086746647953987, 0.04077672213315964, 0.0406230092048645, 0.1263592690229416, 0.040197234600782394, 0.10799195617437363, 0.09919296950101852, 0.03860463947057724, 0.03807441145181656, 0.10139874368906021, 0.03678281977772713, 0.20429930090904236, 0.09176584333181381, 0.03525260463356972, 0.11544864624738693, 0.10531339049339294, 0.03363688290119171, 0.10803818702697754, 0.10882090777158737, 0.10206501185894012, 0.23298360407352448, 0.25670310854911804, 0.2474271059036255, 0.1089058518409729, 0.11067482084035873, 0.10362409055233002, 0.11002379655838013, 0.11256524920463562, 0.11141838878393173, 0.10179657489061356, 0.2436152696609497, 0.10200982540845871, 0.032233547419309616, 0.03224164992570877, 0.10975315421819687, 0.10807718336582184, 0.1115516722202301, 0.10472138971090317, 0.10216369479894638, 0.2377002239227295, 0.10259773582220078, 0.23776952922344208, 0.1070842370390892, 0.03153550624847412, 0.10686568170785904, 0.03162626549601555, 0.11555295437574387, 0.03124312125146389, 0.031018942594528198, 0.11385517567396164, 0.030259139835834503, 0.029875025153160095, 0.029507506638765335, 0.42537444829940796, 0.10594535619020462, 0.11078229546546936, 0.10641566663980484, 0.10343483835458755, 0.4068884253501892, 0.39395445585250854, 0.03113676607608795, 0.03222277760505676, 0.10342506319284439, 0.22193433344364166, 0.21473558247089386, 0.03560713678598404, 0.03630155697464943, 0.03692092001438141, 0.0373290479183197, 0.03747635334730148, 0.21739070117473602, 0.03784521296620369, 0.03794315084815025, 0.10614503920078278, 0.11385323107242584, 0.0375511460006237, 0.22894515097141266, 0.23043525218963623, 0.10443106293678284, 0.10924890637397766, 0.10413918644189835, 0.10901645570993423, 0.10578504204750061, 0.03756108507514, 0.2165500819683075, 0.10256346315145493, 0.03717958182096481, 0.03702728822827339, 0.11389215290546417, 0.11341214925050735, 0.03601561859250069, 0.21059410274028778, 0.2161417305469513, 0.035356659442186356, 0.03534359112381935, 0.10121570527553558, 0.03483300656080246, 0.03447336331009865, 0.21817056834697723, 0.10198410600423813, 0.033695340156555176, 0.22857198119163513, 0.21366576850414276, 0.033603958785533905, 0.11389165371656418, 0.03353170305490494, 0.10061653703451157, 0.208673357963562, 0.03342798724770546, 0.10866057127714157, 0.0332183763384819, 0.10406849533319473, 0.23842361569404602, 0.23116423189640045, 0.033006682991981506, 0.10928771644830704, 0.248727485537529, 0.21779710054397583, 0.12228472530841827, 0.10575211048126221, 0.2040284126996994, 0.11862780153751373, 0.10297021269798279, 0.035993847995996475, 0.19652998447418213, 0.10098810493946075, 0.3694862127304077, 0.1163545772433281, 0.03802037984132767, 0.1066180169582367, 0.107630655169487, 0.12074705213308334, 0.10530409961938858, 0.0398995541036129, 0.103606216609478, 0.37421971559524536, 0.04029741883277893, 0.04045635834336281, 0.0406520701944828, 0.09830810874700546, 0.0406782403588295, 0.04045911133289337, 0.04017731919884682, 0.09565421938896179, 0.10272477567195892, 0.115017369389534, 0.19242270290851593, 0.11556500941514969, 0.03776248171925545, 0.10370530188083649, 0.03704706206917763, 0.10144860297441483, 0.10539958626031876, 0.09975267201662064, 0.10175061970949173, 0.03458951413631439, 0.2190006524324417, 0.03376637026667595, 0.23772166669368744, 0.5793781876564026, 0.10474991798400879, 0.23452597856521606, 0.40387195348739624, 0.09765390306711197, 0.03922777995467186, 0.10692223161458969, 0.10110156983137131, 0.11063936352729797, 0.04335767403244972, 0.04345187544822693, 0.04402129352092743, 0.12501057982444763, 0.11486078053712845, 0.10602276772260666, 0.04373206943273544, 0.04335451498627663, 0.04309328272938728, 0.20999598503112793, 0.041963186115026474, 0.041556328535079956, 0.2084304541349411, 0.1082511693239212, 0.10605471581220627, 0.12426839768886566, 0.039904091507196426, 0.22637496888637543, 0.0391397587954998, 0.038897935301065445, 0.03836449980735779, 0.03785959258675575, 0.037304218858480453, 0.12328178435564041, 0.03596965968608856, 0.10402350127696991, 0.03459128364920616, 0.11400940269231796, 0.10576546937227249, 0.21367251873016357, 0.10198619216680527, 0.10405030101537704, 0.21393516659736633, 0.21406996250152588, 0.09352416545152664, 0.09766621142625809, 0.1198398694396019, 0.09973437339067459, 0.11958326399326324, 0.03147030249238014, 0.101151242852211, 0.24158714711666107, 0.031016189604997635, 0.23965071141719818, 0.12362556904554367, 0.09272172302007675, 0.11093030124902725, 0.031192196533083916, 0.10936923325061798, 0.23979851603507996, 0.10709857195615768, 0.10283210128545761, 0.3933538496494293, 0.23396684229373932, 0.20449380576610565, 0.4085192084312439, 0.09171272069215775, 0.21306782960891724, 0.03778432309627533, 0.22008448839187622, 0.11802959442138672, 0.2141772359609604, 0.21012766659259796, 0.04392029717564583, 0.1024727150797844, 0.1146622896194458, 0.04634908586740494, 0.11503956466913223, 0.04757295548915863, 0.047207340598106384, 0.23140260577201843, 0.04730553552508354, 0.11738403141498566, 0.21634931862354279, 0.11596845090389252, 0.13126635551452637, 0.047340814024209976, 0.046887051314115524, 0.4799203872680664, 0.1259552389383316, 0.047756463289260864, 0.10890177637338638, 0.09795988351106644, 0.21124733984470367, 0.22862312197685242, 0.12485318630933762, 0.20372088253498077, 0.0500028096139431, 0.1902584582567215, 0.05063783749938011, 0.11336388438940048, 0.05099000409245491, 0.09815365821123123, 0.18665185570716858, 0.20269615948200226, 0.0965333804488182, 0.11104906350374222, 0.13382981717586517, 0.05100817605853081, 0.11335843801498413, 0.11055624485015869, 0.19595609605312347, 0.107303187251091, 0.20699965953826904, 0.04930592700839043, 0.04918232187628746, 0.1307106465101242, 0.3264803886413574, 0.10098499059677124, 0.04880571365356445, 0.1961926370859146, 0.12782616913318634, 0.10507740825414658, 0.20830360054969788, 0.048772357404232025, 0.10458151996135712, 0.09905606508255005, 0.04843444377183914, 0.04814024269580841, 0.047622956335544586, 0.11035843938589096, 0.10944429039955139, 0.045941174030303955, 0.19816936552524567, 0.1829734444618225, 0.04433551803231239, 0.11461439728736877, 0.19590499997138977, 0.3555026650428772, 0.21486957371234894, 0.10910402983427048, 0.08929933607578278, 0.23599189519882202, 0.10320437699556351, 0.04644528031349182, 0.046226561069488525, 0.046288974583148956, 0.113992840051651, 0.11163330078125, 0.045375239104032516, 0.2054188847541809, 0.04481076821684837, 0.12292498350143433, 0.11566600948572159, 0.11682206392288208, 0.33282703161239624, 0.22072288393974304, 0.11529923230409622, 0.1098557561635971, 0.04429739713668823, 0.3034168779850006, 0.3107231855392456, 0.11219077557325363, 0.11736083775758743, 0.1238882765173912, 0.20325273275375366, 0.21839411556720734, 0.049517981708049774, 0.1065468117594719, 0.22895507514476776, 0.05102068558335304, 0.3223589360713959, 0.10752947628498077, 0.0530451200902462, 0.053465478122234344, 0.20029115676879883, 0.05394149571657181, 0.12080631405115128, 0.05395681411027908, 0.0538262315094471, 0.11740230023860931, 0.20731398463249207, 0.05285008251667023, 0.05231935530900955, 0.11349725723266602, 0.051362745463848114, 0.17822015285491943, 0.05030745267868042, 0.049968037754297256, 0.048889197409152985, 0.10752217471599579, 0.047295257449150085, 0.0465388149023056, 0.09905324131250381, 0.3549266457557678, 0.09387768059968948, 0.0443386510014534, 0.20807397365570068, 0.04363846778869629, 0.09881313890218735, 0.2034319043159485, 0.23344802856445312, 0.3188786804676056, 0.11988645046949387, 0.1880606859922409, 0.044618841260671616, 0.0989372730255127, 0.19131143391132355, 0.04566927254199982, 0.046198759227991104, 0.2179260551929474, 0.11404110491275787, 0.04642151668667793, 0.04635586217045784, 0.09197631478309631, 0.11953559517860413, 0.11971785873174667, 0.33908554911613464, 0.04622506722807884, 0.1201351135969162, 0.09586521238088608, 0.04554664343595505, 0.22888724505901337, 0.11489244550466537, 0.11226541548967361, 0.1970566064119339, 0.23104692995548248, 0.0455007366836071, 0.13726845383644104, 0.17686434090137482, 0.11596238613128662, 0.09570174664258957, 0.1112130805850029, 0.0949336513876915, 0.20753414928913116, 0.04568009078502655, 0.11730864644050598, 0.04516337066888809, 0.045063938945531845, 0.20274893939495087, 0.04420784115791321, 0.2483537644147873, 0.33622297644615173, 0.1865900754928589, 0.10461448132991791, 0.12032415717840195, 0.12503720819950104, 0.11380428820848465, 0.11728613078594208, 0.08058325946331024, 0.2166367918252945, 0.21900784969329834, 0.04672802984714508, 0.3305833041667938, 0.11226994544267654, 0.04842938855290413, 0.22978325188159943, 0.21403299272060394, 0.04942593351006508, 0.049681033939123154, 0.049823954701423645, 0.09479788690805435, 0.11484701931476593, 0.04971350356936455, 0.11737670004367828, 0.23767825961112976, 0.10616441071033478, 0.10548768192529678, 0.04882078617811203, 0.12234310060739517, 0.04812724143266678, 0.04749968275427818, 0.04645814746618271, 0.045520782470703125, 0.11267434805631638, 0.044102154672145844, 0.04329230263829231, 0.10925721377134323, 0.18844957649707794, 0.22708138823509216, 0.11831051856279373, 0.1050793007016182, 0.11483301222324371, 0.11079014837741852, 0.21465042233467102, 0.1895601898431778, 0.038815464824438095, 0.10864541679620743, 0.03845387324690819, 0.03836781159043312, 0.1239268109202385, 0.10310579836368561, 0.11764230579137802, 0.086990125477314, 0.24436768889427185, 0.1009470671415329, 0.03657368943095207, 0.09392399340867996, 0.09094228595495224, 0.035502463579177856, 0.11400945484638214, 0.21999326348304749, 0.10352107882499695, 0.09801075607538223, 0.11629863828420639, 0.26022517681121826, 0.141282320022583, 0.1897856891155243, 0.12273472547531128, 0.12555290758609772, 0.10999753326177597, 0.12413736432790756, 0.08599117398262024, 0.03434358909726143, 0.18731947243213654, 0.3694804608821869, 0.09230674058198929, 0.034692514687776566, 0.2373696267604828, 0.035642798990011215, 0.03584831953048706, 0.0361914336681366, 0.21313227713108063, 0.36923032999038696, 0.036953285336494446, 0.03755056858062744, 0.11981682479381561, 0.03856413811445236, 0.03842952102422714, 0.23304595053195953, 0.11378201842308044, 0.1093563660979271, 0.21005237102508545, 0.21691076457500458, 0.11547766625881195, 0.039735373109579086, 0.10599823296070099, 0.26157277822494507, 0.11501123756170273, 0.11472193151712418, 0.10084107518196106, 0.04066849872469902, 0.2036229968070984, 0.2303362935781479, 0.11267870664596558, 0.04095509275794029, 0.21212294697761536, 0.11390800774097443, 0.04123047739267349, 0.21191586554050446, 0.04147689417004585, 0.04165229573845863, 0.33941131830215454, 0.230091392993927, 0.0427381806075573, 0.042555227875709534, 0.10915098339319229, 0.10318541526794434, 0.12413422763347626, 0.13491301238536835, 0.04328295215964317, 0.042817212641239166, 0.04233285039663315, 0.09858668595552444, 0.22054430842399597, 0.37199917435646057, 0.10948320478200912, 0.21858607232570648, 0.04217465966939926, 0.11094358563423157, 0.04259192943572998, 0.09622719883918762, 0.25142329931259155, 0.09814479947090149, 0.09852103143930435, 0.04280757158994675, 0.12514489889144897, 0.20134277641773224, 0.10899814963340759, 0.09941641986370087, 0.042266201227903366, 0.2195555716753006, 0.20640315115451813, 0.09674729406833649, 0.04226921498775482, 0.13036906719207764, 0.042503077536821365, 0.09361936151981354, 0.10629750043153763, 0.041415974497795105, 0.04130876436829567, 0.11368335038423538, 0.11148136109113693, 0.36170700192451477, 0.10784901678562164, 0.14031873643398285, 0.12614914774894714, 0.039559975266456604, 0.10896480828523636, 0.03938153386116028, 0.09353195130825043, 0.3878651261329651, 0.11027059704065323, 0.0389762744307518, 0.23575341701507568, 0.23634544014930725, 0.03956408426165581, 0.12288209795951843, 0.03987712785601616, 0.10288307815790176, 0.22172820568084717, 0.5246437191963196, 0.10556401312351227, 0.10260984301567078, 0.042580101639032364, 0.11738230288028717, 0.2016811966896057, 0.2175770252943039, 0.10930986702442169, 0.10954427719116211, 0.10984989255666733, 0.11721578985452652, 0.046767376363277435, 0.10946989059448242, 0.1970764845609665, 0.09788287431001663, 0.1322484165430069, 0.11866813153028488, 0.13706763088703156, 0.10233039408922195, 0.04726628586649895, 0.04642832651734352, 0.10958495736122131, 0.21000966429710388, 0.13077440857887268, 0.3392673432826996, 0.355167955160141, 0.21764948964118958, 0.046685341745615005, 0.10703649371862411, 0.10466457903385162, 0.04834528639912605, 0.04855889081954956, 0.11007317900657654, 0.11603742837905884, 0.04841323569417, 0.1113889217376709, 0.047568537294864655, 0.047474462538957596, 0.20677319169044495, 0.127358540892601, 0.04615750163793564, 0.1076844185590744, 0.045132871717214584, 0.3285655081272125, 0.04457906261086464, 0.044985707849264145, 0.11489218473434448, 0.21943964064121246, 0.1977725476026535, 0.12874752283096313, 0.09740553051233292, 0.043936215341091156, 0.10606490075588226, 0.2070382535457611, 0.04336629807949066, 0.09891796857118607, 0.042770784348249435, 0.11295554041862488, 0.1084619089961052, 0.04191163182258606, 0.04145202785730362, 0.11021989583969116, 0.11463740468025208, 0.3270307183265686, 0.040225643664598465, 0.09233858436346054, 0.23618526756763458, 0.10646934062242508, 0.03974116966128349, 0.1128157302737236, 0.13123475015163422, 0.10468876361846924, 0.10127834230661392, 0.10687773674726486, 0.10620209574699402, 0.20608572661876678, 0.12799879908561707, 0.2176676094532013, 0.03840973228216171, 0.0984070673584938, 0.10856769979000092, 0.037873316556215286, 0.11061535775661469, 0.03729728236794472, 0.1052846610546112, 0.2393990159034729, 0.22456850111484528, 0.19760526716709137, 0.1820525974035263, 0.037237804383039474, 0.3664475381374359, 0.09591595083475113, 0.10853929072618484, 0.03936222195625305, 0.20749366283416748, 0.11890645325183868, 0.1235506683588028, 0.3576829731464386, 0.04139290004968643, 0.09737718105316162, 0.2507573366165161, 0.2282540202140808, 0.22254741191864014, 0.21734420955181122, 0.12171994149684906, 0.04572140425443649, 0.21856586635112762, 0.11513625830411911, 0.09537910670042038, 0.21798241138458252, 0.20809468626976013, 0.20879347622394562, 0.21030916273593903, 0.05078328028321266, 0.12682421505451202, 0.12575487792491913, 0.11969028413295746, 0.12332341074943542, 0.3254527449607849, 0.052764542400836945, 0.053235940635204315, 0.19218464195728302, 0.10500587522983551, 0.053625814616680145, 0.21010996401309967, 0.11162120848894119, 0.19377970695495605, 0.10885201394557953, 0.20896175503730774, 0.1278620958328247, 0.1359112411737442, 0.05465144291520119, 0.19819533824920654, 0.12196695059537888, 0.12463553249835968, 0.20861685276031494, 0.05417913571000099, 0.05402286350727081, 0.21432307362556458, 0.05373529717326164, 0.2007005661725998, 0.11170384287834167, 0.11823751032352448, 0.052894558757543564, 0.19864371418952942, 0.10476764291524887, 0.11483887583017349, 0.0518418624997139, 0.05177048221230507, 0.1905967891216278, 0.30982205271720886, 0.20120970904827118, 0.21331359446048737, 0.10468295216560364, 0.4500744640827179, 0.1900019347667694, 0.21499168872833252, 0.23127348721027374, 0.11308427900075912, 0.057503946125507355, 0.19761458039283752, 0.20209388434886932, 0.05988938733935356, 0.29862818121910095, 0.12210696190595627, 0.12879811227321625, 0.19682961702346802, 0.12034539133310318, 0.11965814977884293, 0.06463465839624405, 0.11711499094963074, 0.10909373313188553, 0.0650094747543335, 0.12220506370067596, 0.1249469444155693, 0.17858687043190002, 0.11060579866170883, 0.11469797044992447, 0.11540281027555466, 0.06273924559354782, 0.11931601166725159, 0.14430806040763855, 0.061130937188863754, 0.06032556667923927, 0.11141077429056168, 0.12486666440963745, 0.20253388583660126, 0.0920092836022377, 0.12075038254261017, 0.12911273539066315, 0.12409574538469315, 0.19839346408843994, 0.11411044001579285, 0.05391338840126991, 0.12372971326112747, 0.2108263522386551, 0.052559882402420044, 0.052231088280677795, 0.051498863846063614, 0.2150837630033493, 0.3257637619972229, 0.11048386991024017, 0.10296755284070969, 0.050008952617645264, 0.21073515713214874, 0.04995923116803169, 0.12271010875701904, 0.04923100024461746, 0.11243239790201187, 0.11172126978635788, 0.11333248019218445, 0.10797635465860367, 0.3469013571739197, 0.20255661010742188, 0.04758062958717346, 0.047482579946517944, 0.04738764464855194, 0.04724019020795822, 0.18254218995571136, 0.04660932719707489, 0.2143397033214569, 0.48148202896118164, 0.20512793958187103, 0.34554558992385864, 0.12454720586538315, 0.050110213458538055, 0.10713206231594086, 0.10372764617204666, 0.05177730321884155, 0.05207071825861931, 0.11475735902786255, 0.052401259541511536, 0.05230461433529854, 0.11781386286020279, 0.09702487289905548, 0.20150116086006165, 0.3245651423931122, 0.10881192982196808, 0.2968233525753021, 0.19153185188770294, 0.11742184311151505, 0.10698186606168747, 0.11408130079507828, 0.1171499714255333, 0.05454140529036522, 0.10863153636455536, 0.05484164506196976, 0.11457231640815735, 0.3359951674938202, 0.05449974909424782, 0.12443602830171585, 0.11771088093519211, 0.3107214570045471, 0.11221596598625183, 0.1119367703795433, 0.05505623295903206, 0.30648353695869446, 0.05545677989721298, 0.1140102818608284, 0.11889473348855972, 0.10038752853870392, 0.05582749471068382, 0.11527116596698761, 0.11730591952800751, 0.0554300956428051, 0.20905731618404388, 0.05444415286183357, 0.05400457605719566, 0.05359934642910957, 0.11531713604927063, 0.11873412877321243, 0.4496780335903168, 0.05225077643990517, 0.1228213757276535, 0.12487275153398514, 0.19200244545936584, 0.20681841671466827, 0.1308133602142334, 0.19270873069763184, 0.11281725764274597, 0.1960499882698059, 0.05308976396918297, 0.2390049248933792, 0.05346810817718506, 0.107820063829422, 0.10880061984062195, 0.11445936560630798, 0.17582830786705017, 0.05305175110697746, 0.05305011197924614, 0.22917714715003967, 0.3258662819862366, 0.2991470694541931, 0.2098313719034195, 0.05413047969341278, 0.05464418604969978, 0.1921926736831665, 0.1111699640750885, 0.05576721578836441, 0.20470689237117767, 0.05629509314894676, 0.1205112487077713, 0.056198760867118835, 0.1312192678451538, 0.10424000769853592, 0.209063321352005, 0.05529313534498215, 0.05511423200368881, 0.1041867807507515, 0.3126257658004761, 0.05429619923233986, 0.22083532810211182, 0.19924594461917877, 0.2031300663948059, 0.11568945646286011, 0.10743358731269836, 0.3030630350112915, 0.20032145082950592, 0.11350909620523453, 0.10866907984018326, 0.056593313813209534, 0.1133841946721077, 0.1153649389743805, 0.1200864166021347, 0.11647847294807434, 0.12350055575370789, 0.1911236196756363, 0.11787229031324387, 0.11088598519563675, 0.11520128697156906, 0.05568285286426544, 0.11580152809619904, 0.05502951145172119, 0.05439161881804466, 0.10801173746585846, 0.05324416607618332, 0.10806616395711899, 0.11054584383964539, 0.1120392456650734, 0.09216278791427612, 0.1856791377067566, 0.32205864787101746, 0.11738964915275574, 0.04973335564136505, 0.049404751509428024, 0.04909791424870491, 0.21200983226299286, 0.10681717097759247, 0.04835065081715584, 0.04796227812767029, 0.04750636965036392, 0.11434539407491684, 0.09949222207069397, 0.102714404463768, 0.11547689139842987, 0.2204868197441101, 0.10992832481861115, 0.04420522600412369, 0.11361487209796906, 0.22017890214920044, 0.1995750516653061, 0.11163438111543655, 0.04302786663174629, 0.5047345757484436, 0.1072017252445221, 0.09686567634344101, 0.04419003799557686, 0.21259170770645142, 0.044734321534633636, 0.04496346414089203, 0.3589533567428589, 0.1194658949971199, 0.045858386904001236, 0.21643735468387604, 0.20683760941028595, 0.2211383879184723, 0.3277311623096466, 0.04828867316246033, 0.12995412945747375, 0.04951227083802223, 0.46068525314331055, 0.19910259544849396, 0.1308639943599701, 0.22701241075992584, 0.12170178443193436, 0.05486857146024704, 0.12011906504631042, 0.10766492784023285, 0.05610436946153641, 0.30261823534965515, 0.19439281523227692, 0.20466077327728271, 0.11272956430912018, 0.11089561134576797, 0.20047658681869507, 0.06002373248338699, 0.06001278758049011, 0.21543055772781372, 0.2699768841266632, 0.06072769686579704, 0.20102019608020782, 0.10621045529842377, 0.06162311136722565, 0.30466461181640625, 0.06253645569086075, 0.10586458444595337, 0.1256919950246811, 0.11008715629577637, 0.29107895493507385, 0.11511524766683578, 0.063533253967762, 0.0633409321308136, 0.12444032728672028, 0.06298815459012985, 0.0634637326002121, 0.06234632432460785, 0.06191350147128105, 0.11370395869016647, 0.06032472476363182, 0.059748344123363495, 0.10840470343828201, 0.11197225004434586, 0.1813669502735138, 0.19486117362976074, 0.056303758174180984, 0.12592414021492004, 0.12347735464572906, 0.22281749546527863, 0.12350992113351822, 0.20112349092960358, 0.11555483937263489, 0.05354955792427063, 0.20163904130458832, 0.11013045907020569, 0.1263643354177475, 0.05229535326361656, 0.05186579003930092, 0.1205657422542572, 0.1946372240781784, 0.19242563843727112, 0.11716672778129578, 0.10147765278816223, 0.12040320038795471, 0.2073431760072708, 0.11942917853593826, 0.049372319132089615, 0.11255446821451187, 0.12463811784982681, 0.19978398084640503, 0.13384245336055756, 0.10746437311172485, 0.21307781338691711, 0.1151682436466217, 0.11437898129224777, 0.2211466282606125, 0.2072378695011139, 0.3144764006137848, 0.048068881034851074, 0.048392459750175476, 0.29361873865127563, 0.1087343618273735, 0.04950690641999245, 0.04966617003083229, 0.04967069998383522, 0.22175124287605286, 0.2072090059518814, 0.20007890462875366, 0.21379593014717102, 0.11087129265069962, 0.05095929279923439, 0.10650644451379776, 0.33622005581855774, 0.18506573140621185, 0.05214367061853409, 0.1219017431139946, 0.1255839616060257, 0.1171945184469223, 0.053005240857601166, 0.11309324949979782, 0.05280553176999092, 0.052587319165468216, 0.05220400542020798, 0.21376238763332367, 0.12062159180641174, 0.11803054809570312, 0.21095281839370728, 0.05075550451874733, 0.10744726657867432, 0.050075363367795944, 0.18810540437698364, 0.32320091128349304, 0.11884497851133347, 0.10751208662986755, 0.20347096025943756, 0.3308618664741516, 0.3302561938762665, 0.3304790258407593, 0.3255782127380371, 0.1079418733716011, 0.12099878489971161, 0.20009338855743408, 0.21599173545837402, 0.11111551523208618, 0.1940339207649231, 0.10178715735673904, 0.06069939583539963, 0.06136389449238777, 0.11884114891290665, 0.20850934088230133, 0.20376072824001312, 0.0626392513513565, 0.0628795474767685, 0.1866566687822342, 0.06291639059782028, 0.19065232574939728, 0.20188438892364502, 0.0631219819188118, 0.13538306951522827, 0.2060832977294922, 0.30648377537727356, 0.06352950632572174, 0.31338855624198914, 0.06409427523612976, 0.06433641910552979, 0.06451878696680069, 0.19282148778438568, 0.06451006233692169, 0.06442221999168396, 0.19593936204910278, 0.1306409388780594, 0.18928742408752441, 0.06364358216524124, 0.12572702765464783, 0.11556802690029144, 0.19092249870300293, 0.1991707980632782, 0.11513330042362213, 0.29001864790916443, 0.06286342442035675, 0.06283459067344666, 0.12248009443283081, 0.1244279220700264, 0.06230808049440384, 0.11430711299180984, 0.061424192041158676, 0.2069512903690338, 0.060631606727838516, 0.11048059910535812, 0.20029066503047943, 0.12583452463150024, 0.05910799279808998, 0.11919442564249039, 0.11035039275884628, 0.05777265876531601, 0.21267648041248322, 0.11694271862506866, 0.1220291331410408, 0.11039569228887558, 0.19873420894145966, 0.11231308430433273, 0.21227753162384033, 0.3164536952972412, 0.05481856316328049, 0.12171028554439545, 0.29978877305984497, 0.19461630284786224, 0.2930493950843811, 0.10747472196817398, 0.056898631155490875, 0.057492535561323166, 0.05752981826663017, 0.12122335284948349, 0.11989547312259674, 0.2011050134897232, 0.11447478830814362, 0.22208769619464874, 0.12236150354146957, 0.20214088261127472, 0.11613737791776657, 0.4195179343223572, 0.12379644811153412, 0.19514121115207672, 0.11926709860563278, 0.059961821883916855, 0.11654119938611984, 0.06048017740249634, 0.060634512454271317, 0.12213985621929169, 0.19908322393894196, 0.1967342644929886, 0.1273028701543808, 0.18955782055854797, 0.06029704213142395, 0.19309461116790771, 0.060281265527009964, 0.1141902282834053, 0.059993088245391846, 0.059637024998664856, 0.05930148810148239, 0.11700210720300674, 0.11722539365291595, 0.20324628055095673, 0.11493788659572601, 0.057204265147447586, 0.31269365549087524, 0.12193784862756729, 0.1940365880727768, 0.18349234759807587, 0.19920121133327484, 0.12168745696544647, 0.12344483286142349, 0.3162212669849396, 0.2977384328842163, 0.1972924768924713, 0.20488479733467102, 0.19276335835456848, 0.06013556569814682, 0.12840493023395538, 0.12096261233091354, 0.061260778456926346, 0.19192880392074585, 0.12181178480386734, 0.2037665843963623, 0.061935946345329285, 0.20447881519794464, 0.12187660485506058, 0.1968706101179123, 0.11909361928701401, 0.1307554841041565, 0.0625012144446373, 0.18652784824371338, 0.19341596961021423, 0.12340477108955383, 0.062311746180057526, 0.062132030725479126, 0.12652449309825897, 0.2842181324958801, 0.20616990327835083, 0.06176368147134781, 0.2948342561721802, 0.11748720705509186, 0.2850024104118347, 0.06263799220323563, 0.06303653120994568, 0.11686697602272034, 0.06314355134963989, 0.06316039711236954, 0.1194685697555542, 0.06265995651483536, 0.11642659455537796, 0.12121301889419556, 0.11408070474863052, 0.3015895187854767, 0.19804349541664124, 0.11638141423463821, 0.06079323589801788, 0.11464927345514297, 0.1986805498600006, 0.19629916548728943, 0.06044096127152443, 0.2066691517829895, 0.06017878279089928, 0.12133515626192093, 0.11759388446807861, 0.10935276746749878, 0.18127299845218658, 0.11619126796722412, 0.12688830494880676, 0.2039206624031067, 0.1207553967833519, 0.05835928022861481, 0.12063349783420563, 0.1975727081298828, 0.12283851206302643, 0.12025342136621475, 0.11470845341682434, 0.123277448117733, 0.18816706538200378, 0.20945389568805695, 0.10940670222043991, 0.10740318894386292, 0.2998940944671631, 0.11697645485401154, 0.20183877646923065, 0.21636845171451569, 0.11638291925191879, 0.11802889406681061, 0.19478437304496765, 0.19557692110538483, 0.057410482317209244, 0.11408451944589615, 0.19692151248455048, 0.057703327387571335, 0.2073291391134262, 0.11925411969423294, 0.19279497861862183, 0.1180596575140953, 0.2002374827861786, 0.19929753243923187, 0.18799851834774017, 0.058640480041503906, 0.10966045409440994, 0.058837663382291794, 0.18606220185756683, 0.12575797736644745, 0.05880599096417427, 0.05871789529919624, 0.058429766446352005, 0.1980106234550476, 0.20292547345161438, 0.20849208533763885, 0.19569632411003113, 0.19172032177448273, 0.057905130088329315, 0.20007866621017456, 0.05803118646144867, 0.20209276676177979, 0.12437885999679565, 0.19644151628017426, 0.11636997014284134, 0.11609861254692078, 0.1956769973039627, 0.05823736637830734, 0.12106259912252426, 0.12603414058685303, 0.11481697857379913, 0.1969662606716156, 0.05770174413919449, 0.19971850514411926, 0.20957651734352112, 0.12344396114349365, 0.11789818108081818, 0.0572027824819088, 0.31365785002708435, 0.0571051649749279, 0.1230473443865776, 0.11134973168373108, 0.2024899274110794, 0.05700116977095604, 0.19819146394729614, 0.11114964634180069, 0.11912349611520767, 0.20542454719543457, 0.05663174390792847, 0.20591962337493896, 0.11214474588632584, 0.121098592877388, 0.2020188868045807, 0.11578335613012314, 0.2045283019542694, 0.12476583570241928, 0.11378563940525055, 0.11498592048883438, 0.05592501536011696, 0.05579214170575142, 0.19477462768554688, 0.19147200882434845, 0.2035256177186966, 0.19941680133342743, 0.11278086155653, 0.309661865234375, 0.2047373354434967, 0.056150928139686584, 0.11093766987323761, 0.056544527411460876, 0.05660880729556084, 0.05662094056606293, 0.056461744010448456, 0.32928815484046936, 0.12235637754201889, 0.11444316059350967, 0.1910054087638855, 0.11911805719137192, 0.19846442341804504, 0.056223515421152115, 0.05607713386416435, 0.055932749062776566, 0.20350924134254456, 0.11689525842666626, 0.11047690361738205, 0.11499009281396866, 0.21118243038654327, 0.1143825426697731, 0.21002404391765594, 0.21203097701072693, 0.12042741477489471, 0.11709050089120865, 0.11381533741950989, 0.11987680941820145, 0.11463885754346848, 0.1952987015247345, 0.11997941136360168, 0.31641992926597595, 0.10941001027822495, 0.2929888963699341, 0.05462729558348656, 0.19605287909507751, 0.193895161151886, 0.11499502509832382, 0.19160902500152588, 0.1154727190732956, 0.05664366856217384, 0.19139358401298523, 0.19016291201114655, 0.1950695663690567, 0.057629067450761795, 0.20226670801639557, 0.058082059025764465, 0.11683378368616104, 0.05827433615922928, 0.20676833391189575, 0.20169706642627716, 0.11135249584913254, 0.12481055408716202, 0.1210055947303772, 0.2017536759376526, 0.12210624665021896, 0.19286811351776123, 0.19709882140159607, 0.18914695084095, 0.058636851608753204, 0.05872005596756935, 0.11913832277059555, 0.11081080883741379, 0.11425245553255081, 0.05820725858211517, 0.11479107290506363, 0.11181264370679855, 0.2898215651512146, 0.1155148446559906, 0.4218735992908478, 0.19366586208343506, 0.05826583504676819, 0.0586356595158577, 0.058797676116228104, 0.05902077257633209, 0.05888790637254715, 0.11709502339363098, 0.11697404831647873, 0.12222455441951752, 0.0579151026904583, 0.29440081119537354, 0.11926846951246262, 0.1126009002327919, 0.12249813228845596, 0.057157449424266815, 0.11465146392583847, 0.11492864787578583, 0.20903339982032776, 0.20133589208126068, 0.12144795805215836, 0.11972535401582718, 0.11756192892789841, 0.055559925734996796, 0.11340290307998657, 0.05496826022863388, 0.11937384307384491, 0.054236650466918945, 0.12178543210029602, 0.11140315234661102, 0.11353051662445068, 0.052279431372880936, 0.3321402370929718, 0.051589079201221466, 0.051439426839351654, 0.19929638504981995, 0.12334286421537399, 0.19515012204647064, 0.19535057246685028, 0.11795488744974136, 0.0504203699529171, 0.05029797554016113, 0.33152756094932556, 0.21989049017429352, 0.11475150287151337, 0.11544004827737808, 0.11130642890930176, 0.050543107092380524, 0.11500239372253418, 0.10944350808858871, 0.21886785328388214, 0.1968863308429718, 0.11044158786535263, 0.05021084472537041, 0.10954858362674713, 0.050041038542985916, 0.21112628281116486, 0.10976631939411163, 0.11404889076948166, 0.04948331415653229, 0.11625119298696518, 0.3288712203502655, 0.20792905986309052, 0.10845321416854858, 0.4626636505126953, 0.21480560302734375, 0.05069548636674881, 0.05123287811875343, 0.20814211666584015, 0.11033733189105988, 0.05235251039266586, 0.1967267245054245, 0.05298039689660072, 0.18975397944450378, 0.22200250625610352, 0.11730416864156723, 0.1936286985874176, 0.20525847375392914, 0.11834084242582321, 0.20163646340370178, 0.19999714195728302, 0.11081798374652863, 0.10972973704338074, 0.19852280616760254, 0.05611181631684303, 0.11381559073925018, 0.11100859194993973, 0.20092737674713135, 0.30885767936706543, 0.05690382793545723, 0.1990349292755127, 0.1124383732676506, 0.11743824183940887, 0.05777924135327339, 0.19509361684322357, 0.21256136894226074, 0.31524547934532166, 0.20724427700042725, 0.19596488773822784, 0.05957281216979027, 0.19394148886203766, 0.0603160597383976, 0.11434321850538254, 0.1242734044790268, 0.11595810204744339, 0.06098394840955734, 0.12545360624790192, 0.06063157692551613, 0.1822541058063507, 0.2939303517341614, 0.20428457856178284, 0.3037787079811096, 0.06112596392631531, 0.11313655227422714, 0.11525805294513702, 0.10916612297296524, 0.06201091408729553, 0.1282271146774292, 0.11666824668645859, 0.061618924140930176, 0.10804787278175354, 0.2940406799316406, 0.12109904736280441, 0.20700670778751373, 0.2030934989452362, 0.0612814836204052, 0.1962711066007614, 0.2952509820461273, 0.11452098190784454, 0.1806795746088028, 0.11632690578699112, 0.13273821771144867, 0.11420346051454544, 0.06280092149972916, 0.20516826212406158, 0.06284812837839127, 0.10673584043979645, 0.06253453344106674, 0.29982030391693115, 0.12661026418209076, 0.1259453445672989, 0.19683660566806793, 0.11559135466814041, 0.062314704060554504, 0.06216006353497505, 0.20414823293685913, 0.1315700262784958, 0.20355403423309326, 0.1976473331451416, 0.12517569959163666, 0.12299583852291107, 0.1250307857990265, 0.2977401912212372, 0.12513437867164612, 0.11336684972047806, 0.11760883778333664, 0.11640850454568863, 0.191290020942688, 0.30629393458366394, 0.1246737390756607, 0.18821388483047485, 0.20203380286693573, 0.062270842492580414, 0.11896541714668274, 0.11983998119831085, 0.18783490359783173, 0.06258968263864517, 0.06251887232065201, 0.0624324306845665, 0.11782978475093842, 0.11290214210748672, 0.1965547800064087, 0.11407724022865295, 0.1169329583644867, 0.060861263424158096, 0.12358276546001434]\n",
            "Val loss 0.13376833311746592\n",
            "Val auc roc 0.48387854629888855\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89c529dc268f435682f7911b40336438",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1323\n",
            "Train Losses : [0.11940232664346695, 0.18589209020137787, 0.20004789531230927, 0.11991294473409653, 0.29648762941360474, 0.12265145033597946, 0.11377160996198654, 0.060148000717163086, 0.06005188822746277, 0.11665762960910797, 0.12146379798650742, 0.060003772377967834, 0.2922959327697754, 0.05992734059691429, 0.05995984375476837, 0.1947057545185089, 0.06000611186027527, 0.11538294702768326, 0.2075740396976471, 0.11456449329853058, 0.05978439003229141, 0.11275126039981842, 0.1260494589805603, 0.19586406648159027, 0.05984165519475937, 0.1914002001285553, 0.20317882299423218, 0.05959935858845711, 0.11663725972175598, 0.0595892108976841, 0.20330874621868134, 0.19193510711193085, 0.20595930516719818, 0.18770284950733185, 0.0595892108976841, 0.3222404718399048, 0.11752251535654068, 0.11147993803024292, 0.11913305521011353, 0.30885541439056396, 0.059549808502197266, 0.10996046662330627, 0.11880578845739365, 0.05966358631849289, 0.1923319697380066, 0.21128807961940765, 0.11397328227758408, 0.05966142565011978, 0.20553255081176758, 0.20093216001987457, 0.11882680654525757, 0.0595843568444252, 0.20300710201263428, 0.05959230288863182, 0.18138538300991058, 0.284108430147171, 0.11427541822195053, 0.11214926838874817, 0.05961790680885315, 0.11533550173044205, 0.05962514877319336, 0.05958284065127373, 0.059567492455244064, 0.11884322017431259, 0.11630716919898987, 0.11041905730962753, 0.11554921418428421, 0.18210247159004211, 0.3028181195259094, 0.11169154196977615, 0.059450142085552216, 0.19968177378177643, 0.11657199263572693, 0.05933612585067749, 0.11910001188516617, 0.19525179266929626, 0.19124361872673035, 0.2900764048099518, 0.11013675481081009, 0.12157529592514038, 0.19146142899990082, 0.12571196258068085, 0.19427038729190826, 0.11333046853542328, 0.0593949630856514, 0.2098623514175415, 0.11292023211717606, 0.05935276299715042, 0.11763203889131546, 0.11065556108951569, 0.059312786906957626, 0.11562035232782364, 0.1963292509317398, 0.19867585599422455, 0.11265874654054642, 0.05932307988405228, 0.059234749525785446, 0.12054318934679031, 0.10408508032560349, 0.059083376079797745, 0.12518534064292908, 0.11980509012937546, 0.05902741849422455, 0.058919649571180344, 0.12178312987089157, 0.11848777532577515, 0.0588240809738636, 0.12072425335645676, 0.19315743446350098, 0.18656355142593384, 0.11388153582811356, 0.1177825927734375, 0.11071973294019699, 0.05846685916185379, 0.12067249417304993, 0.1994096338748932, 0.05853089690208435, 0.19810141623020172, 0.05831611156463623, 0.11509554833173752, 0.20150341093540192, 0.12160440534353256, 0.19710426032543182, 0.05822571739554405, 0.1125641018152237, 0.1125059649348259, 0.19524765014648438, 0.11933177709579468, 0.11888787150382996, 0.187971830368042, 0.0579790398478508, 0.05795688554644585, 0.2046927958726883, 0.18986108899116516, 0.20203576982021332, 0.1227656677365303, 0.057938478887081146, 0.11431177705526352, 0.2109607458114624, 0.19228781759738922, 0.11118213087320328, 0.11262821406126022, 0.11145857721567154, 0.11485926806926727, 0.05782435089349747, 0.19730065762996674, 0.1847296506166458, 0.12398554384708405, 0.19115740060806274, 0.20627954602241516, 0.11647870391607285, 0.11671169102191925, 0.05795169621706009, 0.11640850454568863, 0.1999942809343338, 0.19717517495155334, 0.11805486679077148, 0.2820117771625519, 0.20182466506958008, 0.19839511811733246, 0.19593510031700134, 0.41861772537231445, 0.19364838302135468, 0.11739908903837204, 0.1272771805524826, 0.1127132922410965, 0.11543504893779755, 0.058343563228845596, 0.10537633299827576, 0.05824100971221924, 0.058244626969099045, 0.19956357777118683, 0.058231718838214874, 0.1891223043203354, 0.11857164651155472, 0.05834564194083214, 0.20838449895381927, 0.058246910572052, 0.20238609611988068, 0.124678835272789, 0.19156529009342194, 0.3084723651409149, 0.1917112022638321, 0.12007611244916916, 0.05823144316673279, 0.20626336336135864, 0.18490758538246155, 0.11844488233327866, 0.05831579491496086, 0.0582999549806118, 0.21057844161987305, 0.05841159448027611, 0.12260661274194717, 0.1899106204509735, 0.210405632853508, 0.11785462498664856, 0.1226610615849495, 0.05830131843686104, 0.10983062535524368, 0.11539997905492783, 0.1937294900417328, 0.05817676708102226, 0.19171814620494843, 0.11499117314815521, 0.30723485350608826, 0.058175768703222275, 0.11483585834503174, 0.1120046004652977, 0.1167527362704277, 0.1173943355679512, 0.20463290810585022, 0.0581405907869339, 0.31347450613975525, 0.20957320928573608, 0.19692859053611755, 0.2036931961774826, 0.11417187750339508, 0.11512460559606552, 0.12006241828203201, 0.11010219901800156, 0.19043919444084167, 0.1231582760810852, 0.12033338844776154, 0.11849068105220795, 0.20503616333007812, 0.11807729303836823, 0.2107202708721161, 0.18743953108787537, 0.12152816355228424, 0.1974746286869049, 0.05824081972241402, 0.05827993154525757, 0.292741060256958, 0.05836346372961998, 0.11448079347610474, 0.21323208510875702, 0.1133454218506813, 0.058338381350040436, 0.12415929138660431, 0.29049551486968994, 0.204594224691391, 0.05838027969002724, 0.20255118608474731, 0.05832666903734207, 0.05830018222332001, 0.12272925674915314, 0.05838156118988991, 0.05832560360431671, 0.20015308260917664, 0.3032747507095337, 0.20789580047130585, 0.11546500772237778, 0.10460242629051208, 0.11751248687505722, 0.058325156569480896, 0.2104557305574417, 0.18472175300121307, 0.05826561897993088, 0.12983673810958862, 0.0582747757434845, 0.19118350744247437, 0.05825086310505867, 0.05820981040596962, 0.05817378684878349, 0.12061890959739685, 0.05821448564529419, 0.11801497638225555, 0.058022964745759964, 0.19178925454616547, 0.2041538655757904, 0.11433146893978119, 0.05799396336078644, 0.05797959864139557, 0.18116021156311035, 0.11113511770963669, 0.19934594631195068, 0.05777920037508011, 0.19186566770076752, 0.11119858920574188, 0.11528231203556061, 0.11281783878803253, 0.11011034995317459, 0.11545929312705994, 0.1125716045498848, 0.3038797080516815, 0.20479878783226013, 0.11464010179042816, 0.11427779495716095, 0.05765550956130028, 0.19758066534996033, 0.05758026987314224, 0.1908729523420334, 0.19168537855148315, 0.11035492271184921, 0.30699247121810913, 0.19421054422855377, 0.19208410382270813, 0.12177278101444244, 0.29292142391204834, 0.11323775351047516, 0.11618372052907944, 0.12885287404060364, 0.11423002928495407, 0.11438921093940735, 0.057870861142873764, 0.19860658049583435, 0.057807885110378265, 0.19504469633102417, 0.11624541133642197, 0.05785664543509483, 0.1050620749592781, 0.19400911033153534, 0.12999147176742554, 0.11708013713359833, 0.12058395147323608, 0.200456902384758, 0.20344656705856323, 0.05773548409342766, 0.2018778920173645, 0.11222542077302933, 0.11017169058322906, 0.2030867487192154, 0.057694822549819946, 0.2107219398021698, 0.11423160135746002, 0.11353131383657455, 0.05779749155044556, 0.11837435513734818, 0.05777159705758095, 0.11666488647460938, 0.18105800449848175, 0.12292134016752243, 0.18052929639816284, 0.2863883972167969, 0.12209848314523697, 0.05757590010762215, 0.21336671710014343, 0.1144406720995903, 0.1248515173792839, 0.30026775598526, 0.05761080980300903, 0.1960345357656479, 0.12356593459844589, 0.11602398753166199, 0.05776740983128548, 0.12598131597042084, 0.11157310009002686, 0.11849142611026764, 0.0575745552778244, 0.10768011212348938, 0.20426858961582184, 0.11605259776115417, 0.05749856308102608, 0.19370709359645844, 0.05747690424323082, 0.11748476326465607, 0.05738389864563942, 0.12435469031333923, 0.19386771321296692, 0.1239461824297905, 0.19976237416267395, 0.11996728181838989, 0.11635319143533707, 0.0572933629155159, 0.05726658180356026, 0.05722692981362343, 0.1992919147014618, 0.1875593066215515, 0.20340125262737274, 0.3077719509601593, 0.05706492066383362, 0.11268975585699081, 0.20198923349380493, 0.057094767689704895, 0.21120142936706543, 0.12764862179756165, 0.20226140320301056, 0.1845991313457489, 0.057097427546978, 0.11779629439115524, 0.18523284792900085, 0.11946649104356766, 0.12059244513511658, 0.11719030886888504, 0.21016272902488708, 0.11493757367134094, 0.057196564972400665, 0.12204471230506897, 0.2088368684053421, 0.11565256118774414, 0.05722043290734291, 0.12186971306800842, 0.12160728126764297, 0.05700354278087616, 0.11743798851966858, 0.21081195771694183, 0.12043560296297073, 0.19865912199020386, 0.11488022655248642, 0.30519309639930725, 0.11686860024929047, 0.05695043504238129, 0.1166786402463913, 0.1955036222934723, 0.21786907315254211, 0.1067846268415451, 0.05689827352762222, 0.10454080998897552, 0.056977663189172745, 0.10936693847179413, 0.05694391950964928, 0.056767914444208145, 0.05677364021539688, 0.11001081019639969, 0.12110329419374466, 0.05669495090842247, 0.11433163285255432, 0.11675760895013809, 0.12019307166337967, 0.12183002382516861, 0.11239831894636154, 0.05643374100327492, 0.11878140270709991, 0.05646539852023125, 0.11184491217136383, 0.05625934153795242, 0.19752472639083862, 0.05611664429306984, 0.20676696300506592, 0.207312673330307, 0.1184382289648056, 0.055998481810092926, 0.1963183581829071, 0.12249448150396347, 0.11460129916667938, 0.2030443698167801, 0.12249121069908142, 0.10697949677705765, 0.1909744143486023, 0.05584760010242462, 0.20438872277736664, 0.11491016298532486, 0.1120871752500534, 0.12384254485368729, 0.05583595111966133, 0.12685845792293549, 0.10637160390615463, 0.19517971575260162, 0.1978358030319214, 0.05573033541440964, 0.19487354159355164, 0.12352918088436127, 0.12137602269649506, 0.11310455203056335, 0.12140946835279465, 0.12333543598651886, 0.117122583091259, 0.11726793646812439, 0.20547062158584595, 0.05562945827841759, 0.055521462112665176, 0.11367626488208771, 0.12254511564970016, 0.10716550797224045, 0.055380843579769135, 0.20637719333171844, 0.11793391406536102, 0.055399809032678604, 0.20841223001480103, 0.12739665806293488, 0.19810491800308228, 0.055312905460596085, 0.43133482336997986, 0.055302564054727554, 0.055311087518930435, 0.05538574606180191, 0.12371203303337097, 0.10815733671188354, 0.11164768040180206, 0.19441631436347961, 0.055194489657878876, 0.05518598482012749, 0.20683394372463226, 0.19911547005176544, 0.11969145387411118, 0.055137693881988525, 0.10736425966024399, 0.19745126366615295, 0.11673513799905777, 0.05512940138578415, 0.2040785402059555, 0.055061761289834976, 0.20789538323879242, 0.11442776769399643, 0.11563747376203537, 0.1990181803703308, 0.054990254342556, 0.2156694233417511, 0.20578841865062714, 0.2090177834033966, 0.11839086562395096, 0.13395175337791443, 0.11762876063585281, 0.29938122630119324, 0.20448999106884003, 0.2943209111690521, 0.11409954726696014, 0.1960809975862503, 0.1101628988981247, 0.05525117367506027, 0.055233217775821686, 0.11168710887432098, 0.12573879957199097, 0.11464612931013107, 0.11829105019569397, 0.05517738312482834, 0.0552716888487339, 0.055167462676763535, 0.21786737442016602, 0.22074002027511597, 0.11190582811832428, 0.11688296496868134, 0.12318487465381622, 0.055057015269994736, 0.11421240121126175, 0.12127608060836792, 0.12709739804267883, 0.05498571693897247, 0.19769038259983063, 0.12237478047609329, 0.22073622047901154, 0.12187247723340988, 0.2044660896062851, 0.10971920937299728, 0.29886552691459656, 0.12385925650596619, 0.31503182649612427, 0.05497081205248833, 0.12092006951570511, 0.12148943543434143, 0.20315773785114288, 0.11483622342348099, 0.20447023212909698, 0.05500102788209915, 0.05503783002495766, 0.11156462132930756, 0.202590212225914, 0.21120497584342957, 0.05495211109519005, 0.054936736822128296, 0.20682689547538757, 0.12366334348917007, 0.1139979287981987, 0.11479481309652328, 0.11880716681480408, 0.11614654958248138, 0.12284690886735916, 0.10531117022037506, 0.2017584890127182, 0.31987476348876953, 0.11071760952472687, 0.20069070160388947, 0.054929520934820175, 0.18695347011089325, 0.20464971661567688, 0.05498325079679489, 0.20883344113826752, 0.054935988038778305, 0.11558543890714645, 0.11375238746404648, 0.11097392439842224, 0.11976360529661179, 0.31617143750190735, 0.12521564960479736, 0.054937317967414856, 0.12105193734169006, 0.1940109133720398, 0.11853259056806564, 0.05489720404148102, 0.20479652285575867, 0.2142118662595749, 0.3154045641422272, 0.11716476082801819, 0.11504682898521423, 0.11362855136394501, 0.054994113743305206, 0.1168207973241806, 0.4325031638145447, 0.1984044909477234, 0.31139203906059265, 0.20055973529815674, 0.055127352476119995, 0.055167194455862045, 0.055189572274684906, 0.12074083834886551, 0.11278583109378815, 0.18814848363399506, 0.19929111003875732, 0.05530356615781784, 0.055286407470703125, 0.11298980563879013, 0.10748196393251419, 0.05530930683016777, 0.05524107441306114, 0.1947476714849472, 0.10340447723865509, 0.29711201786994934, 0.4312978982925415, 0.1199374720454216, 0.20580141246318817, 0.11409717798233032, 0.055382534861564636, 0.18958687782287598, 0.055418483912944794, 0.055412404239177704, 0.19776371121406555, 0.20344720780849457, 0.055445678532123566, 0.05556535720825195, 0.11680702120065689, 0.12394144386053085, 0.31340351700782776, 0.055474139750003815, 0.12181618064641953, 0.05557103082537651, 0.19968441128730774, 0.055598799139261246, 0.10770829766988754, 0.2120703160762787, 0.10929276794195175, 0.055419087409973145, 0.310068279504776, 0.21045219898223877, 0.11574816703796387, 0.05546166002750397, 0.05552821606397629, 0.05553818121552467, 0.05554313212633133, 0.18714262545108795, 0.1175486147403717, 0.05537942051887512, 0.055482830852270126, 0.055360097438097, 0.11770325899124146, 0.11273642629384995, 0.055252328515052795, 0.11878331750631332, 0.19958284497261047, 0.10948269814252853, 0.05514410883188248, 0.19853299856185913, 0.19726446270942688, 0.11504752188920975, 0.055122312158346176, 0.05506216362118721, 0.055077340453863144, 0.05504060536623001, 0.12031534314155579, 0.05496396869421005, 0.05493304878473282, 0.12078580260276794, 0.054825399070978165, 0.1144888773560524, 0.11209936439990997, 0.11375892907381058, 0.43401026725769043, 0.1124972254037857, 0.05476623773574829, 0.11137193441390991, 0.3041144609451294, 0.11549488455057144, 0.05472522974014282, 0.11701446026563644, 0.0547395721077919, 0.12036959826946259, 0.054645806550979614, 0.12303196638822556, 0.11410508304834366, 0.0545903705060482, 0.20911678671836853, 0.11894131451845169, 0.21111665666103363, 0.11872333288192749, 0.11720514297485352, 0.11180981248617172, 0.054472666233778, 0.054482314735651016, 0.12001572549343109, 0.19763804972171783, 0.20017313957214355, 0.05445488542318344, 0.11099422723054886, 0.054331690073013306, 0.18669527769088745, 0.20172925293445587, 0.05437684431672096, 0.20416821539402008, 0.2175181359052658, 0.05429219827055931, 0.1229952946305275, 0.05428853631019592, 0.11432342231273651, 0.3164540231227875, 0.054289862513542175, 0.10874909907579422, 0.0542890764772892, 0.1164991557598114, 0.05421328917145729, 0.05429339408874512, 0.05429895222187042, 0.05416928976774216, 0.3143432140350342, 0.12844067811965942, 0.12121685594320297, 0.19544586539268494, 0.3133516311645508, 0.1236499473452568, 0.1131303533911705, 0.05410685017704964, 0.2011304646730423, 0.29814985394477844, 0.11025302112102509, 0.05417676270008087, 0.05412096530199051, 0.05416363105177879, 0.3048374056816101, 0.2053918093442917, 0.20422972738742828, 0.19421017169952393, 0.19368857145309448, 0.19350379705429077, 0.11136066168546677, 0.12063845247030258, 0.05439503490924835, 0.2061101794242859, 0.12226051092147827, 0.21386773884296417, 0.10827495157718658, 0.054400160908699036, 0.11767517775297165, 0.11943703144788742, 0.05432610958814621, 0.11797115206718445, 0.1938467174768448, 0.12271290272474289, 0.10951103270053864, 0.05433701351284981, 0.18546314537525177, 0.19596578180789948, 0.05426054447889328, 0.12417616695165634, 0.11757978051900864, 0.1976432502269745, 0.12362376600503922, 0.3015429377555847, 0.11647575348615646, 0.21372146904468536, 0.22031064331531525, 0.11469657719135284, 0.2084733247756958, 0.054318930953741074, 0.05431355908513069, 0.12317103147506714, 0.12847135961055756, 0.11335668712854385, 0.19957853853702545, 0.05426618456840515, 0.11924467235803604, 0.18835832178592682, 0.21386419236660004, 0.11432697623968124, 0.11462169885635376, 0.05426054447889328, 0.11512154340744019, 0.0542311891913414, 0.11709745973348618, 0.05432700738310814, 0.0541691891849041, 0.11875296384096146, 0.11287550628185272, 0.05423790216445923, 0.12484222650527954, 0.19745005667209625, 0.11971818655729294, 0.11158779263496399, 0.2088414579629898, 0.115153469145298, 0.10923212766647339, 0.05401890352368355, 0.11959756165742874, 0.11661095917224884, 0.053973082453012466, 0.30564531683921814, 0.21749669313430786, 0.11045554280281067, 0.11143819242715836, 0.11353621631860733, 0.13037212193012238, 0.12213028967380524, 0.12039438635110855, 0.11254751682281494, 0.10896312445402145, 0.11144410818815231, 0.11728887259960175, 0.05383145436644554, 0.05375397205352783, 0.11473105847835541, 0.12342224270105362, 0.05374251306056976, 0.05363578349351883, 0.0536399707198143, 0.11483965069055557, 0.11220153421163559, 0.3364689350128174, 0.05350435525178909, 0.11807754635810852, 0.05347678065299988, 0.05346530303359032, 0.11802936345338821, 0.19861553609371185, 0.19785979390144348, 0.30185624957084656, 0.2059788703918457, 0.20717725157737732, 0.05343851074576378, 0.19497644901275635, 0.19314579665660858, 0.11748819053173065, 0.20574091374874115, 0.20250432193279266, 0.11702631413936615, 0.20664511620998383, 0.10756029933691025, 0.20228664577007294, 0.11826077848672867, 0.11562713235616684, 0.2023107260465622, 0.12168540060520172, 0.0536440871655941, 0.2017958164215088, 0.11176254600286484, 0.2094106376171112, 0.19414857029914856, 0.115107960999012, 0.11241907626390457, 0.11836181581020355, 0.29540473222732544, 0.11087265610694885, 0.2075335830450058, 0.10754159092903137, 0.05365993455052376, 0.11070483922958374, 0.1976550966501236, 0.18319286406040192, 0.053709544241428375, 0.1977519690990448, 0.11320848017930984, 0.053698841482400894, 0.05371946468949318, 0.19931387901306152, 0.12084148079156876, 0.19120490550994873, 0.11370803415775299, 0.11613455414772034, 0.11669939756393433, 0.05374385043978691, 0.11501455307006836, 0.11480196565389633, 0.11275116354227066, 0.053651705384254456, 0.12061896920204163, 0.05360395833849907, 0.05365448817610741, 0.20232033729553223, 0.11055027693510056, 0.31247371435165405, 0.11352570354938507, 0.10696655511856079, 0.2118082195520401, 0.05350154638290405, 0.053514208644628525, 0.11838050186634064, 0.1936693787574768, 0.20006060600280762, 0.2171185165643692, 0.12030548602342606, 0.05357285216450691, 0.05349959433078766, 0.053545691072940826, 0.11497341096401215, 0.05347151681780815, 0.21336816251277924, 0.05344192311167717, 0.12430232763290405, 0.31634384393692017, 0.053394775837659836, 0.3047891855239868, 0.12088743597269058, 0.20085154473781586, 0.11604894697666168, 0.10950937867164612, 0.3223905861377716, 0.11719066649675369, 0.114642433822155, 0.11264733970165253, 0.3168155550956726, 0.10858650505542755, 0.21044398844242096, 0.2060852199792862, 0.05360289663076401, 0.12094226479530334, 0.11794345825910568, 0.20955653488636017, 0.1133352518081665, 0.1159544289112091, 0.20970550179481506, 0.31030282378196716, 0.05369100719690323, 0.053649142384529114, 0.21284137666225433, 0.1198454424738884, 0.11855500936508179, 0.11802630126476288, 0.12268931418657303, 0.19745297729969025, 0.19939814507961273, 0.20894233882427216, 0.1980411410331726, 0.21289847791194916, 0.20129263401031494, 0.1159663051366806, 0.18723143637180328, 0.053868431597948074, 0.3033411204814911, 0.30633699893951416, 0.05382273718714714, 0.3001142740249634, 0.054038841277360916, 0.05390888825058937, 0.20550966262817383, 0.10944420099258423, 0.053955744951963425, 0.11926250159740448, 0.054028768092393875, 0.19080278277397156, 0.11896924674510956, 0.2941904664039612, 0.054012659937143326, 0.11494894325733185, 0.11127224564552307, 0.11909404397010803, 0.12021225690841675, 0.05400833114981651, 0.05402284488081932, 0.11152776330709457, 0.20939093828201294, 0.05407770723104477, 0.05412759259343147, 0.053989145904779434, 0.11797316372394562, 0.11438027024269104, 0.3147655427455902, 0.11555296927690506, 0.05392654612660408, 0.21108175814151764, 0.21277181804180145, 0.1175236776471138, 0.10963381826877594, 0.11788319796323776, 0.05387258902192116, 0.1953469216823578, 0.05390067771077156, 0.11326705664396286, 0.31274768710136414, 0.11302435398101807, 0.05385053530335426, 0.05386286973953247, 0.31197357177734375, 0.20801325142383575, 0.19969716668128967, 0.11237236112356186, 0.053870510309934616, 0.11426088213920593, 0.05392330884933472, 0.05388049781322479, 0.19497345387935638, 0.11482994258403778, 0.20294222235679626, 0.05386963486671448, 0.21531270444393158, 0.11284153163433075, 0.053903963416814804, 0.053842056542634964, 0.053824760019779205, 0.1961590200662613, 0.1109219491481781, 0.12206988781690598, 0.31512996554374695, 0.053829941898584366, 0.11806892603635788, 0.053788259625434875, 0.18788926303386688, 0.10713766515254974, 0.1194339171051979, 0.1155264750123024, 0.12304066866636276, 0.05379234626889229, 0.20956580340862274, 0.10659833252429962, 0.31879374384880066, 0.053758326917886734, 0.11352921277284622, 0.053771018981933594, 0.11332851648330688, 0.053742606192827225, 0.1968754082918167, 0.3122505247592926, 0.11142890155315399, 0.11562154442071915, 0.2059246450662613, 0.05382383242249489, 0.1175212413072586, 0.053726594895124435, 0.11183659732341766, 0.19812214374542236, 0.12519985437393188, 0.19821974635124207, 0.43887731432914734, 0.12121568620204926, 0.20427173376083374, 0.11444880813360214, 0.053761813789606094, 0.19776995480060577, 0.20204930007457733, 0.19670896232128143, 0.10903014242649078, 0.05384993925690651, 0.21264149248600006, 0.11460553854703903, 0.05399763584136963, 0.12013871967792511, 0.05386153981089592, 0.11506389826536179, 0.11331186443567276, 0.05386319011449814, 0.05387330800294876, 0.43828922510147095, 0.05383151397109032, 0.1204243078827858, 0.053863272070884705, 0.053836431354284286, 0.20301346480846405, 0.11321485042572021, 0.053891126066446304, 0.05387027934193611, 0.12042978405952454, 0.11710004508495331, 0.12621468305587769, 0.05389289930462837, 0.19461533427238464, 0.05372884124517441, 0.1158488318324089, 0.11484956741333008, 0.12145546078681946, 0.19615501165390015, 0.11231882870197296, 0.05373920872807503, 0.053717684000730515, 0.05367037281394005, 0.12001308798789978, 0.11895011365413666, 0.05359192565083504, 0.18643724918365479, 0.053634945303201675, 0.19760335981845856, 0.11270665377378464, 0.05353550985455513, 0.11974187195301056, 0.10856446623802185, 0.20176780223846436, 0.11633899062871933, 0.1972479373216629, 0.20004980266094208, 0.053526800125837326, 0.18446865677833557, 0.05344622582197189, 0.11343317478895187, 0.053444068878889084, 0.3190980851650238, 0.05356735736131668, 0.19778695702552795, 0.053421325981616974, 0.117047518491745, 0.05348741635680199, 0.12236334383487701, 0.32330599427223206, 0.11093822121620178, 0.05339277163147926, 0.3059692680835724, 0.21688076853752136, 0.19637206196784973, 0.11773473024368286, 0.05356373265385628, 0.11663001775741577, 0.05348249152302742, 0.12794458866119385, 0.20942258834838867, 0.11848771572113037, 0.18992896378040314, 0.11650381982326508, 0.12362276762723923, 0.3046407103538513, 0.11611288785934448, 0.05345504358410835, 0.119954414665699, 0.10925555229187012, 0.11715757101774216, 0.11222147196531296, 0.11157213896512985, 0.1953841596841812, 0.05346355959773064, 0.11100407689809799, 0.11704746633768082, 0.05345833674073219, 0.11714429408311844, 0.11328402161598206, 0.11940652132034302, 0.053415633738040924, 0.19719454646110535, 0.053345419466495514, 0.19363467395305634, 0.20526397228240967, 0.1153906062245369, 0.30902326107025146, 0.1087082177400589, 0.20092767477035522, 0.20052024722099304, 0.1056823879480362, 0.0534101203083992, 0.05342927947640419, 0.20226672291755676, 0.20993466675281525, 0.11009671539068222, 0.19580017030239105, 0.21159453690052032, 0.19560375809669495, 0.19404008984565735, 0.05340712517499924, 0.12118475139141083, 0.12328282743692398, 0.05342904105782509, 0.05341092124581337, 0.05343540757894516, 0.10907217860221863, 0.11608405411243439, 0.11489645391702652, 0.05336726829409599, 0.11754068732261658, 0.053413622081279755, 0.11870798468589783, 0.18972568213939667, 0.2007407695055008, 0.12124653905630112, 0.11805031448602676, 0.11329666525125504, 0.12259925901889801, 0.11247110366821289, 0.10923322290182114, 0.11026644706726074, 0.12116681039333344, 0.05325722694396973, 0.115822434425354, 0.053332120180130005, 0.05323955789208412, 0.11737633496522903, 0.1122056171298027, 0.32409176230430603, 0.05322914943099022, 0.2060459703207016, 0.10828035324811935, 0.053217656910419464, 0.0531497448682785, 0.053159404546022415, 0.20914329588413239, 0.11056287586688995, 0.05309570953249931, 0.11409467458724976, 0.12657316029071808, 0.3258914649486542, 0.05309662967920303, 0.11477182805538177, 0.19651475548744202, 0.11648193746805191, 0.05309677496552467, 0.2006206214427948, 0.11924615502357483, 0.11271442472934723, 0.11293452978134155, 0.10677185654640198, 0.05301358550786972, 0.11009248346090317, 0.12267284840345383, 0.053068533539772034, 0.11496950685977936, 0.10737475752830505, 0.11502370983362198, 0.11956940591335297, 0.11314543336629868, 0.11784245073795319, 0.1114245355129242, 0.053047362715005875, 0.05291600525379181, 0.10530431568622589, 0.1137806624174118, 0.10804738104343414, 0.05284513533115387, 0.11100908368825912, 0.197035014629364, 0.11605675518512726, 0.21164721250534058, 0.11013879626989365, 0.20935474336147308, 0.11318106204271317, 0.19387590885162354, 0.052773259580135345, 0.2013344168663025, 0.11974406242370605, 0.11573966592550278, 0.05282168835401535, 0.11884022504091263, 0.20142677426338196, 0.20324106514453888, 0.052739907056093216, 0.2041563242673874, 0.21784639358520508, 0.21167661249637604, 0.20785410702228546, 0.2015986293554306, 0.05281246080994606, 0.11859000474214554, 0.2025076150894165, 0.11306481808423996, 0.0527651272714138, 0.11268371343612671, 0.3206782639026642, 0.11881040781736374, 0.20342373847961426, 0.20107102394104004, 0.11378058046102524, 0.19646106660366058, 0.052800096571445465, 0.05293813720345497, 0.11293663084506989, 0.11570223420858383, 0.05283668264746666, 0.05291660130023956, 0.21726389229297638, 0.10906977206468582, 0.3218706250190735, 0.052826251834630966, 0.11443617194890976, 0.052782051265239716, 0.18895156681537628, 0.11135994642972946, 0.3164518177509308, 0.11801501363515854, 0.11266259104013443, 0.052945319563150406, 0.21148544549942017, 0.12194464355707169, 0.11974331736564636, 0.052814826369285583, 0.120844267308712, 0.12108670920133591, 0.12071239203214645, 0.05291258916258812, 0.21594631671905518, 0.05278157815337181, 0.11461682617664337, 0.10731387883424759, 0.05284256488084793, 0.11277558654546738, 0.05276030674576759, 0.20514559745788574, 0.12256991118192673, 0.10794343799352646, 0.11899217218160629, 0.11322244256734848, 0.05269382521510124, 0.052889999002218246, 0.11242708563804626, 0.05269431322813034, 0.19673174619674683, 0.05266154929995537, 0.11012443155050278, 0.21023453772068024, 0.12509670853614807, 0.2115975320339203, 0.1961948275566101, 0.12168464064598083, 0.2008885145187378, 0.05263505131006241, 0.052655406296253204, 0.1262993961572647, 0.3065851628780365, 0.11642784625291824, 0.0526406466960907, 0.11230842769145966, 0.19915848970413208, 0.11312906444072723, 0.12779514491558075, 0.1105773001909256, 0.11159378290176392, 0.05261861905455589, 0.20629923045635223, 0.05260176584124565, 0.052585575729608536, 0.05267147719860077, 0.11453656852245331, 0.05256279185414314, 0.11218412965536118, 0.11743976920843124, 0.10785765200853348, 0.19649295508861542, 0.21269993484020233, 0.052534978836774826, 0.19309477508068085, 0.05255752056837082, 0.11195115745067596, 0.1054600328207016, 0.1977798044681549, 0.052550241351127625, 0.115308478474617, 0.11458920687437057, 0.05253035947680473, 0.05246623232960701, 0.0525498129427433, 0.05245161056518555, 0.11153760552406311, 0.11859047412872314, 0.11174933612346649, 0.21336102485656738, 0.12109920382499695, 0.12533040344715118, 0.1119430884718895, 0.05241438373923302, 0.05245845764875412, 0.11346235871315002, 0.05235544964671135, 0.21012257039546967, 0.052398931235075, 0.21077464520931244, 0.11250147223472595, 0.21368589997291565, 0.31009963154792786, 0.11323399841785431, 0.1181606873869896, 0.12382221966981888, 0.052343014627695084, 0.10937803983688354, 0.20415915548801422, 0.21219398081302643, 0.4461936354637146, 0.2036866396665573, 0.11699497699737549, 0.05240073800086975, 0.11501239240169525, 0.05234121158719063, 0.05239497497677803, 0.10819855332374573, 0.11731598526239395, 0.20092841982841492, 0.11228127032518387, 0.20116721093654633, 0.19697973132133484, 0.05236014723777771, 0.3167700171470642, 0.10960060358047485, 0.19585558772087097, 0.05237497761845589, 0.05237744003534317, 0.21218350529670715, 0.10884996503591537, 0.1927701085805893, 0.20063643157482147, 0.05251365900039673, 0.19275137782096863, 0.12130614370107651, 0.11410529911518097, 0.05243147909641266, 0.12547636032104492, 0.20695193111896515, 0.10929176956415176, 0.11920376867055893, 0.10764734447002411, 0.11914932727813721, 0.30971741676330566, 0.12665744125843048, 0.19235987961292267, 0.05238805711269379, 0.2058059722185135, 0.1194213479757309, 0.1982412189245224, 0.110663041472435, 0.20429623126983643, 0.3138619065284729, 0.052417606115341187, 0.11433283239603043, 0.1940094232559204, 0.11028096079826355, 0.05252891033887863, 0.05244138464331627, 0.11354979127645493, 0.05249066650867462, 0.11396335810422897, 0.2014184445142746, 0.12099247425794601, 0.11825244873762131, 0.20246100425720215, 0.05242485553026199, 0.11570306122303009, 0.20220541954040527, 0.052493952214717865, 0.05251605808734894, 0.10827888548374176, 0.05242982134222984, 0.10937096923589706, 0.11632408946752548, 0.1125706359744072, 0.21082589030265808, 0.20213402807712555, 0.19966687262058258, 0.11609231680631638, 0.21248234808444977, 0.11418402940034866, 0.11288437992334366, 0.11605783551931381, 0.10815051943063736, 0.20374639332294464, 0.3081492781639099, 0.11540425568819046, 0.05238332971930504, 0.11719736456871033, 0.12028072029352188, 0.11604523658752441, 0.05239613726735115, 0.0523691326379776, 0.10951358079910278, 0.05238381400704384, 0.31113293766975403, 0.31850939989089966, 0.11828742921352386, 0.19337277114391327, 0.052367959171533585, 0.19572824239730835, 0.12369779497385025, 0.30884191393852234, 0.11268893629312515, 0.05247889831662178, 0.20773279666900635, 0.05239134654402733, 0.214693084359169, 0.05243590101599693, 0.2066808044910431, 0.052396081387996674, 0.05239298194646835, 0.11403469741344452, 0.052402760833501816, 0.2016647309064865, 0.116817906498909, 0.052409447729587555, 0.11678743362426758, 0.1133260652422905, 0.19978465139865875, 0.05239291116595268, 0.05238068476319313, 0.1100505068898201, 0.19407738745212555, 0.21485410630702972, 0.11011265963315964, 0.1099301278591156, 0.11396122723817825, 0.1944483369588852, 0.11326523125171661, 0.052435457706451416, 0.05237458273768425, 0.05241183564066887, 0.05243551358580589, 0.30252906680107117, 0.052387140691280365, 0.20915940403938293, 0.11696245521306992, 0.05239085480570793, 0.20171242952346802, 0.11226571351289749, 0.11259226500988007, 0.052451834082603455, 0.2031392604112625, 0.11733213067054749, 0.20157086849212646, 0.05238260328769684, 0.0523616299033165, 0.2071569859981537, 0.05236227065324783, 0.11439329385757446, 0.0523659884929657, 0.20735637843608856, 0.1322513222694397, 0.19574056565761566, 0.052458614110946655, 0.11974427849054337, 0.11279093474149704, 0.20368221402168274, 0.1943497210741043, 0.19025178253650665, 0.05232275649905205, 0.20455606281757355, 0.12125813215970993, 0.11546193808317184, 0.10883919149637222, 0.12526574730873108, 0.112276591360569, 0.12414821237325668, 0.052374500781297684, 0.05232667550444603, 0.05242590606212616, 0.19762036204338074, 0.11069252341985703, 0.30434650182724, 0.05235733091831207, 0.31224367022514343, 0.05233033001422882, 0.12042471766471863, 0.11106342822313309, 0.05232621356844902, 0.05243959277868271, 0.12216662615537643, 0.19595275819301605, 0.1108686700463295, 0.21436633169651031, 0.2017270177602768, 0.2129896581172943, 0.11750023066997528, 0.11329790204763412, 0.11367743462324142, 0.05239047482609749, 0.0523274764418602, 0.11455870419740677, 0.05231314152479172, 0.1115875318646431, 0.05232603847980499, 0.052361078560352325, 0.21974603831768036, 0.05236903578042984, 0.19791071116924286, 0.11017980426549911, 0.19784416258335114, 0.11555072665214539, 0.05230719596147537, 0.19501285254955292, 0.11595579981803894, 0.18872161209583282, 0.11115943640470505, 0.3177560865879059, 0.1952507495880127, 0.052342489361763, 0.2050011157989502, 0.20994071662425995, 0.19384440779685974, 0.05239783227443695, 0.05236583948135376, 0.11330745369195938, 0.2052045464515686, 0.0523277185857296, 0.11866839230060577, 0.11434663087129593, 0.11309950053691864, 0.12448074668645859, 0.12318503856658936, 0.11913670599460602, 0.11078593879938126, 0.1204155758023262, 0.1136540025472641, 0.10890210419893265, 0.1254517138004303, 0.11976461857557297, 0.20803599059581757, 0.11936265975236893, 0.1169549822807312, 0.11298256367444992, 0.05236177518963814, 0.05230717360973358, 0.11739866435527802, 0.11219898611307144, 0.3050357401371002]\n",
            "Val loss 0.13301413495065575\n",
            "Val auc roc 0.5086612801231062\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c48aaa5-5a65-4952-d74d-c4d54c0ab807"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13187d3c-efe4-4571-ab29-564ea1bb716b"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ff5f577-b333-4593-9fce-0d77774fcfb9"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "9294fe86-a5b7-4a84-b434-bab194018df4"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Text_Only_Informative]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52ffa1a5-168f-4852-f9d1-3d670245b658"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-08-06 10:15:17.006576'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}