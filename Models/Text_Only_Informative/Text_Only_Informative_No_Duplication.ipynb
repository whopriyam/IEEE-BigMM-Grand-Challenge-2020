{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Only_Informative_No_Duplication.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69c2884a47f14e6796afcf72262f29ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df729bb9f5a94e1eb6f91a1baf8803cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa070201c7974e2aa9cd1e11dbf9bd0e",
              "IPY_MODEL_e465e5ab1d4e4a13afc88e70f2029414"
            ]
          }
        },
        "df729bb9f5a94e1eb6f91a1baf8803cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa070201c7974e2aa9cd1e11dbf9bd0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_315414c3509c4a8ab9c010a485c9d5f4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51cbc02cefe647d1a078988e7877e97a"
          }
        },
        "e465e5ab1d4e4a13afc88e70f2029414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e11e9f1957ed446981690350e2111415",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:12&lt;00:00, 19.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0675e03cb24f4198b3f8c7cfc48ba44f"
          }
        },
        "315414c3509c4a8ab9c010a485c9d5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51cbc02cefe647d1a078988e7877e97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e11e9f1957ed446981690350e2111415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0675e03cb24f4198b3f8c7cfc48ba44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2137eede501a4968a30a12170f284287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62769de19aa64f8e83890b9c6de19204",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e67529a9b56a4eaa96a4a23d1c6a72c6",
              "IPY_MODEL_a9bbed0e273b434b8d4fdf9bf6ab6d40"
            ]
          }
        },
        "62769de19aa64f8e83890b9c6de19204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e67529a9b56a4eaa96a4a23d1c6a72c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_20e1dfb3bdcc457b9d77e3a4ca185d1e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f077c4adc2c04556a760e206f4b009ab"
          }
        },
        "a9bbed0e273b434b8d4fdf9bf6ab6d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_168b8904295c48d18418bb07d60dc576",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [46:58&lt;00:00,  1.77s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10c984a2a48544ca8334d71beb1226e1"
          }
        },
        "20e1dfb3bdcc457b9d77e3a4ca185d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f077c4adc2c04556a760e206f4b009ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "168b8904295c48d18418bb07d60dc576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10c984a2a48544ca8334d71beb1226e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fd0b9151ce84806abf0dc17b03a5faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89da126d608d4e13995241a78a092474",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de00882594794b4b8900f12d5cc722f6",
              "IPY_MODEL_4ee7ffe8e8bd40beb738c997a9febffe"
            ]
          }
        },
        "89da126d608d4e13995241a78a092474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de00882594794b4b8900f12d5cc722f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_788227942a634d61989b531640d867cc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43ba8b9bc817425c8de3f3d89aead563"
          }
        },
        "4ee7ffe8e8bd40beb738c997a9febffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9abfb15db1e44778bc68ee20e160ae35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:22&lt;00:00,  1.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5095942968ad46bba97983feb07d6583"
          }
        },
        "788227942a634d61989b531640d867cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43ba8b9bc817425c8de3f3d89aead563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9abfb15db1e44778bc68ee20e160ae35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5095942968ad46bba97983feb07d6583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39d0f9dcd4824d82b5ad91609b96b6f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bde6b225fb3d44a5a73a3e7ae3c27c71",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a0c0bc529d49406697c128fcac5ed36e",
              "IPY_MODEL_54db76481e9c40a589acc8b30e79f20b"
            ]
          }
        },
        "bde6b225fb3d44a5a73a3e7ae3c27c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0c0bc529d49406697c128fcac5ed36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96a9b4e059794a55909e47ea4755e1fe",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f295c161e91f41198722403e0361d499"
          }
        },
        "54db76481e9c40a589acc8b30e79f20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f26e35f42b014699969aba3ed12eb55c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1595/1595 [15:14&lt;00:00,  1.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9980b135db874d96a248e3c2c2f94857"
          }
        },
        "96a9b4e059794a55909e47ea4755e1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f295c161e91f41198722403e0361d499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f26e35f42b014699969aba3ed12eb55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9980b135db874d96a248e3c2c2f94857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ce91c5de-9100-4b7f-d35c-e5fe0e8c8ce1"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/33)\u001b[K\rremote: Counting objects:   6% (2/33)\u001b[K\rremote: Counting objects:   9% (3/33)\u001b[K\rremote: Counting objects:  12% (4/33)\u001b[K\rremote: Counting objects:  15% (5/33)\u001b[K\rremote: Counting objects:  18% (6/33)\u001b[K\rremote: Counting objects:  21% (7/33)\u001b[K\rremote: Counting objects:  24% (8/33)\u001b[K\rremote: Counting objects:  27% (9/33)\u001b[K\rremote: Counting objects:  30% (10/33)\u001b[K\rremote: Counting objects:  33% (11/33)\u001b[K\rremote: Counting objects:  36% (12/33)\u001b[K\rremote: Counting objects:  39% (13/33)\u001b[K\rremote: Counting objects:  42% (14/33)\u001b[K\rremote: Counting objects:  45% (15/33)\u001b[K\rremote: Counting objects:  48% (16/33)\u001b[K\rremote: Counting objects:  51% (17/33)\u001b[K\rremote: Counting objects:  54% (18/33)\u001b[K\rremote: Counting objects:  57% (19/33)\u001b[K\rremote: Counting objects:  60% (20/33)\u001b[K\rremote: Counting objects:  63% (21/33)\u001b[K\rremote: Counting objects:  66% (22/33)\u001b[K\rremote: Counting objects:  69% (23/33)\u001b[K\rremote: Counting objects:  72% (24/33)\u001b[K\rremote: Counting objects:  75% (25/33)\u001b[K\rremote: Counting objects:  78% (26/33)\u001b[K\rremote: Counting objects:  81% (27/33)\u001b[K\rremote: Counting objects:  84% (28/33)\u001b[K\rremote: Counting objects:  87% (29/33)\u001b[K\rremote: Counting objects:  90% (30/33)\u001b[K\rremote: Counting objects:  93% (31/33)\u001b[K\rremote: Counting objects:  96% (32/33)\u001b[K\rremote: Counting objects: 100% (33/33)\u001b[K\rremote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 41.15 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7189192-b928-4862-adcf-89d2ea66ba0f"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91e6d220-bf87-45a7-bd84-41f2c4200686"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "28128aaa-7476-4b43-b465-4f89e7e12ccb"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a03339e7-fe29-4d04-b988-86c1ff56422b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "4a681030-79cb-4b78-92dc-b64875159f30"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "22cc52a8-b003-43e0-cd1a-e0b1274c7ad8"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ee0943b7-fb93-42e8-cdb2-f84a22bcb146"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "ce46597d-8b7d-4111-9d14-dc9eacad059f"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "06f5776c-3348-46f6-a562-86661967af1b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    #MeToo inspires wave of old misconduct reports...\n",
              "1    #MeToo, says @RW_UNP   @virakesari_lk cartoon ...\n",
              "2    When Chunkey Pandey and Neelam said #MeToo #Me...\n",
              "3                MCGOWAN: #MeToo Is Bullsh*t, Lie...  \n",
              "4    \"Let us not doubt people who have made these r...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f83187d-7e2f-4596-b392-bd8c5a97d8b1"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 34869276.43B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a3d1425-2b92-4224-b5b7-edc9b858b1a3"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1221288.75B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ccc7785c-674e-4481-83c8-14d8778b28d9"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc8abacd-9939-4e2f-d9cb-6db73a867672"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        #df2 = self.data[self.data[label_name]==minority_class]\n",
        "        #df2 = df2.copy().reset_index(drop=True)\n",
        "        #df3 = df2.copy().reset_index(drop=True)\n",
        "        #df4 = df2.copy().reset_index(drop=True)\n",
        "        #df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        \n",
        "        #self.data = self.data.append(df2, ignore_index=True)\n",
        "        #self.data = self.data.append(df3, ignore_index=True)\n",
        "        #self.data = self.data.append(df4, ignore_index=True)\n",
        "        #self.data = self.data.append(df5, ignore_index=True)\n",
        "        #self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9babcb1e-1b83-4f01-96fa-245b802ece4c"
      },
      "source": [
        "col_name = \"Text_Only_Informative\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 0 # or 0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794,
          "referenced_widgets": [
            "69c2884a47f14e6796afcf72262f29ba",
            "df729bb9f5a94e1eb6f91a1baf8803cd",
            "fa070201c7974e2aa9cd1e11dbf9bd0e",
            "e465e5ab1d4e4a13afc88e70f2029414",
            "315414c3509c4a8ab9c010a485c9d5f4",
            "51cbc02cefe647d1a078988e7877e97a",
            "e11e9f1957ed446981690350e2111415",
            "0675e03cb24f4198b3f8c7cfc48ba44f",
            "2137eede501a4968a30a12170f284287",
            "62769de19aa64f8e83890b9c6de19204",
            "e67529a9b56a4eaa96a4a23d1c6a72c6",
            "a9bbed0e273b434b8d4fdf9bf6ab6d40",
            "20e1dfb3bdcc457b9d77e3a4ca185d1e",
            "f077c4adc2c04556a760e206f4b009ab",
            "168b8904295c48d18418bb07d60dc576",
            "10c984a2a48544ca8334d71beb1226e1",
            "7fd0b9151ce84806abf0dc17b03a5faf",
            "89da126d608d4e13995241a78a092474",
            "de00882594794b4b8900f12d5cc722f6",
            "4ee7ffe8e8bd40beb738c997a9febffe",
            "788227942a634d61989b531640d867cc",
            "43ba8b9bc817425c8de3f3d89aead563",
            "9abfb15db1e44778bc68ee20e160ae35",
            "5095942968ad46bba97983feb07d6583",
            "39d0f9dcd4824d82b5ad91609b96b6f8",
            "bde6b225fb3d44a5a73a3e7ae3c27c71",
            "a0c0bc529d49406697c128fcac5ed36e",
            "54db76481e9c40a589acc8b30e79f20b",
            "96a9b4e059794a55909e47ea4755e1fe",
            "f295c161e91f41198722403e0361d499",
            "f26e35f42b014699969aba3ed12eb55c",
            "9980b135db874d96a248e3c2c2f94857"
          ]
        },
        "outputId": "556176cb-8dea-46d2-a32a-487ca6bdbf3d"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 6382\n",
            "Old data length : 1596\n",
            "minority class is 0. Duplicating minority class data!\n",
            "New data length : 1596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69c2884a47f14e6796afcf72262f29ba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2137eede501a4968a30a12170f284287",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1470\n",
            "Train Losses : [0.09347540885210037, 0.3398596942424774, 1.3057637214660645, 1.4635217189788818, 0.5566096305847168, 0.0010790402302518487, 3.666896104812622, 0.9280813336372375, 0.364020973443985, 0.46351271867752075, 0.13817104697227478, 0.12985046207904816, 0.3988664448261261, 0.10779136419296265, 0.007654470391571522, 0.4422236680984497, 0.31832632422447205, 0.05088470131158829, 0.15086354315280914, 0.15178728103637695, 1.7631373405456543, 0.7284812331199646, 0.16453416645526886, 1.842032790184021, 0.08071570843458176, 1.091054081916809, 0.46370038390159607, 0.8047055602073669, 0.2950283885002136, 0.07436232268810272, 0.7689887285232544, 0.06044064834713936, 0.15488918125629425, 0.03929443657398224, 0.054281290620565414, 0.2406199723482132, 0.6777175664901733, 0.04240277409553528, 0.06238611787557602, 0.03121480904519558, 0.2672611176967621, 0.27671733498573303, 0.2145991176366806, 0.03709103539586067, 0.027832960709929466, 0.3619694411754608, 0.3777065575122833, 0.03548227995634079, 0.027423303574323654, 0.08562467992305756, 0.06136399134993553, 0.028844963759183884, 0.03456173092126846, 0.26003602147102356, 0.025046681985259056, 0.13938714563846588, 0.29106616973876953, 0.1368873566389084, 0.043298929929733276, 0.020127873867750168, 0.22462090849876404, 0.9353624582290649, 0.029857683926820755, 0.03945787250995636, 0.10529539734125137, 0.5697512030601501, 0.20101237297058105, 0.37563881278038025, 0.029972828924655914, 0.09796182066202164, 0.042099807411432266, 0.24445943534374237, 0.04977542161941528, 0.13347436487674713, 0.3080975115299225, 0.1509791612625122, 0.11459951847791672, 0.36110731959342957, 0.07399407774209976, 0.500348687171936, 0.26868101954460144, 0.13985858857631683, 0.13845783472061157, 0.08684029430150986, 0.3883782625198364, 0.28080618381500244, 0.13613131642341614, 0.2116430401802063, 0.31790050864219666, 0.1588767021894455, 0.1098443865776062, 0.09682302922010422, 0.15211538970470428, 0.22039327025413513, 0.12130993604660034, 0.08786647021770477, 0.17414674162864685, 0.2635324001312256, 0.1342136263847351, 0.29290851950645447, 0.0876699686050415, 0.1513361930847168, 0.08484998345375061, 0.14815209805965424, 0.2667621672153473, 0.11840652674436569, 0.21485215425491333, 0.21456608176231384, 0.09987000375986099, 0.3028596043586731, 0.12274328619241714, 0.26790180802345276, 0.08373330533504486, 0.11883863806724548, 0.29564496874809265, 0.14245450496673584, 0.07572667300701141, 0.2173270583152771, 0.2048940658569336, 0.07654871046543121, 0.06626618653535843, 0.06419674307107925, 0.057646624743938446, 0.1623891294002533, 0.2446109801530838, 0.12125435471534729, 0.19497019052505493, 0.07826773822307587, 0.08711159974336624, 0.029919719323515892, 0.026305442675948143, 0.10852888971567154, 0.021740706637501717, 0.10672483593225479, 0.23309028148651123, 0.072792649269104, 0.017724361270666122, 0.129248708486557, 0.015531548298895359, 0.2354629635810852, 0.01500154659152031, 0.06768503040075302, 0.015465809032320976, 0.014973749406635761, 0.2576582133769989, 0.12607604265213013, 0.01439148560166359, 0.32228153944015503, 0.1418493390083313, 0.1499916911125183, 0.8168668746948242, 0.021350938826799393, 0.02349703200161457, 0.08555545657873154, 0.1787782907485962, 0.23450340330600739, 0.15112823247909546, 0.24860286712646484, 0.08132482320070267, 0.2229158729314804, 0.16068492829799652, 0.3202204704284668, 0.0815248191356659, 0.1511736363172531, 0.14457494020462036, 0.06908836960792542, 0.2062477469444275, 0.15692953765392303, 0.17893433570861816, 0.08430572599172592, 0.10530312359333038, 0.0881814956665039, 0.08195886015892029, 0.4170724153518677, 0.12802854180335999, 0.1425568163394928, 0.15475264191627502, 0.07313869148492813, 0.23787809908390045, 0.10883365571498871, 0.1326206624507904, 0.10375414788722992, 0.13557325303554535, 0.06566024571657181, 0.06324931979179382, 0.09667593985795975, 0.20903757214546204, 0.053235966712236404, 0.06930553913116455, 0.047668009996414185, 0.17353525757789612, 0.04053609445691109, 0.12541432678699493, 0.10793361067771912, 0.17336933314800262, 0.19935515522956848, 0.02832847274839878, 0.027385849505662918, 0.08728141337633133, 0.08388864994049072, 0.14770539104938507, 0.10693810880184174, 0.12680238485336304, 0.23963019251823425, 0.10940680652856827, 0.13270844519138336, 0.01884579285979271, 0.018830107524991035, 0.3519223630428314, 0.1353377401828766, 0.10258655995130539, 0.20213204622268677, 0.09886717051267624, 0.27334102988243103, 0.16883991658687592, 0.10713305324316025, 0.1264907270669937, 0.02449924871325493, 0.2504328787326813, 0.027285698801279068, 0.08157391846179962, 0.17470335960388184, 0.27633652091026306, 0.028465528041124344, 0.11930153518915176, 0.10581271350383759, 0.2170674353837967, 0.10287973284721375, 0.11155824363231659, 0.258756548166275, 0.03333434462547302, 0.25293001532554626, 0.03423444554209709, 0.03459883853793144, 0.15499164164066315, 0.09709342569112778, 0.10762637853622437, 0.034479059278964996, 0.03358311578631401, 0.0768931657075882, 0.13887092471122742, 0.030627861618995667, 0.14347828924655914, 0.028288748115301132, 0.08417017757892609, 0.08617094904184341, 0.10998625308275223, 0.2336500734090805, 0.08818783611059189, 0.24713382124900818, 0.022622792050242424, 0.3296569287776947, 0.5770281553268433, 0.09702187776565552, 0.07485075294971466, 0.029644157737493515, 0.09898970276117325, 0.24341678619384766, 0.2190813571214676, 0.12692148983478546, 0.08003251999616623, 0.12329290807247162, 0.3275661766529083, 0.22235247492790222, 0.04697130247950554, 0.04972483217716217, 0.11069893091917038, 0.1289600133895874, 0.09227848798036575, 0.1758318394422531, 0.18316256999969482, 0.2565471827983856, 0.1876038759946823, 0.06426119804382324, 0.13823889195919037, 0.30347689986228943, 0.06975775957107544, 0.10559283941984177, 0.07627440989017487, 0.17741842567920685, 0.1311865895986557, 0.0749884769320488, 0.06946352124214172, 0.23588132858276367, 0.09394588321447372, 0.1766912043094635, 0.06335947662591934, 0.09549121558666229, 0.159959614276886, 0.055146973580121994, 0.09949993342161179, 0.21561264991760254, 0.13097365200519562, 0.07803095877170563, 0.11080610007047653, 0.04403207451105118, 0.1171007826924324, 0.10625886172056198, 0.2649465799331665, 0.14496776461601257, 0.035168301314115524, 0.17083679139614105, 0.1000930443406105, 0.0312776044011116, 0.1774800717830658, 0.1363115906715393, 0.02903093583881855, 0.03041541576385498, 0.027627481147646904, 0.20170198380947113, 0.21132685244083405, 0.10410250723361969, 0.02495192177593708, 0.12095333635807037, 0.16164840757846832, 0.025244681164622307, 0.19715994596481323, 0.02562001347541809, 0.02619948424398899, 0.11621659249067307, 0.30613815784454346, 0.024193838238716125, 0.02693098969757557, 0.02457740716636181, 0.4873594641685486, 0.24749328196048737, 0.1326698362827301, 0.07675685733556747, 0.1297617256641388, 0.0812285840511322, 0.1837598979473114, 0.08910517394542694, 0.2706824541091919, 0.03392700105905533, 0.13018649816513062, 0.035745587199926376, 0.03648226335644722, 0.1240924522280693, 0.10211256891489029, 0.10092157870531082, 0.03674823045730591, 0.03451293706893921, 0.2964775860309601, 0.2635363042354584, 0.03744214400649071, 0.18373289704322815, 0.2100982964038849, 0.2660834491252899, 0.08648903667926788, 0.23225237429141998, 0.24874965846538544, 0.037807781249284744, 0.43186554312705994, 0.09003367274999619, 0.04529004544019699, 0.28084713220596313, 0.14575089514255524, 0.30623501539230347, 0.1348859667778015, 0.06274601072072983, 0.06397935003042221, 0.06519581377506256, 0.2690471410751343, 0.11879535764455795, 0.15422575175762177, 0.15158016979694366, 0.22252129018306732, 0.200508713722229, 0.06983977556228638, 0.1662570834159851, 0.06930208206176758, 0.0704960897564888, 0.13691149652004242, 0.22865386307239532, 0.10597046464681625, 0.16128376126289368, 0.08062964677810669, 0.06259185820817947, 0.056065041571855545, 0.09092973917722702, 0.17845940589904785, 0.09685955196619034, 0.2921479642391205, 0.1684224158525467, 0.24847173690795898, 0.09313899278640747, 0.08410034328699112, 0.04345932975411415, 0.0875193327665329, 0.28405866026878357, 0.1484452486038208, 0.17573559284210205, 0.10391996055841446, 0.044005852192640305, 0.12559755146503448, 0.11727210134267807, 0.044818323105573654, 0.12450989335775375, 0.28335079550743103, 0.26685139536857605, 0.11593161523342133, 0.09439690411090851, 0.11270904541015625, 0.13430185616016388, 0.14169606566429138, 0.31255075335502625, 0.09143410623073578, 0.04496372118592262, 0.17934264242649078, 0.22923873364925385, 0.21248039603233337, 0.13664738833904266, 0.049083150923252106, 0.2104290872812271, 0.05127926543354988, 0.09582676738500595, 0.050927918404340744, 0.1967054307460785, 0.0504491962492466, 0.12278702855110168, 0.20544202625751495, 0.04953166842460632, 0.04975283518433571, 0.04853511229157448, 0.2358090579509735, 0.17950008809566498, 0.0863153487443924, 0.045675698667764664, 0.10921058058738708, 0.20209291577339172, 0.14457973837852478, 0.18893460929393768, 0.10041477531194687, 0.04266177862882614, 0.10193279385566711, 0.040928468108177185, 0.03955025598406792, 0.12784823775291443, 0.34171292185783386, 0.097773976624012, 0.12998370826244354, 0.1020641103386879, 0.11513292044401169, 0.0673571228981018, 0.1329522281885147, 0.11882304400205612, 0.3120909035205841, 0.22576841711997986, 0.03545847535133362, 0.0796382874250412, 0.035551637411117554, 0.2336069643497467, 0.12465247511863708, 0.24584512412548065, 0.037622496485710144, 0.03694934770464897, 0.036944080144166946, 0.12025847285985947, 0.03597818315029144, 0.10762222111225128, 0.03467227891087532, 0.11087054759263992, 0.14552874863147736, 0.03171331807971001, 0.28866979479789734, 0.12213495373725891, 0.26908111572265625, 0.27938035130500793, 0.39576053619384766, 0.08220893144607544, 0.03258010372519493, 0.03380138799548149, 0.10693615674972534, 0.09458218514919281, 0.1313226968050003, 0.03679972514510155, 0.10315156728029251, 0.07904183119535446, 0.0368427038192749, 0.22480624914169312, 0.12281657010316849, 0.03520569950342178, 0.034448251128196716, 0.09088234603404999, 0.25280001759529114, 0.11545579880475998, 0.26301148533821106, 0.23325614631175995, 0.03488579019904137, 0.12910792231559753, 0.03489973023533821, 0.035296063870191574, 0.2116006463766098, 0.24101516604423523, 0.28355175256729126, 0.2363424301147461, 0.22959138453006744, 0.09734437614679337, 0.0438028909265995, 0.10256883502006531, 0.12392501533031464, 0.046127818524837494, 0.1318870633840561, 0.237180694937706, 0.18904954195022583, 0.047158580273389816, 0.12866026163101196, 0.09355621039867401, 0.10706355422735214, 0.12653176486492157, 0.09372656047344208, 0.1943637877702713, 0.20014631748199463, 0.046894267201423645, 0.09570378810167313, 0.04638231545686722, 0.13070045411586761, 0.12949790060520172, 0.04392140358686447, 0.1220846176147461, 0.1223229393362999, 0.137867271900177, 0.23459510505199432, 0.10323932021856308, 0.2625638544559479, 0.3800112009048462, 0.03830079361796379, 0.20059514045715332, 0.03952924907207489, 0.040450211614370346, 0.04039117693901062, 0.27541854977607727, 0.04125674441456795, 0.09929650276899338, 0.04091493412852287, 0.11906665563583374, 0.15326102077960968, 0.11777418106794357, 0.03761628642678261, 0.09622834622859955, 0.03580084070563316, 0.09165927767753601, 0.0979250818490982, 0.03200902044773102, 0.10604050010442734, 0.030121076852083206, 0.0951966792345047, 0.4134916663169861, 0.24401892721652985, 0.12467987835407257, 0.44632306694984436, 0.27456924319267273, 0.08833533525466919, 0.24910353124141693, 0.38876578211784363, 0.10642882436513901, 0.04267074540257454, 0.10000251233577728, 0.12267010658979416, 0.10232962667942047, 0.12207113951444626, 0.11130379885435104, 0.17050780355930328, 0.33241549134254456, 0.10810405015945435, 0.15326817333698273, 0.11882457882165909, 0.11226832121610641, 0.11605872958898544, 0.10868161916732788, 0.30084729194641113, 0.22626657783985138, 0.17840078473091125, 0.11316739767789841, 0.16082140803337097, 0.12364538758993149, 0.17762786149978638, 0.07171723246574402, 0.14520369470119476, 0.10220309346914291, 0.12259619683027267, 0.1808779388666153, 0.2686832845211029, 0.07248314470052719, 0.13437144458293915, 0.07338014990091324, 0.11403175443410873, 0.07094158232212067, 0.16901177167892456, 0.1845625340938568, 0.0683448314666748, 0.1136774942278862, 0.27210402488708496, 0.22289012372493744, 0.1842113882303238, 0.09854990988969803, 0.15796692669391632, 0.11479508876800537, 0.09679792076349258, 0.11398699879646301, 0.16525520384311676, 0.12174428999423981, 0.18254774808883667, 0.12470588088035583, 0.11475952714681625, 0.220171719789505, 0.06181112304329872, 0.22512595355510712, 0.315530002117157, 0.16872760653495789, 0.25990793108940125, 0.12490373104810715, 0.1479167342185974, 0.0720626637339592, 0.11068055778741837, 0.11212068051099777, 0.1913757026195526, 0.07947729527950287, 0.07849904149770737, 0.15815483033657074, 0.20472122728824615, 0.08050138503313065, 0.0928996279835701, 0.11200501024723053, 0.07582645118236542, 0.07341153919696808, 0.15276233851909637, 0.2434449940919876, 0.12761527299880981, 0.06542573124170303, 0.06060488522052765, 0.060152653604745865, 0.16604523360729218, 0.1214657723903656, 0.09494830667972565, 0.4058757722377777, 0.269785612821579, 0.04706205055117607, 0.06437879055738449, 0.0976041853427887, 0.14764843881130219, 0.045498304069042206, 0.15445873141288757, 0.04247184842824936, 0.21951276063919067, 0.041940100491046906, 0.0425083301961422, 0.2718784511089325, 0.09400369226932526, 0.0685071125626564, 0.038599174469709396, 0.037246741354465485, 0.1399313509464264, 0.035763852298259735, 0.23778477311134338, 0.10531041771173477, 0.23561830818653107, 0.2524353563785553, 0.07912550866603851, 0.1933225840330124, 0.1281387060880661, 0.034001875668764114, 0.2893606722354889, 0.10616230219602585, 0.35189443826675415, 0.1130884662270546, 0.23805969953536987, 0.03942805156111717, 0.1411202996969223, 0.042462028563022614, 0.12456203997135162, 0.19766846299171448, 0.0942065492272377, 0.09940380603075027, 0.044457171112298965, 0.09958404302597046, 0.043762680143117905, 0.19192424416542053, 0.10395292192697525, 0.09397829324007034, 0.10321369022130966, 0.10896562039852142, 0.0977955162525177, 0.1454891860485077, 0.0411696657538414, 0.03948453813791275, 0.225694939494133, 0.10312224924564362, 0.07799729704856873, 0.11391345411539078, 0.03660031035542488, 0.18727274239063263, 0.22119328379631042, 0.22470524907112122, 0.03500596806406975, 0.17068688571453094, 0.2720910310745239, 0.17706935107707977, 0.381181001663208, 0.0945592001080513, 0.04157142713665962, 0.04149454087018967, 0.42977291345596313, 0.07985405623912811, 0.045120611786842346, 0.21177487075328827, 0.04749058932065964, 0.2502569258213043, 0.04943051561713219, 0.1431298702955246, 0.1371338814496994, 0.3484361469745636, 0.053738728165626526, 0.05379645153880119, 0.19884145259857178, 0.1166238784790039, 0.2546098828315735, 0.2690361440181732, 0.09973188489675522, 0.3170647621154785, 0.11277232319116592, 0.09210645407438278, 0.06874411553144455, 0.14770789444446564, 0.13266348838806152, 0.18056614696979523, 0.10580176115036011, 0.2529451549053192, 0.11923446506261826, 0.15993745625019073, 0.1787908673286438, 0.25372591614723206, 0.07618574798107147, 0.07650952786207199, 0.1465436965227127, 0.07525946199893951, 0.20709654688835144, 0.22988145053386688, 0.19416958093643188, 0.07490499317646027, 0.07406977564096451, 0.132126122713089, 0.14490610361099243, 0.181788370013237, 0.09692451357841492, 0.17974531650543213, 0.20782586932182312, 0.08987575769424438, 0.24185751378536224, 0.07217496633529663, 0.11258447915315628, 0.16226671636104584, 0.1962464302778244, 0.1562347114086151, 0.1625748574733734, 0.1581195890903473, 0.07430054992437363, 0.1083667203783989, 0.17384445667266846, 0.19795995950698853, 0.06998258084058762, 0.23399460315704346, 0.12928906083106995, 0.11582938581705093, 0.0696411281824112, 0.1073712632060051, 0.06915919482707977, 0.06653251498937607, 0.10743728280067444, 0.10372433066368103, 0.11677896231412888, 0.26307013630867004, 0.054750874638557434, 0.18915429711341858, 0.17271418869495392, 0.2029561698436737, 0.09935589134693146, 0.05019921809434891, 0.3007515072822571, 0.04989657178521156, 0.13874588906764984, 0.0991290956735611, 0.1683468222618103, 0.04713645577430725, 0.04776743799448013, 0.11717595160007477, 0.1147468239068985, 0.09341519325971603, 0.10485301166772842, 0.09069612622261047, 0.040938571095466614, 0.04018022492527962, 0.08439568430185318, 0.2742692530155182, 0.037774186581373215, 0.4489813446998596, 0.3471405506134033, 0.11005182564258575, 0.39897391200065613, 0.04061901196837425, 0.041421275585889816, 0.32895469665527344, 0.04523245617747307, 0.2203173190355301, 0.04868602007627487, 0.09709607809782028, 0.05293457210063934, 0.124839648604393, 0.11989370733499527, 0.21006426215171814, 0.05383811518549919, 0.11837539821863174, 0.05343623459339142, 0.053592029958963394, 0.053457777947187424, 0.05181853100657463, 0.23022189736366272, 0.04907209053635597, 0.19502000510692596, 0.1165427640080452, 0.09943843632936478, 0.08207499235868454, 0.08919091522693634, 0.044218093156814575, 0.36577415466308594, 0.14193546772003174, 0.35764023661613464, 0.09210514277219772, 0.2147722691297531, 0.04893938824534416, 0.12937584519386292, 0.050753772258758545, 0.0503491647541523, 0.23564141988754272, 0.04984264448285103, 0.1642739623785019, 0.18767854571342468, 0.1279381662607193, 0.13289006054401398, 0.22691026329994202, 0.1422894299030304, 0.13826650381088257, 0.15053504705429077, 0.05199199542403221, 0.13016393780708313, 0.0511554554104805, 0.04993023723363876, 0.04908599331974983, 0.047317568212747574, 0.2082652896642685, 0.11738593131303787, 0.1983153223991394, 0.12632572650909424, 0.3042413890361786, 0.09868313372135162, 0.20235708355903625, 0.2440069168806076, 0.1048571839928627, 0.04563570395112038, 0.0461527556180954, 0.21020756661891937, 0.13799457252025604, 0.30452170968055725, 0.13138660788536072, 0.04805440083146095, 0.20384487509727478, 0.18997715413570404, 0.11810681968927383, 0.2469068318605423, 0.09991542994976044, 0.1273411512374878, 0.1097419336438179, 0.05598803982138634, 0.05442892387509346, 0.2621699869632721, 0.1667564958333969, 0.05356992781162262, 0.05448464676737785, 0.17427465319633484, 0.1362626999616623, 0.09383774548768997, 0.286659836769104, 0.20440427958965302, 0.10724348574876785, 0.08819464594125748, 0.25319474935531616, 0.2486625611782074, 0.13449068367481232, 0.133170023560524, 0.1246015802025795, 0.22646039724349976, 0.059097643941640854, 0.1326087862253189, 0.10179625451564789, 0.28417232632637024, 0.09898994117975235, 0.11931191384792328, 0.060034703463315964, 0.2096964716911316, 0.10560492426156998, 0.1773965060710907, 0.15756748616695404, 0.059979721903800964, 0.2272442877292633, 0.12309830635786057, 0.18128405511379242, 0.1333075612783432, 0.06124579906463623, 0.08305882662534714, 0.10873086750507355, 0.05963696166872978, 0.057641856372356415, 0.12041231244802475, 0.09723172336816788, 0.2002287358045578, 0.05312267690896988, 0.324558287858963, 0.3697863221168518, 0.053148653358221054, 0.10630063712596893, 0.20310050249099731, 0.23318113386631012, 0.13193941116333008, 0.12064492702484131, 0.09686589986085892, 0.05651876702904701, 0.19742299616336823, 0.34437045454978943, 0.22950755059719086, 0.0584932304918766, 0.11310464888811111, 0.12209711968898773, 0.06023406609892845, 0.11999958008527756, 0.10183627903461456, 0.23521670699119568, 0.23644420504570007, 0.059177324175834656, 0.14801731705665588, 0.05883195251226425, 0.05875680595636368, 0.11751803010702133, 0.24311134219169617, 0.11435279250144958, 0.21411453187465668, 0.05585424229502678, 0.11298615485429764, 0.1909436732530594, 0.2194852977991104, 0.054349541664123535, 0.12399275600910187, 0.2012607455253601, 0.1666242778301239, 0.054196231067180634, 0.19044294953346252, 0.1737711876630783, 0.35867470502853394, 0.05625182390213013, 0.20054598152637482, 0.23656103014945984, 0.05965983495116234, 0.06043297424912453, 0.17813265323638916, 0.1618824154138565, 0.1208810806274414, 0.0634734258055687, 0.0634450614452362, 0.1608344316482544, 0.12062858790159225, 0.062074240297079086, 0.2741437256336212, 0.08641228824853897, 0.4161689281463623, 0.22400525212287903, 0.19814983010292053, 0.12372269481420517, 0.06666044145822525, 0.16459961235523224, 0.06924871355295181, 0.06985612958669662, 0.20762376487255096, 0.06977538764476776, 0.09911368042230606, 0.1379782259464264, 0.06835117936134338, 0.0669204518198967, 0.10118243098258972, 0.11792048066854477, 0.1127495989203453, 0.13162438571453094, 0.060557156801223755, 0.2771582007408142, 0.22331896424293518, 0.05807853117585182, 0.05587966740131378, 0.05522298067808151, 0.10506708174943924, 0.052305009216070175, 0.19747434556484222, 0.05007787421345711, 0.05061770975589752, 0.34084171056747437, 0.18975163996219635, 0.22230131924152374, 0.11215625703334808, 0.23733478784561157, 0.1897701770067215, 0.04963983595371246, 0.13512374460697174, 0.20773376524448395, 0.1097477450966835, 0.13574226200580597, 0.05032777413725853, 0.28593888878822327, 0.05086993798613548, 0.17214572429656982, 0.22616201639175415, 0.1907159686088562, 0.055399905890226364, 0.054513152688741684, 0.17806044220924377, 0.124769426882267, 0.2424294501543045, 0.10355732589960098, 0.1279253363609314, 0.057013947516679764, 0.1922331601381302, 0.13870838284492493, 0.14195677638053894, 0.11410605907440186, 0.05449206382036209, 0.2738705277442932, 0.12792876362800598, 0.19927887618541718, 0.10301516205072403, 0.05476944521069527, 0.10399805009365082, 0.1206255853176117, 0.05398133024573326, 0.22990010678768158, 0.05252695456147194, 0.0938650518655777, 0.05125752463936806, 0.049864597618579865, 0.11541634798049927, 0.12396053224802017, 0.1054067388176918, 0.1270170956850052, 0.11510952562093735, 0.20545126497745514, 0.1032942533493042, 0.041460953652858734, 0.1339498907327652, 0.0404963344335556, 0.12303174287080765, 0.24117274582386017, 0.037447892129421234, 0.12210363894701004, 0.1024763435125351, 0.19589291512966156, 0.13050174713134766, 0.11049368977546692, 0.0338805615901947, 0.2258271723985672, 0.11475813388824463, 0.2061607837677002, 0.09252665191888809, 0.10795658081769943, 0.09694129228591919, 0.2541973888874054, 0.08402883261442184, 0.36331331729888916, 0.21449348330497742, 0.20355774462223053, 0.2119738608598709, 0.11281941086053848, 0.111201211810112, 0.2375102937221527, 0.19448255002498627, 0.12739504873752594, 0.047838468104600906, 0.047477614134550095, 0.11675258725881577, 0.1324916034936905, 0.12091252952814102, 0.048847123980522156, 0.04903893172740936, 0.2161351442337036, 0.047915007919073105, 0.19010987877845764, 0.12973229587078094, 0.10252489894628525, 0.046795401722192764, 0.04633514955639839, 0.11716456711292267, 0.045594409108161926, 0.14055202901363373, 0.2246319055557251, 0.1969723105430603, 0.10245979577302933, 0.3755188286304474, 0.09198461472988129, 0.042758870869874954, 0.21929362416267395, 0.08447905629873276, 0.11784042418003082, 0.1009364128112793, 0.263242244720459, 0.11417416483163834, 0.19895294308662415, 0.19726207852363586, 0.045634955167770386, 0.21885725855827332, 0.10921906679868698, 0.18258419632911682, 0.1731061488389969, 0.1614173948764801, 0.0898112952709198, 0.13419127464294434, 0.21371686458587646, 0.1236286386847496, 0.10709718614816666, 0.11978112906217575, 0.10928347706794739, 0.18696905672550201, 0.05403830483555794, 0.05455385893583298, 0.3626934885978699, 0.19476787745952606, 0.08337333053350449, 0.2986660301685333, 0.12354882806539536, 0.15334376692771912, 0.10680084675550461, 0.09340013563632965, 0.17506930232048035, 0.06185692176222801, 0.10249434411525726, 0.062229305505752563, 0.22238664329051971, 0.3206431567668915, 0.10883240401744843, 0.2051927000284195, 0.06294634938240051, 0.16447676718235016, 0.11932888627052307, 0.10878083854913712, 0.1154218316078186, 0.19224996864795685, 0.14535699784755707, 0.06447406113147736, 0.2703060805797577, 0.06514453887939453, 0.1496681421995163, 0.06506001204252243, 0.13803495466709137, 0.16463324427604675, 0.14296872913837433, 0.2753172516822815, 0.11807481199502945, 0.2467702180147171, 0.06380128860473633, 0.0627949982881546, 0.06151477247476578, 0.0979328602552414, 0.05969563126564026, 0.1806180775165558, 0.10600452870130539, 0.21387678384780884, 0.11467301100492477, 0.2304040789604187, 0.1250510811805725, 0.21259798109531403, 0.055160824209451675, 0.15612256526947021, 0.214857280254364, 0.19202131032943726, 0.10816480219364166, 0.09121299535036087, 0.12912312150001526, 0.10565990209579468, 0.11155290901660919, 0.09799756109714508, 0.05242615193128586, 0.05150950700044632, 0.1099693551659584, 0.19768927991390228, 0.09355536848306656, 0.15406177937984467, 0.04825163632631302, 0.0466865710914135, 0.045507557690143585, 0.21328847110271454, 0.1121203675866127, 0.3836689889431, 0.2274562567472458, 0.04383543133735657, 0.20067887008190155, 0.11104606837034225, 0.04369400069117546, 0.24663665890693665, 0.09292732924222946, 0.0450957752764225, 0.04357443377375603, 0.2147710770368576, 0.0926702693104744, 0.20109207928180695, 0.04348260909318924, 0.09164761006832123, 0.04287594184279442, 0.3735508322715759, 0.09194077551364899, 0.043997395783662796, 0.21682310104370117, 0.21530874073505402, 0.09751157462596893, 0.12673722207546234, 0.0472160167992115, 0.04703282192349434, 0.04634702950716019, 0.10695215314626694, 0.1699078530073166, 0.09038962423801422, 0.0898967981338501, 0.3524649739265442, 0.12488558143377304, 0.10642576217651367, 0.047678641974925995, 0.24602778255939484, 0.1032044067978859, 0.12866432964801788, 0.09379756450653076, 0.11538223177194595, 0.12476252764463425, 0.13054940104484558, 0.0895659402012825, 0.22775375843048096, 0.048054005950689316, 0.10177097469568253, 0.11413589864969254, 0.04751351848244667, 0.04540644958615303, 0.27079907059669495, 0.043528858572244644, 0.3058393895626068, 0.21981580555438995, 0.044013913720846176, 0.32705390453338623, 0.04661087319254875, 0.13871754705905914, 0.046904582530260086, 0.10071941465139389, 0.17319385707378387, 0.11648555845022202, 0.17512181401252747, 0.11867397278547287, 0.12069626897573471, 0.20113924145698547, 0.1968495398759842, 0.10283751785755157, 0.0894717425107956, 0.10248202085494995, 0.0492912158370018, 0.15142057836055756, 0.376880019903183, 0.16007691621780396, 0.1939684897661209, 0.12749016284942627, 0.05398615077137947, 0.054784711450338364, 0.05228334665298462, 0.05181187763810158, 0.2284676879644394, 0.09375908225774765, 0.11493106931447983, 0.0510404147207737, 0.04923803359270096, 0.11518634855747223, 0.1076938584446907, 0.08212536573410034, 0.1257520169019699, 0.1671106219291687, 0.158529594540596, 0.04544132202863693, 0.04353528097271919, 0.043415192514657974, 0.16333407163619995, 0.04131438955664635, 0.0395859032869339, 0.10107890516519547, 0.05874555930495262, 0.09191987663507462, 0.037594087421894073, 0.20933213829994202, 0.16066071391105652, 0.09498225897550583, 0.034007299691438675, 0.11532619595527649, 0.03294430300593376, 0.2759546637535095, 0.030628636479377747, 0.174298956990242, 0.12448935210704803, 0.1639999896287918, 0.11032530665397644, 0.2968735694885254, 0.1229722797870636, 0.03064267337322235, 0.19266477227210999, 0.08074858039617538, 0.15733526647090912, 0.23921296000480652, 0.22412119805812836, 0.12704358994960785, 0.2185734361410141, 0.3128688335418701, 0.21827374398708344, 0.11654575914144516, 0.14231452345848083, 0.12742483615875244, 0.0807151049375534, 0.09091340005397797, 0.1317983716726303, 0.07719928026199341, 0.13427358865737915, 0.10772249847650528, 0.16250982880592346, 0.13657154142856598, 0.044728413224220276, 0.2273702770471573, 0.3974853754043579, 0.20168328285217285, 0.12356144189834595, 0.09307287633419037, 0.04815256968140602, 0.10287945717573166, 0.04888775944709778, 0.049948133528232574, 0.048952121287584305, 0.04919319599866867, 0.11715281754732132, 0.04907985404133797, 0.12899520993232727, 0.1972608119249344, 0.04474039003252983, 0.18633554875850677, 0.043508563190698624, 0.043902963399887085, 0.04269752278923988, 0.04436979070305824, 0.039836883544921875, 0.08654942363500595, 0.03972810134291649, 0.03636069968342781, 0.3876550793647766, 0.13590103387832642, 0.036358222365379333, 0.034374404698610306, 0.0339730903506279, 0.11383786052465439, 0.2835276126861572, 0.1350587010383606, 0.034076787531375885, 0.13039833307266235, 0.13521626591682434, 0.031711217015981674, 0.39676883816719055, 0.11101755499839783, 0.20540772378444672, 0.22320665419101715, 0.03262573853135109, 0.03332863375544548, 0.09063936024904251, 0.21383903920650482, 0.09032918512821198, 0.25057485699653625, 0.21792492270469666, 0.10893858224153519, 0.09960007667541504, 0.5573529005050659, 0.22186601161956787, 0.1477176994085312, 0.042445119470357895, 0.044655703008174896, 0.045852430164813995, 0.14361630380153656, 0.04643508791923523, 0.04649084061384201, 0.048386652022600174, 0.04641848802566528, 0.11606396734714508, 0.3724075257778168, 0.22189466655254364, 0.24572861194610596, 0.2427707314491272, 0.1451600193977356, 0.12041112780570984, 0.17038670182228088, 0.18781808018684387, 0.11743386089801788, 0.054197777062654495, 0.09197042137384415, 0.1699478030204773, 0.055893056094646454, 0.17170673608779907, 0.11286116391420364, 0.05637699365615845, 0.2306062877178192, 0.1053999736905098, 0.11946889013051987, 0.10547643899917603, 0.4301226735115051, 0.05966344103217125, 0.11900059878826141, 0.05872839316725731, 0.09549795091152191, 0.059392914175987244, 0.12319564074277878, 0.10259994119405746, 0.13129615783691406, 0.05793316289782524, 0.05692203715443611, 0.08893205225467682, 0.1277293860912323, 0.20241716504096985, 0.1180303767323494, 0.13084332644939423, 0.052615903317928314, 0.052017901092767715, 0.10850512236356735, 0.3356335759162903, 0.13795070350170135, 0.2643006145954132, 0.12186389416456223, 0.17597126960754395, 0.049412745982408524, 0.04773516207933426, 0.2052321434020996, 0.04727114737033844, 0.20128244161605835, 0.35798919200897217, 0.1079133003950119, 0.1785929650068283, 0.25122249126434326, 0.0515206940472126, 0.11511857062578201, 0.09443749487400055, 0.21084147691726685, 0.253121018409729, 0.05250602960586548, 0.052456364035606384, 0.185735285282135, 0.05251292139291763, 0.10420632362365723, 0.08348225057125092, 0.24300254881381989, 0.22204458713531494, 0.05276678875088692, 0.10231535881757736, 0.12318043410778046, 0.05293672904372215, 0.05211901664733887, 0.051015276461839676, 0.05011291429400444, 0.10512173920869827, 0.34072810411453247, 0.11582563072443008, 0.04873620718717575, 0.16464808583259583, 0.11712101846933365, 0.12962950766086578, 0.04549838602542877, 0.1323862373828888, 0.044185344129800797, 0.12802906334400177, 0.04341045394539833, 0.13713645935058594, 0.04072387516498566, 0.03942975774407387, 0.23972244560718536, 0.03780394792556763, 0.11293976753950119, 0.03652293607592583, 0.036294907331466675, 0.09848550707101822, 0.2496618628501892, 0.1158747747540474, 0.4362052083015442, 0.19278232753276825, 0.24531425535678864, 0.11719530075788498, 0.034435927867889404, 0.13613422214984894, 0.22775854170322418, 0.03563852980732918, 0.18780352175235748, 0.03624797239899635, 0.10214373469352722, 0.23300155997276306, 0.3681674003601074, 0.25511330366134644, 0.1133040115237236, 0.04096885398030281, 0.041777145117521286, 0.11766619980335236, 0.10210487991571426, 0.11494027823209763, 0.043084122240543365, 0.09550883620977402, 0.04324152693152428, 0.04291241988539696, 0.1753838211297989, 0.22138582170009613, 0.08778176456689835, 0.23321467638015747, 0.18859167397022247, 0.11792036145925522, 0.22084376215934753, 0.04419366642832756, 0.12302901595830917, 0.22178184986114502, 0.1273624300956726, 0.1941833198070526, 0.13504578173160553, 0.19067686796188354, 0.11025600135326385, 0.04992414638400078, 0.0480697900056839, 0.13486674427986145, 0.046782054007053375, 0.08132912218570709, 0.12749192118644714, 0.49831265211105347, 0.4428741931915283, 0.07289060950279236, 0.16797204315662384, 0.2767163813114166, 0.08200819045305252, 0.15649834275245667, 0.11907996237277985, 0.059050966054201126, 0.058579228818416595, 0.2434263378381729, 0.05923346430063248, 0.1279304027557373, 0.16956603527069092, 0.11087320744991302, 0.06196601316332817, 0.05965056270360947, 0.05765985697507858, 0.21822737157344818, 0.08837799727916718, 0.12835325300693512, 0.0547521635890007, 0.18866467475891113, 0.34469127655029297, 0.276434987783432, 0.09029154479503632, 0.22163733839988708, 0.054921552538871765, 0.05429942160844803, 0.05441025272011757, 0.21068055927753448, 0.08658365905284882, 0.21137148141860962, 0.1283103972673416, 0.1324349194765091, 0.05310375988483429, 0.13273432850837708, 0.3206435739994049, 0.3300258219242096, 0.05482279509305954, 0.17630842328071594, 0.12268981337547302, 0.055786602199077606, 0.11032219231128693, 0.23487794399261475, 0.23509304225444794, 0.17806363105773926, 0.2174854874610901, 0.13847358524799347, 0.10269179940223694, 0.14261463284492493, 0.16209058463573456, 0.16150881350040436, 0.20079505443572998, 0.09211710095405579, 0.060520902276039124, 0.05978510528802872, 0.1996690183877945, 0.12443183362483978, 0.12906447052955627, 0.059414755553007126, 0.19822198152542114, 0.1361166089773178]\n",
            "Val loss 0.1329539310476535\n",
            "Val auc roc 0.5098848332520538\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd0b9151ce84806abf0dc17b03a5faf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1339\n",
            "Train Losses : [0.08309401571750641, 0.1536058932542801, 0.056997738778591156, 0.28697770833969116, 0.05681060627102852, 0.0984518975019455, 0.05713476985692978, 0.4300359785556793, 0.19728879630565643, 0.20980343222618103, 0.22571535408496857, 0.06019968166947365, 0.1438634693622589, 0.28099268674850464, 0.0632971003651619, 0.10688978433609009, 0.10414858162403107, 0.06514065712690353, 0.13140930235385895, 0.21748845279216766, 0.10777438431978226, 0.06487251818180084, 0.15591327846050262, 0.06443183124065399, 0.10941193252801895, 0.11235573887825012, 0.1327679604291916, 0.1250281184911728, 0.13620255887508392, 0.1518746018409729, 0.2266576886177063, 0.05672181770205498, 0.11567100137472153, 0.05518103763461113, 0.10484270006418228, 0.16078394651412964, 0.05194125697016716, 0.1723605841398239, 0.11738816648721695, 0.10471118241548538, 0.21068544685840607, 0.04865593463182449, 0.04909808933734894, 0.047575693577528, 0.08328504115343094, 0.1488569974899292, 0.16480395197868347, 0.04432104527950287, 0.20371586084365845, 0.2154064178466797, 0.11433293670415878, 0.1232721209526062, 0.21759353578090668, 0.04364922642707825, 0.04320494830608368, 0.09658889472484589, 0.09243886172771454, 0.04202425852417946, 0.22616219520568848, 0.11094829440116882, 0.09981516003608704, 0.10760753601789474, 0.039984218776226044, 0.04073255509138107, 0.03922940418124199, 0.25141721963882446, 0.10345897078514099, 0.14785468578338623, 0.09587503969669342, 0.1300325095653534, 0.179576575756073, 0.14566397666931152, 0.03602804243564606, 0.1945219188928604, 0.08340679109096527, 0.23827475309371948, 0.13149172067642212, 0.2759120762348175, 0.2570551335811615, 0.03617550805211067, 0.10861188173294067, 0.037537913769483566, 0.23084275424480438, 0.23840101063251495, 0.1024538055062294, 0.11398574709892273, 0.03751233592629433, 0.0378117710351944, 0.11324586719274521, 0.37236160039901733, 0.10674179345369339, 0.04009260609745979, 0.038833994418382645, 0.1124112531542778, 0.11594362556934357, 0.10065267980098724, 0.21979592740535736, 0.0944676324725151, 0.08134203404188156, 0.13321645557880402, 0.13546526432037354, 0.0391339473426342, 0.03837565332651138, 0.11118298023939133, 0.03777828812599182, 0.12348896265029907, 0.21910613775253296, 0.036747582256793976, 0.24706831574440002, 0.10525170713663101, 0.09277155995368958, 0.2836344838142395, 0.035760559141635895, 0.03582464158535004, 0.12004917114973068, 0.10585375130176544, 0.035721927881240845, 0.10013049840927124, 0.11937934160232544, 0.03419998660683632, 0.22352686524391174, 0.2505253553390503, 0.13470147550106049, 0.2477877140045166, 0.2531445324420929, 0.21265406906604767, 0.38304099440574646, 0.1170731708407402, 0.22802340984344482, 0.0384700708091259, 0.21695218980312347, 0.12234439700841904, 0.2019195556640625, 0.22929394245147705, 0.12173226475715637, 0.10212145745754242, 0.33526408672332764, 0.04662013798952103, 0.21030017733573914, 0.20292073488235474, 0.10100893676280975, 0.11996021121740341, 0.22111786901950836, 0.11543120443820953, 0.1135392040014267, 0.2543022632598877, 0.055741406977176666, 0.1833208203315735, 0.11663000285625458, 0.1299106925725937, 0.11508676409721375, 0.1999974548816681, 0.2985050082206726, 0.18246322870254517, 0.10423438996076584, 0.06107201799750328, 0.061357609927654266, 0.19917577505111694, 0.06160496175289154, 0.06197537109255791, 0.06267909705638885, 0.06122438982129097, 0.10814893245697021, 0.0885641872882843, 0.21663524210453033, 0.058697305619716644, 0.05699916183948517, 0.11046476662158966, 0.1258731335401535, 0.11616074293851852, 0.2390170395374298, 0.05302537605166435, 0.0520784966647625, 0.3371899724006653, 0.051109205931425095, 0.050931479781866074, 0.05052929371595383, 0.09976541996002197, 0.24389337003231049, 0.04839188978075981, 0.04779452830553055, 0.10198415070772171, 0.09587368369102478, 0.13812175393104553, 0.17655234038829803, 0.044609811156988144, 0.04397522658109665, 0.23224610090255737, 0.042864128947257996, 0.11112777888774872, 0.10842408239841461, 0.04162987321615219, 0.040978796780109406, 0.10632693767547607, 0.12628605961799622, 0.22244910895824432, 0.12164688855409622, 0.0376434326171875, 0.23270483314990997, 0.106194406747818, 0.03657521679997444, 0.036248579621315, 0.11091284453868866, 0.0351606123149395, 0.2696162760257721, 0.2248801440000534, 0.11006886512041092, 0.21660813689231873, 0.10534457862377167, 0.11418040841817856, 0.21424493193626404, 0.4165010452270508, 0.09635598957538605, 0.03688851371407509, 0.09668807685375214, 0.193595290184021, 0.03860228881239891, 0.09457584470510483, 0.10862437635660172, 0.22113612294197083, 0.12014435231685638, 0.04026629775762558, 0.2132093608379364, 0.04044317826628685, 0.09238177537918091, 0.358346551656723, 0.04153968021273613, 0.110090471804142, 0.1089727133512497, 0.18281570076942444, 0.2320231795310974, 0.22375942766666412, 0.04409389942884445, 0.23816964030265808, 0.10532384365797043, 0.04600511118769646, 0.10656122863292694, 0.04630294442176819, 0.11058271676301956, 0.18179930746555328, 0.12358468770980835, 0.046138789504766464, 0.11977363377809525, 0.3552355170249939, 0.11020202934741974, 0.21642610430717468, 0.13116355240345, 0.30595090985298157, 0.19560101628303528, 0.11381135880947113, 0.050159964710474014, 0.2232869416475296, 0.18165752291679382, 0.1167721226811409, 0.107819564640522, 0.20172879099845886, 0.11111890524625778, 0.13413254916667938, 0.0546223521232605, 0.05457223579287529, 0.11948071420192719, 0.11967898905277252, 0.12559276819229126, 0.053874991834163666, 0.05287037044763565, 0.0891372561454773, 0.23150038719177246, 0.05126061663031578, 0.0977558046579361, 0.10808590054512024, 0.04955989867448807, 0.21239671111106873, 0.04782514646649361, 0.12755988538265228, 0.047191884368658066, 0.12652897834777832, 0.12885236740112305, 0.04425598680973053, 0.18487580120563507, 0.12743669748306274, 0.11281906813383102, 0.210474893450737, 0.18297405540943146, 0.10612151026725769, 0.2448229193687439, 0.21210245788097382, 0.11113075166940689, 0.09481236338615417, 0.04275493323802948, 0.09966420382261276, 0.1288202702999115, 0.0921536535024643, 0.12449467182159424, 0.1826610416173935, 0.09581504017114639, 0.10034754127264023, 0.2274235635995865, 0.12896806001663208, 0.3358392119407654, 0.13532240688800812, 0.10622337460517883, 0.09932061284780502, 0.11981018632650375, 0.0421408973634243, 0.22219747304916382, 0.10330450534820557, 0.12679310142993927, 0.12079504132270813, 0.20331889390945435, 0.2432396411895752, 0.25117093324661255, 0.04304857924580574, 0.18445175886154175, 0.04414213076233864, 0.12396519631147385, 0.04436543956398964, 0.04401853308081627, 0.2653873860836029, 0.09554313868284225, 0.3288973867893219, 0.04475254565477371, 0.09584827721118927, 0.13112784922122955, 0.19559016823768616, 0.13389813899993896, 0.2156548798084259, 0.04571203142404556, 0.20338328182697296, 0.046056319028139114, 0.32494300603866577, 0.1109868735074997, 0.09307049214839935, 0.11505161225795746, 0.10475538671016693, 0.11918909102678299, 0.11204016953706741, 0.33660051226615906, 0.12729130685329437, 0.11058679223060608, 0.1753474771976471, 0.05051662400364876, 0.2064940184354782, 0.05103083327412605, 0.11988291889429092, 0.051409799605607986, 0.10147605091333389, 0.20972399413585663, 0.05081487074494362, 0.05134112760424614, 0.11688553541898727, 0.1746014803647995, 0.049876198172569275, 0.04978875443339348, 0.2612123191356659, 0.12149213999509811, 0.09932494163513184, 0.24657781422138214, 0.04767318814992905, 0.04891064390540123, 0.0979563295841217, 0.2530827522277832, 0.10388972610235214, 0.09808218479156494, 0.23820672929286957, 0.0973883792757988, 0.22911980748176575, 0.0455668568611145, 0.04603559896349907, 0.10348948836326599, 0.12243258208036423, 0.11716057360172272, 0.11320872604846954, 0.1265294849872589, 0.12003884464502335, 0.04319881275296211, 0.042308155447244644, 0.11134970933198929, 0.04124078527092934, 0.21096493303775787, 0.08908595144748688, 0.039290763437747955, 0.2761681079864502, 0.11578173190355301, 0.19654116034507751, 0.11466699838638306, 0.5417616963386536, 0.03919481858611107, 0.21453966200351715, 0.1105281412601471, 0.21799246966838837, 0.12301436811685562, 0.10650473833084106, 0.11695714294910431, 0.044017937034368515, 0.11297202110290527, 0.2120707780122757, 0.11099963635206223, 0.12664878368377686, 0.21743954718112946, 0.10291123390197754, 0.259556382894516, 0.04666505753993988, 0.10269474983215332, 0.10075430572032928, 0.04681757465004921, 0.04695117846131325, 0.04589047655463219, 0.09046518057584763, 0.1102495789527893, 0.04481174424290657, 0.10375943779945374, 0.20129741728305817, 0.04301602020859718, 0.10770870000123978, 0.09799370914697647, 0.2077898383140564, 0.21482394635677338, 0.04146220535039902, 0.09776951372623444, 0.10912130028009415, 0.2045993208885193, 0.04102925956249237, 0.20980210602283478, 0.11784753203392029, 0.040755659341812134, 0.09740512073040009, 0.10067220032215118, 0.08113723248243332, 0.10949720442295074, 0.03930730000138283, 0.10970837622880936, 0.10811538994312286, 0.23396539688110352, 0.1258138120174408, 0.03805755451321602, 0.11531546711921692, 0.2275070697069168, 0.037202563136816025, 0.03687223792076111, 0.23150163888931274, 0.10078389197587967, 0.03641480207443237, 0.11378563940525055, 0.03588780760765076, 0.035705164074897766, 0.035080913454294205, 0.034845300018787384, 0.110076405107975, 0.10315130650997162, 0.2584736943244934, 0.23360607028007507, 0.03268046677112579, 0.12203977257013321, 0.033218253403902054, 0.21817176043987274, 0.03269406408071518, 0.27772560715675354, 0.032802779227495193, 0.11500126868486404, 0.03205898776650429, 0.18769346177577972, 0.03203137591481209, 0.03232027217745781, 0.03183138743042946, 0.1279802918434143, 0.11141341924667358, 0.13454388082027435, 0.22929202020168304, 0.22842437028884888, 0.031239459291100502, 0.03104650415480137, 0.030957473441958427, 0.24540644884109497, 0.38535237312316895, 0.03150184080004692, 0.2542669177055359, 0.10546665638685226, 0.1090238094329834, 0.03353697806596756, 0.2492591291666031, 0.034426264464855194, 0.22730740904808044, 0.23426468670368195, 0.03556321933865547, 0.09150364995002747, 0.03669899329543114, 0.03656221553683281, 0.11601704359054565, 0.09418878704309464, 0.3999512493610382, 0.10478654503822327, 0.12053242325782776, 0.2292606383562088, 0.12215752899646759, 0.09412448853254318, 0.03899727389216423, 0.23233219981193542, 0.039479710161685944, 0.03976065292954445, 0.2294032871723175, 0.10332392156124115, 0.10256354510784149, 0.5243626832962036, 0.21393415331840515, 0.11227357387542725, 0.12545940279960632, 0.12039277702569962, 0.35603535175323486, 0.19359545409679413, 0.12692373991012573, 0.32451072335243225, 0.21453942358493805, 0.051888007670640945, 0.05293641239404678, 0.1231694445014, 0.31690868735313416, 0.10757423937320709, 0.1262628585100174, 0.28708022832870483, 0.09159477055072784, 0.10689932107925415, 0.061868324875831604, 0.06199111044406891, 0.11172808706760406, 0.06253872066736221, 0.11474452912807465, 0.13093937933444977, 0.10552103817462921, 0.10589837282896042, 0.06105527654290199, 0.11339832097291946, 0.12160224467515945, 0.05915887653827667, 0.10326756536960602, 0.05755072087049484, 0.05683153122663498, 0.056392017751932144, 0.19072282314300537, 0.20521040260791779, 0.20105485618114471, 0.11111696064472198, 0.11859063059091568, 0.13789071142673492, 0.11234139651060104, 0.10220914334058762, 0.12212147563695908, 0.10342491418123245, 0.11567341536283493, 0.048601001501083374, 0.2308017611503601, 0.13041634857654572, 0.10941842943429947, 0.04659845679998398, 0.04591841250658035, 0.04540608823299408, 0.2610139548778534, 0.11645974963903427, 0.10654290020465851, 0.11687202006578445, 0.0428946278989315, 0.22112199664115906, 0.11863862723112106, 0.040958717465400696, 0.5213004946708679, 0.11840875446796417, 0.09568189084529877, 0.04190654680132866, 0.205703005194664, 0.23409803211688995, 0.11147928982973099, 0.3267064094543457, 0.044082026928663254, 0.2212969809770584, 0.1187356635928154, 0.10674051940441132, 0.11902225017547607, 0.0468098409473896, 0.04694661125540733, 0.04762295261025429, 0.3058241307735443, 0.2183600217103958, 0.10727018862962723, 0.09292259067296982, 0.19116367399692535, 0.3187481760978699, 0.11406300216913223, 0.21283270418643951, 0.1271163374185562, 0.05181807652115822, 0.05241519957780838, 0.21604786813259125, 0.1304936408996582, 0.10981106013059616, 0.1980433315038681, 0.05344410240650177, 0.10811817646026611, 0.21982496976852417, 0.09666052460670471, 0.12564533948898315, 0.05407727137207985, 0.19869399070739746, 0.12177804112434387, 0.1160547062754631, 0.11441633105278015, 0.12326501309871674, 0.17826791107654572, 0.20732830464839935, 0.10653659701347351, 0.11114336550235748, 0.14391134679317474, 0.2086694836616516, 0.10543137043714523, 0.20849786698818207, 0.103424571454525, 0.05210723727941513, 0.21488170325756073, 0.20085088908672333, 0.3012291193008423, 0.052806783467531204, 0.22717468440532684, 0.054072365164756775, 0.05396800860762596, 0.05418398603796959, 0.053876347839832306, 0.053789470344781876, 0.11141469329595566, 0.10439667105674744, 0.1255202442407608, 0.13050466775894165, 0.2216614931821823, 0.050887998193502426, 0.3306429088115692, 0.05079435557126999, 0.05038898438215256, 0.17483539879322052, 0.04998159781098366, 0.19010433554649353, 0.12334450334310532, 0.10702260583639145, 0.0493069626390934, 0.19330209493637085, 0.11253489553928375, 0.048650626093149185, 0.04875042662024498, 0.10398529469966888, 0.18563170731067657, 0.11490236222743988, 0.21120049059391022, 0.04742420092225075, 0.10782471299171448, 0.20212753117084503, 0.22566191852092743, 0.04615328088402748, 0.10475525259971619, 0.046025678515434265, 0.11508810520172119, 0.20830704271793365, 0.04509716108441353, 0.11477557569742203, 0.044979196041822433, 0.10197845101356506, 0.20643576979637146, 0.24119994044303894, 0.13957978785037994, 0.10648491978645325, 0.1218949481844902, 0.12576521933078766, 0.04303992912173271, 0.11925207823514938, 0.09567011892795563, 0.12861528992652893, 0.11038121581077576, 0.11903028190135956, 0.12347264587879181, 0.10700332373380661, 0.19986391067504883, 0.10595927387475967, 0.10276620835065842, 0.03917323797941208, 0.10784505307674408, 0.03836629539728165, 0.10412421822547913, 0.22639040648937225, 0.037440963089466095, 0.106147401034832, 0.21035456657409668, 0.11884668469429016, 0.24840083718299866, 0.03638237714767456, 0.22733533382415771, 0.22896593809127808, 0.036663394421339035, 0.2644287943840027, 0.18662665784358978, 0.3704274594783783, 0.10061156749725342, 0.12666469812393188, 0.09385188668966293, 0.040426719933748245, 0.36987316608428955, 0.112163245677948, 0.10917256027460098, 0.2030055820941925, 0.043706052005290985, 0.1239238977432251, 0.11191532015800476, 0.11184144020080566, 0.12248395383358002, 0.1391301453113556, 0.04527343064546585, 0.20023711025714874, 0.09102299064397812, 0.04514581710100174, 0.34802091121673584, 0.10531412810087204, 0.10493248701095581, 0.04578258469700813, 0.3024578392505646, 0.11080145835876465, 0.2229960560798645, 0.04743503779172897, 0.20248816907405853, 0.12005967646837234, 0.048033785074949265, 0.04823493957519531, 0.3320941925048828, 0.23131541907787323, 0.13224172592163086, 0.11299262195825577, 0.05007358640432358, 0.4591880738735199, 0.05123407766222954, 0.05197888985276222, 0.10343699902296066, 0.23003485798835754, 0.10312201082706451, 0.19217322766780853, 0.12562806904315948, 0.09225338697433472, 0.054950084537267685, 0.11435895413160324, 0.055189717561006546, 0.23530559241771698, 0.1204766109585762, 0.11205349862575531, 0.1261875331401825, 0.10549362748861313, 0.053895629942417145, 0.30099591612815857, 0.11836707592010498, 0.21011096239089966, 0.053551722317934036, 0.05371464788913727, 0.12772804498672485, 0.053109120577573776, 0.23142525553703308, 0.1251874566078186, 0.10651340335607529, 0.05191605165600777, 0.11287904530763626, 0.11771229654550552, 0.05041591078042984, 0.13018839061260223, 0.20255880057811737, 0.11766909062862396, 0.12661877274513245, 0.04787449911236763, 0.11261726915836334, 0.04670468717813492, 0.04622700437903404, 0.11594805866479874, 0.044850923120975494, 0.2364223748445511, 0.11150892823934555, 0.11708902567625046, 0.10016941279172897, 0.10742150247097015, 0.04163184389472008, 0.11723688989877701, 0.11322519928216934, 0.03991222009062767, 0.20200759172439575, 0.21223248541355133, 0.11225957423448563, 0.2261660397052765, 0.10779450833797455, 0.10301396995782852, 0.112006276845932, 0.03836970031261444, 0.10786540061235428, 0.2315724492073059, 0.11014378070831299, 0.10609333217144012, 0.36360132694244385, 0.5390535593032837, 0.11194319278001785, 0.1985360085964203, 0.10901086032390594, 0.24148991703987122, 0.3637404441833496, 0.10982856899499893, 0.21116280555725098, 0.11368400603532791, 0.3106054663658142, 0.11102022230625153, 0.05127504840493202, 0.29172149300575256, 0.3235343098640442, 0.2850973606109619, 0.12366332113742828, 0.10031682997941971, 0.06092461943626404, 0.10496296733617783, 0.13140597939491272, 0.12376309931278229, 0.1735084354877472, 0.12346210330724716, 0.18038170039653778, 0.06630385667085648, 0.0671069473028183, 0.105872742831707, 0.2941186726093292, 0.1808655709028244, 0.06807473301887512, 0.135306715965271, 0.1181190088391304, 0.11749912053346634, 0.28444793820381165, 0.06865048408508301, 0.13106997311115265, 0.1766704022884369, 0.12760065495967865, 0.12587568163871765, 0.06858562678098679, 0.11836034059524536, 0.06851466745138168, 0.06710279732942581, 0.20601534843444824, 0.06579164415597916, 0.0651676133275032, 0.06447014212608337, 0.06413958221673965, 0.19356855750083923, 0.062244441360235214, 0.0611947737634182, 0.21641960740089417, 0.12591956555843353, 0.3001302480697632, 0.05808506906032562, 0.05785776302218437, 0.11996840685606003, 0.13824990391731262, 0.11833500117063522, 0.19852329790592194, 0.05504529923200607, 0.17820462584495544, 0.18610143661499023, 0.05479680001735687, 0.12521205842494965, 0.053327351808547974, 0.05326521769165993, 0.19680935144424438, 0.11268264800310135, 0.05142848566174507, 0.05089295282959938, 0.05046435818076134, 0.20894691348075867, 0.04927879944443703, 0.048672981560230255, 0.11591283231973648, 0.10899575799703598, 0.046800971031188965, 0.046688616275787354, 0.04537744075059891, 0.35008636116981506, 0.04424033686518669, 0.10963042825460434, 0.19424939155578613, 0.23186509311199188, 0.11014921218156815, 0.19134625792503357, 0.043262798339128494, 0.09899713844060898, 0.24183622002601624, 0.12167738378047943, 0.1040465235710144, 0.132578507065773, 0.11337404698133469, 0.12344808131456375, 0.08910784870386124, 0.04228321835398674, 0.1338980793952942, 0.22420111298561096, 0.12561795115470886, 0.23932109773159027, 0.34274786710739136, 0.11254972219467163, 0.042178187519311905, 0.04228421673178673, 0.042307764291763306, 0.09859112650156021, 0.19702625274658203, 0.11304658651351929, 0.11445625871419907, 0.20085425674915314, 0.21304117143154144, 0.11745531111955643, 0.04267477989196777, 0.042781367897987366, 0.22478315234184265, 0.09801743179559708, 0.12234167754650116, 0.1971895396709442, 0.042659562081098557, 0.11130160093307495, 0.1104833111166954, 0.21437865495681763, 0.04289278760552406, 0.11843179911375046, 0.042950339615345, 0.18972545862197876, 0.12917788326740265, 0.10663578659296036, 0.19303496181964874, 0.04211868345737457, 0.2347000539302826, 0.108127161860466, 0.11435019969940186, 0.13009080290794373, 0.09286094456911087, 0.0416162945330143, 0.13179634511470795, 0.3582976460456848, 0.09662045538425446, 0.2157827466726303, 0.041775695979595184, 0.1113969162106514, 0.10726357996463776, 0.19452206790447235, 0.11424222588539124, 0.10624722391366959, 0.04244304820895195, 0.1119856983423233, 0.10784699022769928, 0.3281024396419525, 0.10400287806987762, 0.21510307490825653, 0.043045442551374435, 0.11351532489061356, 0.04360490292310715, 0.04309307411313057, 0.1090376153588295, 0.121771901845932, 0.0430205799639225, 0.12183090299367905, 0.09292441606521606, 0.2180963158607483, 0.04170428588986397, 0.2134518325328827, 0.10098113119602203, 0.09392262995243073, 0.041259657591581345, 0.10664520412683487, 0.0408291257917881, 0.2289595901966095, 0.19574731588363647, 0.2009831815958023, 0.121411994099617, 0.040551040321588516, 0.21040482819080353, 0.5210609436035156, 0.04132997617125511, 0.11143345385789871, 0.21549256145954132, 0.11007765680551529, 0.2164389044046402, 0.1995575875043869, 0.22111546993255615, 0.2247585505247116, 0.11162298917770386, 0.10406079888343811, 0.10079286247491837, 0.130201056599617, 0.1306118220090866, 0.3379215896129608, 0.22435997426509857, 0.23837411403656006, 0.22496165335178375, 0.19764086604118347, 0.29331550002098083, 0.11809293180704117, 0.056142423301935196, 0.11245539784431458, 0.10456939786672592, 0.20936952531337738, 0.05865679308772087, 0.11460915207862854, 0.11864717304706573, 0.2055269032716751, 0.12302938103675842, 0.12505213916301727, 0.059384457767009735, 0.11527392268180847, 0.1097225695848465, 0.059007614850997925, 0.05849383398890495, 0.11123109608888626, 0.1693377047777176, 0.18171967566013336, 0.18045596778392792, 0.057054970413446426, 0.19744762778282166, 0.20005249977111816, 0.20748189091682434, 0.05687778815627098, 0.11773473024368286, 0.11914867162704468, 0.0578073151409626, 0.10838044434785843, 0.19682754576206207, 0.213201105594635, 0.20578321814537048, 0.20958444476127625, 0.05646159127354622, 0.17613616585731506, 0.11518832296133041, 0.3057861328125, 0.11198432743549347, 0.0573992058634758, 0.19997325539588928, 0.11166016757488251, 0.18178528547286987, 0.05875469371676445, 0.4166826605796814, 0.17828163504600525, 0.13176271319389343, 0.12732814252376556, 0.1854182481765747, 0.19190585613250732, 0.06252750009298325, 0.06360166519880295, 0.10555673390626907, 0.13138552010059357, 0.12022149562835693, 0.06330714374780655, 0.133033886551857, 0.13071855902671814, 0.17301692068576813, 0.18196611106395721, 0.06267790496349335, 0.11290497332811356, 0.22103793919086456, 0.06222524121403694, 0.2105381190776825, 0.13602587580680847, 0.21980546414852142, 0.10994590818881989, 0.11605198681354523, 0.1831047385931015, 0.17189562320709229, 0.06089070439338684, 0.20402200520038605, 0.13461045920848846, 0.09882669150829315, 0.14092296361923218, 0.22839979827404022, 0.19599086046218872, 0.18126548826694489, 0.060234442353248596, 0.11322100460529327, 0.29724082350730896, 0.18308120965957642, 0.0606515146791935, 0.1127251610159874, 0.06084758788347244, 0.12070942670106888, 0.4083958864212036, 0.1076001226902008, 0.2158922553062439, 0.16890431940555573, 0.17187167704105377, 0.06392263621091843, 0.1196693554520607, 0.1393422931432724, 0.06430140137672424, 0.06472443789243698, 0.12614963948726654, 0.12132292240858078, 0.10572679340839386, 0.12241929024457932, 0.06287261843681335, 0.06182760372757912, 0.061441585421562195, 0.176909402012825, 0.18937231600284576, 0.05977560952305794, 0.18779356777668, 0.10565713047981262, 0.11715361475944519, 0.3326277732849121, 0.0990028902888298, 0.11436236649751663, 0.22632341086864471, 0.18649140000343323, 0.1989762932062149, 0.11157543957233429, 0.13753072917461395, 0.20544886589050293, 0.058499693870544434, 0.12546254694461823, 0.05911498889327049, 0.12170518934726715, 0.08707378804683685, 0.18065372109413147, 0.05811797082424164, 0.13176822662353516, 0.1986280381679535, 0.1279280185699463, 0.19978927075862885, 0.05680961534380913, 0.12498615682125092, 0.14154784381389618, 0.05580418184399605, 0.11684250831604004, 0.12323042005300522, 0.20622146129608154, 0.05371478572487831, 0.11114245653152466, 0.2971175014972687, 0.1105673685669899, 0.0931795984506607, 0.18209117650985718, 0.19723151624202728, 0.3345334231853485, 0.105414479970932, 0.05354241281747818, 0.17743512988090515, 0.054612863808870316, 0.05425717309117317, 0.11288346350193024, 0.21121926605701447, 0.1684924215078354, 0.2694852352142334, 0.0549977570772171, 0.11066612601280212, 0.10581465810537338, 0.1905064880847931, 0.05602214112877846, 0.1286132037639618, 0.11456231772899628, 0.05580802634358406, 0.055423080921173096, 0.05513596907258034, 0.1395304948091507, 0.12702526152133942, 0.3101101815700531, 0.10618358850479126, 0.10432378202676773, 0.32674092054367065, 0.05374938249588013, 0.18011389672756195, 0.05382075905799866, 0.05393313243985176, 0.05374757573008537, 0.12779724597930908, 0.053005244582891464, 0.23378780484199524, 0.09710367769002914, 0.10074307024478912, 0.22352749109268188, 0.09743928164243698, 0.05163560062646866, 0.19384054839611053, 0.09780075401067734, 0.05115123093128204, 0.3145490288734436, 0.13298431038856506, 0.2992251217365265, 0.11739776283502579, 0.19473089277744293, 0.10144621133804321, 0.10352908819913864, 0.05294683203101158, 0.11994829028844833, 0.1216401681303978, 0.052817896008491516, 0.12958411872386932, 0.052143536508083344, 0.10496551543474197, 0.051588013768196106, 0.05125391483306885, 0.05062304064631462, 0.050282470881938934, 0.11514651030302048, 0.04895737022161484, 0.2179633527994156, 0.09725769609212875, 0.10753115266561508, 0.04630807042121887, 0.04590301588177681, 0.10164264589548111, 0.14114302396774292, 0.3394171893596649, 0.11387595534324646, 0.11316636949777603, 0.2169373780488968, 0.22749923169612885, 0.10493955761194229, 0.12719403207302094, 0.11754047870635986, 0.24453046917915344, 0.10510925948619843, 0.10619314759969711, 0.12324360758066177, 0.340740442276001, 0.10553272813558578, 0.10554641485214233, 0.09620920568704605, 0.20080070197582245, 0.21200397610664368, 0.223639577627182, 0.11246832460165024, 0.10371875762939453, 0.21135076880455017, 0.12229681760072708, 0.046876564621925354, 0.10393039882183075, 0.11701582372188568, 0.13524200022220612, 0.11971498280763626, 0.046672891825437546, 0.10477146506309509, 0.04612424224615097, 0.04578851908445358, 0.045660365372896194, 0.2125229686498642, 0.13295243680477142, 0.176078662276268, 0.32310131192207336, 0.044340021908283234, 0.04450755938887596, 0.3330298662185669, 0.2077033966779709, 0.21830475330352783, 0.20989705622196198, 0.046301499009132385, 0.04664430394768715, 0.19424621760845184, 0.1080898717045784, 0.22650279104709625, 0.1245575100183487, 0.11900070309638977, 0.048840172588825226, 0.049367982894182205, 0.24870967864990234, 0.23555299639701843, 0.12602819502353668, 0.11166474968194962, 0.048499032855033875, 0.19773593544960022, 0.31204089522361755, 0.23593416810035706, 0.4645373225212097, 0.05084209889173508, 0.10958828032016754, 0.11933067440986633, 0.052767787128686905, 0.2867145240306854, 0.053874291479587555, 0.12934677302837372, 0.05523279309272766, 0.20058679580688477, 0.1845642477273941, 0.21829621493816376, 0.05641033872961998, 0.05674511939287186, 0.05713003873825073, 0.056582462042570114, 0.131726935505867, 0.13176840543746948, 0.11596987396478653, 0.05602202191948891, 0.13805615901947021, 0.10603561997413635, 0.05536665394902229, 0.05416187271475792, 0.1250162124633789, 0.05306404083967209, 0.05241464078426361, 0.13806791603565216, 0.12441445887088776, 0.12180880457162857, 0.10604474693536758, 0.04921654239296913, 0.04857528209686279, 0.13310350477695465, 0.11148335784673691, 0.04650084674358368, 0.0455484539270401, 0.21410982310771942, 0.04437259957194328, 0.1167123094201088, 0.11005765199661255, 0.22579582035541534, 0.04250477999448776, 0.11450305581092834, 0.11925876140594482, 0.31554287672042847, 0.10659896582365036, 0.10950898379087448, 0.10026133060455322, 0.041155170649290085, 0.3554745018482208, 0.3598417341709137, 0.041478000581264496, 0.2162178009748459, 0.04230521246790886, 0.04336054250597954, 0.12114382535219193, 0.11646473407745361, 0.5020012855529785, 0.20546431839466095, 0.21152769029140472, 0.20885084569454193, 0.09123104810714722, 0.10168711841106415, 0.04770011827349663, 0.23060837388038635, 0.09769834578037262, 0.19430236518383026, 0.12469511479139328, 0.2180703729391098, 0.10490663349628448, 0.19498556852340698, 0.30817437171936035, 0.14044968783855438, 0.052549947053194046, 0.05309732258319855, 0.3052282929420471, 0.1365583837032318, 0.11992250382900238, 0.23265783488750458, 0.1257886290550232, 0.11299138516187668, 0.055473580956459045, 0.05555853247642517, 0.32754653692245483, 0.11077436804771423, 0.05626509711146355, 0.2139151692390442, 0.19881485402584076, 0.11703930795192719, 0.1295216828584671, 0.2933340072631836, 0.12357009947299957, 0.13026496767997742, 0.21694248914718628, 0.1902640163898468, 0.11865813285112381, 0.1877066195011139, 0.12574082612991333, 0.059304822236299515, 0.059193555265665054, 0.19081741571426392, 0.12632662057876587, 0.05907976254820824, 0.1383979171514511, 0.11494738608598709, 0.12817281484603882, 0.20552963018417358, 0.22001515328884125, 0.09792276471853256, 0.05803713947534561, 0.20614683628082275, 0.20153604447841644, 0.2123437225818634, 0.1257387399673462, 0.18302318453788757, 0.1801467388868332, 0.12411098927259445, 0.11819324642419815, 0.10859476774930954, 0.12251856178045273, 0.10699094831943512, 0.1898411214351654, 0.05828161910176277, 0.05804004892706871, 0.11162130534648895, 0.20122413337230682, 0.20914976298809052, 0.11600003391504288, 0.10514501482248306, 0.05706300586462021, 0.1303577721118927, 0.1354645937681198, 0.118795245885849, 0.055939387530088425, 0.21860681474208832, 0.23057310283184052, 0.13447368144989014, 0.21970103681087494, 0.3038252592086792, 0.20221810042858124, 0.2890585660934448, 0.11164691299200058, 0.1236269623041153, 0.056595008820295334, 0.056857895106077194, 0.05709696561098099, 0.12224124372005463, 0.11212729662656784, 0.056712113320827484, 0.10531649738550186, 0.11809679120779037, 0.12332232296466827, 0.12576372921466827, 0.05523071438074112, 0.054809119552373886, 0.31277111172676086, 0.20110489428043365, 0.10603691637516022, 0.11648016422986984, 0.19587969779968262, 0.20895707607269287, 0.10505300015211105, 0.11932046711444855, 0.21109606325626373, 0.2010447233915329, 0.10944144427776337, 0.12152313441038132, 0.10893005132675171, 0.182943657040596, 0.28908923268318176, 0.11732003837823868, 0.12686118483543396, 0.11638140678405762, 0.1265440583229065, 0.055495426058769226, 0.18622948229312897, 0.10644429922103882, 0.05546150356531143, 0.05542217940092087, 0.19994989037513733, 0.11350291967391968, 0.22380292415618896, 0.1853155791759491, 0.1155666708946228, 0.11742440611124039, 0.18491126596927643, 0.054828397929668427, 0.20890477299690247, 0.12221764028072357, 0.18140922486782074, 0.1032465249300003, 0.12871316075325012, 0.3148680329322815, 0.11951393634080887, 0.4313074052333832, 0.05661968141794205, 0.19904109835624695, 0.10982579737901688, 0.10780233889818192, 0.10926800966262817, 0.22176724672317505, 0.22553837299346924, 0.21520118415355682, 0.18822991847991943, 0.11974170058965683, 0.17338474094867706, 0.09988022595643997, 0.10200521349906921, 0.17298375070095062, 0.16729669272899628, 0.10935342311859131, 0.12277975678443909, 0.17938581109046936, 0.06406684219837189, 0.06369811296463013, 0.18627730011940002, 0.06405164301395416, 0.06379381567239761, 0.06343135982751846, 0.20867951214313507, 0.06310688704252243, 0.13148924708366394, 0.1963520050048828, 0.18462911248207092, 0.1997404247522354, 0.06263189762830734, 0.10565479844808578, 0.13305045664310455, 0.11708759516477585, 0.32307127118110657, 0.2132343202829361, 0.18572747707366943, 0.10362466424703598, 0.1960867941379547, 0.118248850107193, 0.11324436962604523, 0.1954100877046585, 0.13526473939418793, 0.19210508465766907, 0.11975647509098053, 0.06252263486385345, 0.1088620275259018, 0.1078873872756958, 0.20029151439666748, 0.062126532196998596, 0.19569553434848785, 0.061748553067445755, 0.12071067094802856, 0.2273710072040558, 0.22236226499080658, 0.23303303122520447, 0.10751180350780487, 0.1294831484556198, 0.21349772810935974, 0.29806745052337646, 0.06141926348209381, 0.19887693226337433, 0.06154246628284454, 0.061553411185741425, 0.06191195175051689, 0.1271064579486847, 0.12158805131912231, 0.17362913489341736, 0.1801067292690277, 0.060642391443252563, 0.12459241598844528, 0.060252148658037186, 0.06000290811061859, 0.3035672903060913, 0.05948774516582489, 0.0596238449215889, 0.09800870716571808, 0.20018720626831055, 0.1158570796251297, 0.058388568460941315, 0.11932506412267685, 0.13017478585243225, 0.20333145558834076, 0.1068815067410469, 0.05674038454890251, 0.11580143868923187, 0.11242760717868805, 0.05532127618789673, 0.11079606413841248, 0.13633045554161072, 0.05464867874979973, 0.11932104825973511, 0.21517039835453033, 0.21051283180713654, 0.125588059425354, 0.19487369060516357, 0.2210858315229416, 0.12973883748054504, 0.05184445530176163, 0.12294477969408035, 0.05208669230341911, 0.10467404872179031, 0.05101244896650314, 0.050603512674570084, 0.20700132846832275, 0.11666172742843628, 0.19560545682907104, 0.11873747408390045, 0.04905145615339279, 0.048897214233875275, 0.11235911399126053, 0.11431244015693665, 0.0476931594312191, 0.04737208038568497, 0.10590096563100815, 0.3381301164627075, 0.10637351125478745, 0.046282943338155746, 0.04590880870819092, 0.04594305530190468, 0.04530368000268936, 0.2159128338098526, 0.12452676892280579, 0.21160626411437988, 0.21667303144931793, 0.04455079883337021, 0.044246573001146317, 0.12137042731046677, 0.11695598065853119, 0.11298882961273193, 0.11173683404922485, 0.2058437615633011, 0.1047217845916748, 0.11653172224760056, 0.19749920070171356, 0.04260419309139252, 0.195710226893425]\n",
            "Val loss 0.13132015618502646\n",
            "Val auc roc 0.47435254490279033\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39d0f9dcd4824d82b5ad91609b96b6f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1328\n",
            "Train Losses : [0.10576333850622177, 0.04262438043951988, 0.12674148380756378, 0.21991002559661865, 0.1958969235420227, 0.09527010470628738, 0.042543768882751465, 0.042673204094171524, 0.12630897760391235, 0.10130655765533447, 0.042450692504644394, 0.2180289924144745, 0.22480259835720062, 0.1082661971449852, 0.20339608192443848, 0.10709552466869354, 0.04261759668588638, 0.1043207049369812, 0.10917944461107254, 0.04247750714421272, 0.10828860849142075, 0.10984662175178528, 0.04260088875889778, 0.10984915494918823, 0.3543407917022705, 0.04240070655941963, 0.1184132769703865, 0.04249494522809982, 0.10910092294216156, 0.3541017174720764, 0.10190177708864212, 0.09246550500392914, 0.09903860092163086, 0.10533036291599274, 0.09480799734592438, 0.20630739629268646, 0.1149805337190628, 0.042579930275678635, 0.21770347654819489, 0.20212772488594055, 0.20260491967201233, 0.04234737157821655, 0.2306622713804245, 0.5067756175994873, 0.09800755232572556, 0.04277625307440758, 0.04286976903676987, 0.04281535744667053, 0.042795248329639435, 0.1111510619521141, 0.10222930461168289, 0.3660551905632019, 0.11736535280942917, 0.21650153398513794, 0.04313528910279274, 0.10911661386489868, 0.10780932009220123, 0.11434229463338852, 0.10538891702890396, 0.22596336901187897, 0.11281773447990417, 0.13767555356025696, 0.042841698974370956, 0.04306495562195778, 0.2107139229774475, 0.10819245874881744, 0.04287182539701462, 0.24337638914585114, 0.09435874968767166, 0.043129973113536835, 0.23172281682491302, 0.252359002828598, 0.3310743570327759, 0.04308969900012016, 0.10295098274946213, 0.11277119070291519, 0.2028055638074875, 0.2353215515613556, 0.19236089289188385, 0.10545200109481812, 0.04321200028061867, 0.04322289675474167, 0.1206345334649086, 0.31672152876853943, 0.11547335237264633, 0.23848854005336761, 0.11021382361650467, 0.04334358870983124, 0.20721250772476196, 0.11848065257072449, 0.04336756467819214, 0.10666073113679886, 0.10536767542362213, 0.10172511637210846, 0.11749283969402313, 0.500717043876648, 0.33111727237701416, 0.12250804901123047, 0.11955837905406952, 0.37583449482917786, 0.10070876032114029, 0.1996573507785797, 0.04382594674825668, 0.11216552555561066, 0.11689020693302155, 0.10785110294818878, 0.20760297775268555, 0.34418168663978577, 0.20194005966186523, 0.11879999935626984, 0.10943253338336945, 0.11411488801240921, 0.1998009979724884, 0.10916300117969513, 0.12818720936775208, 0.18240416049957275, 0.11220063269138336, 0.04427532106637955, 0.04424614459276199, 0.09901878982782364, 0.22681918740272522, 0.04431376978754997, 0.21625587344169617, 0.11071408540010452, 0.19766192138195038, 0.10939915478229523, 0.21643786132335663, 0.04457319900393486, 0.10320590436458588, 0.22295740246772766, 0.10131633281707764, 0.11427049338817596, 0.10814495384693146, 0.04474122077226639, 0.1260906308889389, 0.18951943516731262, 0.10958141833543777, 0.3336097002029419, 0.12887917459011078, 0.10863855481147766, 0.10622097551822662, 0.2107546627521515, 0.21981948614120483, 0.12768499553203583, 0.11557011306285858, 0.04454413801431656, 0.044720157980918884, 0.20669469237327576, 0.1034335270524025, 0.22302813827991486, 0.12661418318748474, 0.21985048055648804, 0.20726053416728973, 0.20867277681827545, 0.04462616518139839, 0.33935320377349854, 0.1127471774816513, 0.04476993903517723, 0.11659184843301773, 0.10259963572025299, 0.1195162758231163, 0.21983905136585236, 0.10743463039398193, 0.044935088604688644, 0.11666908115148544, 0.10993149131536484, 0.1934661567211151, 0.04474762827157974, 0.04510875791311264, 0.10853058099746704, 0.1933196783065796, 0.10360636562108994, 0.11599968373775482, 0.23175688087940216, 0.11660563945770264, 0.04498198255896568, 0.19810159504413605, 0.11840476840734482, 0.10509153455495834, 0.13232636451721191, 0.1150110736489296, 0.04492606595158577, 0.1158532127737999, 0.04469454661011696, 0.11446794122457504, 0.1137605756521225, 0.22533932328224182, 0.3298112750053406, 0.3748890161514282, 0.12633097171783447, 0.1100141853094101, 0.10822708904743195, 0.20924042165279388, 0.04493971914052963, 0.044829029589891434, 0.11664507538080215, 0.10481090843677521, 0.12493687868118286, 0.10614612698554993, 0.11308909207582474, 0.1283819079399109, 0.044800080358982086, 0.04467693716287613, 0.10671434551477432, 0.044736895710229874, 0.04473162814974785, 0.04456769675016403, 0.1202549859881401, 0.11513767391443253, 0.04447690397500992, 0.12821637094020844, 0.04449757933616638, 0.20357964932918549, 0.11535562574863434, 0.044384270906448364, 0.04454966261982918, 0.21571198105812073, 0.22877272963523865, 0.20437294244766235, 0.11467251181602478, 0.2345312088727951, 0.3597087264060974, 0.10434451699256897, 0.12549418210983276, 0.13117000460624695, 0.32073187828063965, 0.10156887024641037, 0.09953023493289948, 0.10070674866437912, 0.11911719292402267, 0.1929129809141159, 0.2098059058189392, 0.22313255071640015, 0.1054404005408287, 0.1114087626338005, 0.21146434545516968, 0.0985431894659996, 0.3508930504322052, 0.11488091200590134, 0.10360308736562729, 0.24184122681617737, 0.04516760632395744, 0.34660908579826355, 0.18947431445121765, 0.044937893748283386, 0.04491627588868141, 0.21627168357372284, 0.0452745221555233, 0.2225302904844284, 0.04496525600552559, 0.10586321353912354, 0.11563684046268463, 0.19667582213878632, 0.10440991818904877, 0.045284900814294815, 0.11739258468151093, 0.04502690210938454, 0.11013487726449966, 0.044959377497434616, 0.12785020470619202, 0.209917351603508, 0.20278139412403107, 0.1120116338133812, 0.2133677750825882, 0.09968891739845276, 0.044935669749975204, 0.12139387428760529, 0.23504814505577087, 0.11638941615819931, 0.0452282540500164, 0.10085336863994598, 0.35352087020874023, 0.37321823835372925, 0.11561884731054306, 0.12089557200670242, 0.04508647695183754, 0.2230193167924881, 0.09774266183376312, 0.35873186588287354, 0.33495739102363586, 0.21530906856060028, 0.21584540605545044, 0.045355185866355896, 0.04571743682026863, 0.045690253376960754, 0.04552760720252991, 0.09417698532342911, 0.045587558299303055, 0.20903606712818146, 0.11006780713796616, 0.19920529425144196, 0.11361893266439438, 0.20249541103839874, 0.04555005580186844, 0.04554634541273117, 0.10681352764368057, 0.21411660313606262, 0.1876078099012375, 0.11969942599534988, 0.22190240025520325, 0.33187443017959595, 0.10755812376737595, 0.11486143618822098, 0.04565925523638725, 0.04567769169807434, 0.13688112795352936, 0.1232241690158844, 0.11384167522192001, 0.04569222033023834, 0.10521477460861206, 0.1058424711227417, 0.10929886251688004, 0.11541987955570221, 0.21357262134552002, 0.04567742720246315, 0.11314615607261658, 0.11475268751382828, 0.0459202378988266, 0.04576968401670456, 0.20609763264656067, 0.04543796554207802, 0.21216796338558197, 0.10861869901418686, 0.10938854515552521, 0.11753128468990326, 0.10968669503927231, 0.10167121142148972, 0.212791308760643, 0.04535842314362526, 0.19225497543811798, 0.04555843397974968, 0.04534301906824112, 0.04529392719268799, 0.12768781185150146, 0.10473790764808655, 0.11795896291732788, 0.1055336520075798, 0.10981518030166626, 0.19181936979293823, 0.12069056183099747, 0.12151245027780533, 0.0451846681535244, 0.21715271472930908, 0.2370602786540985, 0.3489076793193817, 0.1133110299706459, 0.04519790783524513, 0.21948696672916412, 0.21160227060317993, 0.19887231290340424, 0.11387607455253601, 0.04520910605788231, 0.22774145007133484, 0.11442924290895462, 0.045506663620471954, 0.10939190536737442, 0.11404192447662354, 0.10595786571502686, 0.04542890563607216, 0.04516002908349037, 0.04535246267914772, 0.22195348143577576, 0.11388596892356873, 0.10587284713983536, 0.09776358306407928, 0.04511300101876259, 0.12226343899965286, 0.18210247159004211, 0.10726374387741089, 0.3215092122554779, 0.11102896928787231, 0.1185341626405716, 0.04508587345480919, 0.21136215329170227, 0.04500241205096245, 0.21843881905078888, 0.0451505146920681, 0.10575337707996368, 0.2040681391954422, 0.045182965695858, 0.1141725555062294, 0.11887779086828232, 0.2109149545431137, 0.04506902024149895, 0.22165028750896454, 0.11039143055677414, 0.10873012989759445, 0.34498924016952515, 0.10675490647554398, 0.20507076382637024, 0.21788747608661652, 0.19936439394950867, 0.045222606509923935, 0.10666671395301819, 0.22036311030387878, 0.30768823623657227, 0.22079645097255707, 0.21544352173805237, 0.11273222416639328, 0.32312625646591187, 0.11168061941862106, 0.04540933668613434, 0.2039250284433365, 0.2153109312057495, 0.04567214474081993, 0.04571964591741562, 0.2154931277036667, 0.11047864705324173, 0.10611642152070999, 0.1893819272518158, 0.045942388474941254, 0.04603155702352524, 0.11736980080604553, 0.10111858695745468, 0.11055482923984528, 0.21034376323223114, 0.045762352645397186, 0.11403707414865494, 0.04574069753289223, 0.20571863651275635, 0.21924641728401184, 0.1921943873167038, 0.10510300099849701, 0.04587305337190628, 0.045766234397888184, 0.10883796215057373, 0.2215345799922943, 0.045820049941539764, 0.1153886541724205, 0.21953244507312775, 0.04571792483329773, 0.3360128402709961, 0.3444071114063263, 0.09025402367115021, 0.10620129853487015, 0.19865389168262482, 0.3147944509983063, 0.21751660108566284, 0.13392236828804016, 0.2112068235874176, 0.11524418741464615, 0.11830734461545944, 0.20179885625839233, 0.04605529084801674, 0.11130383610725403, 0.20150354504585266, 0.0462084598839283, 0.1080271452665329, 0.09829457849264145, 0.10053864866495132, 0.10872972011566162, 0.12685717642307281, 0.22532348334789276, 0.12774337828159332, 0.1713683307170868, 0.13181276619434357, 0.04633300006389618, 0.046448640525341034, 0.10464717447757721, 0.2195558398962021, 0.2178114950656891, 0.046185530722141266, 0.10938189178705215, 0.22001022100448608, 0.21274685859680176, 0.10567939281463623, 0.20840774476528168, 0.20892174541950226, 0.04620838537812233, 0.04650341346859932, 0.04645032435655594, 0.10722246021032333, 0.046279195696115494, 0.19872063398361206, 0.19970978796482086, 0.21897371113300323, 0.19382694363594055, 0.04647088050842285, 0.04623961076140404, 0.04630119353532791, 0.04628894105553627, 0.10040974617004395, 0.04639136791229248, 0.20120637118816376, 0.1150805875658989, 0.12236687541007996, 0.04613859951496124, 0.1274409294128418, 0.2181578427553177, 0.04615994170308113, 0.046158451586961746, 0.2086104303598404, 0.3281185030937195, 0.20957966148853302, 0.047181062400341034, 0.3138275444507599, 0.12626232206821442, 0.0462648868560791, 0.12631624937057495, 0.12001820653676987, 0.12030753493309021, 0.10870417952537537, 0.34630876779556274, 0.09799663722515106, 0.04629906266927719, 0.2016507089138031, 0.11292091757059097, 0.046363089233636856, 0.04621844366192818, 0.04628150165081024, 0.046334803104400635, 0.11939426511526108, 0.09868174046278, 0.2124413698911667, 0.2312575876712799, 0.12163079530000687, 0.04618191346526146, 0.21055102348327637, 0.04630288854241371, 0.3535406291484833, 0.19585514068603516, 0.19032646715641022, 0.09281561523675919, 0.11706100404262543, 0.10114096850156784, 0.11659840494394302, 0.23460720479488373, 0.11519123613834381, 0.19124969840049744, 0.04633735120296478, 0.341380774974823, 0.11834682524204254, 0.3404994606971741, 0.10976184159517288, 0.046479370445013046, 0.1133766770362854, 0.0465514212846756, 0.10050757974386215, 0.10074357688426971, 0.11266627162694931, 0.04657666012644768, 0.10875722765922546, 0.046785131096839905, 0.22633454203605652, 0.0466141477227211, 0.19221247732639313, 0.04643964394927025, 0.12862427532672882, 0.04697840288281441, 0.11834678798913956, 0.04666895791888237, 0.11317650973796844, 0.046378713101148605, 0.046591442078351974, 0.11379262804985046, 0.1048528254032135, 0.48191943764686584, 0.3217914402484894, 0.11371993273496628, 0.046366993337869644, 0.11107523739337921, 0.116954505443573, 0.11193631589412689, 0.23228304088115692, 0.10060052573680878, 0.04651297256350517, 0.1847105622291565, 0.11691568046808243, 0.046422503888607025, 0.10867337137460709, 0.10947983711957932, 0.19953198730945587, 0.0463448241353035, 0.21776916086673737, 0.10877043008804321, 0.10545415431261063, 0.22391514480113983, 0.0464114248752594, 0.3351929485797882, 0.046375539153814316, 0.11500432342290878, 0.4805235266685486, 0.1352654993534088, 0.11508722603321075, 0.23272567987442017, 0.12571007013320923, 0.10868300497531891, 0.04664347320795059, 0.20121514797210693, 0.1299276202917099, 0.2199399620294571, 0.10809818655252457, 0.20591431856155396, 0.04666280373930931, 0.22083322703838348, 0.046681541949510574, 0.21985700726509094, 0.0466831736266613, 0.04671052470803261, 0.10762958228588104, 0.1955765038728714, 0.21718217432498932, 0.11490198969841003, 0.04684813320636749, 0.19990159571170807, 0.20695124566555023, 0.046775467693805695, 0.10554748773574829, 0.11507879942655563, 0.12264513969421387, 0.11016017198562622, 0.12203902006149292, 0.046925801783800125, 0.10670952498912811, 0.0467071533203125, 0.10371125489473343, 0.12631931900978088, 0.11990571767091751, 0.10731709003448486, 0.04677155241370201, 0.04657812416553497, 0.12033642828464508, 0.12016643583774567, 0.11040958017110825, 0.04673514515161514, 0.11337178945541382, 0.11116282641887665, 0.21150192618370056, 0.0465315617620945, 0.0464153066277504, 0.32675859332084656, 0.3354141414165497, 0.046485159546136856, 0.10763413459062576, 0.1127970963716507, 0.09900786727666855, 0.11928657442331314, 0.04638388007879257, 0.11412858963012695, 0.10499606281518936, 0.04657972231507301, 0.19365428388118744, 0.33231139183044434, 0.0462845116853714, 0.10821180045604706, 0.10968475043773651, 0.04639926925301552, 0.1066022738814354, 0.2381390631198883, 0.22305122017860413, 0.117490254342556, 0.20576483011245728, 0.046329859644174576, 0.23572450876235962, 0.20185023546218872, 0.04635072872042656, 0.0462760366499424, 0.20253223180770874, 0.10591287910938263, 0.04643959179520607, 0.11025352030992508, 0.04648161306977272, 0.10735207051038742, 0.18621543049812317, 0.10912559926509857, 0.20157785713672638, 0.04627181589603424, 0.10991661995649338, 0.04663795232772827, 0.2382466048002243, 0.3259471356868744, 0.33097511529922485, 0.20939406752586365, 0.04627127945423126, 0.12123886495828629, 0.2086673378944397, 0.11287032812833786, 0.046413205564022064, 0.11006674915552139, 0.11519122868776321, 0.12753909826278687, 0.10358648747205734, 0.10792966932058334, 0.046549949795007706, 0.10684625804424286, 0.18566501140594482, 0.046459268778562546, 0.10465041548013687, 0.11338423192501068, 0.11727331578731537, 0.22728927433490753, 0.1231970563530922, 0.09377363324165344, 0.10709349811077118, 0.12590289115905762, 0.04637335240840912, 0.32006850838661194, 0.04633533954620361, 0.10079829394817352, 0.04636327549815178, 0.10951931029558182, 0.04624924808740616, 0.12262685596942902, 0.32209452986717224, 0.11897169798612595, 0.11576377600431442, 0.21901825070381165, 0.04631702974438667, 0.046347133815288544, 0.19741597771644592, 0.04643040895462036, 0.10682684183120728, 0.10377083718776703, 0.11100752651691437, 0.10147972404956818, 0.11831913143396378, 0.11202789098024368, 0.10788080841302872, 0.21944771707057953, 0.046307194977998734, 0.04615529999136925, 0.04624778404831886, 0.046284351497888565, 0.1131700649857521, 0.04608658701181412, 0.10862606018781662, 0.046042826026678085, 0.23521441221237183, 0.1140342727303505, 0.34493982791900635, 0.2132822722196579, 0.04622757062315941, 0.09967448562383652, 0.11852656304836273, 0.12031017988920212, 0.0985352173447609, 0.20931777358055115, 0.19923219084739685, 0.045973777770996094, 0.04610477387905121, 0.2012244313955307, 0.11439879983663559, 0.04595811292529106, 0.04606717452406883, 0.11438938230276108, 0.20355722308158875, 0.22717802226543427, 0.1089106947183609, 0.045944295823574066, 0.4838911294937134, 0.2260182648897171, 0.04590341076254845, 0.04596419259905815, 0.10756519436836243, 0.22044068574905396, 0.04634115844964981, 0.1018725261092186, 0.10385478287935257, 0.046213772147893906, 0.046290744096040726, 0.0461408793926239, 0.04612251743674278, 0.04609452560544014, 0.046065423637628555, 0.046072687953710556, 0.11045946925878525, 0.046105705201625824, 0.22568747401237488, 0.1105305403470993, 0.12013350427150726, 0.21583256125450134, 0.11033255606889725, 0.04599855840206146, 0.04585292562842369, 0.04578353092074394, 0.04598351567983627, 0.04581553116440773, 0.11159402132034302, 0.10092689841985703, 0.10131894052028656, 0.11748947203159332, 0.1254993975162506, 0.04552947357296944, 0.04576133191585541, 0.1029428243637085, 0.11409309506416321, 0.33699825406074524, 0.20239941775798798, 0.13012726604938507, 0.09636390209197998, 0.04543139785528183, 0.21234337985515594, 0.11339867860078812, 0.04569268599152565, 0.22732077538967133, 0.2107812762260437, 0.04541899636387825, 0.19400840997695923, 0.20292893052101135, 0.1992829144001007, 0.045469947159290314, 0.045425157994031906, 0.10066332668066025, 0.04570022597908974, 0.12278319895267487, 0.11363067477941513, 0.04543941468000412, 0.4872731864452362, 0.22909075021743774, 0.2085108608007431, 0.11742404848337173, 0.19941435754299164, 0.11341440677642822, 0.21777784824371338, 0.04551719129085541, 0.10555976629257202, 0.11266257613897324, 0.11080621927976608, 0.11213510483503342, 0.21511486172676086, 0.11520923674106598, 0.04567106440663338, 0.045617539435625076, 0.1255713403224945, 0.2067725956439972, 0.11328177154064178, 0.19861996173858643, 0.04581553861498833, 0.11188489943742752, 0.22066311538219452, 0.10379339009523392, 0.04555383324623108, 0.10623713582754135, 0.04557124897837639, 0.04555138945579529, 0.04551374912261963, 0.32355841994285583, 0.21563103795051575, 0.1158275455236435, 0.20354174077510834, 0.045551810413599014, 0.045543123036623, 0.1133502721786499, 0.222754567861557, 0.3378171920776367, 0.10264881700277328, 0.22157341241836548, 0.2327871173620224, 0.0966484323143959, 0.10269522666931152, 0.1124628484249115, 0.11368798464536667, 0.12611111998558044, 0.10646265000104904, 0.11595302820205688, 0.10588675737380981, 0.04585264250636101, 0.11223872005939484, 0.11962725222110748, 0.11063109338283539, 0.11593568325042725, 0.130316361784935, 0.11364759504795074, 0.4858456552028656, 0.10372366011142731, 0.3462212085723877, 0.11428328603506088, 0.227870911359787, 0.045679327100515366, 0.04588351026177406, 0.11855191737413406, 0.3620750606060028, 0.04596196860074997, 0.10939990729093552, 0.12144036591053009, 0.3495132327079773, 0.21437899768352509, 0.10482995957136154, 0.11095182597637177, 0.2165726274251938, 0.0461689792573452, 0.04584822431206703, 0.11806933581829071, 0.12357404828071594, 0.3461834788322449, 0.10902420431375504, 0.20685552060604095, 0.10340701043605804, 0.11227279901504517, 0.10411462187767029, 0.10042087733745575, 0.09806042909622192, 0.10447418689727783, 0.21483758091926575, 0.22530582547187805, 0.11569403856992722, 0.04611287638545036, 0.10159024596214294, 0.10757867246866226, 0.2212854027748108, 0.4837700426578522, 0.10223744064569473, 0.04605161026120186, 0.216897651553154, 0.04602518677711487, 0.18456117808818817, 0.09916543960571289, 0.1134958267211914, 0.11299416422843933, 0.1967351734638214, 0.11645937711000443, 0.11381718516349792, 0.04636441916227341, 0.11966655403375626, 0.11057823896408081, 0.19387920200824738, 0.046369314193725586, 0.22707004845142365, 0.11296515166759491, 0.11250125616788864, 0.04612230136990547, 0.11553128808736801, 0.0461602620780468, 0.22314004600048065, 0.11246122419834137, 0.11476316303014755, 0.12280602753162384, 0.34310904145240784, 0.23141296207904816, 0.1319415420293808, 0.046229347586631775, 0.04618782177567482, 0.046152032911777496, 0.046204645186662674, 0.3202286660671234, 0.20361129939556122, 0.046235982328653336, 0.04621697589755058, 0.046138714998960495, 0.04613105207681656, 0.11107759922742844, 0.04619690403342247, 0.48255813121795654, 0.1197149008512497, 0.04625086858868599, 0.1144883930683136, 0.3261539340019226, 0.04613610729575157, 0.20331548154354095, 0.10934282839298248, 0.10742904245853424, 0.10477602481842041, 0.12216439098119736, 0.09469279646873474, 0.11478937417268753, 0.12354233860969543, 0.0462234728038311, 0.20572513341903687, 0.13543324172496796, 0.1175195574760437, 0.22324363887310028, 0.10160989314317703, 0.1168808564543724, 0.12427081912755966, 0.10661933571100235, 0.2064613550901413, 0.19267597794532776, 0.19587554037570953, 0.12588955461978912, 0.1210579052567482, 0.046217113733291626, 0.046112142503261566, 0.11307872831821442, 0.1900298148393631, 0.1051681637763977, 0.04613037034869194, 0.11723456531763077, 0.19068112969398499, 0.11803245544433594, 0.19410555064678192, 0.10303574800491333, 0.19787488877773285, 0.0460936464369297, 0.11287146806716919, 0.11778442561626434, 0.09729889035224915, 0.12319609522819519, 0.11348249763250351, 0.10722573101520538, 0.2033630609512329, 0.3238146901130676, 0.09756523370742798, 0.04623427614569664, 0.11950169503688812, 0.10327430814504623, 0.2062593698501587, 0.31174325942993164, 0.11545708775520325, 0.10130521655082703, 0.11029112339019775, 0.046440206468105316, 0.34348317980766296, 0.04661225900053978, 0.32960644364356995, 0.10854486376047134, 0.10412026196718216, 0.10659272968769073, 0.21076203882694244, 0.1262119710445404, 0.1134844571352005, 0.04625967517495155, 0.1008206456899643, 0.1007620170712471, 0.1286441534757614, 0.1110338345170021, 0.10849089175462723, 0.11716398596763611, 0.04624346271157265, 0.046325456351041794, 0.22615043818950653, 0.12263684719800949, 0.1057528480887413, 0.36312365531921387, 0.0464172288775444, 0.04635732248425484, 0.10466783493757248, 0.11548925936222076, 0.11335375905036926, 0.112046979367733, 0.19080953299999237, 0.046202730387449265, 0.10942849516868591, 0.11467987298965454, 0.04623674973845482, 0.34741973876953125, 0.11777862906455994, 0.1313391923904419, 0.21444332599639893, 0.2114366739988327, 0.12036506086587906, 0.20064856112003326, 0.04642576724290848, 0.20432424545288086, 0.3372827470302582, 0.0461786612868309, 0.10602026432752609, 0.046274811029434204, 0.12304218113422394, 0.12215045094490051, 0.11023873090744019, 0.12894052267074585, 0.20408403873443604, 0.3602841794490814, 0.19905459880828857, 0.0463324598968029, 0.04635801166296005, 0.11777577549219131, 0.04647519439458847, 0.1942308247089386, 0.22590729594230652, 0.19616128504276276, 0.12109897285699844, 0.2013910412788391, 0.10192430019378662, 0.1975516974925995, 0.11127803474664688, 0.23504145443439484, 0.04642140865325928, 0.12164754420518875, 0.3133160471916199, 0.21409539878368378, 0.0920700877904892, 0.04645252972841263, 0.11420220881700516, 0.12699514627456665, 0.046429768204689026, 0.046463288366794586, 0.4801056683063507, 0.04647596925497055, 0.1151689738035202, 0.10519227385520935, 0.10983795672655106, 0.3073880970478058, 0.12492652982473373, 0.11435366421937943, 0.19958381354808807, 0.11208666861057281, 0.10492178052663803, 0.11906944215297699, 0.1383216828107834, 0.10700615495443344, 0.10771937668323517, 0.12768596410751343, 0.21568883955478668, 0.11229811608791351, 0.04661984369158745, 0.18929629027843475, 0.11097362637519836, 0.12006597220897675, 0.12426860630512238, 0.1233435645699501, 0.10989543050527573, 0.1040087565779686, 0.12539424002170563, 0.19289718568325043, 0.12882573902606964, 0.21167533099651337, 0.11933610588312149, 0.11334764957427979, 0.2152060717344284, 0.04655676335096359, 0.10041458159685135, 0.046591684222221375, 0.11536546051502228, 0.04666043445467949, 0.21559154987335205, 0.2281298190355301, 0.12362969666719437, 0.046752966940402985, 0.1251930445432663, 0.3372441232204437, 0.09885247051715851, 0.04651828482747078, 0.046443529427051544, 0.225997194647789, 0.11491260677576065, 0.09806568175554276, 0.10786175727844238, 0.10033906251192093, 0.3153357207775116, 0.200905904173851, 0.046434078365564346, 0.10335957258939743, 0.21417959034442902, 0.04681478813290596, 0.04650029167532921, 0.3140091001987457, 0.20191122591495514, 0.11330170929431915, 0.11629839241504669, 0.11262544989585876, 0.11083196103572845, 0.11322937160730362, 0.04653286561369896, 0.04661988839507103, 0.04683496430516243, 0.104340560734272, 0.32197320461273193, 0.046483177691698074, 0.21474550664424896, 0.20043279230594635, 0.10800499469041824, 0.10448196530342102, 0.0465489961206913, 0.04689503833651543, 0.12055040150880814, 0.32473689317703247, 0.3444331884384155, 0.0467565543949604, 0.20488972961902618, 0.04668106883764267, 0.1979334056377411, 0.10872947424650192, 0.19394324719905853, 0.12051510065793991, 0.11981906741857529, 0.046619199216365814, 0.1169217899441719, 0.11672964692115784, 0.10479089617729187, 0.046741731464862823, 0.04659125208854675, 0.04673121124505997, 0.10337354987859726, 0.12318296730518341, 0.046801432967185974, 0.205429345369339, 0.21859753131866455, 0.11861525475978851, 0.12038165330886841, 0.20871910452842712, 0.04690613970160484, 0.20219303667545319, 0.046497780829668045, 0.11251639574766159, 0.12601806223392487, 0.04649296775460243, 0.21355409920215607, 0.04651569202542305, 0.21224987506866455, 0.12467048317193985, 0.3257363736629486, 0.12134088575839996, 0.3416571021080017, 0.10374150425195694, 0.0464828684926033, 0.10966706275939941, 0.22302395105361938, 0.2133321762084961, 0.2130400538444519, 0.04666857793927193, 0.10925131291151047, 0.12984414398670197, 0.11152913421392441, 0.04659752920269966, 0.11928685754537582, 0.11767175793647766, 0.21081312000751495, 0.046603262424468994, 0.046631451696157455, 0.2115398347377777, 0.20985300838947296, 0.2122654914855957, 0.11748576164245605, 0.19936272501945496, 0.04664488881826401, 0.04667157307267189, 0.32355797290802, 0.12804444134235382, 0.13367386162281036, 0.11487812548875809, 0.11111988127231598, 0.10984980314970016, 0.12793777883052826, 0.11341390013694763, 0.11923196166753769, 0.046540647745132446, 0.1059524342417717, 0.10698161274194717, 0.046620480716228485, 0.19440333545207977, 0.0465429425239563, 0.10621543973684311, 0.10327661037445068, 0.04665388539433479, 0.04667734354734421, 0.1084483340382576, 0.12008276581764221, 0.1257036179304123, 0.19563467800617218, 0.2205132246017456, 0.046502213925123215, 0.11942879855632782, 0.11707564443349838, 0.10874059796333313, 0.10678031295537949, 0.04658161476254463, 0.2177133858203888, 0.1202927976846695, 0.11224912852048874, 0.10723475366830826, 0.04653318226337433, 0.046667639166116714, 0.04658612981438637, 0.11545287817716599, 0.04646017774939537, 0.11849304288625717, 0.1095062866806984, 0.04655659943819046, 0.11067643016576767, 0.3191623091697693, 0.19442297518253326, 0.21556876599788666, 0.10349112749099731, 0.1082010418176651, 0.04642901569604874, 0.18390977382659912, 0.0464819073677063, 0.1165415346622467, 0.04635447636246681, 0.22673271596431732, 0.20108331739902496, 0.21103769540786743, 0.1096903383731842, 0.11492796242237091, 0.10320211946964264, 0.2014741450548172, 0.10983147472143173, 0.046291716396808624, 0.10852241516113281, 0.04631662741303444, 0.04648108035326004, 0.2176826000213623, 0.13107410073280334, 0.046621788293123245, 0.11896675825119019, 0.11538077890872955, 0.11093291640281677, 0.2001132220029831, 0.122299425303936, 0.12146391719579697, 0.2019149363040924, 0.21791225671768188, 0.13392521440982819, 0.3545321226119995, 0.3381337821483612, 0.19957232475280762, 0.1884436309337616, 0.046480435878038406, 0.12215399742126465, 0.10893543064594269, 0.046508606523275375, 0.04652966931462288, 0.04641323164105415, 0.046426329761743546, 0.11175869405269623, 0.10718098282814026, 0.32195913791656494, 0.04629308730363846, 0.1225232407450676, 0.04637438803911209, 0.11477036774158478, 0.1892024278640747, 0.1115439161658287, 0.046420205384492874, 0.1025029718875885, 0.12972421944141388, 0.11478666961193085, 0.20251764357089996, 0.12625065445899963, 0.21759244799613953, 0.12161880731582642, 0.10892685502767563, 0.184302419424057, 0.2075330764055252, 0.23316919803619385, 0.1218908280134201, 0.11622362583875656, 0.04636822268366814, 0.21046403050422668, 0.04634471610188484, 0.20412024855613708, 0.11692220717668533, 0.11319322139024734, 0.3215670883655548, 0.11337533593177795, 0.10620512068271637, 0.21462181210517883, 0.11354120075702667, 0.046536222100257874, 0.1212797611951828, 0.046341776847839355, 0.10205407440662384, 0.10264261811971664, 0.04632728919386864, 0.04653352126479149, 0.09701167047023773, 0.04631965607404709, 0.046527422964572906, 0.3467321991920471, 0.046463947743177414, 0.04654816538095474, 0.10908026993274689, 0.1043349951505661, 0.11276160180568695, 0.04626515135169029, 0.04642458260059357, 0.046345047652721405, 0.22132661938667297, 0.046408846974372864, 0.04627025127410889, 0.11498266458511353, 0.20556004345417023, 0.09913045167922974, 0.10552805662155151, 0.11437816172838211, 0.1145346537232399, 0.34213459491729736, 0.10370486229658127, 0.0463029146194458, 0.21033833920955658, 0.10238375514745712, 0.04624961316585541, 0.11618203669786453, 0.11761900037527084, 0.04644571989774704, 0.19703730940818787, 0.21428674459457397, 0.10249858349561691, 0.10501451790332794, 0.046239133924245834, 0.04627625271677971, 0.11602864414453506, 0.12465011328458786, 0.046367716044187546, 0.19549071788787842, 0.11799953877925873, 0.1032896414399147, 0.12406285107135773, 0.3221937417984009, 0.11119266599416733, 0.12081143260002136, 0.046246279031038284, 0.10301807522773743, 0.195023775100708, 0.10054491460323334, 0.09429207444190979, 0.11456310003995895, 0.11051741242408752, 0.046288203448057175, 0.2143051028251648, 0.32610848546028137, 0.0462070032954216, 0.21374326944351196, 0.48150894045829773, 0.11433438211679459, 0.04622441157698631, 0.11003604531288147, 0.11096017807722092, 0.09966841340065002, 0.3258006274700165, 0.046304851770401, 0.04652238264679909, 0.19596941769123077, 0.1015734076499939, 0.11471611261367798, 0.20541425049304962, 0.11948177963495255, 0.11090612411499023, 0.22630560398101807, 0.09879928827285767, 0.4816771447658539, 0.20676349103450775, 0.046363864094018936, 0.20719164609909058, 0.1228712797164917, 0.10448867827653885, 0.11400418728590012, 0.04630565643310547, 0.2215529978275299, 0.21864351630210876, 0.3551395535469055, 0.10663747787475586, 0.09317512065172195, 0.11177649348974228, 0.04647937789559364, 0.48093122243881226, 0.11352629214525223, 0.04657737538218498, 0.046705830842256546, 0.19913238286972046, 0.11569497734308243, 0.20294377207756042, 0.04639650508761406, 0.046471305191516876, 0.04661412164568901, 0.22941748797893524, 0.11203363537788391, 0.10235396027565002, 0.3373511731624603, 0.046657744795084, 0.04653926566243172, 0.1143345832824707, 0.11973493546247482, 0.11483881622552872, 0.2223343551158905, 0.04641376808285713, 0.10641497373580933, 0.04640110582113266, 0.21294929087162018, 0.3319920599460602, 0.11683361977338791, 0.11636792123317719, 0.11158910393714905, 0.10100243240594864, 0.0467405803501606, 0.046427108347415924, 0.20390403270721436, 0.10387585312128067, 0.11910343170166016, 0.18661855161190033, 0.11331886053085327, 0.10547064244747162, 0.04650060459971428, 0.04651081934571266, 0.04635130986571312, 0.04640953615307808, 0.21720357239246368, 0.11647224426269531, 0.046512529253959656, 0.11104127764701843, 0.04634028300642967, 0.22633619606494904, 0.04661928117275238, 0.0949053019285202, 0.04635750874876976, 0.196950301527977, 0.34291791915893555, 0.04639846831560135, 0.10680659860372543, 0.21407361328601837, 0.12471643835306168, 0.10338643938302994, 0.11097358912229538, 0.11100276559591293, 0.12428286671638489, 0.22364629805088043, 0.21504241228103638, 0.20960624516010284, 0.22355832159519196, 0.10390247404575348, 0.11682122945785522, 0.3306361138820648, 0.046366553753614426, 0.2133786529302597, 0.22158969938755035, 0.21169595420360565, 0.10705022513866425, 0.12063491344451904, 0.11367443203926086, 0.046558596193790436, 0.10919354110956192, 0.04645891860127449, 0.3210844099521637, 0.04641765356063843, 0.04638593643903732, 0.046357136219739914, 0.21165186166763306, 0.3372844457626343, 0.10530940443277359, 0.10760744661092758, 0.10571767389774323, 0.11082716286182404, 0.11262518912553787, 0.11154421418905258, 0.11205064505338669, 0.11818663030862808, 0.21633337438106537, 0.046527713537216187, 0.04638918116688728, 0.12108854949474335, 0.19164197146892548, 0.10712363570928574, 0.11408240348100662, 0.20358061790466309, 0.1954096257686615, 0.12093557417392731, 0.11455950886011124, 0.1074422299861908, 0.04660332202911377, 0.046664152294397354, 0.2188742458820343, 0.12057650834321976, 0.04655750095844269, 0.10552575439214706, 0.11637841165065765, 0.04668256267905235, 0.11779334396123886, 0.2064153403043747, 0.11251015216112137, 0.11458735167980194, 0.11300195753574371, 0.04662651941180229, 0.11188581585884094, 0.11861556768417358, 0.21865570545196533, 0.21798299252986908, 0.19950386881828308, 0.11093927919864655, 0.11301563680171967, 0.0463351234793663, 0.04665803164243698, 0.10605726391077042, 0.11255493015050888, 0.2028198540210724, 0.10447533428668976, 0.046367108821868896, 0.10822000354528427, 0.19395814836025238, 0.10794930160045624, 0.10337694734334946, 0.1968182921409607, 0.04638237878680229, 0.04653950408101082, 0.110990971326828, 0.20968663692474365, 0.04637269675731659, 0.2123105525970459, 0.33069711923599243, 0.19587774574756622, 0.12348567694425583, 0.34597259759902954, 0.04632345959544182, 0.11193540692329407, 0.1160215362906456, 0.04647405818104744, 0.046481430530548096, 0.04652280732989311, 0.13605284690856934, 0.046419303864240646, 0.21534515917301178, 0.1998763084411621, 0.1205672025680542, 0.04636378958821297, 0.10704486817121506]\n",
            "Val loss 0.13148014420143941\n",
            "Val auc roc 0.4970255665047001\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fd60724-edab-44f4-c874-1b4053bf8d4e"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "547bf59d-20f4-4f79-b2d0-4eb5cea130d2"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54ad0abf-5252-4004-ceb7-4c98b549496f"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "0738429a-1370-4bf6-e960-bc2673f3467e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Text_Only_Informative]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2136c8c-dc8e-4a6f-c749-002a3d938c22"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-08-05 19:53:12.511322'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}