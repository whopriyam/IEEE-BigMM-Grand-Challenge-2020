{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Only_Informative_One_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2ef96cf2b5a42be8c85815d75d50f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a7dc5bc87ec4bf384ceacdd9a804cb1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_106cc30e35ad4b4ca9abe34c1b4e0ee0",
              "IPY_MODEL_253ca981fe934106b0c9b325085085c2"
            ]
          }
        },
        "6a7dc5bc87ec4bf384ceacdd9a804cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "106cc30e35ad4b4ca9abe34c1b4e0ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9834da69a8354f4e90d6288ece237a4b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9414c0cc34ba481c92cbdc25d0153a31"
          }
        },
        "253ca981fe934106b0c9b325085085c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_366dc4c901dc44e0ba2ac8e8050d055e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:08&lt;00:00, 26.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb6268ed96e34ffe87b26dc5a8f8dbfe"
          }
        },
        "9834da69a8354f4e90d6288ece237a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9414c0cc34ba481c92cbdc25d0153a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "366dc4c901dc44e0ba2ac8e8050d055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb6268ed96e34ffe87b26dc5a8f8dbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "994b092d19a84ca584727f18a125623c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8e495cefa79458596bbf2770e155f76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7dd2907e610e4af5b68a760f1bcbbbb6",
              "IPY_MODEL_54b9f1ef29464ba6965d31c256293d40"
            ]
          }
        },
        "d8e495cefa79458596bbf2770e155f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dd2907e610e4af5b68a760f1bcbbbb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6602b3fe74ae474b942ec888ebc5ce11",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2124,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2124,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b38cf70ccfd49d3aba20e9d24ec3e25"
          }
        },
        "54b9f1ef29464ba6965d31c256293d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8cf2e4be35947b0a54d1a5b6830c502",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2124/2124 [36:19&lt;00:00,  1.03s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b0bfee3ecaa451aad99f3dbea4bb3c3"
          }
        },
        "6602b3fe74ae474b942ec888ebc5ce11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b38cf70ccfd49d3aba20e9d24ec3e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8cf2e4be35947b0a54d1a5b6830c502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b0bfee3ecaa451aad99f3dbea4bb3c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efc282c0310b48678e66bc03fe33d5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59490bf08e644a5ab538366025dc52c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f4e6b30e68b842c886bd22c1d9296cde",
              "IPY_MODEL_066657de446d4e22967887bbd8c65407"
            ]
          }
        },
        "59490bf08e644a5ab538366025dc52c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4e6b30e68b842c886bd22c1d9296cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_daf27b9536394c10a460fcf21c9a80be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2124,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2124,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ce34eea6c8b4d9186c2bed56791a933"
          }
        },
        "066657de446d4e22967887bbd8c65407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b371235328634ea38a46a19944b1b1db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2124/2124 [37:06&lt;00:00,  1.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f24fcca55e2446ee84c440130934c287"
          }
        },
        "daf27b9536394c10a460fcf21c9a80be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ce34eea6c8b4d9186c2bed56791a933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b371235328634ea38a46a19944b1b1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f24fcca55e2446ee84c440130934c287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2378a1ecbd49465ea616aad54870369c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62494246374a4abf884b6064f45705ef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f777257ba5834609a0752b36afe22d77",
              "IPY_MODEL_cfa235c2071e4f6987f975bd6a9d5b69"
            ]
          }
        },
        "62494246374a4abf884b6064f45705ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f777257ba5834609a0752b36afe22d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94ec28d171fc496694b7f0c01a10adaa",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2124,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2124,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_695a1f77aedc447b8d53d52e78ccd176"
          }
        },
        "cfa235c2071e4f6987f975bd6a9d5b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a59ef9b77c94e5b863fa78d7e262653",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2124/2124 [37:15&lt;00:00,  1.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a50b342209594e25a810bd68e5a99ac7"
          }
        },
        "94ec28d171fc496694b7f0c01a10adaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "695a1f77aedc447b8d53d52e78ccd176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a59ef9b77c94e5b863fa78d7e262653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a50b342209594e25a810bd68e5a99ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "43646e00-65d3-4c8b-cf65-ea45e2d9aaaf"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 5.51 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0537e886-7f1a-4ba7-abd4-f16f9d531faf"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78ac0066-157c-4f39-e7fb-5d119b8591f7"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c2e1b720-1393-4856-9bf1-f8bd5fff7c5e"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c2484525-c302-43c4-beb3-dd3ee3f150d2"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "878620a4-234c-4cfd-a0b3-cc349e6acc87"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3034db56-816b-4c55-d3de-8090b25fbfef"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f85be8ac-f179-4647-ed9c-001d745e9f2f"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "e85662f0-3728-4265-8cc1-3c785fc5d6fe"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fcc7d6bc-85b8-4d31-e5fb-982e28d722cd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    @kathyonair @TKras @JPPetersonSprts @CharleyBe...\n",
              "1    #BIGNEWS: Some are using #MeToo campaign for p...\n",
              "2                      Anonymous posts weaken #MeToo  \n",
              "3    Edit | Anonymous posts weaken #MeToo   Those j...\n",
              "4    Taxidermy DeathsHead Moth by Oddity Asylum #ta...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "512494ba-365c-49bf-ecd7-eb2fc90f3869"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:15<00:00, 26912134.64B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cd977d2-5451-4d9f-bf7f-86b402bc067c"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 699982.43B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "233d167a-9f9a-46a6-db63-3d3763cd51df"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47509607-6a5a-458c-dde0-1ede81e5824e"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        df4 = df2.copy().reset_index(drop=True)\n",
        "        df5 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            # random.shuffle(text)\n",
        "            # text3 = ' '.join(text)\n",
        "            # df3['text'][i]=text3\n",
        "            # random.shuffle(text)\n",
        "            # text4 = ' '.join(text)\n",
        "            # df4['text'][i]=text4\n",
        "            # random.shuffle(text)\n",
        "            # text5 = ' '.join(text)\n",
        "            # df5['text'][i]=text5\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        # self.data = self.data.append(df3, ignore_index=True)\n",
        "        # self.data = self.data.append(df4, ignore_index=True)\n",
        "        # self.data = self.data.append(df5, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce82f624-925e-483b-dc32-a1a679acddd5"
      },
      "source": [
        "col_name = \"Image_Only_Informative\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862,
          "referenced_widgets": [
            "b2ef96cf2b5a42be8c85815d75d50f00",
            "6a7dc5bc87ec4bf384ceacdd9a804cb1",
            "106cc30e35ad4b4ca9abe34c1b4e0ee0",
            "253ca981fe934106b0c9b325085085c2",
            "9834da69a8354f4e90d6288ece237a4b",
            "9414c0cc34ba481c92cbdc25d0153a31",
            "366dc4c901dc44e0ba2ac8e8050d055e",
            "eb6268ed96e34ffe87b26dc5a8f8dbfe",
            "994b092d19a84ca584727f18a125623c",
            "d8e495cefa79458596bbf2770e155f76",
            "7dd2907e610e4af5b68a760f1bcbbbb6",
            "54b9f1ef29464ba6965d31c256293d40",
            "6602b3fe74ae474b942ec888ebc5ce11",
            "9b38cf70ccfd49d3aba20e9d24ec3e25",
            "b8cf2e4be35947b0a54d1a5b6830c502",
            "5b0bfee3ecaa451aad99f3dbea4bb3c3",
            "efc282c0310b48678e66bc03fe33d5a7",
            "59490bf08e644a5ab538366025dc52c7",
            "f4e6b30e68b842c886bd22c1d9296cde",
            "066657de446d4e22967887bbd8c65407",
            "daf27b9536394c10a460fcf21c9a80be",
            "7ce34eea6c8b4d9186c2bed56791a933",
            "b371235328634ea38a46a19944b1b1db",
            "f24fcca55e2446ee84c440130934c287",
            "2378a1ecbd49465ea616aad54870369c",
            "62494246374a4abf884b6064f45705ef",
            "f777257ba5834609a0752b36afe22d77",
            "cfa235c2071e4f6987f975bd6a9d5b69",
            "94ec28d171fc496694b7f0c01a10adaa",
            "695a1f77aedc447b8d53d52e78ccd176",
            "7a59ef9b77c94e5b863fa78d7e262653",
            "a50b342209594e25a810bd68e5a99ac7"
          ]
        },
        "outputId": "81356797-3a2f-4562-ddc7-48988bb67205"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New data length : 8497\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 2116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2ef96cf2b5a42be8c85815d75d50f00",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting fresh! Previous model state dict load unsuccessful\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "994b092d19a84ca584727f18a125623c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2124.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1860\n",
            "Train Losses : [0.18224872648715973, 0.08790236711502075, 0.24398978054523468, 0.1829872578382492, 2.8415422439575195, 3.761977434158325, 0.17929457128047943, 0.7617853879928589, 2.9591174125671387, 2.531780242919922, 0.7689111828804016, 0.5033732056617737, 0.26396113634109497, 0.3861364722251892, 0.10269073396921158, 0.2563939094543457, 1.3253235816955566, 0.38967621326446533, 0.5601561069488525, 0.22709354758262634, 0.858349084854126, 0.5180522799491882, 0.7364943623542786, 0.3104453384876251, 0.06512203067541122, 0.5684949159622192, 0.9640961289405823, 0.10928770899772644, 0.16682372987270355, 0.06593719124794006, 0.3187330365180969, 0.0895090326666832, 0.15401920676231384, 0.05719493329524994, 0.26949775218963623, 0.24940751492977142, 0.0743294507265091, 0.15600000321865082, 0.08231199532747269, 0.3272853195667267, 0.2139066457748413, 0.19463038444519043, 0.129813551902771, 0.06385939568281174, 0.6230365037918091, 0.0980260893702507, 0.304776132106781, 0.4915817677974701, 0.11191489547491074, 0.17272810637950897, 0.11225362867116928, 0.6529791355133057, 0.1193917989730835, 0.10191405564546585, 0.3575555682182312, 0.5247180461883545, 0.1645888090133667, 0.07303210347890854, 0.30804193019866943, 0.09507042169570923, 0.17873606085777283, 0.20380809903144836, 0.10864781588315964, 0.38069838285446167, 0.3801048994064331, 0.11409599334001541, 0.2882515490055084, 0.10530765354633331, 0.10658836364746094, 0.3504078984260559, 0.10898805409669876, 0.29602304100990295, 0.09132850915193558, 0.19688962399959564, 0.28903064131736755, 0.25116387009620667, 0.27753689885139465, 0.1408521831035614, 0.19717705249786377, 0.12255563586950302, 0.19957178831100464, 0.13286586105823517, 0.14906489849090576, 0.2397533357143402, 0.19179466366767883, 0.11737830191850662, 0.12622232735157013, 0.18173515796661377, 0.15266449749469757, 0.27684882283210754, 0.2036462277173996, 0.6085587739944458, 0.09841764718294144, 0.3527386486530304, 0.13410203158855438, 0.17903277277946472, 0.43956148624420166, 0.39252036809921265, 0.2209920585155487, 0.30068039894104004, 0.19541338086128235, 0.16327862441539764, 0.2862154543399811, 0.262724369764328, 0.24117545783519745, 0.09468849748373032, 0.23611943423748016, 0.2741684913635254, 0.28722068667411804, 0.16496169567108154, 0.16212236881256104, 0.13067935407161713, 0.14573368430137634, 0.32117754220962524, 0.21012245118618011, 0.21013393998146057, 0.1914067566394806, 0.2091819941997528, 0.11974728107452393, 0.22202244400978088, 0.14368855953216553, 0.1647575944662094, 0.1671847552061081, 0.12954726815223694, 0.18675751984119415, 0.1805778592824936, 0.24440708756446838, 0.1272052675485611, 0.18163138628005981, 0.2969280183315277, 0.16144296526908875, 0.18453219532966614, 0.18625187873840332, 0.1497984230518341, 0.14087887108325958, 0.19078192114830017, 0.12637829780578613, 0.13030384480953217, 0.1923859417438507, 0.12825700640678406, 0.24401061236858368, 0.06524629145860672, 0.2206072360277176, 0.1700679361820221, 0.21823452413082123, 0.08311448246240616, 0.16712114214897156, 0.2706384062767029, 0.2091863751411438, 0.19002315402030945, 0.13581860065460205, 0.45973244309425354, 0.18527957797050476, 0.17667090892791748, 0.1778920590877533, 0.09992524236440659, 0.11840222775936127, 0.16244465112686157, 0.11030225455760956, 0.20056283473968506, 0.24615474045276642, 0.3149838447570801, 0.1257377564907074, 0.20174600183963776, 0.1862267702817917, 0.22125767171382904, 0.18082298338413239, 0.17810676991939545, 0.15167105197906494, 0.24574777483940125, 0.1605646163225174, 0.21245872974395752, 0.20484130084514618, 0.1950112283229828, 0.18636970221996307, 0.19630639255046844, 0.17908836901187897, 0.29099616408348083, 0.17633849382400513, 0.14249803125858307, 0.15121179819107056, 0.1820559799671173, 0.2334877848625183, 0.2122785449028015, 0.18570369482040405, 0.1639397144317627, 0.12258284538984299, 0.13037680089473724, 0.28467416763305664, 0.11588837951421738, 0.15356776118278503, 0.14272192120552063, 0.12338168919086456, 0.1065584048628807, 0.2173282951116562, 0.16162645816802979, 0.2758275866508484, 0.11741012334823608, 0.2415635734796524, 0.15749509632587433, 0.19659404456615448, 0.14031679928302765, 0.09832565486431122, 0.08311892300844193, 0.19584009051322937, 0.1865905076265335, 0.13261725008487701, 0.2292110025882721, 0.191740021109581, 0.27820730209350586, 0.4295957684516907, 0.191706120967865, 0.2710929214954376, 0.29149776697158813, 0.24890482425689697, 0.2751883864402771, 0.13682226836681366, 0.2502133548259735, 0.1346985250711441, 0.1331256628036499, 0.16611018776893616, 0.23244483768939972, 0.14312882721424103, 0.28826624155044556, 0.1791623830795288, 0.2172091156244278, 0.11478634923696518, 0.19630166888237, 0.10429568588733673, 0.20639514923095703, 0.1720288246870041, 0.14831528067588806, 0.13265977799892426, 0.10593045502901077, 0.14843237400054932, 0.16416682302951813, 0.15615792572498322, 0.2155550718307495, 0.2372574508190155, 0.1864681839942932, 0.23282794654369354, 0.1475997418165207, 0.16317909955978394, 0.17217986285686493, 0.1857599914073944, 0.13655656576156616, 0.11465089023113251, 0.15236321091651917, 0.20560897886753082, 0.24987052381038666, 0.2550853192806244, 0.2052811086177826, 0.1918395310640335, 0.14725714921951294, 0.17458607256412506, 0.2497895359992981, 0.1455589085817337, 0.21019308269023895, 0.1322564035654068, 0.20664024353027344, 0.16996492445468903, 0.20544137060642242, 0.28251948952674866, 0.202318474650383, 0.10957778990268707, 0.1718672215938568, 0.24043209850788116, 0.1439042091369629, 0.16582874953746796, 0.1648675948381424, 0.22291578352451324, 0.3095704913139343, 0.206827312707901, 0.24668963253498077, 0.16688400506973267, 0.17588233947753906, 0.15256452560424805, 0.16576410830020905, 0.178831085562706, 0.18508906662464142, 0.16208456456661224, 0.19567330181598663, 0.1603057086467743, 0.20436623692512512, 0.15891259908676147, 0.13654625415802002, 0.11167308688163757, 0.167014479637146, 0.1331559717655182, 0.19634662568569183, 0.2197488397359848, 0.12819617986679077, 0.12852931022644043, 0.2039691060781479, 0.16348451375961304, 0.14429304003715515, 0.18731458485126495, 0.16166506707668304, 0.18285271525382996, 0.16602648794651031, 0.13945063948631287, 0.17285147309303284, 0.11636394262313843, 0.26846843957901, 0.17013822495937347, 0.12514430284500122, 0.10963363945484161, 0.23596106469631195, 0.19371157884597778, 0.22810815274715424, 0.13768544793128967, 0.20006157457828522, 0.19947004318237305, 0.1839454621076584, 0.13481859862804413, 0.1786336600780487, 0.17931371927261353, 0.25956931710243225, 0.2448032796382904, 0.14505934715270996, 0.13938605785369873, 0.2120639830827713, 0.2307865470647812, 0.1639261394739151, 0.10409095883369446, 0.1823221892118454, 0.14095623791217804, 0.18512897193431854, 0.14878417551517487, 0.1457524448633194, 0.22188898921012878, 0.1131349727511406, 0.23560605943202972, 0.16173386573791504, 0.1764785200357437, 0.16888359189033508, 0.1734478622674942, 0.11840801686048508, 0.1568528115749359, 0.16710053384304047, 0.2557995319366455, 0.15818651020526886, 0.1507793366909027, 0.1715484857559204, 0.15423882007598877, 0.12321209162473679, 0.17466765642166138, 0.10805659741163254, 0.22632692754268646, 0.17314240336418152, 0.15112099051475525, 0.197946235537529, 0.32658371329307556, 0.1367442011833191, 0.24284806847572327, 0.1894366592168808, 0.18535274267196655, 0.1092003881931305, 0.1731610894203186, 0.19910481572151184, 0.1890670359134674, 0.15142689645290375, 0.15437060594558716, 0.18642519414424896, 0.11833203583955765, 0.2459198385477066, 0.22608642280101776, 0.21255935728549957, 0.12947194278240204, 0.13999633491039276, 0.17363497614860535, 0.16507454216480255, 0.1621839702129364, 0.1289324164390564, 0.21815578639507294, 0.09392748028039932, 0.15896058082580566, 0.2139371782541275, 0.2641355097293854, 0.20681904256343842, 0.15916062891483307, 0.07448781281709671, 0.14394691586494446, 0.154344379901886, 0.20251771807670593, 0.19978274405002594, 0.2461615651845932, 0.155008926987648, 0.11540816724300385, 0.20202532410621643, 0.1921786367893219, 0.18756923079490662, 0.20933781564235687, 0.1892159879207611, 0.15849731862545013, 0.1581316888332367, 0.2446841150522232, 0.15022127330303192, 0.19899415969848633, 0.17253611981868744, 0.1221870705485344, 0.2555507719516754, 0.1924491822719574, 0.16668958961963654, 0.1935862898826599, 0.155032217502594, 0.185341477394104, 0.13321499526500702, 0.19744692742824554, 0.12397266924381256, 0.1892877221107483, 0.1390615850687027, 0.15566754341125488, 0.24187584221363068, 0.16016560792922974, 0.22962626814842224, 0.19279685616493225, 0.25547781586647034, 0.13995198905467987, 0.179546520113945, 0.20893187820911407, 0.1755802184343338, 0.1969798505306244, 0.18546585738658905, 0.23409509658813477, 0.1830132007598877, 0.1303965151309967, 0.16047462821006775, 0.1788097620010376, 0.2680586874485016, 0.16275881230831146, 0.16305379569530487, 0.1496565043926239, 0.16518859565258026, 0.09954006969928741, 0.15496794879436493, 0.12324847280979156, 0.17254741489887238, 0.17647211253643036, 0.16325068473815918, 0.21757405996322632, 0.2561796009540558, 0.18194758892059326, 0.1751912236213684, 0.14299502968788147, 0.22784987092018127, 0.16592596471309662, 0.20050176978111267, 0.15622593462467194, 0.2569464147090912, 0.1735856831073761, 0.15349605679512024, 0.1811164766550064, 0.1783190220594406, 0.15474234521389008, 0.1358865201473236, 0.16977626085281372, 0.17589916288852692, 0.14856630563735962, 0.14124594628810883, 0.11267705261707306, 0.16344054043293, 0.23601020872592926, 0.12253082543611526, 0.23289240896701813, 0.20513871312141418, 0.1527530997991562, 0.17852900922298431, 0.20632191002368927, 0.18848758935928345, 0.1893884688615799, 0.19315297901630402, 0.1892811357975006, 0.1254003345966339, 0.21862472593784332, 0.12122566252946854, 0.16147415339946747, 0.12849386036396027, 0.17332647740840912, 0.16965709626674652, 0.16785433888435364, 0.1729879528284073, 0.1914304941892624, 0.14780296385288239, 0.16627870500087738, 0.18179821968078613, 0.1364404559135437, 0.1420072317123413, 0.19631442427635193, 0.13219833374023438, 0.20799389481544495, 0.15549245476722717, 0.17764532566070557, 0.14312641322612762, 0.19196994602680206, 0.2114405483007431, 0.1590372920036316, 0.18602615594863892, 0.2019481062889099, 0.169663667678833, 0.18794535100460052, 0.177894726395607, 0.20899127423763275, 0.18549129366874695, 0.20640654861927032, 0.18449054658412933, 0.23212756216526031, 0.2063954472541809, 0.10487983375787735, 0.13313479721546173, 0.21555602550506592, 0.22334013879299164, 0.24132001399993896, 0.1571418195962906, 0.15902337431907654, 0.17694151401519775, 0.10981476306915283, 0.16321340203285217, 0.13264231383800507, 0.19900524616241455, 0.2219218760728836, 0.22501668334007263, 0.22806362807750702, 0.19882085919380188, 0.1902291476726532, 0.16373930871486664, 0.18189653754234314, 0.12176291644573212, 0.14931371808052063, 0.2089543491601944, 0.12368562817573547, 0.172686368227005, 0.12994909286499023, 0.17620933055877686, 0.15313510596752167, 0.3084539771080017, 0.23755380511283875, 0.17014195024967194, 0.1486164629459381, 0.18591471016407013, 0.21711435914039612, 0.1833803951740265, 0.21415190398693085, 0.24295008182525635, 0.18467049300670624, 0.15170305967330933, 0.15787909924983978, 0.14352057874202728, 0.13429957628250122, 0.1787462830543518, 0.20014484226703644, 0.17034022510051727, 0.1716604232788086, 0.20458835363388062, 0.1251250058412552, 0.14535506069660187, 0.21957194805145264, 0.16309651732444763, 0.16437079012393951, 0.22166912257671356, 0.13023576140403748, 0.16074858605861664, 0.12507496774196625, 0.2111411988735199, 0.2026234269142151, 0.17096787691116333, 0.17032305896282196, 0.20665371417999268, 0.18819837272167206, 0.14804647862911224, 0.19017837941646576, 0.1718449592590332, 0.16400013864040375, 0.20593930780887604, 0.16834625601768494, 0.1911180317401886, 0.19822324812412262, 0.17493237555027008, 0.190691739320755, 0.196590855717659, 0.1830010563135147, 0.18303893506526947, 0.16344238817691803, 0.15416784584522247, 0.20060668885707855, 0.19214873015880585, 0.1807003915309906, 0.19149956107139587, 0.14303232729434967, 0.20621715486049652, 0.12377210706472397, 0.11909125000238419, 0.16535399854183197, 0.1461474597454071, 0.20901158452033997, 0.16711291670799255, 0.14033626019954681, 0.15725786983966827, 0.19718559086322784, 0.1850418597459793, 0.23056846857070923, 0.11519046127796173, 0.20001015067100525, 0.17203842103481293, 0.202421173453331, 0.20192648470401764, 0.18958181142807007, 0.12285784631967545, 0.13228857517242432, 0.1606707125902176, 0.1952916830778122, 0.20133757591247559, 0.15131671726703644, 0.18260008096694946, 0.14579695463180542, 0.15267983078956604, 0.19162127375602722, 0.16941918432712555, 0.17575877904891968, 0.19204622507095337, 0.15335659682750702, 0.18390335142612457, 0.18221937119960785, 0.16783222556114197, 0.20173093676567078, 0.15042974054813385, 0.17727524042129517, 0.15575219690799713, 0.16308872401714325, 0.2246989607810974, 0.19996380805969238, 0.18060413002967834, 0.14665520191192627, 0.19156506657600403, 0.16872207820415497, 0.15523198246955872, 0.16900724172592163, 0.1535717099905014, 0.15854983031749725, 0.2180050015449524, 0.19673079252243042, 0.1765259951353073, 0.18820245563983917, 0.24649953842163086, 0.15733014047145844, 0.23140044510364532, 0.1681099683046341, 0.1854485720396042, 0.2414095103740692, 0.18509653210639954, 0.14110323786735535, 0.1774994283914566, 0.18881411850452423, 0.18885967135429382, 0.16494391858577728, 0.16786274313926697, 0.18959955871105194, 0.19544266164302826, 0.14932818710803986, 0.16298438608646393, 0.15994954109191895, 0.1263561248779297, 0.17694534361362457, 0.20381581783294678, 0.1800648719072342, 0.18197545409202576, 0.17070811986923218, 0.1814476102590561, 0.22032660245895386, 0.25618401169776917, 0.17220093309879303, 0.18352672457695007, 0.19760653376579285, 0.15530134737491608, 0.16896964609622955, 0.16132164001464844, 0.21279697120189667, 0.1743842512369156, 0.16837109625339508, 0.15235832333564758, 0.1802952140569687, 0.17834864556789398, 0.1565132737159729, 0.1709405481815338, 0.2090592086315155, 0.17171716690063477, 0.18178291618824005, 0.17092861235141754, 0.15143127739429474, 0.16739241778850555, 0.18000254034996033, 0.17107854783535004, 0.17127732932567596, 0.13097728788852692, 0.2130703628063202, 0.15662936866283417, 0.15417611598968506, 0.18000435829162598, 0.1577763557434082, 0.16406971216201782, 0.16649311780929565, 0.21478284895420074, 0.1560518443584442, 0.16385947167873383, 0.19237005710601807, 0.16075824201107025, 0.19731464982032776, 0.18018880486488342, 0.16994844377040863, 0.19069071114063263, 0.17282861471176147, 0.12857569754123688, 0.1666858196258545, 0.15917527675628662, 0.21143537759780884, 0.15647804737091064, 0.16698674857616425, 0.18244610726833344, 0.12029140442609787, 0.15789876878261566, 0.18459174036979675, 0.14938847720623016, 0.17687834799289703, 0.15023094415664673, 0.18631449341773987, 0.16357460618019104, 0.19140082597732544, 0.15462209284305573, 0.20599403977394104, 0.18691764771938324, 0.18427884578704834, 0.20913244783878326, 0.20693804323673248, 0.20623385906219482, 0.1467832773923874, 0.19052152335643768, 0.1636086255311966, 0.20241683721542358, 0.18071459233760834, 0.23970867693424225, 0.17670607566833496, 0.15592849254608154, 0.1653883159160614, 0.22727221250534058, 0.16149617731571198, 0.18512140214443207, 0.20970726013183594, 0.1649627983570099, 0.1666811853647232, 0.20542676746845245, 0.17345988750457764, 0.19119596481323242, 0.21377702057361603, 0.19130724668502808, 0.1636837124824524, 0.11879854649305344, 0.17034578323364258, 0.17132554948329926, 0.17804881930351257, 0.16298037767410278, 0.17925412952899933, 0.15705932676792145, 0.16109590232372284, 0.15251101553440094, 0.1584579050540924, 0.1710374653339386, 0.17082731425762177, 0.17199452221393585, 0.16313959658145905, 0.16772021353244781, 0.14374148845672607, 0.16463741660118103, 0.169075608253479, 0.18074901401996613, 0.17942380905151367, 0.18117918074131012, 0.15750069916248322, 0.178447425365448, 0.18213889002799988, 0.19726978242397308, 0.14772632718086243, 0.1742473542690277, 0.15686064958572388, 0.20654556155204773, 0.13592198491096497, 0.2086009383201599, 0.1845361441373825, 0.2154356986284256, 0.2174292951822281, 0.21351908147335052, 0.1568170189857483, 0.17627936601638794, 0.17645759880542755, 0.1629772037267685, 0.16453403234481812, 0.1900644749403, 0.1322770118713379, 0.1763412356376648, 0.20803910493850708, 0.18009251356124878, 0.16797086596488953, 0.18195047974586487, 0.19201844930648804, 0.1885760873556137, 0.1818094104528427, 0.1819000393152237, 0.21890407800674438, 0.15791410207748413, 0.1609971970319748, 0.15244659781455994, 0.1637190878391266, 0.15521760284900665, 0.19906705617904663, 0.1651773750782013, 0.13285788893699646, 0.16743215918540955, 0.1541662961244583, 0.17788384854793549, 0.13860578835010529, 0.19678954780101776, 0.16974441707134247, 0.19412045180797577, 0.15867064893245697, 0.22154295444488525, 0.14846891164779663, 0.16840125620365143, 0.16227205097675323, 0.125775545835495, 0.16827549040317535, 0.21316692233085632, 0.1683901995420456, 0.1569535732269287, 0.16955219209194183, 0.19348572194576263, 0.11944696307182312, 0.16669312119483948, 0.1802845001220703, 0.2403624802827835, 0.18657483160495758, 0.15416350960731506, 0.14775021374225616, 0.14717845618724823, 0.1611708551645279, 0.18456992506980896, 0.1509179323911667, 0.19757215678691864, 0.14550639688968658, 0.1656305342912674, 0.1844092309474945, 0.156839519739151, 0.14651021361351013, 0.1657356470823288, 0.14611348509788513, 0.16788426041603088, 0.15580634772777557, 0.19692230224609375, 0.154417484998703, 0.17055219411849976, 0.1621074676513672, 0.19165661931037903, 0.14051015675067902, 0.19063615798950195, 0.19081789255142212, 0.1905488520860672, 0.17585574090480804, 0.15014484524726868, 0.1877388209104538, 0.16310153901576996, 0.17639271914958954, 0.17600740492343903, 0.16339033842086792, 0.14849674701690674, 0.15526044368743896, 0.18777522444725037, 0.1860727220773697, 0.1988719254732132, 0.16980046033859253, 0.14019377529621124, 0.11030510812997818, 0.2072206288576126, 0.13353103399276733, 0.1654021441936493, 0.1450345665216446, 0.2116633653640747, 0.17923907935619354, 0.19642284512519836, 0.1515795886516571, 0.25771552324295044, 0.20699353516101837, 0.19547291100025177, 0.159272700548172, 0.17693066596984863, 0.14676563441753387, 0.1652621328830719, 0.1961406171321869, 0.1463087797164917, 0.1902027577161789, 0.12908704578876495, 0.17135266959667206, 0.20509284734725952, 0.21496273577213287, 0.18969815969467163, 0.2140624225139618, 0.19729197025299072, 0.1782226860523224, 0.18402241170406342, 0.1716279536485672, 0.16939546167850494, 0.17815153300762177, 0.18281695246696472, 0.17540794610977173, 0.15498073399066925, 0.18007387220859528, 0.18871065974235535, 0.17757941782474518, 0.1754831075668335, 0.17713011801242828, 0.17864836752414703, 0.16922175884246826, 0.17779649794101715, 0.16021700203418732, 0.17111429572105408, 0.20022335648536682, 0.17380006611347198, 0.1768663227558136, 0.16984319686889648, 0.18157991766929626, 0.16540448367595673, 0.220937117934227, 0.2212917059659958, 0.23288951814174652, 0.17137554287910461, 0.19338327646255493, 0.1382647305727005, 0.15852931141853333, 0.16562019288539886, 0.19333000481128693, 0.19604340195655823, 0.15210291743278503, 0.17419344186782837, 0.20983178913593292, 0.17169111967086792, 0.1577177345752716, 0.14730970561504364, 0.18448418378829956, 0.1847316473722458, 0.166781947016716, 0.20859754085540771, 0.17224247753620148, 0.18594494462013245, 0.1765766590833664, 0.17091497778892517, 0.20117349922657013, 0.1338520348072052, 0.13770480453968048, 0.19775202870368958, 0.1580989807844162, 0.19559316337108612, 0.24148835241794586, 0.1705622673034668, 0.2018250972032547, 0.1362656205892563, 0.16674676537513733, 0.17297211289405823, 0.19206763803958893, 0.18454678356647491, 0.1771775782108307, 0.1778874546289444, 0.14363734424114227, 0.18886370956897736, 0.13501900434494019, 0.1578391045331955, 0.17601199448108673, 0.18646465241909027, 0.19750361144542694, 0.13256613910198212, 0.17416460812091827, 0.1785825490951538, 0.17669233679771423, 0.16379663348197937, 0.17162610590457916, 0.22352445125579834, 0.17094506323337555, 0.17559869587421417, 0.17356714606285095, 0.15400251746177673, 0.18179009854793549, 0.1697428822517395, 0.15943321585655212, 0.19357949495315552, 0.14079336822032928, 0.1760925054550171, 0.13948161900043488, 0.1524476557970047, 0.16697177290916443, 0.18582509458065033, 0.18742002546787262, 0.18600012362003326, 0.16389445960521698, 0.16045895218849182, 0.2040168195962906, 0.16743257641792297, 0.21013441681861877, 0.16742254793643951, 0.14852124452590942, 0.19842301309108734, 0.16782459616661072, 0.171202152967453, 0.16636110842227936, 0.17948898673057556, 0.1649714857339859, 0.1734730303287506, 0.15881387889385223, 0.17146436870098114, 0.17864416539669037, 0.17033761739730835, 0.19304902851581573, 0.151956245303154, 0.16588450968265533, 0.1833907514810562, 0.19737622141838074, 0.17774221301078796, 0.13160505890846252, 0.1916770339012146, 0.22096310555934906, 0.15537525713443756, 0.18689629435539246, 0.18476466834545135, 0.14284156262874603, 0.15651240944862366, 0.18542717397212982, 0.16480180621147156, 0.18575511872768402, 0.18536998331546783, 0.14185389876365662, 0.18001249432563782, 0.13669517636299133, 0.17086823284626007, 0.1333180069923401, 0.20863394439220428, 0.1329212486743927, 0.22618116438388824, 0.1836543083190918, 0.14751417934894562, 0.15927357971668243, 0.18147949874401093, 0.1485767513513565, 0.16677525639533997, 0.1606070101261139, 0.16595016419887543, 0.2144055813550949, 0.16628174483776093, 0.16522856056690216, 0.22454454004764557, 0.17763735353946686, 0.14554549753665924, 0.18382230401039124, 0.15541021525859833, 0.15966859459877014, 0.22266104817390442, 0.17844246327877045, 0.16560997068881989, 0.16148391366004944, 0.18285955488681793, 0.1906101554632187, 0.18258455395698547, 0.15690122544765472, 0.1919899731874466, 0.15703225135803223, 0.182204008102417, 0.18353112041950226, 0.16979265213012695, 0.161900132894516, 0.15338467061519623, 0.13878142833709717, 0.16211673617362976, 0.15948472917079926, 0.19786199927330017, 0.19129633903503418, 0.17290548980236053, 0.1288498193025589, 0.15361514687538147, 0.22600510716438293, 0.2060658484697342, 0.19640754163265228, 0.14369770884513855, 0.17214947938919067, 0.17929202318191528, 0.14410512149333954, 0.17762373387813568, 0.15390218794345856, 0.17039456963539124, 0.16017664968967438, 0.21681910753250122, 0.15375417470932007, 0.16555428504943848, 0.18198755383491516, 0.18803520500659943, 0.18327198922634125, 0.17266780138015747, 0.18934884667396545, 0.24221111834049225, 0.2071073204278946, 0.14194464683532715, 0.16412560641765594, 0.20103870332241058, 0.11840630322694778, 0.13459095358848572, 0.17818474769592285, 0.2675521671772003, 0.22971759736537933, 0.14031068980693817, 0.17255190014839172, 0.1598636656999588, 0.13602477312088013, 0.15337088704109192, 0.18990109860897064, 0.17824310064315796, 0.1540941298007965, 0.12881483137607574, 0.1991017907857895, 0.15046316385269165, 0.1460549682378769, 0.18050913512706757, 0.14869056642055511, 0.18630678951740265, 0.1845310628414154, 0.18333059549331665, 0.16218900680541992, 0.18155108392238617, 0.1807192713022232, 0.12585106492042542, 0.188773512840271, 0.17136846482753754, 0.1902349591255188, 0.16579748690128326, 0.19619642198085785, 0.19656218588352203, 0.17230215668678284, 0.15199896693229675, 0.22036971151828766, 0.22120466828346252, 0.17255830764770508, 0.20150750875473022, 0.15812228620052338, 0.1576240360736847, 0.21728357672691345, 0.15082775056362152, 0.15789154171943665, 0.22104182839393616, 0.17908428609371185, 0.18545666337013245, 0.1610230654478073, 0.17811520397663116, 0.19379429519176483, 0.15779057145118713, 0.16128800809383392, 0.1600867509841919, 0.17634621262550354, 0.2002745121717453, 0.16840939223766327, 0.13407190144062042, 0.18971660733222961, 0.17193810641765594, 0.14756639301776886, 0.173326775431633, 0.17646710574626923, 0.1847863793373108, 0.21135137975215912, 0.15910151600837708, 0.16997605562210083, 0.16207392513751984, 0.16595867276191711, 0.20585963129997253, 0.16636055707931519, 0.22438126802444458, 0.1776909977197647, 0.18145471811294556, 0.1552126556634903, 0.16467684507369995, 0.1887122392654419, 0.13113228976726532, 0.1820726841688156, 0.1942121684551239, 0.14578646421432495, 0.1645108461380005, 0.1625775694847107, 0.1889389008283615, 0.16888932883739471, 0.15746280550956726, 0.14872556924819946, 0.1783517599105835, 0.15243585407733917, 0.16560232639312744, 0.1856379359960556, 0.20355917513370514, 0.18013088405132294, 0.17982254922389984, 0.17802397906780243, 0.20754660665988922, 0.16085149347782135, 0.1792522370815277, 0.15945187211036682, 0.21085841953754425, 0.1449725329875946, 0.20020118355751038, 0.13588516414165497, 0.2232009470462799, 0.18736113607883453, 0.19210447371006012, 0.1876610666513443, 0.19053274393081665, 0.16671183705329895, 0.16732893884181976, 0.21634118258953094, 0.15660828351974487, 0.22228264808654785, 0.2082337588071823, 0.1871120035648346, 0.13596853613853455, 0.19960013031959534, 0.17385388910770416, 0.1667117029428482, 0.1569465547800064, 0.20863185822963715, 0.167939692735672, 0.23688475787639618, 0.16821855306625366, 0.18647529184818268, 0.17355449497699738, 0.16082268953323364, 0.15684854984283447, 0.182514488697052, 0.18802618980407715, 0.18948237597942352, 0.17146135866641998, 0.16690364480018616, 0.1498037576675415, 0.16835612058639526, 0.14418447017669678, 0.2085779458284378, 0.17671261727809906, 0.19066444039344788, 0.18817290663719177, 0.16615533828735352, 0.17946457862854004, 0.1610885113477707, 0.1605413258075714, 0.16190578043460846, 0.18635468184947968, 0.1718015819787979, 0.18720927834510803, 0.15476195514202118, 0.22672437131404877, 0.20708729326725006, 0.1923021674156189, 0.15038219094276428, 0.17090708017349243, 0.14640603959560394, 0.20228895545005798, 0.15960527956485748, 0.20179037749767303, 0.16941119730472565, 0.18016110360622406, 0.17595070600509644, 0.19645701348781586, 0.1815769523382187, 0.14210917055606842, 0.1676311194896698, 0.16653135418891907, 0.17403548955917358, 0.1929078847169876, 0.16052314639091492, 0.1985529512166977, 0.18695619702339172, 0.14110933244228363, 0.17038072645664215, 0.14818519353866577, 0.1792750209569931, 0.17906923592090607, 0.145725816488266, 0.178206205368042, 0.14704088866710663, 0.18806493282318115, 0.15748628973960876, 0.1811949759721756, 0.15263903141021729, 0.16261173784732819, 0.16143402457237244, 0.16126197576522827, 0.13800303637981415, 0.19133645296096802, 0.20061473548412323, 0.1724025011062622, 0.17310892045497894, 0.18229076266288757, 0.17544244229793549, 0.1954653114080429, 0.17695200443267822, 0.13591638207435608, 0.18466630578041077, 0.16168001294136047, 0.15287692844867706, 0.18580858409404755, 0.19276970624923706, 0.13315117359161377, 0.17619188129901886, 0.1786760538816452, 0.16371583938598633, 0.1463574469089508, 0.20162074267864227, 0.1718011498451233, 0.13300065696239471, 0.17856799066066742, 0.13530193269252777, 0.14001820981502533, 0.17726312577724457, 0.1891539841890335, 0.18427734076976776, 0.1780521422624588, 0.13382399082183838, 0.14641232788562775, 0.17489856481552124, 0.14878663420677185, 0.15762624144554138, 0.1396019458770752, 0.19805708527565002, 0.19100865721702576, 0.15364310145378113, 0.17201481759548187, 0.1985340118408203, 0.19116522371768951, 0.17097757756710052, 0.24704010784626007, 0.16375264525413513, 0.1912417709827423, 0.17216575145721436, 0.14772431552410126, 0.1231985092163086, 0.18118463456630707, 0.19188503921031952, 0.1396504044532776, 0.21637174487113953, 0.17901234328746796, 0.17182596027851105, 0.13729146122932434, 0.20371799170970917, 0.18945518136024475, 0.18157994747161865, 0.14708349108695984, 0.1733696609735489, 0.19357457756996155, 0.21140995621681213, 0.1861613243818283, 0.2202948033809662, 0.15905305743217468, 0.20042210817337036, 0.1802862137556076, 0.16637501120567322, 0.12838143110275269, 0.2022252380847931, 0.1715816855430603, 0.1509794145822525, 0.17368634045124054, 0.19618307054042816, 0.1318667083978653, 0.1838310807943344, 0.17063720524311066, 0.14655418694019318, 0.20027299225330353, 0.13946329057216644, 0.17871691286563873, 0.16639962792396545, 0.17830230295658112, 0.19400127232074738, 0.19820165634155273, 0.17940130829811096, 0.16777865588665009, 0.1574297994375229, 0.18599547445774078, 0.17032846808433533, 0.18658094108104706, 0.18669497966766357, 0.1943507194519043, 0.22507955133914948, 0.15851950645446777, 0.15779681503772736, 0.15240225195884705, 0.2096768319606781, 0.1560327261686325, 0.18321189284324646, 0.19016259908676147, 0.22774899005889893, 0.18091781437397003, 0.1576905995607376, 0.1654035449028015, 0.17948053777217865, 0.17641253769397736, 0.17453224956989288, 0.15967489778995514, 0.1774967461824417, 0.16720324754714966, 0.1786969006061554, 0.1749202013015747, 0.17395702004432678, 0.15610335767269135, 0.19601841270923615, 0.21731610596179962, 0.19859226047992706, 0.1570417284965515, 0.23411500453948975, 0.17676421999931335, 0.14646688103675842, 0.168535977602005, 0.17857791483402252, 0.1766878068447113, 0.16416481137275696, 0.18373245000839233, 0.13439646363258362, 0.15078102052211761, 0.1801385134458542, 0.17217515408992767, 0.15493926405906677, 0.1527431160211563, 0.20747846364974976, 0.15459300577640533, 0.1623232066631317, 0.19770190119743347, 0.14443157613277435, 0.16450896859169006, 0.16910217702388763, 0.21643245220184326, 0.1741800308227539, 0.19624094665050507, 0.15000095963478088, 0.16899532079696655, 0.19582383334636688, 0.20375798642635345, 0.18405506014823914, 0.15188364684581757, 0.2044724076986313, 0.141755610704422, 0.1916208118200302, 0.16931882500648499, 0.17811259627342224, 0.18597827851772308, 0.20348592102527618, 0.17164203524589539, 0.1946457028388977, 0.19808532297611237, 0.19321738183498383, 0.15741656720638275, 0.17927396297454834, 0.16763891279697418, 0.17806659638881683, 0.16374064981937408, 0.14047488570213318, 0.19271127879619598, 0.21149633824825287, 0.16034278273582458, 0.18402710556983948, 0.17838303744792938, 0.17763051390647888, 0.1470419317483902, 0.14569495618343353, 0.1711769849061966, 0.17337669432163239, 0.1793002337217331, 0.1504366397857666, 0.20460158586502075, 0.20510470867156982, 0.16834840178489685, 0.17716635763645172, 0.17845970392227173, 0.17310991883277893, 0.19944196939468384, 0.17129014432430267, 0.1830570101737976, 0.1942743957042694, 0.15968190133571625, 0.17758435010910034, 0.14990831911563873, 0.15057304501533508, 0.17572596669197083, 0.21969914436340332, 0.17316095530986786, 0.13559490442276, 0.2103549838066101, 0.17214226722717285, 0.17409543693065643, 0.18126651644706726, 0.17475032806396484, 0.1762169897556305, 0.19657203555107117, 0.18218781054019928, 0.16212408244609833, 0.17837202548980713, 0.143213152885437, 0.17275618016719818, 0.17879202961921692, 0.17685844004154205, 0.1842740774154663, 0.13349764049053192, 0.17908066511154175, 0.176528662443161, 0.20296524465084076, 0.17192022502422333, 0.18558382987976074, 0.12073571979999542, 0.1457720547914505, 0.17113009095191956, 0.17790141701698303, 0.17022740840911865, 0.15767475962638855, 0.19326753914356232, 0.17868074774742126, 0.16009852290153503, 0.1518600881099701, 0.16669437289237976, 0.15631400048732758, 0.18002790212631226, 0.21776016056537628, 0.19152921438217163, 0.1903143674135208, 0.19408103823661804, 0.22316928207874298, 0.17899063229560852, 0.15049344301223755, 0.20186226069927216, 0.1842261403799057, 0.18975034356117249, 0.15779443085193634, 0.14568784832954407, 0.17928427457809448, 0.1577143520116806, 0.22198377549648285, 0.18333843350410461, 0.14595477283000946, 0.1719825714826584, 0.17548811435699463, 0.1708962470293045, 0.19553712010383606, 0.16499106585979462, 0.15557122230529785, 0.14570878446102142, 0.15990905463695526, 0.16537287831306458, 0.19433237612247467, 0.19276005029678345, 0.18068157136440277, 0.20992648601531982, 0.15761138498783112, 0.17381884157657623, 0.11559619009494781, 0.18321149051189423, 0.1670871078968048, 0.16236431896686554, 0.1585862636566162, 0.16065023839473724, 0.1714583933353424, 0.16823093593120575, 0.1469869613647461, 0.19514258205890656, 0.17477238178253174, 0.19356536865234375, 0.15591439604759216, 0.20063571631908417, 0.1724579781293869, 0.19074220955371857, 0.19232162833213806, 0.17108121514320374, 0.16009344160556793, 0.1621207743883133, 0.18208922445774078, 0.18518543243408203, 0.15183837711811066, 0.17791512608528137, 0.17925290763378143, 0.19440080225467682, 0.18008488416671753, 0.17909690737724304, 0.17349937558174133, 0.1486303061246872, 0.15660856664180756, 0.1774100810289383, 0.16755054891109467, 0.16237331926822662, 0.15927569568157196, 0.22849684953689575, 0.1500805765390396, 0.17324262857437134, 0.17398642003536224, 0.16527815163135529, 0.15369103848934174, 0.14941547811031342, 0.12230314314365387, 0.2068195641040802, 0.1744282990694046, 0.17641329765319824, 0.11892244964838028, 0.15344354510307312, 0.18560349941253662, 0.1590670347213745, 0.2296883761882782, 0.1468585580587387, 0.13592635095119476, 0.2184561938047409, 0.14443325996398926, 0.17846526205539703, 0.19781845808029175, 0.16362492740154266, 0.17321725189685822, 0.17096661031246185, 0.164295956492424, 0.18775758147239685, 0.15105921030044556, 0.11567629128694534, 0.21943092346191406, 0.17428629100322723, 0.2246120721101761, 0.13978350162506104, 0.16391845047473907, 0.1704905480146408, 0.1881401240825653, 0.16256685554981232, 0.13482481241226196, 0.20737577974796295, 0.17566417157649994, 0.17499367892742157, 0.2347697913646698, 0.22889460623264313, 0.1359875202178955, 0.17661353945732117, 0.16861547529697418, 0.16317586600780487, 0.17081667482852936, 0.1588333398103714, 0.19761395454406738, 0.19350796937942505, 0.17360162734985352, 0.17275956273078918, 0.16578546166419983, 0.17966267466545105, 0.15683914721012115, 0.18145789206027985, 0.17774339020252228, 0.19569680094718933, 0.15551523864269257, 0.1921052783727646, 0.13753074407577515, 0.21423012018203735, 0.18676236271858215, 0.1423998475074768, 0.20380209386348724, 0.18210653960704803, 0.16671957075595856, 0.11831095814704895, 0.20272399485111237, 0.18197424709796906, 0.18381664156913757, 0.20079049468040466, 0.15663234889507294, 0.15865440666675568, 0.208645299077034, 0.18354688584804535, 0.14505904912948608, 0.1704600602388382, 0.17260561883449554, 0.1515532284975052, 0.17180389165878296, 0.18764954805374146, 0.1804180145263672, 0.1543729305267334, 0.15030649304389954, 0.2090778350830078, 0.15017279982566833, 0.1741882562637329, 0.16565534472465515, 0.17038387060165405, 0.24301326274871826, 0.21236552298069, 0.1641818881034851, 0.18753358721733093, 0.1632363349199295, 0.17566509544849396, 0.14010538160800934, 0.1916293054819107, 0.17802397906780243, 0.17361631989479065, 0.16297590732574463, 0.17099399864673615, 0.2264317274093628, 0.17969273030757904, 0.17761357128620148, 0.17210496962070465, 0.19892627000808716, 0.17743097245693207, 0.20610347390174866, 0.20883134007453918, 0.17555376887321472, 0.13552804291248322, 0.13883757591247559, 0.18424339592456818, 0.1792818307876587, 0.1390816867351532, 0.15590070188045502, 0.16096219420433044, 0.14212137460708618, 0.13796915113925934, 0.20305992662906647, 0.18665245175361633, 0.14862138032913208, 0.15830779075622559, 0.1653953492641449, 0.16867564618587494, 0.16240274906158447, 0.19052517414093018, 0.19768597185611725, 0.1765771508216858, 0.20907756686210632, 0.17344851791858673, 0.1953488439321518, 0.14226669073104858, 0.18525263667106628, 0.17248781025409698, 0.1777336448431015, 0.15676620602607727, 0.20717285573482513, 0.14898712933063507, 0.18776796758174896, 0.15207892656326294, 0.17751455307006836, 0.17624931037425995, 0.17825646698474884, 0.1802811324596405, 0.18047036230564117, 0.17589786648750305, 0.1833466738462448, 0.18037430942058563, 0.20883207023143768, 0.1579611599445343, 0.1756361573934555, 0.18309345841407776, 0.18191486597061157, 0.1753760576248169, 0.1619747281074524, 0.17791257798671722, 0.15567778050899506, 0.16225352883338928, 0.20556055009365082, 0.1797761470079422, 0.17949675023555756, 0.15791727602481842, 0.18125586211681366, 0.18284685909748077, 0.1738867461681366, 0.16418488323688507, 0.1732049435377121, 0.195802241563797, 0.18556763231754303, 0.17465616762638092, 0.16631336510181427, 0.1753026247024536, 0.17371788620948792, 0.18989482522010803, 0.1744421422481537, 0.16773289442062378, 0.18119657039642334, 0.1745242178440094, 0.19670283794403076, 0.18551018834114075, 0.16379329562187195, 0.16358888149261475, 0.1572408527135849, 0.16316337883472443, 0.1672537922859192, 0.15833936631679535, 0.1958446502685547, 0.18985724449157715, 0.1840204894542694, 0.17883838713169098, 0.2036522924900055, 0.17427675426006317, 0.16418364644050598, 0.15179523825645447, 0.1670645922422409, 0.13218291103839874, 0.15716539323329926, 0.2036077380180359, 0.18355342745780945, 0.2094484269618988, 0.17710958421230316, 0.13450369238853455, 0.1875305026769638, 0.15633241832256317, 0.19005641341209412, 0.1736462414264679, 0.1672101616859436, 0.16959477961063385, 0.17197106778621674, 0.18306252360343933, 0.19584594666957855, 0.17069490253925323, 0.1416635364294052, 0.18628528714179993, 0.14489756524562836, 0.15641725063323975, 0.1701493114233017, 0.18122734129428864, 0.16675668954849243, 0.16454775631427765, 0.18472251296043396, 0.1752023547887802, 0.16208167374134064, 0.17084544897079468, 0.2122008353471756, 0.20104849338531494, 0.1668643355369568, 0.16770514845848083, 0.1875247210264206, 0.18093274533748627, 0.18915016949176788, 0.16247619688510895, 0.1553855836391449, 0.1827920526266098, 0.1451789289712906, 0.17916354537010193, 0.1836795210838318, 0.1768861562013626, 0.16108554601669312, 0.2110525369644165, 0.15262342989444733, 0.18899258971214294, 0.15207622945308685, 0.16749463975429535, 0.1810782104730606, 0.1708795130252838, 0.169931560754776, 0.1973225325345993, 0.1630006581544876, 0.17093607783317566, 0.1898602694272995, 0.17226703464984894, 0.18076619505882263, 0.1583828628063202, 0.17361091077327728, 0.1833251267671585, 0.19676408171653748, 0.15768522024154663, 0.17825284600257874, 0.17413298785686493, 0.1547684669494629, 0.20282742381095886, 0.16316485404968262, 0.21288903057575226, 0.17021024227142334, 0.18491999804973602, 0.16015872359275818, 0.18316124379634857, 0.17733819782733917, 0.17336632311344147, 0.1701500564813614, 0.18660889565944672, 0.1752191185951233, 0.1374005675315857, 0.18003785610198975, 0.16316187381744385, 0.1818002313375473, 0.18029741942882538, 0.17525677382946014, 0.15154460072517395, 0.18634623289108276, 0.16873888671398163, 0.15223294496536255, 0.1855453997850418, 0.1714150309562683, 0.14821279048919678, 0.1939903050661087, 0.15047143399715424, 0.18743683397769928, 0.14691269397735596, 0.15908782184123993, 0.19434386491775513, 0.18471847474575043, 0.17252111434936523, 0.14796525239944458, 0.22076718509197235, 0.172212615609169, 0.18133606016635895, 0.15755653381347656, 0.14782990515232086, 0.16373120248317719, 0.1671220362186432, 0.16314713656902313, 0.1653176099061966, 0.20837874710559845, 0.15688872337341309, 0.15275561809539795, 0.19513338804244995, 0.16491460800170898, 0.16029854118824005, 0.18136045336723328, 0.17519909143447876, 0.16177614033222198, 0.18450555205345154, 0.16350291669368744, 0.1933280974626541, 0.18045032024383545, 0.18469452857971191, 0.16349348425865173, 0.19150613248348236, 0.13777534663677216, 0.16634994745254517, 0.15430894494056702, 0.18347033858299255, 0.17292547225952148, 0.18145377933979034, 0.16918601095676422, 0.157506063580513, 0.14891746640205383, 0.20312140882015228, 0.15362513065338135, 0.19487081468105316, 0.17555701732635498, 0.1643216460943222, 0.14081406593322754, 0.16143563389778137, 0.1434011608362198, 0.1868225336074829, 0.16216513514518738, 0.16447561979293823, 0.16870509088039398, 0.18366628885269165, 0.18469154834747314, 0.1591995507478714, 0.16562150418758392, 0.16962121427059174, 0.19044464826583862, 0.21133892238140106, 0.141127347946167, 0.15769875049591064, 0.2104489505290985, 0.12991473078727722, 0.17622262239456177, 0.21478170156478882, 0.18510702252388, 0.18166399002075195, 0.15731294453144073, 0.18787474930286407, 0.19077694416046143, 0.16502900421619415, 0.18142259120941162, 0.18208537995815277, 0.15932327508926392, 0.13716840744018555, 0.18654100596904755, 0.15177226066589355, 0.1509968489408493, 0.13295993208885193, 0.17673304677009583, 0.1317203789949417, 0.1633024960756302, 0.1611047238111496, 0.18803945183753967, 0.16723039746284485, 0.188755601644516, 0.1836799681186676, 0.18949094414710999, 0.19887323677539825, 0.19693078100681305, 0.2091914266347885, 0.1859404742717743, 0.20394748449325562, 0.19014401733875275, 0.13989652693271637, 0.1600933074951172, 0.21153324842453003, 0.1397688239812851, 0.1760225147008896, 0.17967158555984497, 0.1302906572818756, 0.22146160900592804, 0.1511249542236328, 0.1994611918926239, 0.21666693687438965, 0.19912201166152954, 0.18697476387023926, 0.1599688082933426, 0.19741682708263397, 0.1563369184732437, 0.15796004235744476, 0.17921370267868042, 0.17605635523796082, 0.20537348091602325, 0.18597178161144257, 0.17429830133914948, 0.20603443682193756, 0.17000512778759003, 0.16635429859161377, 0.16296198964118958, 0.18159040808677673, 0.17565295100212097, 0.15948913991451263, 0.2082434594631195, 0.16561661660671234, 0.17520655691623688, 0.16675524413585663, 0.17168965935707092, 0.17125512659549713, 0.15106815099716187, 0.15278421342372894, 0.16820639371871948, 0.18301531672477722, 0.17512089014053345, 0.17436625063419342, 0.17995432019233704, 0.1811615377664566, 0.17094555497169495, 0.17792929708957672, 0.19374118745326996, 0.19613686203956604, 0.17096492648124695, 0.14331527054309845, 0.16118155419826508, 0.1406390219926834, 0.16698206961154938, 0.17966143786907196, 0.16608113050460815, 0.18302926421165466, 0.1800891011953354, 0.1798861026763916, 0.20788978040218353, 0.18476369976997375, 0.1627601683139801, 0.18135027587413788, 0.18332666158676147, 0.182665154337883, 0.17677685618400574, 0.1527545303106308, 0.1482710987329483, 0.1610690951347351, 0.1634770929813385, 0.1760062426328659, 0.16554000973701477, 0.18049931526184082, 0.1693304181098938, 0.15229445695877075, 0.18990324437618256, 0.18751022219657898, 0.20364655554294586, 0.18598376214504242, 0.17022503912448883, 0.16971389949321747, 0.16270077228546143, 0.19607089459896088, 0.17513735592365265, 0.15718835592269897, 0.1729033887386322, 0.17388901114463806, 0.20373329520225525, 0.16336308419704437, 0.16570614278316498, 0.1738652139902115, 0.17110703885555267, 0.18045614659786224, 0.16261233389377594, 0.1614411622285843, 0.1686539649963379, 0.17910516262054443, 0.14965112507343292, 0.16390803456306458, 0.1586044877767563, 0.16937735676765442, 0.20050232112407684, 0.19561819732189178, 0.2022111713886261, 0.15541067719459534, 0.18573826551437378, 0.17440806329250336, 0.17168228328227997, 0.1598649024963379, 0.184381902217865, 0.18822768330574036, 0.2100355625152588, 0.19649703800678253, 0.15619368851184845, 0.1815185844898224, 0.1987500637769699, 0.15372733771800995, 0.16156478226184845, 0.16856327652931213, 0.17652368545532227, 0.1808626800775528, 0.17837660014629364, 0.19221633672714233, 0.1963653266429901, 0.18517860770225525, 0.15951448678970337, 0.1964075267314911, 0.18096250295639038, 0.18320801854133606, 0.14531365036964417, 0.1597631722688675, 0.18049666285514832, 0.18214187026023865, 0.16254229843616486, 0.16755357384681702, 0.16939112544059753, 0.18871940672397614, 0.16761207580566406, 0.1949431449174881, 0.1668621152639389, 0.1711800992488861, 0.18659541010856628, 0.17930768430233002, 0.1801881045103073, 0.15115274488925934, 0.18144118785858154, 0.19742470979690552, 0.17311084270477295, 0.16886292397975922, 0.15417468547821045, 0.17872187495231628, 0.1527276486158371]\n",
            "Val loss 0.17342783358574815\n",
            "Val auc roc 0.5085877180440378\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efc282c0310b48678e66bc03fe33d5a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2124.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1741\n",
            "Train Losses : [0.17988154292106628, 0.1688029021024704, 0.16716039180755615, 0.19926206767559052, 0.18474268913269043, 0.17611157894134521, 0.17494039237499237, 0.18689553439617157, 0.16965912282466888, 0.15723156929016113, 0.1863851547241211, 0.1621180772781372, 0.19168449938297272, 0.1725756824016571, 0.1913403570652008, 0.16851483285427094, 0.1829129308462143, 0.1690703183412552, 0.1758507937192917, 0.1656571328639984, 0.17221803963184357, 0.14790615439414978, 0.1732349544763565, 0.16909542679786682, 0.16900815069675446, 0.16801127791404724, 0.16723962128162384, 0.18854539096355438, 0.20227237045764923, 0.18333877623081207, 0.18027649819850922, 0.18014028668403625, 0.1618279367685318, 0.16894476115703583, 0.16764332354068756, 0.18070092797279358, 0.2004781812429428, 0.16729386150836945, 0.20160053670406342, 0.15594637393951416, 0.17449820041656494, 0.18091744184494019, 0.18083471059799194, 0.16036356985569, 0.1941012591123581, 0.1758870929479599, 0.16958220303058624, 0.170546293258667, 0.1620170623064041, 0.1936161071062088, 0.16828346252441406, 0.19115914404392242, 0.1809592992067337, 0.16360296308994293, 0.14971868693828583, 0.17940333485603333, 0.17194271087646484, 0.16748636960983276, 0.16388460993766785, 0.1979576051235199, 0.18174506723880768, 0.18180130422115326, 0.16876870393753052, 0.15689215064048767, 0.17564979195594788, 0.15287692844867706, 0.169086292386055, 0.1693994402885437, 0.19716119766235352, 0.17073345184326172, 0.19242045283317566, 0.18090900778770447, 0.17609290778636932, 0.18066084384918213, 0.1669783741235733, 0.1988973617553711, 0.16035790741443634, 0.17861998081207275, 0.1790449619293213, 0.17778874933719635, 0.1704702377319336, 0.183125302195549, 0.17213116586208344, 0.1729750782251358, 0.1684054434299469, 0.16925044357776642, 0.15716588497161865, 0.15879712998867035, 0.15712307393550873, 0.18151044845581055, 0.18560843169689178, 0.16505686938762665, 0.18769535422325134, 0.17917929589748383, 0.17814776301383972, 0.1801312118768692, 0.19209566712379456, 0.17954063415527344, 0.16783742606639862, 0.1860889047384262, 0.17012539505958557, 0.16767340898513794, 0.1914798617362976, 0.19160053133964539, 0.17629112303256989, 0.17706021666526794, 0.17660003900527954, 0.1864289790391922, 0.17794960737228394, 0.15353530645370483, 0.17435482144355774, 0.1759853959083557, 0.18821734189987183, 0.17288702726364136, 0.1745043396949768, 0.17991037666797638, 0.1723337322473526, 0.18179181218147278, 0.17912456393241882, 0.14512185752391815, 0.15701602399349213, 0.14553797245025635, 0.17293283343315125, 0.1643141508102417, 0.1694422960281372, 0.18103867769241333, 0.17373059689998627, 0.1705474853515625, 0.17761878669261932, 0.17734578251838684, 0.19991184771060944, 0.15801340341567993, 0.15258635580539703, 0.16766828298568726, 0.17710667848587036, 0.16543979942798615, 0.1898149996995926, 0.1600794643163681, 0.1671123504638672, 0.17411834001541138, 0.16992126405239105, 0.17600926756858826, 0.18204207718372345, 0.17160649597644806, 0.16470269858837128, 0.1861453652381897, 0.17563585937023163, 0.1677580624818802, 0.1682468205690384, 0.17904986441135406, 0.15605862438678741, 0.17408017814159393, 0.16965679824352264, 0.18806758522987366, 0.20168906450271606, 0.1928487867116928, 0.17002925276756287, 0.16805320978164673, 0.17069949209690094, 0.19564050436019897, 0.17798104882240295, 0.17923566699028015, 0.1785399168729782, 0.15585090219974518, 0.16953539848327637, 0.16209524869918823, 0.2034383863210678, 0.2221565842628479, 0.17755377292633057, 0.18172834813594818, 0.1697002649307251, 0.1685166358947754, 0.1954498589038849, 0.1672598272562027, 0.18721605837345123, 0.17568399012088776, 0.1624830812215805, 0.1706046164035797, 0.17182877659797668, 0.17687419056892395, 0.16172996163368225, 0.17980408668518066, 0.16467437148094177, 0.17744964361190796, 0.17729851603507996, 0.17658697068691254, 0.19215154647827148, 0.16859382390975952, 0.18436665832996368, 0.1824028640985489, 0.18779006600379944, 0.1975659430027008, 0.16914217174053192, 0.1801370531320572, 0.17407631874084473, 0.18155166506767273, 0.17381468415260315, 0.1860535591840744, 0.18308715522289276, 0.17684942483901978, 0.1723763793706894, 0.17972888052463531, 0.16665785014629364, 0.18151693046092987, 0.1703980565071106, 0.18188931047916412, 0.1669890135526657, 0.17618687450885773, 0.1591862589120865, 0.17150063812732697, 0.16234086453914642, 0.17695286870002747, 0.1707109957933426, 0.16641879081726074, 0.18480563163757324, 0.1618753969669342, 0.16831432282924652, 0.16708828508853912, 0.17222294211387634, 0.1671726405620575, 0.1747339963912964, 0.16498994827270508, 0.16590559482574463, 0.1832154542207718, 0.17314310371875763, 0.17812277376651764, 0.17242880165576935, 0.16200996935367584, 0.19504445791244507, 0.18785232305526733, 0.1831488311290741, 0.2085559070110321, 0.17040081322193146, 0.16570548713207245, 0.16963142156600952, 0.17884692549705505, 0.17504657804965973, 0.16582368314266205, 0.1705254316329956, 0.16851218044757843, 0.16199755668640137, 0.1577717363834381, 0.1554732471704483, 0.16091640293598175, 0.15089304745197296, 0.1743413805961609, 0.16538473963737488, 0.17821809649467468, 0.17469246685504913, 0.16662688553333282, 0.17501820623874664, 0.16600795090198517, 0.15778981149196625, 0.16540838778018951, 0.1720498949289322, 0.16483423113822937, 0.1744745969772339, 0.16875119507312775, 0.19089548289775848, 0.1812761127948761, 0.17611685395240784, 0.17078694701194763, 0.17374257743358612, 0.1699252724647522, 0.1666090339422226, 0.17229336500167847, 0.19481414556503296, 0.18286707997322083, 0.1577000766992569, 0.1831049621105194, 0.1706790179014206, 0.17491932213306427, 0.16065092384815216, 0.19670990109443665, 0.18080398440361023, 0.17935746908187866, 0.16012164950370789, 0.17920778691768646, 0.15614192187786102, 0.17833007872104645, 0.1974792182445526, 0.19388584792613983, 0.16860410571098328, 0.16519558429718018, 0.15805959701538086, 0.19838127493858337, 0.1525629758834839, 0.16801303625106812, 0.1724526584148407, 0.15171095728874207, 0.16040246188640594, 0.1666422188282013, 0.18446065485477448, 0.1798916608095169, 0.20473217964172363, 0.15264807641506195, 0.17651447653770447, 0.15733303129673004, 0.16439667344093323, 0.18306709825992584, 0.15047253668308258, 0.1677735298871994, 0.18403135240077972, 0.17596234381198883, 0.1715380996465683, 0.167883962392807, 0.1878557801246643, 0.15516717731952667, 0.16253983974456787, 0.18400469422340393, 0.16419921815395355, 0.18050293624401093, 0.16117623448371887, 0.14556655287742615, 0.156640887260437, 0.16563467681407928, 0.16521991789340973, 0.18064309656620026, 0.1830412894487381, 0.16512343287467957, 0.16375787556171417, 0.16455063223838806, 0.18696512281894684, 0.18949760496616364, 0.20084407925605774, 0.16365480422973633, 0.17533600330352783, 0.17479072511196136, 0.18225839734077454, 0.19228103756904602, 0.19169598817825317, 0.19306643307209015, 0.1861398071050644, 0.1859409064054489, 0.16755995154380798, 0.18387958407402039, 0.17073869705200195, 0.15243147313594818, 0.15718306601047516, 0.20120853185653687, 0.1774858832359314, 0.16301022469997406, 0.18723493814468384, 0.1901436299085617, 0.1621701568365097, 0.20204675197601318, 0.16043783724308014, 0.17874382436275482, 0.17918278276920319, 0.14519600570201874, 0.18292687833309174, 0.16570676863193512, 0.174155130982399, 0.18137623369693756, 0.18438926339149475, 0.16161856055259705, 0.18576264381408691, 0.16724832355976105, 0.15130969882011414, 0.17041897773742676, 0.17198623716831207, 0.16892467439174652, 0.16258201003074646, 0.17072148621082306, 0.1916123628616333, 0.17555831372737885, 0.17107756435871124, 0.19011352956295013, 0.1733148843050003, 0.1801084727048874, 0.16776569187641144, 0.16078999638557434, 0.151978999376297, 0.1731112003326416, 0.16738151013851166, 0.20359957218170166, 0.1752677708864212, 0.1690523624420166, 0.17792770266532898, 0.1636708527803421, 0.1724839210510254, 0.16625013947486877, 0.18188664317131042, 0.1779697835445404, 0.16265255212783813, 0.1475381702184677, 0.1772562861442566, 0.17565113306045532, 0.14673194289207458, 0.183218851685524, 0.1891823559999466, 0.1634259968996048, 0.18256661295890808, 0.16922643780708313, 0.16592596471309662, 0.16637445986270905, 0.16169707477092743, 0.19487807154655457, 0.17587296664714813, 0.1860324591398239, 0.17590074241161346, 0.19538666307926178, 0.16618314385414124, 0.15321233868598938, 0.18253450095653534, 0.18185265362262726, 0.17062383890151978, 0.1662522703409195, 0.16552400588989258, 0.18166866898536682, 0.18602025508880615, 0.1728786826133728, 0.1822875738143921, 0.17798155546188354, 0.18223273754119873, 0.16116555035114288, 0.18091906607151031, 0.17236687242984772, 0.17156477272510529, 0.18504655361175537, 0.17074896395206451, 0.1684267371892929, 0.1677131950855255, 0.16449332237243652, 0.14212267100811005, 0.18829123675823212, 0.16882435977458954, 0.16353924572467804, 0.16338902711868286, 0.15709124505519867, 0.16017618775367737, 0.18089714646339417, 0.1947028934955597, 0.17071467638015747, 0.18501390516757965, 0.18546462059020996, 0.18132100999355316, 0.19981706142425537, 0.16291198134422302, 0.16678756475448608, 0.17652900516986847, 0.1671009510755539, 0.15405648946762085, 0.16099503636360168, 0.16714182496070862, 0.16448000073432922, 0.1608169823884964, 0.1796829104423523, 0.1871698945760727, 0.17151276767253876, 0.20176362991333008, 0.18610504269599915, 0.19055677950382233, 0.19237260520458221, 0.15818603336811066, 0.15757831931114197, 0.1776876598596573, 0.18309451639652252, 0.17314296960830688, 0.192299947142601, 0.19406944513320923, 0.17497046291828156, 0.1556916981935501, 0.17542807757854462, 0.16800802946090698, 0.1882602870464325, 0.1870875358581543, 0.19249974191188812, 0.15888828039169312, 0.1565256118774414, 0.1739419549703598, 0.1891379952430725, 0.15673179924488068, 0.17846110463142395, 0.18774810433387756, 0.19438354671001434, 0.16544771194458008, 0.1525699943304062, 0.16473188996315002, 0.16348962485790253, 0.1663125455379486, 0.18324151635169983, 0.1936076283454895, 0.16199199855327606, 0.16696693003177643, 0.18172720074653625, 0.15756690502166748, 0.17925910651683807, 0.1655745804309845, 0.1755254864692688, 0.18236348032951355, 0.16143402457237244, 0.1577441543340683, 0.18501563370227814, 0.195002481341362, 0.1925397366285324, 0.16901203989982605, 0.16920612752437592, 0.17471714317798615, 0.1778849959373474, 0.19435198605060577, 0.191217303276062, 0.1574832648038864, 0.16725420951843262, 0.1692156046628952, 0.18569904565811157, 0.19075901806354523, 0.17277739942073822, 0.19590303301811218, 0.1692429780960083, 0.16326533257961273, 0.15431033074855804, 0.17603540420532227, 0.1816619485616684, 0.18828994035720825, 0.17760421335697174, 0.17100898921489716, 0.16038236021995544, 0.17438898980617523, 0.1752324402332306, 0.17303481698036194, 0.1664520800113678, 0.18687684834003448, 0.18639157712459564, 0.1854867786169052, 0.16733378171920776, 0.1800922006368637, 0.19188471138477325, 0.1691162884235382, 0.17429952323436737, 0.17691679298877716, 0.17314261198043823, 0.1779545396566391, 0.1614573895931244, 0.1637430638074875, 0.17118798196315765, 0.20837968587875366, 0.15923161804676056, 0.16014666855335236, 0.16029618680477142, 0.182545006275177, 0.1467117816209793, 0.1838676780462265, 0.1829230934381485, 0.16279277205467224, 0.15900853276252747, 0.1701238453388214, 0.1799512505531311, 0.16793788969516754, 0.1600256860256195, 0.17058156430721283, 0.15661168098449707, 0.15566912293434143, 0.16713398694992065, 0.1822669953107834, 0.17600680887699127, 0.17308995127677917, 0.16717638075351715, 0.16884571313858032, 0.18822705745697021, 0.1807386428117752, 0.1730147898197174, 0.18120570480823517, 0.18953438103199005, 0.17917174100875854, 0.18964159488677979, 0.17062969505786896, 0.15826191008090973, 0.16321679949760437, 0.1708512306213379, 0.16123157739639282, 0.1781534105539322, 0.1789930909872055, 0.16511480510234833, 0.1669209748506546, 0.1968522071838379, 0.18903113901615143, 0.17121537029743195, 0.15398377180099487, 0.18126848340034485, 0.19483645260334015, 0.1535850465297699, 0.18632739782333374, 0.17743951082229614, 0.1607033610343933, 0.1817697137594223, 0.1759260594844818, 0.1862836480140686, 0.18503645062446594, 0.15232974290847778, 0.17487919330596924, 0.17500686645507812, 0.17600886523723602, 0.18827907741069794, 0.1927371770143509, 0.15192298591136932, 0.19022732973098755, 0.1868460774421692, 0.17145857214927673, 0.16732624173164368, 0.1753322035074234, 0.18155646324157715, 0.17577849328517914, 0.18682298064231873, 0.16293585300445557, 0.16162608563899994, 0.19784918427467346, 0.17160648107528687, 0.16386394202709198, 0.19681374728679657, 0.18387563526630402, 0.18769550323486328, 0.1742626577615738, 0.18971417844295502, 0.185185506939888, 0.1665201336145401, 0.19384844601154327, 0.15271563827991486, 0.1662922352552414, 0.18388397991657257, 0.1886294186115265, 0.16854402422904968, 0.1782788783311844, 0.20136597752571106, 0.1714157909154892, 0.18196065723896027, 0.1842893660068512, 0.18545767664909363, 0.1648341417312622, 0.17979004979133606, 0.20152662694454193, 0.1718963235616684, 0.16431285440921783, 0.17049024999141693, 0.1656012237071991, 0.1678946614265442, 0.177524134516716, 0.18904376029968262, 0.173540398478508, 0.18218083679676056, 0.18660902976989746, 0.1673811972141266, 0.18689000606536865, 0.18693768978118896, 0.16592340171337128, 0.18124228715896606, 0.1934388428926468, 0.17948594689369202, 0.15169909596443176, 0.1653069704771042, 0.16708041727542877, 0.1726454645395279, 0.16768121719360352, 0.1650478094816208, 0.16989141702651978, 0.17245125770568848, 0.18261142075061798, 0.17798130214214325, 0.18313783407211304, 0.17183004319667816, 0.17404548823833466, 0.16796018183231354, 0.17196638882160187, 0.1774781048297882, 0.18250277638435364, 0.1853601038455963, 0.18079078197479248, 0.17434030771255493, 0.18175330758094788, 0.1706637293100357, 0.1849031001329422, 0.17364124953746796, 0.19293594360351562, 0.16430972516536713, 0.17794300615787506, 0.1788853257894516, 0.18861836194992065, 0.1867409348487854, 0.16864554584026337, 0.1733270138502121, 0.16374501585960388, 0.1641456037759781, 0.1797640025615692, 0.1756647378206253, 0.17328566312789917, 0.18835242092609406, 0.1739785522222519, 0.1835835576057434, 0.1638951152563095, 0.16391292214393616, 0.1643623411655426, 0.1644832044839859, 0.17336338758468628, 0.18379920721054077, 0.17217469215393066, 0.16635070741176605, 0.16863691806793213, 0.1881094127893448, 0.17803281545639038, 0.1836288422346115, 0.17376179993152618, 0.18335095047950745, 0.1696116030216217, 0.16404566168785095, 0.1765148788690567, 0.16720536351203918, 0.18342719972133636, 0.1757531762123108, 0.17647239565849304, 0.1646149903535843, 0.16952286660671234, 0.16712576150894165, 0.1829921454191208, 0.16312068700790405, 0.1788666993379593, 0.17754976451396942, 0.18313193321228027, 0.1830117404460907, 0.17431902885437012, 0.16471295058727264, 0.17219702899456024, 0.17167814075946808, 0.18994860351085663, 0.16381222009658813, 0.17554472386837006, 0.17097708582878113, 0.17265979945659637, 0.17483840882778168, 0.1781996339559555, 0.1781884729862213, 0.18410998582839966, 0.17359596490859985, 0.1797991245985031, 0.15323498845100403, 0.16700203716754913, 0.18348997831344604, 0.17928436398506165, 0.16544345021247864, 0.16555729508399963, 0.16089150309562683, 0.17936359345912933, 0.18020759522914886, 0.16080307960510254, 0.17153197526931763, 0.17744286358356476, 0.16284167766571045, 0.1657550185918808, 0.16497497260570526, 0.16854265332221985, 0.18024681508541107, 0.17489933967590332, 0.17275233566761017, 0.17169761657714844, 0.18495096266269684, 0.15695562958717346, 0.16167297959327698, 0.18032199144363403, 0.1708698719739914, 0.17791420221328735, 0.17696218192577362, 0.1662234216928482, 0.16925138235092163, 0.17154768109321594, 0.16561736166477203, 0.18015025556087494, 0.17296159267425537, 0.16283003985881805, 0.16494472324848175, 0.16238223016262054, 0.18840980529785156, 0.16623573005199432, 0.1997968703508377, 0.17722776532173157, 0.15985313057899475, 0.17941215634346008, 0.16740258038043976, 0.17784254252910614, 0.1870432198047638, 0.16160792112350464, 0.1776425540447235, 0.1757611483335495, 0.17126843333244324, 0.1600702852010727, 0.18786558508872986, 0.15847589075565338, 0.18064190447330475, 0.1833701729774475, 0.17744891345500946, 0.1781473308801651, 0.18681076169013977, 0.16571667790412903, 0.16095320880413055, 0.15507392585277557, 0.1720496118068695, 0.17762616276741028, 0.17366275191307068, 0.17189174890518188, 0.17959560453891754, 0.17504945397377014, 0.19118458032608032, 0.17758435010910034, 0.1642182171344757, 0.17266950011253357, 0.1781633049249649, 0.18121084570884705, 0.1665489673614502, 0.16181929409503937, 0.18785881996154785, 0.1685333251953125, 0.17586232721805573, 0.16791200637817383, 0.1613922417163849, 0.1703556925058365, 0.19126854836940765, 0.1713917851448059, 0.19438162446022034, 0.15830877423286438, 0.17389889061450958, 0.19148586690425873, 0.16973167657852173, 0.1611790955066681, 0.16278322041034698, 0.18429936468601227, 0.16491568088531494, 0.16173751652240753, 0.16924382746219635, 0.19461241364479065, 0.16178497672080994, 0.1616194248199463, 0.17093192040920258, 0.1690458208322525, 0.17087605595588684, 0.16610455513000488, 0.16682234406471252, 0.1865067481994629, 0.16205251216888428, 0.18214964866638184, 0.186595618724823, 0.1929589956998825, 0.18193723261356354, 0.18297022581100464, 0.16460350155830383, 0.1745770424604416, 0.15132267773151398, 0.16714927554130554, 0.18351657688617706, 0.16415154933929443, 0.18787653744220734, 0.1552690714597702, 0.1764708161354065, 0.16507534682750702, 0.1893901526927948, 0.19832776486873627, 0.17199933528900146, 0.17400667071342468, 0.17357729375362396, 0.16452376544475555, 0.16336524486541748, 0.17311598360538483, 0.16689831018447876, 0.16807128489017487, 0.14915147423744202, 0.1796799898147583, 0.18634140491485596, 0.16967004537582397, 0.19207698106765747, 0.18529415130615234, 0.17570701241493225, 0.18056127429008484, 0.16751334071159363, 0.17487262189388275, 0.16532349586486816, 0.1646275669336319, 0.1763777732849121, 0.1921224147081375, 0.16719284653663635, 0.1868176907300949, 0.15882645547389984, 0.18785759806632996, 0.1945776492357254, 0.15875600278377533, 0.18040581047534943, 0.19551539421081543, 0.15366992354393005, 0.17165972292423248, 0.18017999827861786, 0.1707620471715927, 0.1628926694393158, 0.16150976717472076, 0.18502721190452576, 0.18333135545253754, 0.17398589849472046, 0.183439701795578, 0.1762201339006424, 0.17249728739261627, 0.16802279651165009, 0.19074967503547668, 0.16818657517433167, 0.19596534967422485, 0.1887526959180832, 0.16630804538726807, 0.15546546876430511, 0.17485399544239044, 0.1777830421924591, 0.18427670001983643, 0.15908101201057434, 0.15992581844329834, 0.15952995419502258, 0.1767890602350235, 0.17629055678844452, 0.16844704747200012, 0.16580305993556976, 0.17999176681041718, 0.14887362718582153, 0.17494843900203705, 0.16679657995700836, 0.16903914511203766, 0.1662883162498474, 0.18578827381134033, 0.1957385390996933, 0.17805296182632446, 0.17201772332191467, 0.18832220137119293, 0.17600537836551666, 0.1670721173286438, 0.18191975355148315, 0.17074094712734222, 0.19035887718200684, 0.17387495934963226, 0.16133596003055573, 0.17491324245929718, 0.1808132827281952, 0.16522599756717682, 0.16074974834918976, 0.15478326380252838, 0.18086634576320648, 0.16709139943122864, 0.17700086534023285, 0.15670403838157654, 0.17591828107833862, 0.15701061487197876, 0.17878535389900208, 0.16560637950897217, 0.183199942111969, 0.17158155143260956, 0.1757066547870636, 0.16131974756717682, 0.18013609945774078, 0.17034555971622467, 0.17428165674209595, 0.16397136449813843, 0.17815881967544556, 0.17355799674987793, 0.14983266592025757, 0.17335322499275208, 0.17692749202251434, 0.16161571443080902, 0.15823465585708618, 0.1810249388217926, 0.16436411440372467, 0.1729874461889267, 0.1879720687866211, 0.1661582887172699, 0.2009088546037674, 0.16763024032115936, 0.17711259424686432, 0.15323594212532043, 0.19969898462295532, 0.18900474905967712, 0.1773957461118698, 0.1644285023212433, 0.15168900787830353, 0.19099806249141693, 0.1559460610151291, 0.1598776876926422, 0.17931249737739563, 0.164339080452919, 0.16348865628242493, 0.18005821108818054, 0.18749487400054932, 0.18696224689483643, 0.19262193143367767, 0.1871814727783203, 0.18319225311279297, 0.16503240168094635, 0.16454309225082397, 0.16738873720169067, 0.16082242131233215, 0.18514075875282288, 0.1787632703781128, 0.18261529505252838, 0.18732061982154846, 0.1724151223897934, 0.1684756577014923, 0.17909982800483704, 0.1610330045223236, 0.1764397919178009, 0.18577885627746582, 0.18680259585380554, 0.1614120900630951, 0.17549149692058563, 0.18553666770458221, 0.1838712990283966, 0.1688583642244339, 0.18649724125862122, 0.18593190610408783, 0.18473505973815918, 0.187458798289299, 0.1946137696504593, 0.19262315332889557, 0.1978205144405365, 0.16857562959194183, 0.18926335871219635, 0.1658940613269806, 0.18459036946296692, 0.1803004890680313, 0.1719846874475479, 0.19488443434238434, 0.18266651034355164, 0.17498108744621277, 0.16394522786140442, 0.1751127690076828, 0.16286881268024445, 0.17882558703422546, 0.16467730700969696, 0.16278375685214996, 0.17425492405891418, 0.16885894536972046, 0.17908553779125214, 0.17786164581775665, 0.16963596642017365, 0.1763286143541336, 0.17355386912822723, 0.16628216207027435, 0.16650813817977905, 0.17217645049095154, 0.19113413989543915, 0.1813865602016449, 0.17350424826145172, 0.16183750331401825, 0.15818707644939423, 0.17379122972488403, 0.1825474500656128, 0.16724105179309845, 0.1679254025220871, 0.19407473504543304, 0.1702737957239151, 0.18188399076461792, 0.1655747890472412, 0.16442671418190002, 0.16695643961429596, 0.1629086583852768, 0.18147528171539307, 0.16577932238578796, 0.16526710987091064, 0.18386030197143555, 0.15999414026737213, 0.19651193916797638, 0.1689780205488205, 0.16901066899299622, 0.17757956683635712, 0.17070439457893372, 0.1647457629442215, 0.166397824883461, 0.18479059636592865, 0.18081693351268768, 0.16772885620594025, 0.16557849943637848, 0.1816900074481964, 0.17825883626937866, 0.16447342932224274, 0.1859748810529709, 0.16524827480316162, 0.15006579458713531, 0.1905727982521057, 0.1645147055387497, 0.1681656837463379, 0.15436778962612152, 0.1593741923570633, 0.1715088188648224, 0.1844349205493927, 0.18781396746635437, 0.16929571330547333, 0.1742859184741974, 0.17482821643352509, 0.16294662654399872, 0.16401472687721252, 0.17046603560447693, 0.15849249064922333, 0.189021497964859, 0.1595328152179718, 0.16799573600292206, 0.1710984706878662, 0.18970538675785065, 0.1856997162103653, 0.16229690611362457, 0.17550860345363617, 0.17270340025424957, 0.1706395447254181, 0.20234234631061554, 0.19637709856033325, 0.18304096162319183, 0.17056548595428467, 0.16878972947597504, 0.16581836342811584, 0.17503857612609863, 0.17326396703720093, 0.1894380897283554, 0.18454988300800323, 0.16324910521507263, 0.16884548962116241, 0.16331219673156738, 0.18419933319091797, 0.16173946857452393, 0.17484167218208313, 0.17384716868400574, 0.16166889667510986, 0.15861551463603973, 0.18375128507614136, 0.19036723673343658, 0.17530623078346252, 0.18389171361923218, 0.16769962012767792, 0.16487593948841095, 0.16497087478637695, 0.17401322722434998, 0.16939924657344818, 0.1826915442943573, 0.16975714266300201, 0.1828579604625702, 0.18508456647396088, 0.16809245944023132, 0.16439171135425568, 0.18288154900074005, 0.17237113416194916, 0.15669533610343933, 0.1718042641878128, 0.18389953672885895, 0.18584686517715454, 0.1712523102760315, 0.19542616605758667, 0.17707136273384094, 0.1768522560596466, 0.18237943947315216, 0.17032523453235626, 0.17655542492866516, 0.16188789904117584, 0.17667220532894135, 0.1821812093257904, 0.16541437804698944, 0.17326700687408447, 0.16528838872909546, 0.18235400319099426, 0.17118270695209503, 0.17108626663684845, 0.18231286108493805, 0.16922727227210999, 0.16154444217681885, 0.17271879315376282, 0.18194647133350372, 0.17591044306755066, 0.17280656099319458, 0.1571875661611557, 0.1713946908712387, 0.18162290751934052, 0.16611628234386444, 0.18122971057891846, 0.18217012286186218, 0.17795340716838837, 0.1782829463481903, 0.1729324758052826, 0.16665732860565186, 0.17333878576755524, 0.18086503446102142, 0.1756049245595932, 0.15323638916015625, 0.181041419506073, 0.1877608299255371, 0.18615016341209412, 0.19405610859394073, 0.16830599308013916, 0.1668432503938675, 0.17578773200511932, 0.17564187943935394, 0.16613757610321045, 0.1735866814851761, 0.17175383865833282, 0.17523004114627838, 0.1789487600326538, 0.1655091792345047, 0.15454065799713135, 0.18239369988441467, 0.16389895975589752, 0.1793709397315979, 0.16445773839950562, 0.16212712228298187, 0.15399396419525146, 0.1800554096698761, 0.18003065884113312, 0.16376693546772003, 0.15466925501823425, 0.17267602682113647, 0.17171379923820496, 0.16929541528224945, 0.16824102401733398, 0.17526374757289886, 0.18515756726264954, 0.17959006130695343, 0.17285993695259094, 0.15651361644268036, 0.18728819489479065, 0.18360541760921478, 0.16766691207885742, 0.17577598989009857, 0.167090505361557, 0.19504119455814362, 0.18272778391838074, 0.16399408876895905, 0.17399907112121582, 0.1702536642551422, 0.17999513447284698, 0.1721011847257614, 0.1713973730802536, 0.1608557105064392, 0.1693667620420456, 0.18727551400661469, 0.1615203469991684, 0.15811190009117126, 0.1726788729429245, 0.17743103206157684, 0.1667032688856125, 0.17547836899757385, 0.16921919584274292, 0.17146070301532745, 0.16704466938972473, 0.192987859249115, 0.1774216741323471, 0.18425177037715912, 0.16085582971572876, 0.16466987133026123, 0.16582228243350983, 0.16988535225391388, 0.18869034945964813, 0.18015818297863007, 0.16320794820785522, 0.1436084657907486, 0.1766161024570465, 0.19660218060016632, 0.17868287861347198, 0.15657316148281097, 0.15461395680904388, 0.18609310686588287, 0.17274349927902222, 0.1750272512435913, 0.1643424928188324, 0.18763528764247894, 0.1857423633337021, 0.16875135898590088, 0.1798820048570633, 0.17323794960975647, 0.164442777633667, 0.1652175635099411, 0.16555915772914886, 0.185505673289299, 0.1609206348657608, 0.19908885657787323, 0.17226766049861908, 0.17859584093093872, 0.17614969611167908, 0.1596829742193222, 0.16995152831077576, 0.1787766069173813, 0.1983295977115631, 0.17564955353736877, 0.1580818146467209, 0.16725842654705048, 0.1555938720703125, 0.1758727729320526, 0.17147094011306763, 0.16701944172382355, 0.16727952659130096, 0.17420867085456848, 0.1810571402311325, 0.176692932844162, 0.16948673129081726, 0.18799187242984772, 0.1751829832792282, 0.15755414962768555, 0.16036731004714966, 0.1943855881690979, 0.1768801063299179, 0.19292837381362915, 0.16525748372077942, 0.1721949726343155, 0.1764756143093109, 0.15065710246562958, 0.16318272054195404, 0.17253251373767853, 0.17554302513599396, 0.14950163662433624, 0.20929524302482605, 0.15244029462337494, 0.19128912687301636, 0.15211790800094604, 0.18079951405525208, 0.16670532524585724, 0.18778079748153687, 0.1773206740617752, 0.1962369978427887, 0.15370051562786102, 0.16188116371631622, 0.1705145537853241, 0.19547335803508759, 0.17186135053634644, 0.16271281242370605, 0.17670977115631104, 0.1822705864906311, 0.15918850898742676, 0.2078944444656372, 0.16971543431282043, 0.18090958893299103, 0.20057885348796844, 0.16741152107715607, 0.1772175133228302, 0.19210615754127502, 0.14616891741752625, 0.17630381882190704, 0.1645813137292862, 0.18524016439914703, 0.1612146943807602, 0.17764107882976532, 0.20548737049102783, 0.1626642644405365, 0.1713384985923767, 0.16350680589675903, 0.2012663036584854, 0.17671118676662445, 0.17683525383472443, 0.17019642889499664, 0.1794099360704422, 0.15772274136543274, 0.17436903715133667, 0.17971442639827728, 0.19848360121250153, 0.20756542682647705, 0.17900726199150085, 0.16057957708835602, 0.1687057763338089, 0.17523019015789032, 0.14767226576805115, 0.14340369403362274, 0.16486969590187073, 0.17410272359848022, 0.1819474846124649, 0.1863933652639389, 0.1747177690267563, 0.16037581861019135, 0.19159457087516785, 0.2070864737033844, 0.19383150339126587, 0.18443429470062256, 0.15030770003795624, 0.16151778399944305, 0.1751452386379242, 0.1880142092704773, 0.17830896377563477, 0.17293597757816315, 0.15801069140434265, 0.17492976784706116, 0.1518341600894928, 0.15932974219322205, 0.15449360013008118, 0.15760758519172668, 0.1549311876296997, 0.15917500853538513, 0.15875107049942017, 0.19869846105575562, 0.18426623940467834, 0.1816321611404419, 0.15328508615493774, 0.18817849457263947, 0.1847749799489975, 0.20064012706279755, 0.15508852899074554, 0.1792203038930893, 0.16600415110588074, 0.1728583723306656, 0.19978034496307373, 0.15249085426330566, 0.1544402539730072, 0.17352229356765747, 0.15648533403873444, 0.18553227186203003, 0.17180252075195312, 0.17637792229652405, 0.1613042801618576, 0.19560211896896362, 0.16275611519813538, 0.15931083261966705, 0.18552565574645996, 0.1614111363887787, 0.1895223706960678, 0.16344548761844635, 0.1852758526802063, 0.14878137409687042, 0.17704159021377563, 0.1611248105764389, 0.15714101493358612, 0.16299450397491455, 0.1597895324230194, 0.15864212810993195, 0.14822319149971008, 0.18976522982120514, 0.16783367097377777, 0.15991613268852234, 0.17883868515491486, 0.19901998341083527, 0.16081030666828156, 0.1798764020204544, 0.1580391675233841, 0.15164317190647125, 0.16203784942626953, 0.16591963171958923, 0.1813475489616394, 0.17858974635601044, 0.1963530331850052, 0.18672588467597961, 0.16150040924549103, 0.18387190997600555, 0.16123414039611816, 0.2129141092300415, 0.16584213078022003, 0.1953994631767273, 0.18562857806682587, 0.14828112721443176, 0.16785621643066406, 0.17534054815769196, 0.19122323393821716, 0.1743663251399994, 0.15747930109500885, 0.16131889820098877, 0.16810578107833862, 0.2009182870388031, 0.17278385162353516, 0.1314603090286255, 0.165791854262352, 0.16468574106693268, 0.18827512860298157, 0.1644522100687027, 0.19883017241954803, 0.18414883315563202, 0.201842799782753, 0.15746933221817017, 0.19018316268920898, 0.15965339541435242, 0.18777340650558472, 0.17140986025333405, 0.2006176859140396, 0.15512239933013916, 0.1713634878396988, 0.188768669962883, 0.1932041347026825, 0.13448648154735565, 0.17158196866512299, 0.1562718003988266, 0.18974271416664124, 0.17845578491687775, 0.1781798005104065, 0.17160724103450775, 0.16593888401985168, 0.16591998934745789, 0.17230837047100067, 0.18756131827831268, 0.1669137179851532, 0.15799276530742645, 0.17029273509979248, 0.15295538306236267, 0.17806798219680786, 0.1636250764131546, 0.17444516718387604, 0.1896822601556778, 0.1630714237689972, 0.17007212340831757, 0.1864064484834671, 0.18335787951946259, 0.1624603569507599, 0.18678314983844757, 0.15163297951221466, 0.16820605099201202, 0.18872123956680298, 0.19820141792297363, 0.16473747789859772, 0.16442979872226715, 0.14899122714996338, 0.18626858294010162, 0.1782967746257782, 0.20569190382957458, 0.18870802223682404, 0.16811801493167877, 0.17796166241168976, 0.17044897377490997, 0.18822847306728363, 0.1808914691209793, 0.16648435592651367, 0.17149151861667633, 0.17674776911735535, 0.17432652413845062, 0.16977885365486145, 0.1838708519935608, 0.16976864635944366, 0.14283204078674316, 0.1666003316640854, 0.18740887939929962, 0.1461639106273651, 0.18150454759597778, 0.16055604815483093, 0.1551138013601303, 0.17889223992824554, 0.15289932489395142, 0.17065435647964478, 0.16371551156044006, 0.16783937811851501, 0.16384480893611908, 0.19904576241970062, 0.17789223790168762, 0.17068544030189514, 0.165828138589859, 0.1830308735370636, 0.16412903368473053, 0.18649260699748993, 0.15779006481170654, 0.16653811931610107, 0.17765162885189056, 0.14765077829360962, 0.18159747123718262, 0.1647140383720398, 0.15373261272907257, 0.15524044632911682, 0.19950954616069794, 0.182620108127594, 0.15676724910736084, 0.17493565380573273, 0.169083833694458, 0.18302156031131744, 0.17331524193286896, 0.21002282202243805, 0.17940104007720947, 0.16844585537910461, 0.1796124279499054, 0.18380294740200043, 0.1752510368824005, 0.16314567625522614, 0.1737813949584961, 0.20362421870231628, 0.1841486692428589, 0.1719779521226883, 0.17171797156333923, 0.18482914566993713, 0.17874810099601746, 0.2025567889213562, 0.16365058720111847, 0.16714830696582794, 0.14447720348834991, 0.16602776944637299, 0.16062042117118835, 0.17954178154468536, 0.1479741334915161, 0.18059751391410828, 0.17220810055732727, 0.17368043959140778, 0.1624550074338913, 0.18348684906959534, 0.1856357604265213, 0.16782771050930023, 0.15585732460021973, 0.18220508098602295, 0.14996206760406494, 0.16464459896087646, 0.19127990305423737, 0.1777050644159317, 0.15065400302410126, 0.18578015267848969, 0.1792965531349182, 0.16369342803955078, 0.20369566977024078, 0.17664971947669983, 0.1820887178182602, 0.1616905927658081, 0.17272688448429108, 0.17265012860298157, 0.19238588213920593, 0.17893847823143005, 0.20633280277252197, 0.17429594695568085, 0.15706434845924377, 0.20726774632930756, 0.20070870220661163, 0.14934071898460388, 0.14698176085948944, 0.1762084662914276, 0.1736777424812317, 0.17346538603305817, 0.17665906250476837, 0.16096211969852448, 0.14984247088432312, 0.1446911096572876, 0.1652294397354126, 0.21306802332401276, 0.16542424261569977, 0.1969252973794937, 0.15455491840839386, 0.17700962722301483, 0.1881060004234314, 0.17148610949516296, 0.1785314381122589, 0.1621522605419159, 0.16396306455135345, 0.18621711432933807, 0.19154930114746094, 0.20909088850021362, 0.18138396739959717, 0.1709734946489334, 0.18516266345977783, 0.14651408791542053, 0.2230272740125656, 0.17557568848133087, 0.20337001979351044, 0.18380026519298553, 0.16762523353099823, 0.16796281933784485, 0.18176351487636566, 0.16070780158042908, 0.19024917483329773, 0.1782979965209961, 0.15731176733970642, 0.15775367617607117, 0.17672520875930786, 0.21493782103061676, 0.1917514055967331, 0.18067914247512817, 0.16074295341968536, 0.19421520829200745, 0.18955758213996887, 0.17677272856235504, 0.16960518062114716, 0.16182468831539154, 0.18377509713172913, 0.16584675014019012, 0.17999447882175446, 0.17404091358184814, 0.14509662985801697, 0.18812675774097443, 0.1901864856481552, 0.19362711906433105, 0.1884378045797348, 0.1772349625825882, 0.1916433721780777, 0.1716616004705429, 0.1796366274356842, 0.1909240037202835, 0.16949282586574554, 0.17559537291526794, 0.17217935621738434, 0.1970951408147812, 0.15272478759288788, 0.16649116575717926, 0.15369729697704315, 0.16948233544826508, 0.15045683085918427, 0.17250238358974457, 0.19178567826747894, 0.17649178206920624, 0.16275747120380402, 0.17805171012878418, 0.16557350754737854, 0.17325422167778015, 0.15726783871650696, 0.18778899312019348, 0.16296371817588806, 0.16782565414905548, 0.16533762216567993, 0.16303594410419464, 0.18687282502651215, 0.15564334392547607, 0.180123433470726, 0.1821487545967102, 0.16602705419063568, 0.14784720540046692, 0.17346437275409698, 0.19320489466190338, 0.1956051141023636, 0.15553008019924164, 0.17600710690021515, 0.1567939668893814, 0.18976470828056335, 0.1388711780309677, 0.14495375752449036, 0.17661572992801666, 0.17595356702804565, 0.1550893932580948, 0.1781962513923645, 0.1807512491941452, 0.1469748169183731, 0.2105855792760849, 0.1666835993528366, 0.17767439782619476, 0.2013637274503708, 0.19358442723751068, 0.1912134736776352, 0.18058933317661285, 0.17564719915390015, 0.20320124924182892, 0.17372386157512665, 0.18259862065315247, 0.1731540560722351, 0.18801502883434296, 0.15775944292545319, 0.1836196333169937, 0.19701336324214935, 0.19055107235908508, 0.187265083193779, 0.141500324010849, 0.17372426390647888, 0.1754527986049652, 0.1781378835439682, 0.19909150898456573, 0.16811321675777435, 0.15196378529071808, 0.17370162904262543, 0.18030056357383728, 0.1720174252986908, 0.16374549269676208, 0.20039361715316772, 0.14749309420585632, 0.17523619532585144, 0.18860331177711487, 0.17577508091926575, 0.19905692338943481, 0.1805984228849411, 0.14239531755447388, 0.1631234735250473, 0.1734192669391632, 0.18141473829746246, 0.179583340883255, 0.16623850166797638, 0.17389129102230072, 0.19380486011505127, 0.17985253036022186, 0.16219277679920197, 0.18572409451007843, 0.17797814309597015, 0.18333372473716736, 0.17213428020477295, 0.17516925930976868, 0.15820518136024475, 0.17622287571430206, 0.1745566427707672, 0.1646667718887329, 0.19134148955345154, 0.16916799545288086, 0.16839055716991425, 0.1744861751794815, 0.18620893359184265, 0.1778613179922104, 0.16679120063781738, 0.15729422867298126, 0.178651362657547, 0.17315974831581116, 0.18122345209121704, 0.1650935560464859, 0.16111789643764496, 0.19108301401138306, 0.1711936742067337, 0.18047688901424408, 0.1665232926607132, 0.1598891317844391, 0.18685978651046753, 0.18338340520858765, 0.16277357935905457, 0.17325207591056824, 0.15958964824676514, 0.17176365852355957, 0.19260969758033752, 0.16911964118480682, 0.1754174679517746, 0.1729831099510193, 0.17325769364833832, 0.17411862313747406, 0.17403647303581238, 0.1661122590303421, 0.17388539016246796, 0.17096483707427979, 0.16594548523426056, 0.18391376733779907, 0.18564122915267944, 0.17796167731285095, 0.18225368857383728, 0.19415223598480225, 0.1622709482908249, 0.16483423113822937, 0.18032020330429077, 0.16073356568813324, 0.19328100979328156, 0.17189978063106537, 0.18251095712184906, 0.1719885617494583, 0.1743503212928772, 0.17688284814357758, 0.1713772863149643, 0.1698818951845169, 0.16155482828617096, 0.17151714861392975, 0.15554969012737274, 0.17022712528705597, 0.1808020919561386, 0.1556473970413208, 0.18844060599803925, 0.188204824924469, 0.16427317261695862, 0.15639086067676544, 0.16784261167049408, 0.15608882904052734, 0.17292878031730652, 0.1630336195230484, 0.18647444248199463, 0.20281147956848145, 0.18708421289920807, 0.16913087666034698, 0.14980389177799225, 0.18239891529083252, 0.16947610676288605, 0.1719343364238739, 0.17982394993305206, 0.1691426932811737, 0.18920162320137024, 0.1631775051355362, 0.17292508482933044, 0.1845124214887619, 0.17457358539104462, 0.15353409945964813, 0.16703163087368011, 0.17946182191371918, 0.16780850291252136, 0.1531813144683838, 0.171791210770607, 0.1548919677734375, 0.18218210339546204, 0.18182386457920074, 0.21009492874145508, 0.13669098913669586, 0.1869623214006424, 0.1717269867658615, 0.17063631117343903, 0.17311014235019684, 0.18843679130077362, 0.1656610369682312, 0.1759214848279953, 0.1704069972038269, 0.1647476851940155, 0.17132356762886047, 0.16742947697639465, 0.17509225010871887, 0.16613171994686127, 0.19173108041286469, 0.15499909222126007, 0.20445482432842255, 0.1786079704761505, 0.17347249388694763, 0.1904214322566986, 0.17644397914409637, 0.18232469260692596, 0.16217514872550964, 0.174021378159523, 0.16043423116207123, 0.17740009725093842, 0.18026594817638397, 0.1661428064107895, 0.16951094567775726, 0.20341497659683228, 0.18950527906417847, 0.1817345768213272, 0.18531127274036407, 0.17962166666984558, 0.15214045345783234, 0.18350708484649658, 0.16330507397651672, 0.1565672606229782, 0.1806509792804718, 0.18012475967407227, 0.17231044173240662, 0.17861607670783997, 0.16958610713481903, 0.1634666472673416, 0.16901715099811554, 0.16221870481967926, 0.1748817265033722, 0.18742166459560394, 0.1796046495437622, 0.17407402396202087, 0.19710394740104675, 0.17894276976585388, 0.1737729161977768, 0.18234451115131378, 0.19346605241298676, 0.17814971506595612, 0.171880304813385, 0.18074800074100494, 0.17313216626644135, 0.17392635345458984, 0.20887655019760132, 0.16881203651428223, 0.1845923364162445, 0.1871356964111328, 0.17100653052330017, 0.1993846446275711, 0.1728280633687973, 0.17729873955249786, 0.15968483686447144, 0.18203268945217133, 0.1687638759613037, 0.18210892379283905, 0.15501722693443298, 0.16365446150302887, 0.16751624643802643, 0.18957757949829102, 0.18467579782009125, 0.169745534658432, 0.1799299567937851, 0.17246216535568237, 0.17046652734279633, 0.1604844629764557, 0.17891627550125122, 0.17260512709617615, 0.14984220266342163, 0.1764017790555954, 0.18796898424625397, 0.16231484711170197, 0.1491485983133316, 0.16744117438793182, 0.17976291477680206, 0.16269537806510925, 0.16777092218399048, 0.18275277316570282, 0.18713143467903137, 0.16167616844177246, 0.16450738906860352, 0.1764909327030182, 0.19726550579071045, 0.18219968676567078, 0.1721758097410202, 0.1891033947467804, 0.1816570907831192, 0.17425160109996796, 0.17151284217834473, 0.17450152337551117, 0.18043486773967743, 0.17124131321907043, 0.1740296483039856, 0.17621013522148132, 0.17262248694896698, 0.1695796102285385, 0.17508341372013092, 0.19035644829273224, 0.17935463786125183, 0.16382990777492523, 0.19071856141090393, 0.1651153713464737, 0.17795325815677643, 0.18208062648773193, 0.18341650068759918, 0.1601763665676117, 0.1681741625070572, 0.18747921288013458, 0.18272851407527924, 0.17589008808135986, 0.1940312534570694, 0.16516871750354767, 0.17449678480625153, 0.19158397614955902, 0.17127540707588196, 0.17207162082195282, 0.16666021943092346, 0.17823772132396698, 0.1867469847202301, 0.18023161590099335, 0.17638671398162842, 0.17805637419223785, 0.16720053553581238, 0.1675637811422348, 0.18199893832206726, 0.1659383326768875, 0.17136424779891968, 0.16032035648822784, 0.17551614344120026, 0.16763903200626373, 0.181825652718544, 0.16425617039203644, 0.15437547862529755, 0.1724664866924286, 0.17108231782913208, 0.15243160724639893, 0.18477751314640045, 0.17127414047718048, 0.17145302891731262, 0.15990763902664185, 0.17151989042758942, 0.16318896412849426, 0.1856570690870285, 0.1706833690404892, 0.1670045554637909, 0.18515272438526154, 0.1600891500711441, 0.180769681930542, 0.18055683374404907, 0.17251941561698914, 0.19243091344833374, 0.14969287812709808, 0.16054297983646393, 0.1778968870639801, 0.1567462682723999, 0.1659051775932312, 0.16176651418209076, 0.19666390120983124, 0.17777109146118164, 0.17425666749477386, 0.17671602964401245, 0.1706753671169281, 0.17569909989833832, 0.15668180584907532, 0.16986146569252014, 0.15956991910934448, 0.1673574149608612, 0.1672980636358261, 0.17947864532470703, 0.17065709829330444, 0.18799617886543274, 0.18309707939624786, 0.17037680745124817, 0.18180765211582184, 0.177651047706604, 0.16864395141601562, 0.1880951225757599, 0.17010226845741272, 0.16219063103199005, 0.1702425628900528, 0.1701771318912506, 0.1760043203830719, 0.16825991868972778, 0.1709049791097641, 0.17697203159332275, 0.18716387450695038, 0.17992056906223297, 0.17761501669883728, 0.1907988041639328, 0.1654501110315323, 0.16569730639457703, 0.17765766382217407, 0.16986341774463654, 0.16977520287036896, 0.17704766988754272, 0.1713496595621109, 0.17572668194770813, 0.17697535455226898, 0.18073342740535736, 0.15277476608753204, 0.16696085035800934, 0.18822871148586273, 0.1770263910293579, 0.16001346707344055, 0.17476952075958252, 0.17790022492408752, 0.19118161499500275, 0.16878660023212433, 0.17522740364074707, 0.19240154325962067, 0.16546623408794403, 0.16779877245426178, 0.17341668903827667, 0.16944760084152222, 0.17686013877391815, 0.166311115026474, 0.16921156644821167, 0.1743047833442688, 0.1779620349407196, 0.17067614197731018, 0.16448335349559784, 0.1900644302368164, 0.18197163939476013, 0.15186002850532532, 0.14961612224578857, 0.1688472330570221, 0.17052502930164337, 0.1844162493944168, 0.1850663274526596, 0.1735827624797821, 0.17083601653575897, 0.1860225945711136, 0.17145437002182007, 0.16875435411930084, 0.15841460227966309, 0.16404810547828674, 0.16754969954490662, 0.1810661256313324, 0.1685204654932022, 0.159184068441391, 0.1775999516248703, 0.17692027986049652, 0.18607331812381744, 0.1706736832857132, 0.16470281779766083, 0.1771402209997177, 0.17433422803878784, 0.15942054986953735, 0.18165618181228638, 0.16589583456516266, 0.17448249459266663, 0.17930489778518677, 0.1747119426727295, 0.16523683071136475, 0.17891260981559753, 0.178807333111763, 0.18380653858184814, 0.185369074344635, 0.15201056003570557]\n",
            "Val loss 0.17359996768725167\n",
            "Val auc roc 0.5\n",
            "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     2: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2378a1ecbd49465ea616aad54870369c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2124.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1737\n",
            "Train Losses : [0.17872805893421173, 0.17565259337425232, 0.1873406022787094, 0.1687057763338089, 0.16848787665367126, 0.1705625206232071, 0.16623148322105408, 0.17966462671756744, 0.17407719790935516, 0.1686307191848755, 0.17464013397693634, 0.17866644263267517, 0.18298140168190002, 0.17234492301940918, 0.1903253048658371, 0.14374388754367828, 0.16832852363586426, 0.17617358267307281, 0.17865754663944244, 0.1865909844636917, 0.1799023300409317, 0.18075504899024963, 0.15992207825183868, 0.16947142779827118, 0.17376461625099182, 0.1795683205127716, 0.17036695778369904, 0.18997624516487122, 0.17800003290176392, 0.18347331881523132, 0.18009133636951447, 0.16556060314178467, 0.16513146460056305, 0.17844702303409576, 0.16954517364501953, 0.1759757399559021, 0.16843223571777344, 0.16120083630084991, 0.18209382891654968, 0.1687028706073761, 0.19580473005771637, 0.1681337058544159, 0.1730150580406189, 0.18179172277450562, 0.1769707202911377, 0.17263519763946533, 0.1638384759426117, 0.19037696719169617, 0.16296038031578064, 0.18243582546710968, 0.16902337968349457, 0.16912931203842163, 0.17518340051174164, 0.17700837552547455, 0.175800621509552, 0.15841525793075562, 0.1688612997531891, 0.1768735945224762, 0.17095020413398743, 0.17581234872341156, 0.18801724910736084, 0.17907533049583435, 0.16680385172367096, 0.17619220912456512, 0.1505216807126999, 0.18407027423381805, 0.16612114012241364, 0.17963822185993195, 0.1787669062614441, 0.16911259293556213, 0.17998374998569489, 0.17032241821289062, 0.16013167798519135, 0.1929244101047516, 0.18245865404605865, 0.1669790893793106, 0.1488187462091446, 0.19618715345859528, 0.17946882545948029, 0.18288910388946533, 0.18415363132953644, 0.17607350647449493, 0.17294937372207642, 0.1741311252117157, 0.14739586412906647, 0.16894768178462982, 0.19945789873600006, 0.165508434176445, 0.18059858679771423, 0.1671266108751297, 0.16238871216773987, 0.17855410277843475, 0.15646949410438538, 0.18978853523731232, 0.1856756955385208, 0.1822529286146164, 0.1828383207321167, 0.174935981631279, 0.16662123799324036, 0.16942629218101501, 0.17323310673236847, 0.16872268915176392, 0.18054701387882233, 0.18272042274475098, 0.16742628812789917, 0.1688176840543747, 0.17406901717185974, 0.17458917200565338, 0.18015804886817932, 0.17859004437923431, 0.19999732077121735, 0.16850122809410095, 0.17758429050445557, 0.17849504947662354, 0.18482659757137299, 0.17192164063453674, 0.16537731885910034, 0.1795312613248825, 0.17279136180877686, 0.16677317023277283, 0.178901806473732, 0.17962945997714996, 0.16587945818901062, 0.17284053564071655, 0.1738552451133728, 0.17470993101596832, 0.1591377854347229, 0.17331252992153168, 0.17396146059036255, 0.1784413754940033, 0.1892361044883728, 0.16996203362941742, 0.1732621192932129, 0.18118442595005035, 0.15470774471759796, 0.17383207380771637, 0.20051324367523193, 0.17067404091358185, 0.17856787145137787, 0.16251027584075928, 0.1767772138118744, 0.16772277653217316, 0.17364300787448883, 0.19214949011802673, 0.16850672662258148, 0.16769756376743317, 0.16866829991340637, 0.172811821103096, 0.17862771451473236, 0.169218048453331, 0.16155287623405457, 0.17164114117622375, 0.17685632407665253, 0.16373346745967865, 0.17043828964233398, 0.1763041913509369, 0.19941943883895874, 0.16864848136901855, 0.1791272610425949, 0.18869879841804504, 0.1756954938173294, 0.17019504308700562, 0.19169378280639648, 0.1799171417951584, 0.17884619534015656, 0.1715429574251175, 0.161628857254982, 0.18810813128948212, 0.1763686239719391, 0.17359031736850739, 0.1715586930513382, 0.16726255416870117, 0.16672934591770172, 0.19764818251132965, 0.1876547932624817, 0.17157091200351715, 0.1669742465019226, 0.1898318976163864, 0.17753560841083527, 0.18598707020282745, 0.16253837943077087, 0.16812072694301605, 0.16856423020362854, 0.17832337319850922, 0.18171077966690063, 0.17954212427139282, 0.16255013644695282, 0.1736069619655609, 0.17490755021572113, 0.17802616953849792, 0.16509665548801422, 0.18459384143352509, 0.16868969798088074, 0.2070113569498062, 0.18495284020900726, 0.16249032318592072, 0.1694665551185608, 0.169915571808815, 0.18263694643974304, 0.14847099781036377, 0.17407628893852234, 0.1819096952676773, 0.17823462188243866, 0.15998892486095428, 0.15542015433311462, 0.17870855331420898, 0.1688431203365326, 0.184914231300354, 0.17836147546768188, 0.16732166707515717, 0.18379512429237366, 0.17959678173065186, 0.1568763256072998, 0.17406588792800903, 0.17836768925189972, 0.16108332574367523, 0.18195247650146484, 0.16879133880138397, 0.16187801957130432, 0.1601260006427765, 0.1699274480342865, 0.15045525133609772, 0.18343420326709747, 0.17599183320999146, 0.17161306738853455, 0.1625083088874817, 0.16975687444210052, 0.15242373943328857, 0.1704028993844986, 0.17920733988285065, 0.1839347630739212, 0.17827574908733368, 0.19483599066734314, 0.17840790748596191, 0.18140621483325958, 0.18297019600868225, 0.16726568341255188, 0.17565599083900452, 0.17835037410259247, 0.17863978445529938, 0.1856427937746048, 0.17324298620224, 0.17770624160766602, 0.1710817664861679, 0.18029847741127014, 0.1837889552116394, 0.17110572755336761, 0.1688474416732788, 0.1801663488149643, 0.17148976027965546, 0.16882485151290894, 0.16585850715637207, 0.16921256482601166, 0.1750655174255371, 0.18476144969463348, 0.16373680531978607, 0.16599509119987488, 0.15802253782749176, 0.1662316918373108, 0.18624041974544525, 0.16526556015014648, 0.18465644121170044, 0.17267808318138123, 0.16087456047534943, 0.17407159507274628, 0.1755131334066391, 0.17322514951229095, 0.18260203301906586, 0.1681823432445526, 0.17052699625492096, 0.17820575833320618, 0.1869705468416214, 0.18221573531627655, 0.17077167332172394, 0.15881960093975067, 0.1722116321325302, 0.17794924974441528, 0.16252955794334412, 0.16156168282032013, 0.182170107960701, 0.16563798487186432, 0.18498504161834717, 0.16907943785190582, 0.16922861337661743, 0.1804545819759369, 0.1704697459936142, 0.1764729619026184, 0.17434637248516083, 0.17169061303138733, 0.17300961911678314, 0.18866318464279175, 0.1748792827129364, 0.16905607283115387, 0.1795983612537384, 0.172027125954628, 0.1691567599773407, 0.17679858207702637, 0.15687792003154755, 0.1743452548980713, 0.15874192118644714, 0.16611891984939575, 0.16528429090976715, 0.17652355134487152, 0.190619096159935, 0.16141444444656372, 0.16919152438640594, 0.1772385686635971, 0.18098945915699005, 0.16894078254699707, 0.17835313081741333, 0.153096005320549, 0.16112951934337616, 0.16766779124736786, 0.1772400140762329, 0.19291143119335175, 0.1824880987405777, 0.17988111078739166, 0.18473036587238312, 0.17572900652885437, 0.16920599341392517, 0.16721569001674652, 0.1691189408302307, 0.1851915866136551, 0.17996105551719666, 0.1912255436182022, 0.17730554938316345, 0.16818305850028992, 0.17960353195667267, 0.17291733622550964, 0.1699182093143463, 0.1654096394777298, 0.1712867170572281, 0.1779792159795761, 0.17934560775756836, 0.19050784409046173, 0.18126174807548523, 0.17789416015148163, 0.1691538542509079, 0.1859489232301712, 0.1704111099243164, 0.16472703218460083, 0.1633925437927246, 0.17856177687644958, 0.17120447754859924, 0.18080295622348785, 0.16142770648002625, 0.1690066158771515, 0.1656804233789444, 0.1783648431301117, 0.15955322980880737, 0.17993468046188354, 0.17144504189491272, 0.17029014229774475, 0.17666608095169067, 0.15914039313793182, 0.16786198318004608, 0.17012043297290802, 0.18204651772975922, 0.19134920835494995, 0.18964898586273193, 0.16897587478160858, 0.15849876403808594, 0.17641189694404602, 0.16601143777370453, 0.18995700776576996, 0.17761392891407013, 0.178064227104187, 0.18921032547950745, 0.16887420415878296, 0.1777649074792862, 0.19682645797729492, 0.179177924990654, 0.17870447039604187, 0.19989165663719177, 0.17609162628650665, 0.1680464893579483, 0.16607053577899933, 0.1728326976299286, 0.1844000667333603, 0.16016680002212524, 0.17789481580257416, 0.19343852996826172, 0.15784449875354767, 0.18362395465373993, 0.16928425431251526, 0.1693800836801529, 0.17811477184295654, 0.15947788953781128, 0.17417331039905548, 0.16778269410133362, 0.15738119184970856, 0.17197740077972412, 0.17056787014007568, 0.15799228847026825, 0.16960623860359192, 0.1780550330877304, 0.1867995262145996, 0.17197343707084656, 0.18012328445911407, 0.17809568345546722, 0.15819650888442993, 0.1643804907798767, 0.1916729211807251, 0.17630894482135773, 0.15562044084072113, 0.17107388377189636, 0.18769817054271698, 0.16899701952934265, 0.17604109644889832, 0.15358026325702667, 0.16954903304576874, 0.17594507336616516, 0.17779558897018433, 0.1759856641292572, 0.17246463894844055, 0.183695450425148, 0.17350530624389648, 0.19374383985996246, 0.18051652610301971, 0.17466400563716888, 0.17859871685504913, 0.16176636517047882, 0.1692410409450531, 0.16838820278644562, 0.1770114302635193, 0.16448073089122772, 0.17054018378257751, 0.191883847117424, 0.1738761067390442, 0.17241120338439941, 0.17717888951301575, 0.1777535378932953, 0.17833852767944336, 0.1806756854057312, 0.15644565224647522, 0.16640588641166687, 0.15912598371505737, 0.17655281722545624, 0.16931094229221344, 0.1601555198431015, 0.1781928390264511, 0.1972784548997879, 0.1701413094997406, 0.17627406120300293, 0.16980740427970886, 0.18100182712078094, 0.18818429112434387, 0.1673719435930252, 0.17064212262630463, 0.18250100314617157, 0.19257435202598572, 0.17088499665260315, 0.16031767427921295, 0.1756765991449356, 0.17553159594535828, 0.1715729534626007, 0.17346765100955963, 0.174693763256073, 0.16808584332466125, 0.18163584172725677, 0.18563659489154816, 0.16451279819011688, 0.17861169576644897, 0.1625465601682663, 0.18893581628799438, 0.1673523485660553, 0.16951869428157806, 0.1638675481081009, 0.17716336250305176, 0.17555557191371918, 0.1832789182662964, 0.15807202458381653, 0.1684296578168869, 0.17450135946273804, 0.1750938892364502, 0.15667791664600372, 0.15850423276424408, 0.17753058671951294, 0.1487348973751068, 0.1783483773469925, 0.16789458692073822, 0.1660040020942688, 0.1903999000787735, 0.16611865162849426, 0.17736579477787018, 0.16780279576778412, 0.182723730802536, 0.15969698131084442, 0.18231436610221863, 0.1578119546175003, 0.15728536248207092, 0.1811293512582779, 0.18132942914962769, 0.18864582479000092, 0.17313045263290405, 0.17969171702861786, 0.17807048559188843, 0.18772630393505096, 0.16282425820827484, 0.17589759826660156, 0.18374961614608765, 0.1745644211769104, 0.16559016704559326, 0.15812401473522186, 0.18024000525474548, 0.18110138177871704, 0.1610090136528015, 0.17761386930942535, 0.17992234230041504, 0.1848219484090805, 0.17767348885536194, 0.17389170825481415, 0.16940957307815552, 0.17744114995002747, 0.1719406694173813, 0.16942739486694336, 0.1774907261133194, 0.17765603959560394, 0.16838543117046356, 0.16212345659732819, 0.19625546038150787, 0.17225612699985504, 0.1748456358909607, 0.1828005164861679, 0.16703428328037262, 0.17756763100624084, 0.16333751380443573, 0.15537722408771515, 0.17664971947669983, 0.17490974068641663, 0.166694775223732, 0.1805800497531891, 0.1904764622449875, 0.17611482739448547, 0.16015022993087769, 0.16139160096645355, 0.1719425469636917, 0.18253862857818604, 0.1851498782634735, 0.17848077416419983, 0.16683511435985565, 0.17487406730651855, 0.18397527933120728, 0.16977345943450928, 0.16682539880275726, 0.16064991056919098, 0.18480847775936127, 0.16412636637687683, 0.17816098034381866, 0.16953714191913605, 0.17693667113780975, 0.16372984647750854, 0.16496601700782776, 0.16803541779518127, 0.19290770590305328, 0.1771283596754074, 0.16939915716648102, 0.1812436282634735, 0.19235216081142426, 0.1778658628463745, 0.1682257056236267, 0.17602165043354034, 0.17482292652130127, 0.1712150275707245, 0.16737131774425507, 0.17530107498168945, 0.16442804038524628, 0.17230568826198578, 0.15461470186710358, 0.18144924938678741, 0.1814170628786087, 0.1647724211215973, 0.16941894590854645, 0.18063600361347198, 0.16254861652851105, 0.16991350054740906, 0.17627137899398804, 0.14386804401874542, 0.1758788824081421, 0.16410304605960846, 0.1765369325876236, 0.1710691899061203, 0.18419252336025238, 0.17358525097370148, 0.17645426094532013, 0.16783034801483154, 0.1696820855140686, 0.16638600826263428, 0.17972564697265625, 0.1662503480911255, 0.16690205037593842, 0.16773173213005066, 0.17337611317634583, 0.17480744421482086, 0.1623917818069458, 0.16939377784729004, 0.17113840579986572, 0.1635885387659073, 0.17101089656352997, 0.16959519684314728, 0.16095523536205292, 0.16881063580513, 0.18730351328849792, 0.17465214431285858, 0.17167317867279053, 0.16704273223876953, 0.15825940668582916, 0.16955573856830597, 0.1879885196685791, 0.16688507795333862, 0.16821613907814026, 0.16905008256435394, 0.17884576320648193, 0.17074404656887054, 0.15870755910873413, 0.16422505676746368, 0.17753368616104126, 0.16952553391456604, 0.16940036416053772, 0.1591479629278183, 0.1762610673904419, 0.16324278712272644, 0.18570448458194733, 0.16982784867286682, 0.1533108502626419, 0.17560608685016632, 0.16772575676441193, 0.1749110072851181, 0.16229365766048431, 0.1681348830461502, 0.1595769226551056, 0.16925360262393951, 0.1832927018404007, 0.16804111003875732, 0.1756230592727661, 0.1819043755531311, 0.1698753982782364, 0.14766426384449005, 0.16979265213012695, 0.1736256331205368, 0.19123059511184692, 0.1621742695569992, 0.16951146721839905, 0.1693163812160492, 0.16950316727161407, 0.16873839497566223, 0.1590992510318756, 0.16661061346530914, 0.17836689949035645, 0.18697869777679443, 0.1777770221233368, 0.1692950278520584, 0.1778690367937088, 0.17567148804664612, 0.175095796585083, 0.18008890748023987, 0.18479233980178833, 0.18648715317249298, 0.16525204479694366, 0.1753440946340561, 0.17761793732643127, 0.17942193150520325, 0.17974993586540222, 0.17528612911701202, 0.17639604210853577, 0.15799792110919952, 0.17774857580661774, 0.16916102170944214, 0.14890940487384796, 0.1717940866947174, 0.19296136498451233, 0.16540449857711792, 0.16908720135688782, 0.17766611278057098, 0.18837152421474457, 0.174552783370018, 0.18806007504463196, 0.17780068516731262, 0.1613977551460266, 0.17954206466674805, 0.17187759280204773, 0.1792459487915039, 0.1801270991563797, 0.18160146474838257, 0.16600129008293152, 0.18689323961734772, 0.16899259388446808, 0.19013705849647522, 0.1817619949579239, 0.1615482121706009, 0.19219188392162323, 0.17622578144073486, 0.18600420653820038, 0.1623314619064331, 0.1713355928659439, 0.18560057878494263, 0.18624795973300934, 0.183112233877182, 0.17893491685390472, 0.17067039012908936, 0.20283697545528412, 0.16773083806037903, 0.16343028843402863, 0.1596084088087082, 0.16955924034118652, 0.16029980778694153, 0.18198829889297485, 0.17159391939640045, 0.1778009980916977, 0.17774787545204163, 0.19339536130428314, 0.1691398173570633, 0.16985566914081573, 0.16456502676010132, 0.1857968270778656, 0.17843233048915863, 0.17758281528949738, 0.17968611419200897, 0.1756180226802826, 0.1760222166776657, 0.19446557760238647, 0.18531601130962372, 0.1693914383649826, 0.1946486085653305, 0.16955654323101044, 0.16473627090454102, 0.17912261188030243, 0.1772545725107193, 0.17752574384212494, 0.16141194105148315, 0.17237712442874908, 0.18531274795532227, 0.17730900645256042, 0.18252946436405182, 0.17806515097618103, 0.17711909115314484, 0.1693086326122284, 0.1657230705022812, 0.16114765405654907, 0.176290363073349, 0.1582489162683487, 0.18466801941394806, 0.18035650253295898, 0.16600221395492554, 0.1677764654159546, 0.15772028267383575, 0.16670532524585724, 0.16748258471488953, 0.17059765756130219, 0.1889551877975464, 0.15862751007080078, 0.1620270311832428, 0.1732749193906784, 0.167949840426445, 0.15840809047222137, 0.17162249982357025, 0.17981602251529694, 0.1784168928861618, 0.18994255363941193, 0.15957584977149963, 0.1609145849943161, 0.16815754771232605, 0.1728803813457489, 0.17689557373523712, 0.17683327198028564, 0.16844648122787476, 0.15271343290805817, 0.16600316762924194, 0.1883183866739273, 0.17657363414764404, 0.1579732894897461, 0.16429826617240906, 0.15959040820598602, 0.17064127326011658, 0.18107952177524567, 0.1610805094242096, 0.17693614959716797, 0.17874395847320557, 0.17504571378231049, 0.16241556406021118, 0.1820128858089447, 0.15622086822986603, 0.18173472583293915, 0.16261780261993408, 0.16628147661685944, 0.17457561194896698, 0.16391688585281372, 0.18457365036010742, 0.18175241351127625, 0.154259592294693, 0.19859075546264648, 0.18250775337219238, 0.18431773781776428, 0.17680279910564423, 0.17518582940101624, 0.1739862561225891, 0.1778244972229004, 0.18313007056713104, 0.17586234211921692, 0.15613733232021332, 0.1796366274356842, 0.16950099170207977, 0.1741987019777298, 0.17604713141918182, 0.2004251927137375, 0.1800832748413086, 0.16266563534736633, 0.14829809963703156, 0.1693999469280243, 0.15936534106731415, 0.17876507341861725, 0.1587793380022049, 0.15915435552597046, 0.1720065325498581, 0.17958098649978638, 0.1695115715265274, 0.17625539004802704, 0.16913720965385437, 0.17212961614131927, 0.15046587586402893, 0.17003212869167328, 0.1503409445285797, 0.187433660030365, 0.16073927283287048, 0.18288950622081757, 0.16384918987751007, 0.17123736441135406, 0.1810477077960968, 0.175869882106781, 0.16217906773090363, 0.16131147742271423, 0.1869950294494629, 0.1637747585773468, 0.17215704917907715, 0.16715729236602783, 0.1635276824235916, 0.17585240304470062, 0.18223372101783752, 0.1776815503835678, 0.17165043950080872, 0.15834487974643707, 0.14935609698295593, 0.16826748847961426, 0.16896121203899384, 0.15693797171115875, 0.1663046032190323, 0.17790810763835907, 0.20289966464042664, 0.17614881694316864, 0.16170503199100494, 0.15884195268154144, 0.17241014540195465, 0.1778201162815094, 0.17783385515213013, 0.1638466864824295, 0.16923166811466217, 0.16349028050899506, 0.16682352125644684, 0.1614462286233902, 0.16811677813529968, 0.1744956076145172, 0.16538801789283752, 0.1734040230512619, 0.17753268778324127, 0.1877632737159729, 0.17088739573955536, 0.1810554713010788, 0.1742710918188095, 0.1742897480726242, 0.15202513337135315, 0.18749253451824188, 0.16201044619083405, 0.18731383979320526, 0.16935530304908752, 0.16015182435512543, 0.17873534560203552, 0.1795123964548111, 0.20101796090602875, 0.1692103147506714, 0.18019044399261475, 0.16162583231925964, 0.17663024365901947, 0.17077535390853882, 0.16304141283035278, 0.1661941558122635, 0.16136839985847473, 0.17839987576007843, 0.17401310801506042, 0.14753226935863495, 0.16944286227226257, 0.17037609219551086, 0.18715696036815643, 0.1672978699207306, 0.18601259589195251, 0.1515054851770401, 0.16507694125175476, 0.16871032118797302, 0.18286271393299103, 0.17436483502388, 0.16901366412639618, 0.18439817428588867, 0.18066687881946564, 0.1807698905467987, 0.18097436428070068, 0.16440078616142273, 0.15863125026226044, 0.17452695965766907, 0.18016426265239716, 0.1690124124288559, 0.1527564525604248, 0.1714426577091217, 0.1893007755279541, 0.17905516922473907, 0.1795044094324112, 0.1813686341047287, 0.18596120178699493, 0.16740073263645172, 0.1768757700920105, 0.18318161368370056, 0.16332367062568665, 0.18141092360019684, 0.1756742298603058, 0.16736508905887604, 0.16924040019512177, 0.17571064829826355, 0.17260414361953735, 0.1733541637659073, 0.17519213259220123, 0.17265380918979645, 0.1785178929567337, 0.17289607226848602, 0.168190136551857, 0.1824817657470703, 0.19432583451271057, 0.1750471293926239, 0.17681173980236053, 0.1776176542043686, 0.17027680575847626, 0.1718282848596573, 0.17153450846672058, 0.1693207174539566, 0.17997175455093384, 0.17197047173976898, 0.17899835109710693, 0.17494051158428192, 0.17684656381607056, 0.19229216873645782, 0.1736648976802826, 0.1642783284187317, 0.16760604083538055, 0.17025883495807648, 0.17926502227783203, 0.16920354962348938, 0.16272367537021637, 0.1788492649793625, 0.17769798636436462, 0.19301174581050873, 0.1740192174911499, 0.18277189135551453, 0.16931723058223724, 0.16012580692768097, 0.16486546397209167, 0.1773136556148529, 0.17892570793628693, 0.1791832596063614, 0.18021157383918762, 0.17374418675899506, 0.17583152651786804, 0.17649057507514954, 0.17281784117221832, 0.17274247109889984, 0.19141219556331635, 0.16899769008159637, 0.18976227939128876, 0.189833402633667, 0.19952356815338135, 0.17019957304000854, 0.1772300899028778, 0.16059163212776184, 0.1775771975517273, 0.16315089166164398, 0.18295833468437195, 0.17167353630065918, 0.19364075362682343, 0.19219428300857544, 0.18251094222068787, 0.18916969001293182, 0.17983724176883698, 0.17532092332839966, 0.1692228615283966, 0.17024466395378113, 0.17849011719226837, 0.1717720478773117, 0.1745341569185257, 0.16288532316684723, 0.16253119707107544, 0.1865936517715454, 0.1702219694852829, 0.16198338568210602, 0.18219779431819916, 0.18705247342586517, 0.17336773872375488, 0.1752856969833374, 0.16428543627262115, 0.17760123312473297, 0.16092199087142944, 0.16375122964382172, 0.17122673988342285, 0.1691315472126007, 0.1804601550102234, 0.17757485806941986, 0.17908895015716553, 0.1646314561367035, 0.1818951964378357, 0.17922134697437286, 0.16919508576393127, 0.16914384067058563, 0.17548029124736786, 0.15979361534118652, 0.17361846566200256, 0.17542831599712372, 0.17331452667713165, 0.17506307363510132, 0.1687123328447342, 0.17955012619495392, 0.17829443514347076, 0.1788540631532669, 0.1778603494167328, 0.16075684130191803, 0.17665870487689972, 0.1776295304298401, 0.17847934365272522, 0.17433831095695496, 0.18148766458034515, 0.16935688257217407, 0.1729147881269455, 0.16945214569568634, 0.1817949265241623, 0.17112311720848083, 0.18311505019664764, 0.18903979659080505, 0.17760464549064636, 0.16302815079689026, 0.18583139777183533, 0.16939420998096466, 0.18065762519836426, 0.17115247249603271, 0.16359950602054596, 0.16773152351379395, 0.17885921895503998, 0.18229037523269653, 0.16592305898666382, 0.19102813303470612, 0.18974898755550385, 0.16166700422763824, 0.1694260835647583, 0.18138884007930756, 0.1589869111776352, 0.1636635959148407, 0.17604227364063263, 0.18185429275035858, 0.16936494410037994, 0.17128950357437134, 0.1723584532737732, 0.17764708399772644, 0.15758322179317474, 0.17979224026203156, 0.1785140186548233, 0.16994446516036987, 0.1654617041349411, 0.1693590134382248, 0.1841476559638977, 0.185496985912323, 0.18276506662368774, 0.20474499464035034, 0.16581515967845917, 0.17884480953216553, 0.1739518642425537, 0.1762331873178482, 0.1712973415851593, 0.16912472248077393, 0.186620831489563, 0.18290241062641144, 0.16871888935565948, 0.19959580898284912, 0.17788836359977722, 0.1737736165523529, 0.1919509619474411, 0.17210790514945984, 0.15187177062034607, 0.16965682804584503, 0.17180943489074707, 0.16325624287128448, 0.17951028048992157, 0.17970985174179077, 0.16431142389774323, 0.16943244636058807, 0.17617081105709076, 0.18510597944259644, 0.17774727940559387, 0.16144075989723206, 0.1812959760427475, 0.18537883460521698, 0.18268130719661713, 0.16921645402908325, 0.17146718502044678, 0.18339094519615173, 0.17506258189678192, 0.1736171692609787, 0.1694202870130539, 0.16186471283435822, 0.16916914284229279, 0.16952009499073029, 0.16855420172214508, 0.17787469923496246, 0.16918498277664185, 0.17845293879508972, 0.17667986452579498, 0.17327918112277985, 0.17391365766525269, 0.17060716450214386, 0.16258275508880615, 0.18687725067138672, 0.19443581998348236, 0.16582007706165314, 0.16486969590187073, 0.186968132853508, 0.1696482002735138, 0.17950448393821716, 0.17054040729999542, 0.16059574484825134, 0.17142896354198456, 0.16255976259708405, 0.1545340120792389, 0.17033874988555908, 0.15675607323646545, 0.16914616525173187, 0.18322749435901642, 0.1568729132413864, 0.18570859730243683, 0.1953871101140976, 0.17123089730739594, 0.18398724496364594, 0.16919063031673431, 0.14362318813800812, 0.174688920378685, 0.17472386360168457, 0.1787530481815338, 0.17760051786899567, 0.1715574860572815, 0.17224398255348206, 0.16262421011924744, 0.1742810755968094, 0.18335513770580292, 0.17766033113002777, 0.1811811327934265, 0.16656632721424103, 0.17078237235546112, 0.16944429278373718, 0.16405220329761505, 0.1740466058254242, 0.1763201802968979, 0.1779245138168335, 0.18335527181625366, 0.16123278439044952, 0.18087553977966309, 0.16237175464630127, 0.16922958195209503, 0.1923755556344986, 0.1717473268508911, 0.171229749917984, 0.16362003982067108, 0.16982343792915344, 0.16961099207401276, 0.1861291080713272, 0.1779116541147232, 0.17053358256816864, 0.17110304534435272, 0.18186239898204803, 0.18542195856571198, 0.16432589292526245, 0.17112895846366882, 0.16235004365444183, 0.16871832311153412, 0.19629421830177307, 0.1813727766275406, 0.1825152188539505, 0.18055467307567596, 0.183038130402565, 0.16861233115196228, 0.17748868465423584, 0.18505679070949554, 0.18423821032047272, 0.1626223474740982, 0.16467685997486115, 0.17324256896972656, 0.17870111763477325, 0.17391785979270935, 0.174320250749588, 0.16435906291007996, 0.16322991251945496, 0.17166760563850403, 0.16917134821414948, 0.18547001481056213, 0.16773195564746857, 0.15768009424209595, 0.17123344540596008, 0.17849987745285034, 0.17725642025470734, 0.16137905418872833, 0.16752219200134277, 0.1642404943704605, 0.17254909873008728, 0.17940321564674377, 0.1777234822511673, 0.17069125175476074, 0.17059892416000366, 0.17768938839435577, 0.1786290854215622, 0.1556522250175476, 0.17653362452983856, 0.17103488743305206, 0.17083173990249634, 0.19173261523246765, 0.17238615453243256, 0.17476622760295868, 0.18027649819850922, 0.16919466853141785, 0.18923363089561462, 0.17536379396915436, 0.17248116433620453, 0.16958975791931152, 0.18868830800056458, 0.17259687185287476, 0.16868112981319427, 0.1712782382965088, 0.18696989119052887, 0.16032084822654724, 0.17690333724021912, 0.16968360543251038, 0.1539299637079239, 0.18687185645103455, 0.14686599373817444, 0.17780114710330963, 0.16946233808994293, 0.1777561604976654, 0.19665007293224335, 0.19011689722537994, 0.1626451313495636, 0.17263276875019073, 0.17767417430877686, 0.17751963436603546, 0.16441819071769714, 0.17989425361156464, 0.17235790193080902, 0.16815288364887238, 0.16815185546875, 0.17163781821727753, 0.17924560606479645, 0.16669036448001862, 0.1710403710603714, 0.17966870963573456, 0.1791621595621109, 0.17757774889469147, 0.15423230826854706, 0.17509521543979645, 0.17584839463233948, 0.17096546292304993, 0.17858503758907318, 0.1815645843744278, 0.19173461198806763, 0.1779792308807373, 0.16258160769939423, 0.1713738888502121, 0.18936333060264587, 0.18362826108932495, 0.15796568989753723, 0.17563116550445557, 0.17406824231147766, 0.174507275223732, 0.18905775249004364, 0.1600116342306137, 0.17600524425506592, 0.16937841475009918, 0.17809079587459564, 0.16559292376041412, 0.1785501092672348, 0.17112985253334045, 0.16991280019283295, 0.16931548714637756, 0.1741412729024887, 0.16131900250911713, 0.1654934138059616, 0.16641588509082794, 0.1774229109287262, 0.18243199586868286, 0.17135760188102722, 0.18385154008865356, 0.17393018305301666, 0.1657424420118332, 0.1745164841413498, 0.1765056550502777, 0.181423619389534, 0.17068998515605927, 0.19331961870193481, 0.1652807891368866, 0.1722492128610611, 0.1863701194524765, 0.1836652308702469, 0.17765603959560394, 0.14412669837474823, 0.1697232872247696, 0.16933149099349976, 0.17879430949687958, 0.1793915331363678, 0.1606707125902176, 0.1996738463640213, 0.1768033504486084, 0.16930876672267914, 0.1767192780971527, 0.17086948454380035, 0.16667814552783966, 0.1617938131093979, 0.16953261196613312, 0.16207261383533478, 0.1652691811323166, 0.1624404788017273, 0.1692517250776291, 0.17029140889644623, 0.17475277185440063, 0.1840898096561432, 0.17768548429012299, 0.17679554224014282, 0.1597461700439453, 0.1693958193063736, 0.17594768106937408, 0.1696409434080124, 0.1585131138563156, 0.17154338955879211, 0.17391106486320496, 0.17147715389728546, 0.16939406096935272, 0.1809031069278717, 0.18054023385047913, 0.18309806287288666, 0.16647887229919434, 0.1691870391368866, 0.1637759953737259, 0.17475345730781555, 0.19076499342918396, 0.18744035065174103, 0.18575553596019745, 0.17531125247478485, 0.17251905798912048, 0.17990730702877045, 0.1804492026567459, 0.16598552465438843, 0.18763290345668793, 0.18443983793258667, 0.1903829127550125, 0.15445776283740997, 0.18656249344348907, 0.16517309844493866, 0.19412481784820557, 0.16568461060523987, 0.16937024891376495, 0.16844555735588074, 0.16774903237819672, 0.17781773209571838, 0.17067895829677582, 0.16817747056484222, 0.16821207106113434, 0.16092129051685333, 0.15332657098770142, 0.1775909662246704, 0.1746736317873001, 0.1727466583251953, 0.1744917780160904, 0.174503356218338, 0.18424281477928162, 0.16440275311470032, 0.17659582197666168, 0.17778532207012177, 0.169534832239151, 0.16633230447769165, 0.17432378232479095, 0.17815576493740082, 0.18965370953083038, 0.17543165385723114, 0.18281006813049316, 0.17577557265758514, 0.18439190089702606, 0.15562395751476288, 0.1734388917684555, 0.17765562236309052, 0.18410155177116394, 0.1692703515291214, 0.16525019705295563, 0.15673355758190155, 0.16480767726898193, 0.16194383800029755, 0.17113037407398224, 0.17206592857837677, 0.17466087639331818, 0.1646370142698288, 0.16308103501796722, 0.17785437405109406, 0.16677850484848022, 0.1707933396100998, 0.15431095659732819, 0.17790399491786957, 0.17600898444652557, 0.16939106583595276, 0.1757432222366333, 0.19542649388313293, 0.19137625396251678, 0.17791345715522766, 0.1744171380996704, 0.19042019546031952, 0.1772078275680542, 0.1677124947309494, 0.1627318561077118, 0.18108364939689636, 0.178025484085083, 0.16388751566410065, 0.16602569818496704, 0.17530450224876404, 0.167586088180542, 0.18029339611530304, 0.1794118583202362, 0.1892171949148178, 0.17788094282150269, 0.15895147621631622, 0.17168307304382324, 0.1786530762910843, 0.1657969206571579, 0.17967350780963898, 0.16448244452476501, 0.18291626870632172, 0.1730046421289444, 0.1809641569852829, 0.19470740854740143, 0.18539896607398987, 0.18497036397457123, 0.15935640037059784, 0.19145554304122925, 0.1739935427904129, 0.16340744495391846, 0.178446963429451, 0.1697547733783722, 0.1585620492696762, 0.19046054780483246, 0.17265845835208893, 0.17374476790428162, 0.1713060885667801, 0.16880060732364655, 0.18176236748695374, 0.16708222031593323, 0.16243769228458405, 0.16828541457653046, 0.16329726576805115, 0.16528873145580292, 0.16374115645885468, 0.17267461121082306, 0.18813009560108185, 0.17554393410682678, 0.179911270737648, 0.1728026419878006, 0.17827758193016052, 0.17081619799137115, 0.18567906320095062, 0.16499365866184235, 0.16258452832698822, 0.16510221362113953, 0.18598750233650208, 0.17165830731391907, 0.16878533363342285, 0.1664709597826004, 0.16931304335594177, 0.16213031113147736, 0.16504807770252228, 0.16935698688030243, 0.16669368743896484, 0.1875368356704712, 0.18089677393436432, 0.16550981998443604, 0.17523890733718872, 0.16991937160491943, 0.17874395847320557, 0.1691412776708603, 0.1552736908197403, 0.17969141900539398, 0.18313436210155487, 0.19310513138771057, 0.16374856233596802, 0.19063693284988403, 0.1918659657239914, 0.17157703638076782, 0.17477867007255554, 0.1666577160358429, 0.1857999861240387, 0.1779128760099411, 0.1821979433298111, 0.16944395005702972, 0.1808985471725464, 0.18297018110752106, 0.1693814992904663, 0.1741643249988556, 0.17325222492218018, 0.16692323982715607, 0.1844986528158188, 0.19318798184394836, 0.16605427861213684, 0.1692984402179718, 0.17170192301273346, 0.1980879157781601, 0.1648680865764618, 0.1694333553314209, 0.1840595304965973, 0.1785474419593811, 0.1778545081615448, 0.1761622130870819, 0.1689455360174179, 0.16917462646961212, 0.15948347747325897, 0.17920535802841187, 0.18290571868419647, 0.17755135893821716, 0.18406659364700317, 0.18384183943271637, 0.16101661324501038, 0.16436932981014252, 0.17155306041240692, 0.19342945516109467, 0.17346033453941345, 0.1777047961950302, 0.17565666139125824, 0.19847993552684784, 0.1725083589553833, 0.18805311620235443, 0.17551735043525696, 0.1742093861103058, 0.18421758711338043, 0.17772293090820312, 0.16451400518417358, 0.16946206986904144, 0.18120479583740234, 0.17754049599170685, 0.1776180863380432, 0.16553786396980286, 0.16252386569976807, 0.16938744485378265, 0.16766691207885742, 0.19014550745487213, 0.1923643797636032, 0.17605067789554596, 0.18426206707954407, 0.1710774004459381, 0.1784847229719162, 0.17325925827026367, 0.1778169423341751, 0.14884814620018005, 0.16380806267261505, 0.16466419398784637, 0.1735798418521881, 0.17794004082679749, 0.1692226082086563, 0.1790786236524582, 0.17263829708099365, 0.18448568880558014, 0.19076837599277496, 0.17589879035949707, 0.170439213514328, 0.15393438935279846, 0.1692080795764923, 0.17830607295036316, 0.1926962286233902, 0.15439075231552124, 0.1666058599948883, 0.1907421350479126, 0.14535164833068848, 0.19139285385608673, 0.17764496803283691, 0.16945317387580872, 0.16636304557323456, 0.17024068534374237, 0.16959957778453827, 0.1578674018383026, 0.18187972903251648, 0.18157432973384857, 0.17845891416072845, 0.18686383962631226, 0.1801871359348297, 0.16445335745811462, 0.17272743582725525, 0.1776379495859146, 0.18789927661418915, 0.17770466208457947, 0.16555556654930115, 0.16928187012672424, 0.19151464104652405, 0.15352009236812592, 0.18571069836616516, 0.17239689826965332, 0.1760009378194809, 0.1597447395324707, 0.1814713031053543, 0.17762906849384308, 0.16658934950828552, 0.16499949991703033, 0.16646674275398254, 0.17333629727363586, 0.18504805862903595, 0.1797623634338379, 0.1809217780828476, 0.1765834242105484, 0.19152991473674774, 0.17218825221061707, 0.1650291234254837, 0.17401009798049927, 0.18213342130184174, 0.16920199990272522, 0.17892922461032867, 0.17772923409938812, 0.17511355876922607, 0.17046336829662323, 0.17356091737747192, 0.17554853856563568, 0.16076618432998657, 0.15933457016944885, 0.15057986974716187, 0.16363683342933655, 0.17505547404289246, 0.15855200588703156, 0.17604857683181763, 0.18038730323314667, 0.1831483244895935, 0.18347600102424622, 0.178104966878891, 0.16317035257816315, 0.17270871996879578, 0.16753599047660828, 0.16704867780208588, 0.1558704823255539, 0.17685112357139587, 0.17703405022621155, 0.1762280911207199, 0.16285157203674316, 0.16042545437812805, 0.1772482693195343, 0.15599769353866577, 0.15576930344104767, 0.16347573697566986, 0.17441464960575104, 0.17520083487033844, 0.16548578441143036, 0.1607956439256668, 0.156772181391716, 0.18422897160053253, 0.17462487518787384, 0.20279881358146667, 0.16009610891342163, 0.1544790267944336, 0.1600993275642395, 0.17749708890914917, 0.16722875833511353, 0.16993997991085052, 0.17028893530368805, 0.1870139092206955, 0.18377897143363953, 0.17118017375469208, 0.16897346079349518, 0.16934815049171448, 0.1370222568511963, 0.1715937852859497, 0.16706809401512146, 0.19091029465198517, 0.17795448005199432, 0.16525617241859436, 0.1682787984609604, 0.1759452372789383, 0.17749126255512238, 0.1627364307641983, 0.17060799896717072, 0.16892793774604797, 0.1677139699459076, 0.17808978259563446, 0.17324939370155334, 0.16929355263710022, 0.17218747735023499, 0.17751632630825043, 0.1822446584701538, 0.1593991219997406, 0.1805463433265686, 0.1774422824382782, 0.18206940591335297, 0.17705601453781128, 0.1776147186756134, 0.17868800461292267, 0.17277008295059204, 0.1692708432674408, 0.17981553077697754, 0.18526776134967804, 0.1477075219154358, 0.18957559764385223, 0.2033071368932724, 0.1758834570646286, 0.18147949874401093, 0.17628715932369232, 0.18054091930389404, 0.17589688301086426, 0.18571650981903076, 0.16736051440238953, 0.18047133088111877, 0.19410276412963867, 0.1706315129995346, 0.16971062123775482, 0.17935119569301605, 0.1892915517091751, 0.17357535660266876, 0.1669401079416275, 0.17092663049697876, 0.17116771638393402, 0.17096252739429474, 0.17792551219463348, 0.16993768513202667, 0.16665923595428467, 0.170802041888237, 0.17758707702159882, 0.17880764603614807, 0.17271894216537476, 0.1836414635181427, 0.1750921905040741, 0.1816718429327011, 0.18442535400390625, 0.16544502973556519, 0.17261432111263275, 0.1722298115491867, 0.15852494537830353, 0.15247242152690887, 0.1829955130815506, 0.1688288301229477, 0.15632732212543488, 0.19347938895225525, 0.1732466071844101, 0.17156140506267548, 0.17576701939105988, 0.16840288043022156, 0.15073157846927643, 0.18084824085235596, 0.17748942971229553, 0.17292623221874237, 0.16940629482269287, 0.18935327231884003, 0.1725407987833023, 0.1656598299741745, 0.17352202534675598, 0.16520297527313232, 0.17676977813243866, 0.17759595811367035, 0.18406079709529877, 0.17165729403495789, 0.17766155302524567, 0.16988779604434967, 0.16669955849647522, 0.18012721836566925, 0.17936697602272034, 0.1749233603477478, 0.16728374361991882, 0.18208369612693787, 0.1775854527950287, 0.1702142208814621, 0.1793476939201355, 0.17033259570598602, 0.16777314245700836, 0.17772264778614044, 0.1772577464580536, 0.17760199308395386, 0.18046271800994873, 0.17716196179389954, 0.168531596660614, 0.17279253900051117, 0.15633836388587952, 0.19801662862300873, 0.1704878807067871, 0.16983143985271454, 0.17350110411643982, 0.16952477395534515, 0.17153336107730865, 0.1674933284521103, 0.16799712181091309, 0.16848725080490112, 0.1738283634185791, 0.16798801720142365, 0.17136460542678833, 0.18937066197395325, 0.18701483309268951, 0.18028844892978668, 0.1799773871898651, 0.16258682310581207, 0.1777268499135971, 0.16680169105529785, 0.16201698780059814, 0.18059737980365753, 0.16251040995121002, 0.17786751687526703, 0.17320401966571808, 0.16638407111167908, 0.16510576009750366, 0.17257001996040344, 0.16932077705860138, 0.17261843383312225, 0.16486647725105286, 0.15503503382205963, 0.18273065984249115, 0.1601867526769638, 0.17385630309581757, 0.17425280809402466, 0.15680301189422607, 0.19717377424240112, 0.17031806707382202, 0.1792004108428955, 0.16393893957138062, 0.16837358474731445, 0.16185995936393738, 0.1811118870973587, 0.18444402515888214, 0.1895410269498825, 0.18609358370304108, 0.18252775073051453, 0.17683470249176025, 0.17707109451293945, 0.15789981186389923, 0.16810773313045502, 0.17633342742919922, 0.1777627319097519, 0.18538324534893036, 0.17749296128749847, 0.19030103087425232, 0.16733407974243164, 0.1679857224225998, 0.16882960498332977, 0.17770333588123322, 0.1679254025220871, 0.18807372450828552, 0.17353567481040955, 0.17729851603507996, 0.16068176925182343, 0.1829637885093689, 0.16328196227550507, 0.15138821303844452, 0.1885276436805725, 0.1775187849998474, 0.16265423595905304, 0.16498804092407227, 0.1890752762556076, 0.17787978053092957, 0.18766289949417114, 0.17039968073368073, 0.16244710981845856, 0.19700738787651062, 0.1844908446073532, 0.1756274402141571, 0.1782396286725998, 0.18146370351314545, 0.16202287375926971, 0.17776411771774292, 0.18906626105308533, 0.17444555461406708, 0.16534283757209778, 0.16210222244262695, 0.16250623762607574, 0.16986504197120667, 0.18049070239067078, 0.16835913062095642, 0.1771276444196701, 0.1796393245458603, 0.16752545535564423, 0.19114117324352264, 0.17067067325115204, 0.16744326055049896, 0.16775605082511902, 0.16302704811096191, 0.16731072962284088, 0.17754317820072174, 0.16934752464294434, 0.17268282175064087, 0.1961294710636139, 0.16671256721019745, 0.165767103433609, 0.1828337013721466, 0.1673486828804016, 0.16320030391216278, 0.16671936213970184, 0.16160616278648376, 0.16322077810764313, 0.16822078824043274, 0.17076341807842255, 0.1774086356163025, 0.16943682730197906, 0.14588850736618042, 0.16932281851768494, 0.17185230553150177, 0.1868140697479248, 0.16970418393611908, 0.16455164551734924, 0.16217446327209473, 0.1782516986131668, 0.15891581773757935, 0.18736490607261658, 0.17018668353557587, 0.1662537306547165, 0.17092472314834595, 0.17757239937782288, 0.177300825715065, 0.18162404000759125, 0.17793655395507812, 0.19403448700904846, 0.18123368918895721, 0.17923392355442047, 0.17845433950424194, 0.16599039733409882, 0.18213394284248352, 0.1700238287448883, 0.1694408357143402, 0.1772455871105194, 0.16941113770008087, 0.18248140811920166, 0.17839457094669342, 0.16305893659591675, 0.1677592396736145, 0.1692393571138382, 0.17826537787914276, 0.17395257949829102, 0.15815414488315582, 0.17373286187648773, 0.17955362796783447, 0.18611648678779602, 0.18007272481918335, 0.15941853821277618, 0.18052569031715393, 0.17604370415210724, 0.16859407722949982, 0.171945720911026, 0.163290336728096, 0.1790587455034256, 0.17477352917194366, 0.17082171142101288, 0.16575762629508972, 0.169577494263649, 0.16939757764339447, 0.16168035566806793, 0.17499519884586334, 0.1693355292081833, 0.17888180911540985, 0.17739279568195343, 0.17825132608413696, 0.17619311809539795, 0.18186284601688385, 0.1831924170255661, 0.18385420739650726, 0.1662767231464386, 0.17184829711914062, 0.17731550335884094, 0.17735661566257477, 0.16954058408737183, 0.17747460305690765, 0.18866653740406036, 0.16635429859161377, 0.18448962271213531, 0.16945604979991913, 0.17654073238372803, 0.1705125868320465, 0.18139901757240295, 0.1745082139968872, 0.17614926397800446, 0.17835305631160736, 0.19762402772903442, 0.16237108409404755, 0.18256907165050507, 0.17306536436080933, 0.16739600896835327, 0.15550118684768677, 0.18386311829090118, 0.17987069487571716, 0.1838211715221405, 0.17427214980125427, 0.17630372941493988, 0.1818225383758545, 0.15916654467582703, 0.17719773948192596, 0.18802385032176971, 0.17908824980258942, 0.16989155113697052, 0.16943731904029846, 0.17915397882461548, 0.18107673525810242, 0.17938144505023956, 0.16829462349414825, 0.16116034984588623, 0.18341174721717834, 0.1651221364736557, 0.18063674867153168, 0.1810135692358017, 0.17521928250789642, 0.17836539447307587, 0.17799443006515503, 0.19423983991146088, 0.1694900244474411, 0.177761971950531, 0.177683025598526, 0.15514518320560455, 0.1700369417667389, 0.17757506668567657, 0.17763835191726685, 0.16099020838737488, 0.1842719316482544, 0.16015857458114624, 0.18496854603290558, 0.16418160498142242, 0.16576574742794037, 0.17751207947731018, 0.17370107769966125, 0.18751445412635803, 0.1731954962015152, 0.16928644478321075, 0.17333659529685974, 0.17174217104911804, 0.16934625804424286, 0.1674226075410843, 0.16633981466293335, 0.15719151496887207, 0.1893477886915207, 0.16564752161502838, 0.17079855501651764, 0.15322445333003998, 0.18589232861995697, 0.16971440613269806, 0.1774413138628006, 0.18136200308799744, 0.1773294061422348, 0.1849175989627838, 0.17564718425273895, 0.16179832816123962, 0.17122706770896912, 0.16900140047073364, 0.18092797696590424, 0.16640017926692963, 0.17451868951320648, 0.17758168280124664, 0.15932492911815643, 0.17316819727420807, 0.18032680451869965, 0.17306841909885406, 0.17172487080097198, 0.17886608839035034, 0.1900736391544342, 0.1582852602005005, 0.16118119657039642, 0.1834503412246704, 0.17045213282108307, 0.17255334556102753, 0.19034865498542786, 0.17531728744506836, 0.17814356088638306, 0.16667409241199493, 0.16659529507160187, 0.1786859780550003, 0.18278610706329346, 0.16341036558151245, 0.1784089207649231, 0.18515194952487946, 0.18159836530685425, 0.17293275892734528, 0.1774263232946396, 0.18864977359771729, 0.17143864929676056, 0.16267544031143188, 0.17087534070014954, 0.17501004040241241, 0.17430667579174042, 0.16704495251178741, 0.15476396679878235, 0.16914162039756775, 0.17855636775493622, 0.17789015173912048, 0.1775825470685959, 0.1837799847126007, 0.16975633800029755, 0.19934172928333282, 0.15789689123630524, 0.16637744009494781, 0.17744311690330505, 0.17948520183563232, 0.1784333735704422, 0.16105128824710846, 0.16265667974948883, 0.1853981614112854, 0.17745034396648407, 0.17888006567955017, 0.17525094747543335, 0.17773951590061188, 0.18384407460689545, 0.16931644082069397, 0.16114383935928345, 0.17300641536712646, 0.16173577308654785, 0.16709451377391815, 0.16442516446113586, 0.16103947162628174, 0.16653460264205933, 0.1554708182811737, 0.17536509037017822, 0.18016694486141205, 0.16688351333141327, 0.16781401634216309, 0.18588833510875702, 0.16292926669120789, 0.18688422441482544, 0.18879394233226776, 0.1821819245815277, 0.17779794335365295, 0.18294696509838104, 0.19185255467891693]\n",
            "Val loss 0.17365065524055287\n",
            "Val auc roc 0.5062321275378896\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-05.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb49931-1ae1-4527-9520-05e132a68e88"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}