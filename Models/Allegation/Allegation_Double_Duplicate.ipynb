{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Allegation_Double_Duplicate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f7ca443ef9549c0938d6cf8ce58bb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_210ae5e4b0ad455fa17c11f9a225cfae",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7a7d12f3cd34532a1c54ddda75f37c8",
              "IPY_MODEL_ad08224dba794873b80559f573460c85"
            ]
          }
        },
        "210ae5e4b0ad455fa17c11f9a225cfae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7a7d12f3cd34532a1c54ddda75f37c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_21bb2b3472514864ad27c6c557828ede",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1770,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edf1998359d44bb2a4f67bdbca2b8571"
          }
        },
        "ad08224dba794873b80559f573460c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed60a8cc56fc4a67a5786493dc572c05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1770/1770 [17:00&lt;00:00,  1.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c08fe0d350043f68aac2704065dfc45"
          }
        },
        "21bb2b3472514864ad27c6c557828ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edf1998359d44bb2a4f67bdbca2b8571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed60a8cc56fc4a67a5786493dc572c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c08fe0d350043f68aac2704065dfc45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6cf58ed355040439685955a6f92bd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64e34c45816d4713b0b1fb77f88d5dd6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97457992502d4ed8b707852791e50c66",
              "IPY_MODEL_9de27d673c8c4b18a6fb242152faa27d"
            ]
          }
        },
        "64e34c45816d4713b0b1fb77f88d5dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97457992502d4ed8b707852791e50c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2e3510a23794a3cb7b34694c76db6de",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1770,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28f3425ce822484eaf2a7058e9b6b22f"
          }
        },
        "9de27d673c8c4b18a6fb242152faa27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66a3cd99149448b98f71818439a7d816",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1770/1770 [16:34&lt;00:00,  1.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e00f15f80d14378902780e5daa24378"
          }
        },
        "e2e3510a23794a3cb7b34694c76db6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28f3425ce822484eaf2a7058e9b6b22f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66a3cd99149448b98f71818439a7d816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e00f15f80d14378902780e5daa24378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61af8ce243c7424cb881612bda138989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_915f8b7299dc4593aa413dd6ad4a3708",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a7fe4aa90edb4c86b3bf5662a5a8a80e",
              "IPY_MODEL_ea1ce1095e8b4cd4a9f751eaed277ee1"
            ]
          }
        },
        "915f8b7299dc4593aa413dd6ad4a3708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7fe4aa90edb4c86b3bf5662a5a8a80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1212436e4aef4d03a592343b2c50024a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1770,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9321d148832459983d82233428aba80"
          }
        },
        "ea1ce1095e8b4cd4a9f751eaed277ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b83e474d48254932b35399c6695afa04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1770/1770 [16:33&lt;00:00,  1.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f2eacc28fe7434ea1aa1d5f05313bbf"
          }
        },
        "1212436e4aef4d03a592343b2c50024a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9321d148832459983d82233428aba80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b83e474d48254932b35399c6695afa04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f2eacc28fe7434ea1aa1d5f05313bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie9t7l91U2t",
        "colab_type": "text"
      },
      "source": [
        "# Data Import from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh1JATeBylTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "282c85a6-67f8-42e2-9960-2c189aa587c3"
      },
      "source": [
        "# %cd ..\n",
        "# %pwd\n",
        "# !cp '/content/drive/My Drive/IEEE BigMM/ieee-bigmm-images.zip' './'\n",
        "!git clone 'https://github.com/sohamtiwari3120/ieee-bigmm-images.git'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ieee-bigmm-images'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7175 (delta 12), reused 8 (delta 3), pack-reused 7142\u001b[K\n",
            "Receiving objects: 100% (7175/7175), 592.44 MiB | 44.67 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (8551/8551), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hno1BI3eIQb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M7H8jCyzjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0217f4d3-7cb8-433d-c7b5-2d003993132b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mieee-bigmm-images\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaUvnWy2y97N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "# !unzip ieee-bigmm-images.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkUI93xgzRFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9249b9d4-8c1c-48f1-a9c7-bc1cb7672638"
      },
      "source": [
        "%cd ieee-bigmm-images/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ieee-bigmm-images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYp3BrmFb4EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "48661442-16b2-4cc0-94cc-2a77c0a6f48a"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/sohamtiwari3120/ieee-bigmm-images\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-J3t5rG0EwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fff0eded-d554-408e-9b45-fef43a1c2304"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_datav5.csv                README.md\n",
            "clean_datav6.csv                test_data_cleaned.csv\n",
            "Data_without-invalid_cells.csv  \u001b[0m\u001b[01;34mtest_images\u001b[0m/\n",
            "final_dataset.csv               test_tweet_2.csv\n",
            "final_test2.csv                 \u001b[01;34mtrain_images\u001b[0m/\n",
            "final_test3_unpreprocessed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17uVz_YI1dty",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dghuwTb1t2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "4a33fad4-639d-4be1-edbb-914f3ea9f52a"
      },
      "source": [
        "# %%capture\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision\n",
        "! pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install imbalanced-learn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 26.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.33)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWr-9J1AAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f2bGeBK_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "619f6a3a-d3bd-43f4-edf8-1bec99216645"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftb6j_3C1uSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0cd1c791-2d08-49dc-e64d-7b22b86a1a5b"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phuvcx_b2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "1bb00d88-3b11-4271-f761-cf7798ab4874"
      },
      "source": [
        "df = pd.read_csv('./clean_datav6.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>missing_text</th>\n",
              "      <th>Text_Only_Informative</th>\n",
              "      <th>Image_Only_Informative</th>\n",
              "      <th>Directed_Hate</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "      <th>Sarcasm</th>\n",
              "      <th>Allegation</th>\n",
              "      <th>Justification</th>\n",
              "      <th>Refutation</th>\n",
              "      <th>Support</th>\n",
              "      <th>Oppose</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1052237153789390853</td>\n",
              "      <td>New post (Domestic Violence Awareness Hasn't C...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1052207832081129472</td>\n",
              "      <td>Domestic Violence Awareness Hasn’t Caught Up W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1052183746344960000</td>\n",
              "      <td>Mother Nature’s #MeToo</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1052156864840908800</td>\n",
              "      <td>ption - no:2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1052095305133510656</td>\n",
              "      <td>It is 'high time' #MeToo named and shamed men ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  ...  Refutation Support  Oppose\n",
              "0           0             0               0  ...         0.0     1.0     0.0\n",
              "1           1             1               1  ...         0.0     1.0     0.0\n",
              "2           2             2               2  ...         0.0     0.0     0.0\n",
              "3           3             3               3  ...         0.0     0.0     1.0\n",
              "4           4             4               4  ...         0.0     1.0     0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SOPiJUN2PoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e1fc6a8f-28cc-4ed8-d6db-82d2547400c4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, train_size=0.8, shuffle = True )\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()\n",
        "train_df['text'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      How India's #MeToo campaign different from US  \n",
              "1    What's next for #MeToo after Kavanaugh's confi...\n",
              "2    Fans lose calm as TV personality Mani makes fu...\n",
              "3    Karan johar so painful these days.... Every ac...\n",
              "4    ICYMI - PERRY: The secret lives of sleaze and ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gsQ0q72XPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_transformations = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "#             transforms.Resize((224, 244)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFomlns02fvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79a78d35-36d0-4ad7-827f-198ec02f7c1d"
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 34551873.12B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ScheMbt2_6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d230853b-532f-4bee-a02d-ecd2f6e9d6c5"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 913805.24B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZacy6uP3F-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c32f59ae-d6c0-4a27-953f-c62eccc97082"
      },
      "source": [
        "(bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize('new post domestic violence awareness caught me zzzzzx83272@xxxx')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2047,\n",
              " 2695,\n",
              " 4968,\n",
              " 4808,\n",
              " 7073,\n",
              " 3236,\n",
              " 2033,\n",
              " 1062,\n",
              " 13213,\n",
              " 13213,\n",
              " 2595,\n",
              " 2620,\n",
              " 16703,\n",
              " 2581,\n",
              " 2475,\n",
              " 1030,\n",
              " 22038,\n",
              " 20348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRJVGDJmA8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "101f4d97-cc8d-45c4-f10a-e6719e7e46c6"
      },
      "source": [
        "bert_tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 100, 101, 102, 103]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxbHMxJEbdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert)\n",
        "# Help on BertModel in module pytorch_pretrained_bert.modeling object:\n",
        "\n",
        "# class BertModel(BertPreTrainedModel)\n",
        "#  |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "#  |  \n",
        "#  |  Params:\n",
        "#  |      config: a BertConfig class instance with the configuration to build a new model\n",
        "#  |  \n",
        "#  |  Inputs:\n",
        "#  |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "#  |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "#  |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "#  |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "#  |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "#  |          a `sentence B` token (see BERT paper for more details).\n",
        "#  |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "#  |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "#  |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "#  |          a batch has varying length sentences.\n",
        "#  |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "#  |  \n",
        "#  |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "#  |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "#  |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "#  |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "#  |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "#  |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "#  |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "#  |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "#  |          classifier pretrained on top of the hidden state associated to the first character of the\n",
        "#  |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper). \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-TvFY8oB6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# help(bert.encoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabXmZJl3KVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNImageDataset(Dataset):\n",
        "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab, minority_class):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.label_name = label_name\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "        df2 = self.data[self.data[label_name]==minority_class]\n",
        "        df2 = df2.copy().reset_index(drop=True)\n",
        "        df3 = df2.copy().reset_index(drop=True)\n",
        "        # print(df2)\n",
        "        print(f\"Old data length : {len(self.data)}\")\n",
        "        print(f'minority class is {minority_class}. Duplicating minority class data!')\n",
        "        for i in range(len(df2)):\n",
        "            text = df2['text'][i]\n",
        "            text = text.split(' ')\n",
        "            random.shuffle(text)\n",
        "            text2 = ' '.join(text)\n",
        "            df2['text'][i]=text2\n",
        "            random.shuffle(text)\n",
        "            text3 = ' '.join(text)\n",
        "            df3['text'][i]=text3\n",
        "        self.data = self.data.append(df2, ignore_index=True)\n",
        "        self.data = self.data.append(df3, ignore_index=True)\n",
        "        self.data = self.data.reset_index(drop=True)\n",
        "        print(f\"New data length : {len(self.data)}\")\n",
        "\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['tweet_id'][index]\n",
        "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, label, image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        model = torchvision.models.resnet152(pretrained=True)\n",
        "        modules = list(model.children())[:-2]\n",
        "        # we are removing the last adaptive average pooling layer and the \n",
        "        # the classification layer\n",
        "        self.model = nn.Sequential(*modules)\n",
        "        if(torch.cuda.is_available()):\n",
        "            self.model = self.model.cuda()\n",
        "        # self.model = self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = (self.model(x))\n",
        "        # print('Model output', out.size())\n",
        "\n",
        "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
        "        # to be obtained after pooling\n",
        "        # print('Pooling output', out.size())\n",
        "\n",
        "        out = torch.flatten(out, start_dim=2)\n",
        "        # print('Flattening output', out.size())\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        # print('Transpose output', out.size())\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi={}#string to index dictionary\n",
        "            self.itos=[]#index to string dictionary\n",
        "            self.vocab_size=0\n",
        "        else:\n",
        "            self.stoi={\n",
        "                w:i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_size = len(self.itos)\n",
        "    \n",
        "    def add(self, words):\n",
        "        counter = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w]=counter\n",
        "            counter+=1\n",
        "            self.itos.append(w)\n",
        "        self.vocab_size = len(self.itos)\n",
        "\n",
        "class ImageEmbeddingsForBert(nn.Module):\n",
        "    def __init__(self, embeddings, vocabObject):\n",
        "        super(ImageEmbeddingsForBert, self).__init__()\n",
        "        self.vocab = vocabObject\n",
        "#       the embeddins received as input are the \n",
        "#       all the embeddings provided by the bert model from pytorch\n",
        "        self.img_embeddings = nn.Linear(2048, 768)\n",
        "#       above is linear layer is used to convert the flattened images \n",
        "#       logits obtained after pooling from Image encoder which have 2048\n",
        "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
        "        \n",
        "        self.position_embeddings = embeddings.position_embeddings\n",
        "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
        "        self.word_embeddings = embeddings.word_embeddings\n",
        "        self.LayerNorm = embeddings.LayerNorm\n",
        "        self.dropout = embeddings.dropout\n",
        "        \n",
        "    def forward(self, batch_input_imgs, token_type_ids):\n",
        "        batch_size = batch_input_imgs.size(0)\n",
        "        seq_length = 7 + 2\n",
        "#         since we are assuming that from each image we will obtain\n",
        "#         7 image embeddings of 768 dimensions each\n",
        "        \n",
        "        cls_id = torch.LongTensor([101])\n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "            self.word_embeddings = self.word_embeddings.cuda()\n",
        "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            cls_id = cls_id.cuda()\n",
        "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
        "        \n",
        "        sep_id = torch.LongTensor([102])\n",
        "        if torch.cuda.is_available():\n",
        "            sep_id = sep_id.cuda()\n",
        "            self.img_embeddings = self.img_embeddings.cuda()\n",
        "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
        "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
        "        \n",
        "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
        "        \n",
        "        token_embeddings = torch.cat(\n",
        "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
        "        \n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            position_ids = position_ids.cuda()\n",
        "            self.position_embeddings = self.position_embeddings.cuda()\n",
        "            self.token_type_embeddings= self.token_type_embeddings.cuda()\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "        \n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
        "        if torch.cuda.is_available():\n",
        "            embeddings = embeddings.cuda()\n",
        "            self.LayerNorm=self.LayerNorm.cuda()\n",
        "            self.dropout=self.dropout.cuda()\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiModalBertEncoder(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertEncoder, self).__init__()\n",
        "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embeddings = bert.embeddings\n",
        "        self.vocab=Vocab()\n",
        "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.encoder = bert.encoder\n",
        "        self.pooler = bert.pooler\n",
        "        self.clf = nn.Linear(768, no_of_classes)\n",
        "        \n",
        "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
        "        batch_size = input_text.size(0)\n",
        "# input text is a tensor of encoded texts!\n",
        "        temp = torch.ones(batch_size, 7+2).long()\n",
        "        if torch.cuda.is_available():\n",
        "            temp = temp.cuda()\n",
        "            self.encoder = self.encoder.cuda()\n",
        "            self.pooler = self.pooler.cuda()\n",
        "        attention_mask = torch.cat(\n",
        "            [\n",
        "                temp, text_attention_mask\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "#         print(attention_mask.shape, extended_attention_mask.shape)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype\n",
        "        )\n",
        "        # extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
        "        \n",
        "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
        "        if(torch.cuda.is_available()):\n",
        "            image_token_type_ids= image_token_type_ids.cuda()\n",
        "        \n",
        "        image = self.image_encoder(input_image)\n",
        "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
        "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
        "#         print('Image embeddings: ', image_embedding_out.size())\n",
        "        \n",
        "        text_embedding_out = self.embeddings(input_text, text_segment)\n",
        "#         print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
        "#         print(input_text, text_embedding_out)\n",
        "        \n",
        "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
        "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
        "    \n",
        "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
        "        # above function returns the hidden states off all the layers L in the bert model. in case of bert base, L = 12;\n",
        "        # if output all encoded layers is false, then only returns the hidden state of the last self attention layer\n",
        "        # print('ENCODED_LAYERS',encoded_layers[-1],'enc layers2', encoded_layers[-1][:][0])\n",
        "        final = self.pooler(encoded_layers[-1])\n",
        "        # print('FINAL POOLED LAYERS', final, final.size())\n",
        "#         print('encoded layers', encoded_layers)\n",
        "        return final\n",
        "        # how to extract CLS layer\n",
        "        \n",
        "\n",
        "class MultiModalBertClf(nn.Module):\n",
        "    def __init__(self, no_of_classes, tokenizer):\n",
        "        super(MultiModalBertClf, self).__init__()\n",
        "        self.no_of_classes = no_of_classes\n",
        "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
        "        # self.layer1 = nn.Linear(768, 512)\n",
        "        # self.layer2 = nn.Linear(512, 256)\n",
        "        self.batch_norm = nn.BatchNorm1d(768)\n",
        "        self.clf = nn.Linear(768, self.no_of_classes)\n",
        "    \n",
        "    def forward(self, text, text_attention_mask, text_segment, image):\n",
        "        if(torch.cuda.is_available()):\n",
        "            text = text.cuda()\n",
        "            text_attention_mask=text_attention_mask.cuda()\n",
        "            text_segment=text_segment.cuda()\n",
        "            image = image.cuda()\n",
        "            self.clf = self.clf.cuda()\n",
        "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
        "        # x = F.relu(self.layer1(x))\n",
        "        # x = F.relu(self.layer2(x))\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.clf(x)\n",
        "        # print('Sigmoid output: ',torch.sigmoid(x))\n",
        "        return x \n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    # read the focal loss paper\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(-1), y_true.squeeze(-1), reduce = None)#this automatically  takes sigmoid of logits\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(y_pred, y_true, reduce = None)\n",
        "            \n",
        "        pt = torch.exp(-BCE_loss)\n",
        "#       # pt = p if y = 1\n",
        "#       # pt = 1 - p if y = else\n",
        "#       p is the predicted value, y is the target label\n",
        "        # pt is used to indicate if the prediction matches the target or not\n",
        "        # if pt->1, then proper classification, else if pt->0, then misclassification\n",
        "        # so focal loss basically downweights the loss generated in a proper classification\n",
        "        # but does not change downweight the loss in a miss classification\n",
        "        F_loss =self.alpha * ((1-pt)**self.gamma) * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        return F_loss\n",
        "        \n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, logits = True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, logits=True, smooth=1):\n",
        "        if(logits):\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred*y_true).sum()\n",
        "        pred_sum = (y_pred*y_pred).sum()\n",
        "        true_sum = (y_true*y_true).sum()\n",
        "\n",
        "        return 1 - (2 * intersection + smooth) / (pred_sum + true_sum+smooth)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kS4hVKn3OBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    \n",
        "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
        "    \n",
        "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    if task_type=='multilabel':\n",
        "        label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "    \n",
        "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
        "\n",
        "\n",
        "def get_optimizer(model, train_data_len, batch_size = 4, gradient_accumulation_steps=1, max_epochs=3, lr=0.001):\n",
        "    total_steps = (\n",
        "        train_data_len\n",
        "        / batch_size\n",
        "        / gradient_accumulation_steps\n",
        "        * max_epochs\n",
        "    )\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "    ]\n",
        "    # print('OPTIMIZER PARAMS', optimizer_grouped_parameters)\n",
        "    optimizer = BertAdam(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=lr,\n",
        "#         warmup=args.warmup,\n",
        "        t_total=total_steps,\n",
        "    )\n",
        "#     optimizer = optim.Adam(\n",
        "#         optimizer_grouped_parameters,\n",
        "#         lr=lr,\n",
        "# #         warmup=args.warmup,\n",
        "#         t_total=total_steps,\n",
        "#     )\n",
        "    return optimizer\n",
        "\n",
        "def model_forward(i_epoch, model, criterion, batch):\n",
        "    txt, tgt, segment, mask, img= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    if(torch.cuda.is_available()):\n",
        "        tgt = tgt.cuda()\n",
        "    # print()\n",
        "    loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return loss, out, tgt\n",
        "\n",
        "\n",
        "def store_preds_to_disk(tgts, preds, savedir):\n",
        "    str_time = str(datetime.datetime.now())\n",
        "    with open(os.path.join(savedir, \"./test_labels_pred_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in preds]))\n",
        "    with open(os.path.join(savedir, \"./test_labels_actual_\"+str_time+\"_.txt\"), \"w\") as fw:\n",
        "        fw.write(\"\\n\".join([str(x) for x in tgts]))\n",
        "#     with open(os.path.join(savedir, \"test_labels.txt\"), \"w\") as fw:\n",
        "#         fw.write(\" \".join([str(l) for l in alabels]))\n",
        "\n",
        "\n",
        "def model_eval(i_epoch, data, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts = [], [], []\n",
        "        for batch in data:\n",
        "            loss, out, tgt = model_forward(i_epoch, model, criterion, batch)\n",
        "            losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "                \n",
        "            preds.append(pred)\n",
        "            tgt = tgt.cpu().detach().numpy()\n",
        "            tgts.append(tgt)\n",
        "\n",
        "    metrics = {\"loss\": np.mean(losses)}\n",
        "    tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    if store_preds:\n",
        "        store_preds_to_disk(tgts, preds, './')\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA_xWa87RDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SubmissionDataset(Dataset):\n",
        "    def __init__(self, data, image_path, transforms, tokenizer, vocab):\n",
        "        self.data = data\n",
        "        self.image_path = (image_path)\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_sent_len = 128 - 7 - 2 #since there will be [CLS] <7 image embeddings> [SEP] <the text embeddings>\n",
        "        self.vocab = vocab\n",
        "    def __getitem__(self,  index):\n",
        "        text = self.data['text'][index]\n",
        "        text = str(text)\n",
        "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
        "        text = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(text))\n",
        "        tweet_id = self.data['TweetId'][index]\n",
        "#         label = torch.LongTensor([self.data[self.label_name][index]])\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(\n",
        "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
        "            ).convert(\"RGB\")\n",
        "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
        "#             image.show()\n",
        "            image = self.transforms(image)\n",
        "        except:\n",
        "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        return text, image, tweet_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def collate_function_for_submission(batch, task_type='singlelabel'):\n",
        "    lengths = [len(row[0]) for row in batch]\n",
        "    batch_size = len(batch)\n",
        "    max_sent_len = max(lengths)\n",
        "    if(max_sent_len>128-7-2):\n",
        "        max_sent_len=128-7-2\n",
        "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
        "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
        "    batch_image_tensors = torch.stack([row[1] for row in batch])\n",
        "    tweet_id_tensors = torch.zeros(batch_size, 1).long()\n",
        "    \n",
        "    # label_tensors = torch.cat([row[1] for row in batch]).long()\n",
        "    # if task_type=='multilabel':\n",
        "        # label_tensors = torch.stack([row[1] for row in batch])\n",
        "#     note there is a difference between stack and cat, refer link below if needed\n",
        "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
        "    \n",
        "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
        "        text_tokens = row[0]\n",
        "        if(length>128-7-2):\n",
        "            length = 128-7-2\n",
        "        text_tensors[i, :length] = text_tokens\n",
        "        text_segment[i, :length] = 1#since images will constitute segment 0\n",
        "        text_attention_mask[i, :length]=1\n",
        "        tweet_id_tensors[i, 0]=row[2]\n",
        "    \n",
        "    return text_tensors, text_segment, text_attention_mask, batch_image_tensors, tweet_id_tensors"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroLei1K7M2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(label_name, no_of_classes, max_epochs, train_df, val_df, img_transformations, bert_tokenizer, vocab, gradient_accumulation_steps=1, patience=0):\n",
        "    \n",
        "    train_dataset = TextNImageDataset(train_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    val_dataset = TextNImageDataset(val_df, './train_images', label_name, img_transformations, bert_tokenizer, vocab, minority_class)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader, drop_last=True)\n",
        "\n",
        "    model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "        print('Loaded previous model state successfully!')\n",
        "    except:\n",
        "        print('Starting fresh! Previous model state dict load unsuccessful')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if no_of_classes==1:\n",
        "        print('using '+str(chosen_criteria)+' loss')\n",
        "        criterion = chosen_criteria\n",
        "    optimizer = get_optimizer(model, train_dataset.__len__(), max_epochs=max_epochs, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \"max\", \n",
        "        patience=patience, \n",
        "        verbose=True, \n",
        "#         factor=args.lr_factor\n",
        "    )\n",
        "    if(torch.cuda.is_available()):\n",
        "        model=model.cuda()\n",
        "\n",
        "\n",
        "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "\n",
        "    print(\"Training..\")\n",
        "    for i_epoch in range(start_epoch, max_epochs):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in tqdm.notebook.tqdm(train_loader, total=len(train_loader)):\n",
        "            loss, _, _ = model_forward(i_epoch, model, criterion, batch)\n",
        "            # if gradient_accumulation_steps > 1:\n",
        "            #     loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            global_step += 1\n",
        "            if global_step % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        metrics = model_eval(i_epoch, val_loader, model, criterion, no_of_classes, True)\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
        "        print('Train Losses :', train_losses)\n",
        "        print(\"Val loss\", metrics['loss'])\n",
        "        # print(metrics['acc'])\n",
        "        # print(metrics['classification_report'])\n",
        "        print('Val auc roc', metrics['roc_auc_score'])\n",
        "        tuning_metric = ( metrics['roc_auc_score'])\n",
        "        scheduler.step(tuning_metric)\n",
        "        is_improvement = tuning_metric > best_metric\n",
        "        if is_improvement:\n",
        "            best_metric = tuning_metric\n",
        "            n_no_improve = 0\n",
        "        else:\n",
        "            n_no_improve += 1\n",
        "        \n",
        "        torch.save(model.state_dict(), './model_state_dict.pth')\n",
        "        print(f'Saved model state dict for epoch {i_epoch} ')\n",
        "        # if n_no_improve >= patience:\n",
        "        #     print(\"No improvement. Breaking out of loop.\")\n",
        "        #     break\n",
        "\n",
        "#     load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
        "#     model.eval()\n",
        "# #     for test_name, test_loader in test_loaders.items():\n",
        "#     test_metrics = model_eval(\n",
        "#         np.inf, val_loader, model, criterion, no_of_classes, store_preds=True\n",
        "#     )\n",
        "#     print(f\"Test - \", test_metrics['loss'])\n",
        "#     print(test_metrics['acc'])\n",
        "#     print(test_metrics['classification_report'])\n",
        "#     print(test_metrics['roc_auc_score'])\n",
        "\n",
        "#     torch.save(model.state_dict(), './modelv1.pth')\n",
        "    return model\n",
        "    # return model, test_metrics\n",
        "\n",
        "\n",
        "def model_forward_predict(i_epoch, model, criterion, batch):\n",
        "    txt, segment, mask, img, tweet_id= batch\n",
        "\n",
        "#         for param in model.enc.img_encoder.parameters():\n",
        "#             param.requires_grad = not freeze_img\n",
        "#         for param in model.enc.encoder.parameters():\n",
        "#             param.requires_grad = not freeze_txt\n",
        "    if(torch.cuda.is_available()):\n",
        "        txt, img = txt.cuda(), img.cuda()\n",
        "        mask, segment = mask.cuda(), segment.cuda()\n",
        "    out = model(txt, mask, segment, img)\n",
        "    # if(torch.cuda.is_available()):\n",
        "    #     tgt = tgt.cuda()\n",
        "    # loss = criterion(out*1.0, tgt.unsqueeze(1)*1.0)\n",
        "    return out, tweet_id\n",
        "\n",
        "\n",
        "def model_predict(dataloader, model, criterion, no_of_classes, store_preds=False):\n",
        "    with torch.no_grad():\n",
        "        losses, preds, tgts, tweet_ids = [], [], [], []\n",
        "        for batch in dataloader:\n",
        "            out, tweet_id = model_forward_predict(1, model, criterion, batch)\n",
        "            # losses.append(loss.item())\n",
        "            if no_of_classes==1:\n",
        "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
        "                # for i in range(pred.shape[0]):\n",
        "                #     if pred[i][0]>0.5:\n",
        "                #         pred[i][0]=1\n",
        "                #     else:\n",
        "                #         pred[i][0]=0\n",
        "            else:\n",
        "                pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "            # for i in range(4):\n",
        "            #     if(pred[i])\n",
        "            \n",
        "            # print('preddhd', pred)\n",
        "            # if pred > 0.5:\n",
        "            #     preds.append(1)\n",
        "            # else:\n",
        "            #     preds.append(0)\n",
        "\n",
        "            preds.append(pred)\n",
        "            # tgt = tgt.cpu().detach().numpy()\n",
        "            # tgts.append(tgt)\n",
        "            tweet_id = tweet_id.cpu().detach().numpy()\n",
        "            tweet_ids.append(tweet_id)\n",
        "\n",
        "    # metrics = {\"loss\": np.mean(losses)}\n",
        "    # tgts = [l for sl in tgts for l in sl]\n",
        "    preds = [l for sl in preds for l in sl]\n",
        "    # for i in len(preds):\n",
        "    #     if preds[i]>0.5:\n",
        "    #         preds[i]=1\n",
        "    #     else:\n",
        "    #         preds[i]=0\n",
        "    tweet_ids = [l for sl in tweet_ids for l in sl]\n",
        "    # metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
        "    # metrics['classification_report'] = classification_report(tgts, preds)\n",
        "    # metrics['roc_auc_score'] = roc_auc_score(tgts, preds)\n",
        "\n",
        "    # if store_preds:\n",
        "    #     store_preds_to_disk(tweet_ids, preds, './')\n",
        "\n",
        "    return preds, tweet_ids"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEETPiGryzOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "630cc97a-1bbc-4e6e-d809-5844e6cd848c"
      },
      "source": [
        "col_name = \"Allegation\"\n",
        "train_epochs = 3\n",
        "losses = [FocalLoss, DiceLoss, nn.BCEWithLogitsLoss]\n",
        "chosen_criteria = losses[0]()\n",
        "no_of_classes = 1\n",
        "print(str(chosen_criteria))\n",
        "minority_class = 1 # or 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FocalLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kABURr7vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5z7hFf4D3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762,
          "referenced_widgets": [
            "9f7ca443ef9549c0938d6cf8ce58bb88",
            "210ae5e4b0ad455fa17c11f9a225cfae",
            "e7a7d12f3cd34532a1c54ddda75f37c8",
            "ad08224dba794873b80559f573460c85",
            "21bb2b3472514864ad27c6c557828ede",
            "edf1998359d44bb2a4f67bdbca2b8571",
            "ed60a8cc56fc4a67a5786493dc572c05",
            "8c08fe0d350043f68aac2704065dfc45",
            "c6cf58ed355040439685955a6f92bd64",
            "64e34c45816d4713b0b1fb77f88d5dd6",
            "97457992502d4ed8b707852791e50c66",
            "9de27d673c8c4b18a6fb242152faa27d",
            "e2e3510a23794a3cb7b34694c76db6de",
            "28f3425ce822484eaf2a7058e9b6b22f",
            "66a3cd99149448b98f71818439a7d816",
            "2e00f15f80d14378902780e5daa24378",
            "61af8ce243c7424cb881612bda138989",
            "915f8b7299dc4593aa413dd6ad4a3708",
            "a7fe4aa90edb4c86b3bf5662a5a8a80e",
            "ea1ce1095e8b4cd4a9f751eaed277ee1",
            "1212436e4aef4d03a592343b2c50024a",
            "e9321d148832459983d82233428aba80",
            "b83e474d48254932b35399c6695afa04",
            "1f2eacc28fe7434ea1aa1d5f05313bbf"
          ]
        },
        "outputId": "5749b2fe-b83e-4a7c-ceca-957a98d81d31"
      },
      "source": [
        "model = train(col_name, no_of_classes, train_epochs, train_df , val_df, img_transformations, bert_tokenizer, vocab)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old data length : 6382\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 7082\n",
            "Old data length : 1596\n",
            "minority class is 1. Duplicating minority class data!\n",
            "New data length : 1764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n",
            "using FocalLoss() loss\n",
            "Training..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f7ca443ef9549c0938d6cf8ce58bb88",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1770.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0819\n",
            "Train Losses : [0.08424893766641617, 0.08117010444402695, 0.025195011869072914, 0.017514029517769814, 0.009773624129593372, 0.1069190502166748, 0.10312513262033463, 0.0040606544353067875, 0.004054075572639704, 0.10979986190795898, 0.004531772341579199, 0.009336224757134914, 0.5200749635696411, 0.022727739065885544, 0.17586319148540497, 0.01094800140708685, 0.23263822495937347, 0.013969662599265575, 0.11403282731771469, 0.43437325954437256, 0.012887854129076004, 0.014526022598147392, 0.027642883360385895, 0.0233226977288723, 0.22696708142757416, 0.015860240906476974, 0.014974507503211498, 0.015575414523482323, 0.519061267375946, 0.014161776751279831, 0.14356689155101776, 0.19883723556995392, 0.07806914299726486, 0.1574183702468872, 0.02137463353574276, 0.021346790716052055, 0.07943370193243027, 0.08003638684749603, 0.49307578802108765, 0.1087309718132019, 0.09375204890966415, 0.11558113992214203, 0.04393377900123596, 0.04702647402882576, 0.04882235825061798, 0.046844251453876495, 0.04325035214424133, 0.03817083314061165, 0.03217102587223053, 0.028639113530516624, 0.021389415487647057, 0.01870662160217762, 0.013368159532546997, 0.011026921682059765, 0.008278596214950085, 0.006502488628029823, 0.004791920073330402, 0.0036521744914352894, 0.0029936651699244976, 0.09886489808559418, 0.0018323377007618546, 0.13933928310871124, 0.5850287675857544, 0.0021819784305989742, 0.4128769338130951, 0.004433115012943745, 0.006519145332276821, 0.11289102584123611, 0.01161586306989193, 0.10024001449346542, 0.017052743583917618, 0.2883082628250122, 0.023891214281320572, 0.029288949444890022, 0.17534016072750092, 0.03207651898264885, 0.17663700878620148, 0.2537403702735901, 0.11033999919891357, 0.044161804020404816, 0.046457529067993164, 0.04813295975327492, 0.10763305425643921, 0.044685009866952896, 0.041174232959747314, 0.038003068417310715, 0.03268437087535858, 0.02996205911040306, 0.0928686186671257, 0.02208983153104782, 0.08408612757921219, 0.08386332541704178, 0.015040692873299122, 0.08189001679420471, 0.011546391062438488, 0.010448964312672615, 0.06244080886244774, 0.008436976000666618, 0.008401632308959961, 0.008468136191368103, 0.4287024140357971, 0.19173741340637207, 0.20238329470157623, 0.008288792334496975, 0.29100632667541504, 0.01015779934823513, 0.08348802477121353, 0.12616249918937683, 0.10203968733549118, 0.015367922373116016, 0.017415588721632957, 0.01717725582420826, 0.01789509877562523, 0.018467292189598083, 0.1238333135843277, 0.01919364556670189, 0.017782917246222496, 0.017487652599811554, 0.1425532102584839, 0.016083046793937683, 0.01591605693101883, 0.14483872056007385, 0.08288522064685822, 0.10486188530921936, 0.012929962947964668, 0.1002735123038292, 0.0119200823828578, 0.1384810209274292, 0.011278190650045872, 0.010986397974193096, 0.010206892155110836, 0.0100992601364851, 0.00951413158327341, 0.3265300393104553, 0.0995059534907341, 0.08715887367725372, 0.6522765159606934, 0.012519503012299538, 0.09481694549322128, 0.01777614839375019, 0.020022207871079445, 0.23879919946193695, 0.02473132312297821, 0.027278026565909386, 0.029290569946169853, 0.12367750704288483, 0.23086422681808472, 0.03396350517868996, 0.03486957773566246, 0.560458779335022, 0.1051783636212349, 0.045276690274477005, 0.09896589070558548, 0.054946787655353546, 0.06014937534928322, 0.08209389448165894, 0.0842592790722847, 0.057910993695259094, 0.08404859900474548, 0.0573735274374485, 0.14210380613803864, 0.050916340202093124, 0.05212501063942909, 0.1983361393213272, 0.14203856885433197, 0.041328851133584976, 0.03974425047636032, 0.08126849681138992, 0.035957079380750656, 0.0320773720741272, 0.178622305393219, 0.11681638658046722, 0.09059225022792816, 0.163218155503273, 0.12017444521188736, 0.08558821678161621, 0.023347944021224976, 0.023749660700559616, 0.022481650114059448, 0.020383276045322418, 0.018867410719394684, 0.018101871013641357, 0.018192632123827934, 0.015212415717542171, 0.014704239554703236, 0.11872412264347076, 0.09681462496519089, 0.011622022837400436, 0.011175421997904778, 0.010187950916588306, 0.009841659106314182, 0.11140581965446472, 0.10052405297756195, 0.07652337104082108, 0.0730333924293518, 0.007899178192019463, 0.2913751006126404, 0.00797216035425663, 0.008769335225224495, 0.00856103003025055, 0.009372415952384472, 0.2406594455242157, 0.009508335962891579, 0.42139992117881775, 0.009918978437781334, 0.3737056255340576, 0.35959115624427795, 0.09944353252649307, 0.015259906649589539, 0.08378548175096512, 0.2214760035276413, 0.11519373208284378, 0.0910516232252121, 0.1093810424208641, 0.1051991730928421, 0.23532462120056152, 0.031585849821567535, 0.081419438123703, 0.0967848151922226, 0.035902250558137894, 0.09821411222219467, 0.2317621260881424, 0.12346067279577255, 0.0888066440820694, 0.0952354222536087, 0.11705943942070007, 0.04024653881788254, 0.04010039567947388, 0.03928113728761673, 0.039859261363744736, 0.11627889424562454, 0.2511214017868042, 0.03554427996277809, 0.10635867714881897, 0.03363659977912903, 0.03278938680887222, 0.030994459986686707, 0.030104203149676323, 0.028277572244405746, 0.1131395474076271, 0.12941628694534302, 0.023609645664691925, 0.09480367600917816, 0.021899811923503876, 0.09231599420309067, 0.13681542873382568, 0.018377525731921196, 0.017129307612776756, 0.09050775319337845, 0.13840264081954956, 0.10043049603700638, 0.014197902753949165, 0.013615666888654232, 0.012982573360204697, 0.5881874561309814, 0.013135004788637161, 0.013883748091757298, 0.10378934442996979, 0.015010405331850052, 0.015353605151176453, 0.015618596225976944, 0.015730008482933044, 0.015710290521383286, 0.015653688460588455, 0.015469947829842567, 0.015162591822445393, 0.014824504032731056, 0.09817976504564285, 0.2887163758277893, 0.014434043318033218, 0.014433059841394424, 0.01446075364947319, 0.10158196091651917, 0.014472386799752712, 0.11332141607999802, 0.014107556082308292, 0.013975458219647408, 0.11840393394231796, 0.013538338243961334, 0.013350263237953186, 0.012948323972523212, 0.09257300943136215, 0.012333757244050503, 0.10780689865350723, 0.011769273318350315, 0.31834855675697327, 0.011726724915206432, 0.01192550454288721, 0.012021705508232117, 0.10642404854297638, 0.09834425151348114, 0.10849396884441376, 0.31513553857803345, 0.293550580739975, 0.10178710520267487, 0.11168285459280014, 0.26086780428886414, 0.01767718233168125, 0.10742039978504181, 0.020255347713828087, 0.1029164269566536, 0.022229989990592003, 0.1010284498333931, 0.2582521438598633, 0.02460589073598385, 0.09861006587743759, 0.10801858454942703, 0.02680128440260887, 0.4116813540458679, 0.09864041954278946, 0.030451085418462753, 0.03169850632548332, 0.11524242907762527, 0.03309404477477074, 0.03350873664021492, 0.11754297465085983, 0.03330174833536148, 0.0330459401011467, 0.2167573720216751, 0.032330144196748734, 0.03204689547419548, 0.10051596909761429, 0.10313128679990768, 0.030403148382902145, 0.02962529845535755, 0.09283225238323212, 0.027841173112392426, 0.10372765362262726, 0.02604987472295761, 0.02492772787809372, 0.09134368598461151, 0.09942293167114258, 0.02214205078780651, 0.2535882890224457, 0.0918949544429779, 0.020605331286787987, 0.09253738820552826, 0.10519959777593613, 0.01957905851304531, 0.10945846140384674, 0.0188877135515213, 0.018341660499572754, 0.017815101891756058, 0.017234792932868004, 0.01668202504515648, 0.27584654092788696, 0.10496123880147934, 0.08767417073249817, 0.12964504957199097, 0.01579182595014572, 0.2576003074645996, 0.30282095074653625, 0.10736794769763947, 0.09075086563825607, 0.018466142937541008, 0.019169438630342484, 0.019524378702044487, 0.019624747335910797, 0.019819388166069984, 0.01977413147687912, 0.019512927159667015, 0.019288918003439903, 0.018858714029192924, 0.48593541979789734, 0.12551981210708618, 0.019965119659900665, 0.020543672144412994, 0.021103791892528534, 0.02118033543229103, 0.021402671933174133, 0.11096005886793137, 0.021244117990136147, 0.08625086396932602, 0.11450125277042389, 0.02042013593018055, 0.020148586481809616, 0.11890443414449692, 0.019259916618466377, 0.01873377338051796, 0.2703897953033447, 0.018324967473745346, 0.1039261594414711, 0.3027712404727936, 0.018573056906461716, 0.10282818973064423, 0.2551042139530182, 0.0197178665548563, 0.0203175600618124, 0.020571349188685417, 0.24024741351604462, 0.021327631548047066, 0.09301238507032394, 0.11370039731264114, 0.022464197129011154, 0.022617410868406296, 0.26956039667129517, 0.11997973918914795, 0.10429853945970535, 0.02378254383802414, 0.024009419605135918, 0.023746032267808914, 0.10256777703762054, 0.023402418941259384, 0.02314021624624729, 0.02253330498933792, 0.02210453897714615, 0.10547196120023727, 0.0996512919664383, 0.020238017663359642, 0.09925435483455658, 0.2978225648403168, 0.11000248044729233, 0.019272685050964355, 0.01921384036540985, 0.2439519166946411, 0.01914113573729992, 0.01940017379820347, 0.12129765003919601, 0.019369017332792282, 0.1057765930891037, 0.018926938995718956, 0.10727065056562424, 0.09787741303443909, 0.10302091389894485, 0.09330341964960098, 0.01798749342560768, 0.017721522599458694, 0.017402419820427895, 0.5037873387336731, 0.1021420806646347, 0.018679169937968254, 0.01926516368985176, 0.01963094435632229, 0.10308228433132172, 0.02029433846473694, 0.020143907517194748, 0.08567681908607483, 0.020025961101055145, 0.02001030184328556, 0.019683338701725006, 0.11070553213357925, 0.019041892141103745, 0.10894666612148285, 0.10421483963727951, 0.01798657327890396, 0.017435815185308456, 0.10177575796842575, 0.016753116622567177, 0.016271211206912994, 0.10632726550102234, 0.10958877950906754, 0.014994105324149132, 0.2717908024787903, 0.014748151414096355, 0.014681207947432995, 0.25705599784851074, 0.3277895748615265, 0.10242099314928055, 0.01653256081044674, 0.017201866954565048, 0.2904111444950104, 0.01839236356317997, 0.019114630296826363, 0.019649023190140724, 0.1020762026309967, 0.0204301904886961, 0.10018474608659744, 0.10876116156578064, 0.020877595990896225, 0.22846221923828125, 0.10358763486146927, 0.09024670720100403, 0.021884214133024216, 0.12185341864824295, 0.022141166031360626, 0.08654488623142242, 0.09542407095432281, 0.2847157418727875, 0.24032378196716309, 0.023198211565613747, 0.02398253045976162, 0.02416142448782921, 0.08496231585741043, 0.1095854640007019, 0.024716852232813835, 0.02455998584628105, 0.024716196581721306, 0.024132054299116135, 0.10228738188743591, 0.02311890386044979, 0.022602815181016922, 0.02204308658838272, 0.10858859866857529, 0.020966019481420517, 0.0201579499989748, 0.019479500129818916, 0.01874825917184353, 0.11024219542741776, 0.26920899748802185, 0.10074052959680557, 0.10486963391304016, 0.0167827308177948, 0.016638632863759995, 0.01641637273132801, 0.016035020351409912, 0.015794208273291588, 0.11022257804870605, 0.014964176341891289, 0.10101845115423203, 0.10317031294107437, 0.28119829297065735, 0.5547857284545898, 0.015335701406002045, 0.016412321478128433, 0.01730172336101532, 0.01808897964656353, 0.12122781574726105, 0.019530879333615303, 0.019646789878606796, 0.09585905075073242, 0.02027331292629242, 0.020300518721342087, 0.020188288763165474, 0.020242109894752502, 0.019705617800354958, 0.019503364339470863, 0.01903337985277176, 0.018582673743367195, 0.12258470058441162, 0.017571741715073586, 0.10194945335388184, 0.016601936891674995, 0.016225766390562057, 0.09908311069011688, 0.015234679915010929, 0.014789324253797531, 0.014452807605266571, 0.28293377161026, 0.30208033323287964, 0.014238394796848297, 0.014580908231437206, 0.29182836413383484, 0.015331205911934376, 0.015915045514702797, 0.016288042068481445, 0.09796988219022751, 0.016743043437600136, 0.11337976157665253, 0.0890839546918869, 0.016840657219290733, 0.1149507686495781, 0.017016150057315826, 0.016821160912513733, 0.09340231120586395, 0.10163886845111847, 0.016339655965566635, 0.016283322125673294, 0.2606904208660126, 0.01631888560950756, 0.016212627291679382, 0.016170991584658623, 0.5505109429359436, 0.017232747748494148, 0.4993538558483124, 0.0987103208899498, 0.10025566816329956, 0.02291027083992958, 0.02439037151634693, 0.12501384317874908, 0.02633468620479107, 0.027069004252552986, 0.027819447219371796, 0.028381627053022385, 0.20649969577789307, 0.028642842546105385, 0.02902122214436531, 0.029024040326476097, 0.09925021231174469, 0.028748927637934685, 0.10426701605319977, 0.08717849105596542, 0.02782457135617733, 0.10690546035766602, 0.0268645528703928, 0.1080130785703659, 0.025577988475561142, 0.025135140866041183, 0.09063674509525299, 0.02383156679570675, 0.09501263499259949, 0.022318508476018906, 0.021936900913715363, 0.02101064659655094, 0.0201897993683815, 0.08727231621742249, 0.10356232523918152, 0.018392957746982574, 0.089333675801754, 0.11222205311059952, 0.09234101325273514, 0.2970010042190552, 0.016358356922864914, 0.01630830019712448, 0.25512391328811646, 0.29449182748794556, 0.017094017937779427, 0.10238657146692276, 0.11443047225475311, 0.01838253065943718, 0.1276610791683197, 0.11470536887645721, 0.01931648701429367, 0.01935579814016819, 0.01927955448627472, 0.09920485317707062, 0.01903553679585457, 0.12099052220582962, 0.018720464780926704, 0.018509073182940483, 0.2414456307888031, 0.01833835244178772, 0.018346669152379036, 0.018227413296699524, 0.10149949789047241, 0.017875999212265015, 0.017673972994089127, 0.017429756000638008, 0.10012488812208176, 0.09267625212669373, 0.016468659043312073, 0.016399497166275978, 0.0887267217040062, 0.553087592124939, 0.09149310737848282, 0.01682817004621029, 0.017378559336066246, 0.017825257033109665, 0.01794825680553913, 0.01833229884505272, 0.0181301087141037, 0.018159613013267517, 0.01798764429986477, 0.11830414831638336, 0.25442689657211304, 0.10893797129392624, 0.01801573671400547, 0.10854073613882065, 0.2556029260158539, 0.018593719229102135, 0.09038121998310089, 0.2606094777584076, 0.019796961918473244, 0.020240947604179382, 0.2737424671649933, 0.021262915804982185, 0.021716440096497536, 0.11277496814727783, 0.02241026982665062, 0.09935639798641205, 0.2286912351846695, 0.10723695904016495, 0.10403264313936234, 0.02400083839893341, 0.024185849353671074, 0.02422814629971981, 0.024055369198322296, 0.023841965943574905, 0.023504920303821564, 0.10129635035991669, 0.11095588654279709, 0.022408276796340942, 0.2773101031780243, 0.11484888941049576, 0.02191510982811451, 0.02182851731777191, 0.25172334909439087, 0.09794805198907852, 0.10508085787296295, 0.09972404688596725, 0.02203068882226944, 0.10865728557109833, 0.09111589938402176, 0.2698974311351776, 0.09367749840021133, 0.08912191540002823, 0.022729719057679176, 0.022768406197428703, 0.10121975094079971, 0.435771644115448, 0.09026166051626205, 0.11076942831277847, 0.11138719320297241, 0.09691464900970459, 0.02634510025382042, 0.02671797387301922, 0.0267956480383873, 0.026897061616182327, 0.09213171154260635, 0.026794174686074257, 0.026400288566946983, 0.026074355468153954, 0.025439882650971413, 0.02496214769780636, 0.11471991240978241, 0.023559192195534706, 0.2786899507045746, 0.022773779928684235, 0.02259102836251259, 0.09674882888793945, 0.021862175315618515, 0.02142295055091381, 0.08652479201555252, 0.10817252844572067, 0.10990983992815018, 0.10842268168926239, 0.11214090883731842, 0.2925952672958374, 0.11907238513231277, 0.10822048038244247, 0.11501648277044296, 0.09273959696292877, 0.11701451987028122, 0.23753735423088074, 0.019984500482678413, 0.02016565389931202, 0.020369388163089752, 0.09965634346008301, 0.02037162333726883, 0.020244183018803596, 0.020188696682453156, 0.019784413278102875, 0.019554195925593376, 0.019180933013558388, 0.018773483112454414, 0.11966346949338913, 0.01785740815103054, 0.10588499158620834, 0.10665128380060196, 0.016637522727251053, 0.0980023518204689, 0.015939243137836456, 0.09525973349809647, 0.01530384086072445, 0.2764783203601837, 0.28914162516593933, 0.1161046251654625, 0.28811147809028625, 0.11686539649963379, 0.09238521754741669, 0.2540379762649536, 0.09502014517784119, 0.019452206790447235, 0.020087525248527527, 0.020682478323578835, 0.02099107764661312, 0.021237017586827278, 0.10302258282899857, 0.021512463688850403, 0.11258330941200256, 0.11203189939260483, 0.021424176171422005, 0.10589500516653061, 0.02111208438873291, 0.10629221796989441, 0.020630739629268646, 0.09516017884016037, 0.020233185961842537, 0.2719061076641083, 0.11318577080965042, 0.09723736345767975, 0.0922708585858345, 0.019933098927140236, 0.096869096159935, 0.2634814381599426, 0.11334110796451569, 0.47790372371673584, 0.10430747270584106, 0.022646814584732056, 0.10489897429943085, 0.024162238463759422, 0.024830516427755356, 0.09440523386001587, 0.02557257004082203, 0.1131196841597557, 0.02599746733903885, 0.11432661861181259, 0.10472411662340164, 0.02577030286192894, 0.11045821011066437, 0.10658067464828491, 0.025190597400069237, 0.11730644106864929, 0.024630922824144363, 0.02425382100045681, 0.25882554054260254, 0.02358357422053814, 0.25805583596229553, 0.023537244647741318, 0.10383293777704239, 0.10073445737361908, 0.023609839379787445, 0.09945883601903915, 0.023363236337900162, 0.2591719925403595, 0.02330147475004196, 0.023292778059840202, 0.1029156893491745, 0.023062484338879585, 0.10997829586267471, 0.02264842763543129, 0.022352440282702446, 0.10935718566179276, 0.23895378410816193, 0.021719029173254967, 0.11862558126449585, 0.10893712192773819, 0.021442580968141556, 0.02127707377076149, 0.10033193975687027, 0.11032914370298386, 0.10383500158786774, 0.10258600115776062, 0.10403469204902649, 0.10728410631418228, 0.10158270597457886, 0.01952205039560795, 0.019331688061356544, 0.01902502402663231, 0.09956280887126923, 0.018341191112995148, 0.09612055122852325, 0.017682531848549843, 0.01731855794787407, 0.10206292569637299, 0.09862080216407776, 0.016275720670819283, 0.015942715108394623, 0.015625281259417534, 0.26869407296180725, 0.015193927101790905, 0.10151061415672302, 0.015088695101439953, 0.014980854466557503, 0.014778430573642254, 0.10153751820325851, 0.01441142801195383, 0.014204615727066994, 0.013983530923724174, 0.013716135174036026, 0.01343106385320425, 0.013114936649799347, 0.2851472496986389, 0.10765257477760315, 0.10636577010154724, 0.10038094222545624, 0.10223093628883362, 0.013106363825500011, 0.31382936239242554, 0.013489572331309319, 0.10753867775201797, 0.014013722538948059, 0.014213408343493938, 0.014359716325998306, 0.014368078671395779, 0.014365569688379765, 0.09453534334897995, 0.014250560663640499, 0.10606058686971664, 0.10787898302078247, 0.014011247083544731, 0.09540225565433502, 0.09708815813064575, 0.013769849203526974, 0.10254696756601334, 0.013624012470245361, 0.013526279479265213, 0.013304791413247585, 0.09764783084392548, 0.01296619139611721, 0.01277386024594307, 0.012591223232448101, 0.012386700138449669, 0.012084297835826874, 0.011818048544228077, 0.10814457386732101, 0.011360899545252323, 0.011095280759036541, 0.10588563978672028, 0.010645531117916107, 0.010445804335176945, 0.09854517132043839, 0.10414163768291473, 0.009905772283673286, 0.009796805679798126, 0.009626038372516632, 0.33093103766441345, 0.10232073813676834, 0.00980289001017809, 0.09384278953075409, 0.1148684099316597, 0.010161932557821274, 0.11083384603261948, 0.11038851737976074, 0.10569417476654053, 0.010645408183336258, 0.010767273604869843, 0.10247448086738586, 0.09990774840116501, 0.010935294441878796, 0.01097013894468546, 0.010962660424411297, 0.010918661020696163, 0.010883168317377567, 0.01078067161142826, 0.01061820425093174, 0.11543995141983032, 0.10489558428525925, 0.010298382490873337, 0.010212215594947338, 0.3233065605163574, 0.01029444020241499, 0.010488002561032772, 0.0105386758223176, 0.10734836757183075, 0.010659695602953434, 0.09960297495126724, 0.11478911340236664, 0.010842726565897465, 0.010845969431102276, 0.09571337699890137, 0.010857492685317993, 0.09677557647228241, 0.010842992924153805, 0.11943648755550385, 0.11143741011619568, 0.09727753698825836, 0.010841003619134426, 0.010828180238604546, 0.11207357794046402, 0.11207923293113708, 0.10151340812444687, 0.010864285752177238, 0.010824983939528465, 0.3174033463001251, 0.10940130800008774, 0.011360122822225094, 0.2969922721385956, 0.012081949040293694, 0.10506533831357956, 0.0944528728723526, 0.013306207954883575, 0.09633368998765945, 0.28216317296028137, 0.10187388211488724, 0.015084934420883656, 0.26506564021110535, 0.11222590506076813, 0.01699969731271267, 0.017600541934370995, 0.018070008605718613, 0.10288845002651215, 0.09683097153902054, 0.25428810715675354, 0.10395105928182602, 0.020138300955295563, 0.020478934049606323, 0.1074865460395813, 0.021009374409914017, 0.021079545840620995, 0.09856443107128143, 0.021072031930088997, 0.10271624475717545, 0.02092774398624897, 0.02068156935274601, 0.020440733060240746, 0.02012498490512371, 0.01977791264653206, 0.019348280504345894, 0.10303498059511185, 0.1070047989487648, 0.018112914636731148, 0.017710711807012558, 0.01728314533829689, 0.10705248266458511, 0.016419077292084694, 0.016006603837013245, 0.11316867172718048, 0.015210224315524101, 0.10278000682592392, 0.09703157842159271, 0.28257742524147034, 0.11388018727302551, 0.10658513009548187, 0.27211812138557434, 0.014805254526436329, 0.10711595416069031, 0.015409771353006363, 0.1136423721909523, 0.09687037765979767, 0.01596255972981453, 0.01605955697596073, 0.016043279320001602, 0.016038093715906143, 0.015924056991934776, 0.27441978454589844, 0.015950743108987808, 0.11355576664209366, 0.016128327697515488, 0.10096488147974014, 0.09787579625844955, 0.016176724806427956, 0.10142426937818527, 0.10438598692417145, 0.01614311896264553, 0.01601867377758026, 0.015923121944069862, 0.01574202999472618, 0.015511639416217804, 0.015240644104778767, 0.2747358977794647, 0.10181283950805664, 0.11233501136302948, 0.015135855413973331, 0.015119326300919056, 0.10959679633378983, 0.3000887334346771, 0.015295946970582008, 0.09933558106422424, 0.27712321281433105, 0.25890016555786133, 0.0957443043589592, 0.11199679970741272, 0.09475286304950714, 0.01899619773030281, 0.019456585869193077, 0.2625005841255188, 0.02046755515038967, 0.2765597403049469, 0.10529409348964691, 0.10913798958063126, 0.09478189051151276, 0.023792536929249763, 0.2409968376159668, 0.09605516493320465, 0.10228832811117172, 0.09411558508872986, 0.026563579216599464, 0.026794789358973503, 0.026964429765939713, 0.09662924706935883, 0.10839033126831055, 0.02685661055147648, 0.026660505682229996, 0.026392685249447823, 0.10697374492883682, 0.025577640160918236, 0.025144506245851517, 0.024596579372882843, 0.024027274921536446, 0.0233986247330904, 0.1014496460556984, 0.26988789439201355, 0.1006791889667511, 0.2630661725997925, 0.10475141555070877, 0.021993054077029228, 0.11452192068099976, 0.10021194070577621, 0.11443699151277542, 0.10048479586839676, 0.021877942606806755, 0.02175133116543293, 0.09695134311914444, 0.0983155369758606, 0.11194786429405212, 0.02097408100962639, 0.24466094374656677, 0.02074318937957287, 0.0993083044886589, 0.020726973190903664, 0.020615918561816216, 0.020436428487300873, 0.020170381292700768, 0.10251222550868988, 0.11465197056531906, 0.019313838332891464, 0.10110454261302948, 0.018717333674430847, 0.10020800679922104, 0.10056571662425995, 0.2808878719806671, 0.017917726188898087, 0.017932103946805, 0.09546924382448196, 0.017753466963768005, 0.017628716304898262, 0.09673666208982468, 0.0970197394490242, 0.017183290794491768, 0.016973884776234627, 0.016695315018296242, 0.09099634736776352, 0.016209179535508156, 0.10112680494785309, 0.01568290963768959, 0.015430632047355175, 0.10749327391386032, 0.10726729035377502, 0.014614456333220005, 0.2890326976776123, 0.2904266119003296, 0.014857010915875435, 0.10009501874446869, 0.015407200902700424, 0.015608134679496288, 0.11317932605743408, 0.01587996631860733, 0.1039196252822876, 0.2761494815349579, 0.016302814707159996, 0.01655338890850544, 0.016780301928520203, 0.016831906512379646, 0.016835305839776993, 0.01679418608546257, 0.11187122017145157, 0.11545325815677643, 0.01648538000881672, 0.3006514310836792, 0.09377936273813248, 0.016731414943933487, 0.11194273084402084, 0.016939667984843254, 0.10256904363632202, 0.09590869396924973, 0.2758587896823883, 0.10640454292297363, 0.017678091302514076, 0.017818573862314224, 0.01793079450726509, 0.01798774115741253, 0.10030883550643921, 0.017893454059958458, 0.5223984122276306, 0.018484920263290405, 0.019084634259343147, 0.11092863231897354, 0.019960176199674606, 0.02028609625995159, 0.020471172407269478, 0.10522535443305969, 0.11079523712396622, 0.09788008779287338, 0.0206820759922266, 0.0205752644687891, 0.020433209836483, 0.02024506777524948, 0.019993925467133522, 0.01963431015610695, 0.019267356023192406, 0.2784792482852936, 0.25662243366241455, 0.09536343812942505, 0.10027109086513519, 0.2649795711040497, 0.11026795953512192, 0.24686390161514282, 0.021200131624937057, 0.021740876138210297, 0.10901965200901031, 0.2684345245361328, 0.023236921057105064, 0.2411343902349472, 0.024453051388263702, 0.025046516209840775, 0.025493111461400986, 0.4346941411495209, 0.0268228929489851, 0.2421039193868637, 0.028777344152331352, 0.02966414950788021, 0.11258243769407272, 0.10546431690454483, 0.10994932800531387, 0.23250775039196014, 0.03236670792102814, 0.032785531133413315, 0.2348538041114807, 0.10901792347431183, 0.1043916568160057, 0.03421173617243767, 0.10822218656539917, 0.10151122510433197, 0.10886102169752121, 0.03424384817481041, 0.0340249203145504, 0.033621594309806824, 0.03314034640789032, 0.03251556679606438, 0.031826768070459366, 0.10449337959289551, 0.10573118180036545, 0.10546887665987015, 0.10685472190380096, 0.10695686936378479, 0.027820296585559845, 0.027160096913576126, 0.026467230170965195, 0.025758573785424232, 0.24898771941661835, 0.024630125612020493, 0.09979712963104248, 0.10773437470197678, 0.10250107198953629, 0.023070283234119415, 0.022650880739092827, 0.022190168499946594, 0.021677114069461823, 0.2560671269893646, 0.02096676081418991, 0.020733244717121124, 0.10962750762701035, 0.020153971388936043, 0.019826440140604973, 0.10306283086538315, 0.019127806648612022, 0.10604089498519897, 0.10470159351825714, 0.018141021952033043, 0.017815006896853447, 0.10227631032466888, 0.10768590867519379, 0.5108345150947571, 0.017348354682326317, 0.10129192471504211, 0.018065545707941055, 0.018325120210647583, 0.09832217544317245, 0.26003676652908325, 0.25974753499031067, 0.2520822584629059, 0.10754168778657913, 0.02143828757107258, 0.022117353975772858, 0.10002350062131882, 0.023122232407331467, 0.023484069854021072, 0.10557682067155838, 0.023857420310378075, 0.02390008606016636, 0.023848704993724823, 0.24058957397937775, 0.10631486773490906, 0.10982757061719894, 0.1099248081445694, 0.2465129792690277, 0.024484816938638687, 0.02468397095799446, 0.02475617453455925, 0.10181430727243423, 0.09852969646453857, 0.1099257841706276, 0.10750871896743774, 0.02443068102002144, 0.0242682546377182, 0.023983122780919075, 0.023646172136068344, 0.09844915568828583, 0.022886943072080612, 0.10238633304834366, 0.09887427091598511, 0.021726619452238083, 0.10149618983268738, 0.020954888314008713, 0.0205142330378294, 0.020069453865289688, 0.01959899626672268, 0.019123652949929237, 0.018619971349835396, 0.10390517860651016, 0.017640545964241028, 0.10163971781730652, 0.2824464738368988, 0.016716860234737396, 0.01659526862204075, 0.11261091381311417, 0.10115162283182144, 0.016142742708325386, 0.09569668769836426, 0.10555853694677353, 0.1057240217924118, 0.015584900975227356, 0.09735161811113358, 0.015320335514843464, 0.0151543989777565, 0.10653312504291534, 0.014782454818487167, 0.014598408713936806, 0.014380116946995258, 0.1029665395617485, 0.09665162116289139, 0.013747563585639, 0.0135544678196311, 0.10823516547679901, 0.013134067878127098, 0.012949185445904732, 0.11126065254211426, 0.10029542446136475, 0.09982585906982422, 0.11019495129585266, 0.5823051929473877, 0.11162806302309036, 0.013324675150215626, 0.013788689859211445, 0.09991897642612457, 0.01454714685678482, 0.10602740943431854, 0.0975593626499176, 0.09988527745008469, 0.10531052201986313, 0.015809468924999237, 0.09747526794672012, 0.10954684764146805, 0.016257215291261673, 0.10478795319795609, 0.2811318635940552, 0.09696322679519653, 0.10715783387422562, 0.10345003753900528, 0.10323057323694229, 0.2602216601371765, 0.018343156203627586, 0.10577382147312164, 0.019055165350437164, 0.019315633922815323, 0.0194756630808115, 0.10283979028463364, 0.019584883004426956, 0.10345935076475143, 0.01952505297958851, 0.01944112777709961, 0.019313044846057892, 0.11277011036872864, 0.01888185739517212, 0.10760220885276794, 0.018415363505482674, 0.27932119369506836, 0.10087713599205017, 0.018251094967126846, 0.018213557079434395, 0.10302723944187164, 0.018086615949869156, 0.017913730815052986, 0.11252707988023758, 0.01756196655333042, 0.10149358958005905, 0.09762218594551086, 0.017022354528307915, 0.1001519113779068, 0.11380635201931, 0.01649465411901474, 0.01631438359618187, 0.016070038080215454, 0.01581437885761261, 0.015538305975496769, 0.11367403715848923, 0.2941872477531433, 0.015012471005320549, 0.10221774131059647, 0.10815517604351044, 0.015041838400065899, 0.5507082939147949, 0.26938119530677795, 0.016480589285492897, 0.10083217173814774, 0.2697301208972931, 0.018834499642252922, 0.2588479816913605, 0.020597180351614952, 0.10325463116168976, 0.10529273748397827, 0.02293190360069275, 0.10265448689460754, 0.023943085223436356, 0.02428700029850006, 0.024517381563782692, 0.024624936282634735, 0.10091941058635712, 0.02462911419570446, 0.024498943239450455, 0.02426404319703579, 0.02397776208817959, 0.023649640381336212, 0.11072628945112228, 0.24549749493598938, 0.022823520004749298, 0.022732751443982124, 0.022527683526277542, 0.022250572219491005, 0.09715235978364944, 0.10516336560249329, 0.2666768729686737, 0.10914593935012817, 0.09583748131990433, 0.021329527720808983, 0.021255208179354668, 0.021123871207237244, 0.10067468136548996, 0.020693093538284302, 0.09734707325696945, 0.02019360102713108, 0.46216094493865967, 0.020394016057252884, 0.020681269466876984, 0.02086879499256611, 0.2452227622270584, 0.021384689956903458, 0.021625615656375885, 0.2473084181547165, 0.02221076190471649, 0.022539690136909485, 0.10632120817899704, 0.10138903558254242, 0.022996680811047554, 0.02300420217216015, 0.022936277091503143, 0.022736234590411186, 0.02245485410094261, 0.45877155661582947, 0.262184739112854, 0.023318259045481682, 0.023853516206145287, 0.23838648200035095, 0.1114412248134613, 0.09875123202800751, 0.11088235676288605, 0.26211071014404297, 0.02692425437271595, 0.10231328010559082, 0.02790147252380848, 0.0280738715082407, 0.02829008735716343, 0.22347450256347656, 0.028521394357085228, 0.10946816951036453, 0.10240381211042404, 0.028753764927387238, 0.02864328771829605, 0.02846384048461914, 0.09688810259103775, 0.09878157824277878, 0.027555635198950768, 0.027175066992640495, 0.02680058963596821, 0.026276737451553345, 0.24770520627498627, 0.025638870894908905, 0.0252921711653471, 0.02495788410305977, 0.024615000933408737, 0.10401006788015366, 0.10994601249694824, 0.02334040217101574, 0.10252203047275543, 0.1098649650812149, 0.25582489371299744, 0.11352235823869705, 0.021870478987693787, 0.021833177655935287, 0.021570755168795586, 0.021275004372000694, 0.09074541926383972, 0.10965438932180405, 0.10304698348045349, 0.020222172141075134, 0.11837860941886902, 0.28401613235473633, 0.10328739136457443, 0.019811032339930534, 0.2701043486595154, 0.01993742398917675, 0.11061989516019821, 0.10582584142684937, 0.11008976399898529, 0.020324498414993286, 0.10263048857450485, 0.02035384625196457, 0.10332362353801727, 0.02014801651239395, 0.10186926275491714, 0.0993720293045044, 0.0197979174554348, 0.01966254599392414, 0.0193805992603302, 0.019180260598659515, 0.0188149306923151, 0.5002689361572266, 0.018853753805160522, 0.019117657095193863, 0.1123589277267456, 0.108496755361557, 0.019620100036263466, 0.10439528524875641, 0.019723055884242058, 0.10434966534376144, 0.019678473472595215, 0.2656554579734802, 0.019728848710656166, 0.019831160083413124, 0.11415483802556992, 0.019884487614035606, 0.01981600932776928, 0.4835548400878906, 0.02021818608045578, 0.09283385425806046, 0.11258305609226227, 0.10199711471796036, 0.021657807752490044, 0.021851077675819397, 0.10108458995819092, 0.022052766755223274, 0.1103188619017601, 0.022024929523468018, 0.021973272785544395, 0.10049141943454742, 0.09813299775123596, 0.02155739814043045, 0.10483895987272263, 0.021128814667463303, 0.09971700608730316, 0.02069985307753086, 0.020404532551765442, 0.11223480105400085, 0.11151080578565598, 0.10006963461637497, 0.2682928144931793, 0.01933773048222065, 0.019306905567646027, 0.09456916153430939, 0.019134417176246643, 0.01901145838201046, 0.01885395124554634, 0.01865224912762642, 0.09698938578367233, 0.018109658733010292, 0.01790144294500351, 0.01756083406507969, 0.2671107351779938, 0.017224770039319992, 0.01717532053589821, 0.01699215918779373, 0.1122162714600563, 0.10539419203996658, 0.09585735946893692, 0.10341455787420273, 0.016368811950087547, 0.01623995415866375, 0.016033533960580826, 0.2887520492076874, 0.015920791774988174, 0.015953009948134422, 0.015875857323408127, 0.10827761888504028, 0.10733001679182053, 0.10522641986608505, 0.01564747281372547, 0.015546823851764202, 0.0154360206797719, 0.01527177169919014, 0.10442068427801132, 0.014959762804210186, 0.11177387833595276, 0.10928787291049957, 0.10446760803461075, 0.014383040368556976, 0.014260764233767986, 0.014112238772213459, 0.013936921022832394, 0.013689085841178894, 0.10052620619535446, 0.013314055278897285, 0.09711246192455292, 0.012940713204443455, 0.012756898067891598, 0.11575030535459518, 0.29104259610176086, 0.012475568801164627, 0.012522592209279537, 0.11589177697896957, 0.10551772266626358, 0.11257907003164291, 0.012628762051463127, 0.012631761841475964, 0.11369134485721588, 0.2973564565181732, 0.11129176616668701, 0.013033946044743061, 0.013203714042901993, 0.09896162152290344, 0.1072985827922821, 0.11131046712398529, 0.1135009229183197, 0.013762148097157478, 0.013820016756653786, 0.013844720087945461, 0.01384000014513731, 0.28604552149772644, 0.013972750864923, 0.0140727823600173, 0.10461165755987167, 0.1033145859837532, 0.014319805428385735, 0.014349772594869137, 0.10772126168012619, 0.014346469193696976, 0.27995145320892334, 0.1086115911602974, 0.10177858173847198, 0.014859510585665703, 0.014976223930716515, 0.015051707625389099, 0.10762979090213776, 0.2693255841732025, 0.1018761470913887, 0.10941722989082336, 0.015802381560206413, 0.015949919819831848, 0.016050081700086594, 0.016088534146547318, 0.016074106097221375, 0.016013702377676964, 0.01590411178767681, 0.015763435512781143, 0.015592495910823345, 0.10776760429143906, 0.015217823907732964, 0.1087651476264, 0.01486580166965723, 0.014676784165203571, 0.09912761300802231, 0.014268294908106327, 0.014071132987737656, 0.01384733710438013, 0.29885339736938477, 0.10544407367706299, 0.01369907520711422, 0.1012692078948021, 0.10176922380924225, 0.09640435874462128, 0.2854267358779907, 0.10400369018316269, 0.09706351161003113, 0.1110675260424614, 0.2916163504123688, 0.015126556158065796, 0.27896353602409363, 0.10296899080276489, 0.10932917892932892, 0.017084158957004547, 0.27461737394332886, 0.018098393455147743, 0.26848262548446655, 0.019297253340482712, 0.10086996853351593, 0.10216070711612701, 0.020917413756251335, 0.10864169150590897, 0.1048353835940361, 0.02190699800848961, 0.1071353629231453, 0.02225383184850216, 0.11169393360614777, 0.022363243624567986, 0.022338509559631348, 0.10145466774702072, 0.25931793451309204, 0.02227957174181938, 0.10501974821090698, 0.02239704318344593, 0.1062697321176529, 0.104104183614254, 0.24669943749904633, 0.02244972437620163, 0.10812472552061081, 0.2545090317726135, 0.02292576991021633, 0.023143500089645386, 0.10451797395944595, 0.023333199322223663, 0.10039807856082916, 0.023304689675569534, 0.023174749687314034, 0.02301982045173645, 0.10779595375061035, 0.02257370948791504, 0.022313451394438744, 0.022002920508384705, 0.02163616754114628, 0.021251613274216652, 0.02083168365061283, 0.020408952608704567, 0.019955983385443687, 0.11095117032527924, 0.10865581780672073, 0.10328337550163269, 0.10072454065084457, 0.018069952726364136, 0.28277596831321716, 0.017713669687509537, 0.017632434144616127, 0.09854481369256973, 0.017364786937832832, 0.26730668544769287, 0.10608430206775665, 0.0174129419028759, 0.017416520044207573, 0.017403041943907738, 0.017306271940469742, 0.01718773879110813, 0.10717279464006424, 0.016899138689041138, 0.10935522615909576, 0.01655227690935135, 0.016348006203770638, 0.01615181751549244, 0.015912223607301712, 0.015637466683983803, 0.2875078618526459, 0.10050202906131744, 0.015348549000918865, 0.01533614844083786, 0.015247508883476257, 0.01510556973516941, 0.014975071884691715, 0.014804854057729244, 0.10218779742717743, 0.01441535446792841, 0.09990234673023224, 0.014063429087400436, 0.10083514451980591, 0.013734161853790283, 0.11066056787967682, 0.11281507462263107, 0.013336313888430595, 0.10265965759754181, 0.10203704237937927, 0.10934712737798691, 0.5567998290061951, 0.2994142472743988, 0.014071540907025337, 0.10742959380149841, 0.01524812076240778, 0.015725433826446533, 0.0991605594754219, 0.016465771943330765, 0.10573206096887589, 0.10130305588245392, 0.017234351485967636, 0.017410872504115105, 0.01751091703772545, 0.0175490640103817, 0.017548369243741035, 0.10470875352621078, 0.017396263778209686, 0.01731477491557598, 0.10183544456958771, 0.26162856817245483, 0.1000102236866951, 0.27774807810783386, 0.01758284494280815]\n",
            "Val loss 0.07431575105168652\n",
            "Val auc roc 0.5003306878306879\n",
            "Saved model state dict for epoch 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6cf58ed355040439685955a6f92bd64",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1770.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0796\n",
            "Train Losses : [0.09946853667497635, 0.11461564898490906, 0.018274759873747826, 0.018371978774666786, 0.01843928173184395, 0.01845403015613556, 0.01841912604868412, 0.01836344599723816, 0.018198274075984955, 0.2785152792930603, 0.01801246777176857, 0.2651466727256775, 0.2624928951263428, 0.10715702921152115, 0.09425804018974304, 0.019488364458084106, 0.11483435332775116, 0.24057818949222565, 0.0204562209546566, 0.09358995407819748, 0.10647036135196686, 0.1084497794508934, 0.02158477157354355, 0.10433924943208694, 0.4520784616470337, 0.11564837396144867, 0.023178938776254654, 0.1040700376033783, 0.1026851013302803, 0.024559766054153442, 0.024882711470127106, 0.025002600625157356, 0.02509196661412716, 0.1122761219739914, 0.025034062564373016, 0.10989142954349518, 0.024698317050933838, 0.02453296072781086, 0.024265024811029434, 0.023948555812239647, 0.02363443188369274, 0.11083541065454483, 0.25158900022506714, 0.09620875865221024, 0.022760974243283272, 0.022499805316329002, 0.11396648734807968, 0.09448940306901932, 0.09840397536754608, 0.021716995164752007, 0.1071430966258049, 0.02127852849662304, 0.0210558008402586, 0.02071274444460869, 0.10882070660591125, 0.02014397829771042, 0.10439299046993256, 0.01949913240969181, 0.019202662631869316, 0.01891123689711094, 0.018476411700248718, 0.018142668530344963, 0.017713693901896477, 0.09251219034194946, 0.016982533037662506, 0.016672903671860695, 0.10580950230360031, 0.01594427227973938, 0.01563166081905365, 0.10809699445962906, 0.10457979142665863, 0.014764935709536076, 0.014507369138300419, 0.014249532483518124, 0.09523826837539673, 0.11712748557329178, 0.01360449381172657, 0.013356449082493782, 0.013111129403114319, 0.10098819434642792, 0.10609070956707001, 0.012580718845129013, 0.012371274642646313, 0.012201124802231789, 0.0941842645406723, 0.10954558849334717, 0.011709338054060936, 0.011609604582190514, 0.31634074449539185, 0.09668435156345367, 0.011572912335395813, 0.10371774435043335, 0.10525038838386536, 0.3020796477794647, 0.11709266901016235, 0.10459743440151215, 0.012551702558994293, 0.0956098735332489, 0.01296082790941, 0.10457363724708557, 0.013263585045933723, 0.10950551927089691, 0.1160101369023323, 0.11058296263217926, 0.013605541549623013, 0.013719000853598118, 0.013732638210058212, 0.09998351335525513, 0.013679085299372673, 0.09789460152387619, 0.104559525847435, 0.013617248274385929, 0.11516209691762924, 0.013530004769563675, 0.10784327238798141, 0.11119899153709412, 0.09987939149141312, 0.10927905887365341, 0.10252886265516281, 0.013338670134544373, 0.0952799916267395, 0.013320011086761951, 0.28950637578964233, 0.11129039525985718, 0.01360121089965105, 0.09348680824041367, 0.10528592020273209, 0.01398894377052784, 0.10922566056251526, 0.014116840437054634, 0.2907610237598419, 0.30344098806381226, 0.5371450781822205, 0.015749221667647362, 0.016625968739390373, 0.10000582039356232, 0.5085505247116089, 0.11124007403850555, 0.24331317842006683, 0.02168370969593525, 0.23838262259960175, 0.10666105151176453, 0.10635197162628174, 0.23472252488136292, 0.027399949729442596, 0.0284025426954031, 0.02927997335791588, 0.1017172560095787, 0.03053702786564827, 0.10950171947479248, 0.11240735650062561, 0.03151962161064148, 0.11031488329172134, 0.03177658095955849, 0.031737860292196274, 0.031654249876737595, 0.10083410143852234, 0.03118477761745453, 0.10717473924160004, 0.03055470436811447, 0.030165567994117737, 0.02974897064268589, 0.029282957315444946, 0.10722819715738297, 0.028247028589248657, 0.027750510722398758, 0.2292761653661728, 0.026845108717679977, 0.09907766431570053, 0.09914663434028625, 0.02584167942404747, 0.025510085746645927, 0.10204993933439255, 0.1023406907916069, 0.024385882541537285, 0.1096469983458519, 0.10692287236452103, 0.10352194309234619, 0.022968599572777748, 0.25561878085136414, 0.022532211616635323, 0.022410767152905464, 0.10091174393892288, 0.10723934322595596, 0.021805284544825554, 0.1014983132481575, 0.02141532488167286, 0.11335127800703049, 0.020884862169623375, 0.020632648840546608, 0.11651862412691116, 0.020078102126717567, 0.019777124747633934, 0.10867225378751755, 0.10496007651090622, 0.0189068540930748, 0.01862397789955139, 0.01829971745610237, 0.017947247251868248, 0.09981529414653778, 0.102789506316185, 0.2736218571662903, 0.01703910529613495, 0.09905670583248138, 0.09830459207296371, 0.01685767062008381, 0.016810765489935875, 0.01665465347468853, 0.01651657372713089, 0.27973875403404236, 0.016368890181183815, 0.0996483638882637, 0.2695534825325012, 0.01657560095191002, 0.016762740910053253, 0.10445117950439453, 0.01691259630024433, 0.27789247035980225, 0.01721825636923313, 0.10757266730070114, 0.017555641010403633, 0.10523439943790436, 0.11346393078565598, 0.10108809918165207, 0.10525976866483688, 0.10246828198432922, 0.017977215349674225, 0.01798926666378975, 0.10868120938539505, 0.11073391884565353, 0.1120966225862503, 0.017871951684355736, 0.11028104275465012, 0.09848025441169739, 0.017632823437452316, 0.01757095567882061, 0.01740124449133873, 0.10484550893306732, 0.017076939344406128, 0.25740522146224976, 0.10914018005132675, 0.10627571493387222, 0.11031907051801682, 0.09755470603704453, 0.01711714081466198, 0.10707331448793411, 0.017058877274394035, 0.11053074151277542, 0.016938867047429085, 0.016864024102687836, 0.016776951029896736, 0.10131533443927765, 0.016524625942111015, 0.1055130586028099, 0.2795746326446533, 0.09801751375198364, 0.10453122854232788, 0.01639270782470703, 0.016419164836406708, 0.2679421305656433, 0.10032003372907639, 0.1034570187330246, 0.10697567462921143, 0.01709487847983837, 0.01719144731760025, 0.09982641786336899, 0.017237767577171326, 0.09841658174991608, 0.0171889029443264, 0.017153548076748848, 0.10869312286376953, 0.10465797036886215, 0.10893706977367401, 0.016814440488815308, 0.01674107275903225, 0.10216943174600601, 0.016459673643112183, 0.09694375842809677, 0.26697608828544617, 0.01630169153213501, 0.01636374182999134, 0.10599935054779053, 0.01633106917142868, 0.10592249780893326, 0.10937293618917465, 0.016182418912649155, 0.016128825023770332, 0.01602907106280327, 0.10116223245859146, 0.01580760069191456, 0.01566881500184536, 0.10897804796695709, 0.2940046787261963, 0.015401999466121197, 0.09896738827228546, 0.10749001055955887, 0.10722534358501434, 0.015555335208773613, 0.10888901352882385, 0.2929512560367584, 0.015770215541124344, 0.11056853830814362, 0.01608525775372982, 0.10984960198402405, 0.10201837867498398, 0.01634478010237217, 0.09870638698339462, 0.1054903119802475, 0.10985074192285538, 0.016495367512106895, 0.01648097112774849, 0.01640007644891739, 0.016305632889270782, 0.01618334837257862, 0.1073438823223114, 0.2887887954711914, 0.2936936616897583, 0.10144089162349701, 0.016572600230574608, 0.01676197163760662, 0.10284457355737686, 0.11068232357501984, 0.09999914467334747, 0.017278656363487244, 0.01735740341246128, 0.10816118866205215, 0.10439633578062057, 0.01734033040702343, 0.10268864780664444, 0.017258811742067337, 0.11044192314147949, 0.26745742559432983, 0.1030006930232048, 0.017401954159140587, 0.10110823065042496, 0.1050662025809288, 0.01760607399046421, 0.10489263385534286, 0.10117285698652267, 0.10353601723909378, 0.10191651433706284, 0.10929125547409058, 0.2666488587856293, 0.09986565262079239, 0.018063034862279892, 0.018190814182162285, 0.10375559329986572, 0.018321853131055832, 0.018320754170417786, 0.11135751754045486, 0.1004343256354332, 0.01819862611591816, 0.10023622214794159, 0.09893999248743057, 0.017984716221690178, 0.09924229979515076, 0.09845729917287827, 0.017691850662231445, 0.264778733253479, 0.10039953142404556, 0.0177252646535635, 0.017754342406988144, 0.017741672694683075, 0.11371565610170364, 0.01762254349887371, 0.017534373328089714, 0.017393698915839195, 0.01725112833082676, 0.01704062521457672, 0.016815736889839172, 0.01656987890601158, 0.10638110339641571, 0.01611929200589657, 0.015872323885560036, 0.10006716847419739, 0.10505101829767227, 0.5469555258750916, 0.015570309944450855, 0.01584765315055847, 0.016016248613595963, 0.09900966286659241, 0.1111975833773613, 0.10238727927207947, 0.10314963012933731, 0.016666900366544724, 0.016721442341804504, 0.10041102766990662, 0.016757752746343613, 0.016732007265090942, 0.01666383072733879, 0.016575973480939865, 0.2837914824485779, 0.016513511538505554, 0.016526807099580765, 0.01651616394519806, 0.09952040761709213, 0.016446925699710846, 0.016353484243154526, 0.016230538487434387, 0.016108224168419838, 0.015927178785204887, 0.015729553997516632, 0.015539344400167465, 0.01532681379467249, 0.015104559250175953, 0.014838461764156818, 0.014586112461984158, 0.014337479136884212, 0.11156411468982697, 0.013849735260009766, 0.01363286655396223, 0.013382486067712307, 0.01314620766788721, 0.10877399891614914, 0.11629483103752136, 0.012539929710328579, 0.012358277104794979, 0.10776987671852112, 0.10816729068756104, 0.011880281381309032, 0.01174155529588461, 0.011606881394982338, 0.10627155005931854, 0.011309904046356678, 0.09992876648902893, 0.01107318140566349, 0.010946644470095634, 0.010824158787727356, 0.010685330256819725, 0.10993495583534241, 0.010437462478876114, 0.10927382111549377, 0.010227369144558907, 0.33038511872291565, 0.010196251794695854, 0.010270590893924236, 0.107930988073349, 0.010357800871133804, 0.32201963663101196, 0.010614517144858837, 0.09830957651138306, 0.010939674451947212, 0.01107525359839201, 0.102309450507164, 0.011256519705057144, 0.10224839299917221, 0.01140341442078352, 0.10557660460472107, 0.3170452117919922, 0.011734304949641228, 0.10242213308811188, 0.3032427728176117, 0.3073362410068512, 0.2983863055706024, 0.10260012000799179, 0.5418145060539246, 0.10407581180334091, 0.016419481486082077, 0.2771604061126709, 0.2707289755344391, 0.01956096477806568, 0.10838184505701065, 0.021577229723334312, 0.022430753335356712, 0.02316349558532238, 0.10549654066562653, 0.024280153214931488, 0.10192485898733139, 0.025032931938767433, 0.025287659838795662, 0.10540089756250381, 0.10436879098415375, 0.025713765993714333, 0.10665325820446014, 0.11013758182525635, 0.02567976713180542, 0.10607194155454636, 0.11109668761491776, 0.02537604235112667, 0.10530152171850204, 0.02507615089416504, 0.09889272600412369, 0.024628523737192154, 0.23348468542099, 0.02431878261268139, 0.02424067258834839, 0.024058494716882706, 0.23678401112556458, 0.023826757445931435, 0.10394681245088577, 0.023685647174715996, 0.02354859746992588, 0.24163947999477386, 0.10188103467226028, 0.02344731241464615, 0.10637469589710236, 0.10057882219552994, 0.25439295172691345, 0.023357976227998734, 0.023421160876750946, 0.023418650031089783, 0.10830306261777878, 0.26521584391593933, 0.023387255147099495, 0.023420939221978188, 0.023428358137607574, 0.10652238875627518, 0.10686029493808746, 0.023158395662903786, 0.24712008237838745, 0.023062273859977722, 0.023066412657499313, 0.02301061525940895, 0.022872747853398323, 0.02271566540002823, 0.022485656663775444, 0.0222276970744133, 0.021929891780018806, 0.10750508308410645, 0.25399288535118103, 0.10385444015264511, 0.10507286339998245, 0.02113192342221737, 0.021048514172434807, 0.10335282981395721, 0.10025355219841003, 0.020584234967827797, 0.11159946769475937, 0.02024044655263424, 0.10449734330177307, 0.10972782224416733, 0.10316319018602371, 0.09909458458423615, 0.10181734710931778, 0.26493361592292786, 0.0193480234593153, 0.019344890490174294, 0.019312329590320587, 0.01924477517604828, 0.10557948052883148, 0.25374919176101685, 0.10731679946184158, 0.01917731948196888, 0.019215330481529236, 0.10861137509346008, 0.019155485555529594, 0.019080089405179024, 0.018971392884850502, 0.0980164036154747, 0.01867290586233139, 0.09614439308643341, 0.018349364399909973, 0.10022169351577759, 0.10736158490180969, 0.017834318801760674, 0.017689039930701256, 0.01744418777525425, 0.017202962189912796, 0.016969123855233192, 0.01670779287815094, 0.10153903067111969, 0.2824154496192932, 0.016147425398230553, 0.11312208324670792, 0.10999632626771927, 0.10078364610671997, 0.10675656795501709, 0.26682248711586, 0.10183587670326233, 0.10416898131370544, 0.2735590934753418, 0.016856087371706963, 0.017086846753954887, 0.10863955318927765, 0.017492251470685005, 0.10148852318525314, 0.017708633095026016, 0.2748444378376007, 0.01802784949541092, 0.10398794710636139, 0.0183381550014019, 0.2782379686832428, 0.10100893676280975, 0.2612363398075104, 0.019405314698815346, 0.2577039897441864, 0.020255301147699356, 0.10335805267095566, 0.10615913569927216, 0.021332474425435066, 0.1081518605351448, 0.10150659829378128, 0.2468506246805191, 0.02231006883084774, 0.10295930504798889, 0.10718289762735367, 0.10831025242805481, 0.10376409441232681, 0.023273875936865807, 0.2433147430419922, 0.02355593629181385, 0.10973595827817917, 0.09966953098773956, 0.023921776562929153, 0.02394215762615204, 0.02390645258128643, 0.10929780453443527, 0.09828109294176102, 0.023619208484888077, 0.02344258315861225, 0.09306575357913971, 0.023093411698937416, 0.0228149201720953, 0.10942908376455307, 0.022317003458738327, 0.02202322520315647, 0.11257927119731903, 0.021428344771265984, 0.10347528755664825, 0.020886525511741638, 0.020588623359799385, 0.10258214175701141, 0.019929297268390656, 0.01965240202844143, 0.10542252659797668, 0.019013725221157074, 0.0187444519251585, 0.018407393246889114, 0.018088262528181076, 0.017740361392498016, 0.2663118243217468, 0.10298246145248413, 0.017186913639307022, 0.10352284461259842, 0.016938216984272003, 0.016772450879216194, 0.016606243327260017, 0.016450153663754463, 0.10885459929704666, 0.10279940813779831, 0.10707750171422958, 0.015734076499938965, 0.10899471491575241, 0.10061348974704742, 0.01535818725824356, 0.28772568702697754, 0.10577993094921112, 0.01534583792090416, 0.2985563576221466, 0.015557948499917984, 0.015720615163445473, 0.0944674089550972, 0.09609635174274445, 0.01598413847386837, 0.01606115885078907, 0.016039390116930008, 0.016017671674489975, 0.09920380264520645, 0.015883810818195343, 0.015842802822589874, 0.26777955889701843, 0.015784677118062973, 0.015809528529644012, 0.01579888164997101, 0.100828617811203, 0.015759004279971123, 0.10953984409570694, 0.015640780329704285, 0.015531168319284916, 0.015439389273524284, 0.015315062366425991, 0.0151543989777565, 0.10698969662189484, 0.11077529937028885, 0.01470628660172224, 0.09971145540475845, 0.014465108513832092, 0.014302143827080727, 0.01418272778391838, 0.01398860290646553, 0.2842298448085785, 0.10074757039546967, 0.01383624691516161, 0.2823340594768524, 0.1096031665802002, 0.09566789865493774, 0.014340253546833992, 0.28278079628944397, 0.014728452078998089, 0.014950361102819443, 0.10803329944610596, 0.10318516939878464, 0.1065608337521553, 0.01557957474142313, 0.11593703180551529, 0.01575736328959465, 0.015789587050676346, 0.09902843832969666, 0.10263006389141083, 0.10767580568790436, 0.015835177153348923, 0.015822485089302063, 0.2714863419532776, 0.10423498600721359, 0.11203709989786148, 0.016137797385454178, 0.0938691571354866, 0.016259361058473587, 0.27379584312438965, 0.10843256115913391, 0.2810398042201996, 0.01701788790524006, 0.017282474786043167, 0.01751457154750824, 0.49775075912475586, 0.018253197893500328, 0.09939461946487427, 0.019226301461458206, 0.10500830411911011, 0.019938234239816666, 0.020188486203551292, 0.25473886728286743, 0.02073022536933422, 0.02096979320049286, 0.021198460832238197, 0.11245466768741608, 0.11271277815103531, 0.021461505442857742, 0.10206801444292068, 0.021527931094169617, 0.02145697921514511, 0.021368427202105522, 0.021242531016469002, 0.021069008857011795, 0.020859060809016228, 0.02064201980829239, 0.10597902536392212, 0.02017504908144474, 0.1051705852150917, 0.019661955535411835, 0.01944645307958126, 0.2696293294429779, 0.10196206718683243, 0.019022736698389053, 0.01893136091530323, 0.018825924023985863, 0.018625225871801376, 0.10653356462717056, 0.01826612465083599, 0.018085787072777748, 0.01786353811621666, 0.01763683184981346, 0.10253433883190155, 0.10684330761432648, 0.016980890184640884, 0.016788678243756294, 0.016555985435843468, 0.016316166147589684, 0.016062883660197258, 0.01581505872309208, 0.015573269687592983, 0.09729910641908646, 0.015074403025209904, 0.014835188165307045, 0.10513479262590408, 0.014378851279616356, 0.09936937689781189, 0.01399939227849245, 0.10977066308259964, 0.013631356880068779, 0.10341939330101013, 0.10170525312423706, 0.29424598813056946, 0.013255657628178596, 0.29729530215263367, 0.1063820943236351, 0.013682191260159016, 0.013817675411701202, 0.2745380699634552, 0.10640250891447067, 0.10281170904636383, 0.1030801460146904, 0.014863148331642151, 0.015019715763628483, 0.09769048541784286, 0.015238775871694088, 0.01531335711479187, 0.015398850664496422, 0.01535883080214262, 0.28471657633781433, 0.09654881805181503, 0.015595793724060059, 0.10362956672906876, 0.015717756003141403, 0.01578643172979355, 0.09462865442037582, 0.015764547511935234, 0.015763040632009506, 0.1039188951253891, 0.10213841497898102, 0.26261159777641296, 0.01572786271572113, 0.01582164317369461, 0.01585523597896099, 0.01588347926735878, 0.015839064493775368, 0.015826767310500145, 0.01568906009197235, 0.2696400582790375, 0.09643739461898804, 0.015711424872279167, 0.015772981569170952, 0.28287625312805176, 0.015895245596766472, 0.016079077497124672, 0.016095856204628944, 0.016108298674225807, 0.1064254492521286, 0.016119005158543587, 0.09489987045526505, 0.27914750576019287, 0.1034117266535759, 0.016361070796847343, 0.016455842182040215, 0.26596516370773315, 0.016670694574713707, 0.016798024997115135, 0.10323752462863922, 0.016989775002002716, 0.2559809386730194, 0.09331833571195602, 0.017432888969779015, 0.017541933804750443, 0.01767578162252903, 0.017684342339634895, 0.0176998283714056, 0.11540977656841278, 0.2466752678155899, 0.10963508486747742, 0.11033734679222107, 0.017938457429409027, 0.11017390340566635, 0.0939706414937973, 0.018104540184140205, 0.10031811147928238, 0.2619522213935852, 0.018335262313485146, 0.11154034733772278, 0.10790486633777618, 0.096474289894104, 0.01863432303071022, 0.018670516088604927, 0.018677109852433205, 0.018639374524354935, 0.018532471731305122, 0.018398888409137726, 0.01823493465781212, 0.018109945580363274, 0.10227876901626587, 0.01780337281525135, 0.017490217462182045, 0.11391593515872955, 0.2771717607975006, 0.01715383678674698, 0.017071587964892387, 0.09514624625444412, 0.01695932075381279, 0.11668027937412262, 0.01682218722999096, 0.016693560406565666, 0.10008523613214493, 0.01645064167678356, 0.27823296189308167, 0.01637488603591919, 0.28484922647476196, 0.016609441488981247, 0.016676709055900574, 0.25422823429107666, 0.016948485746979713, 0.5249316692352295, 0.017739424481987953, 0.11019761115312576, 0.09901463985443115, 0.019115561619400978, 0.09452906250953674, 0.27922961115837097, 0.020260440185666084, 0.02061915025115013, 0.27988365292549133, 0.02138805389404297, 0.021649332717061043, 0.021921776235103607, 0.2667388319969177, 0.23428906500339508, 0.022964684292674065, 0.023324081674218178, 0.23935964703559875, 0.10307089984416962, 0.024388020858168602, 0.024729078635573387, 0.1136111468076706, 0.025025689974427223, 0.23682717978954315, 0.025339214131236076, 0.1167111024260521, 0.11481935530900955, 0.09815685451030731, 0.025802308693528175, 0.025813406333327293, 0.10476644337177277, 0.025701358914375305, 0.025541985407471657, 0.02550419420003891, 0.11155983805656433, 0.0250630471855402, 0.10606146603822708, 0.10673821717500687, 0.2551432251930237, 0.11914820969104767, 0.0944756492972374, 0.09725926071405411, 0.024096209555864334, 0.11554508656263351, 0.24942466616630554, 0.1015886440873146, 0.24672922492027283, 0.10567306727170944, 0.024437151849269867, 0.024511583149433136, 0.11262999475002289, 0.024553338065743446, 0.02450849674642086, 0.02439737506210804, 0.02427634224295616, 0.024140458554029465, 0.10899534821510315, 0.023657266050577164, 0.023372884839773178, 0.023139862343668938, 0.022860752418637276, 0.10862486064434052, 0.022260498255491257, 0.10694929212331772, 0.02161838486790657, 0.02134300209581852, 0.021022509783506393, 0.24797579646110535, 0.020556535571813583, 0.020372575148940086, 0.1057894378900528, 0.1068197712302208, 0.019833706319332123, 0.4721388816833496, 0.09904472529888153, 0.02016218565404415, 0.10533683001995087, 0.10896092653274536, 0.10045817494392395, 0.020675206556916237, 0.02074173465371132, 0.25725963711738586, 0.10967061668634415, 0.021048903465270996, 0.02111666649580002, 0.021108444780111313, 0.02110886760056019, 0.02100256457924843, 0.11264663189649582, 0.020836161449551582, 0.0206774715334177, 0.020522335544228554, 0.26497745513916016, 0.02032957226037979, 0.02024632878601551, 0.02017335593700409, 0.02000417187809944, 0.019879182800650597, 0.1079721599817276, 0.01951024867594242, 0.019320674240589142, 0.01913999207317829, 0.257245808839798, 0.01885119453072548, 0.11017099022865295, 0.10124940425157547, 0.2728925049304962, 0.10924343764781952, 0.10415484756231308, 0.26094743609428406, 0.019108492881059647, 0.019290795549750328, 0.019404198974370956, 0.01946997456252575, 0.019446538761258125, 0.01942785643041134, 0.26144471764564514, 0.09505482017993927, 0.019553203135728836, 0.09624569863080978, 0.10256382822990417, 0.019650699570775032, 0.01963244006037712, 0.09741896390914917, 0.10576064884662628, 0.2651316225528717, 0.10585010796785355, 0.01965802162885666, 0.019690873101353645, 0.019710440188646317, 0.10301439464092255, 0.26671648025512695, 0.019734863191843033, 0.019795596599578857, 0.10225138068199158, 0.019872907549142838, 0.01980610564351082, 0.10574386268854141, 0.10721832513809204, 0.019613679498434067, 0.019543437287211418, 0.1063859760761261, 0.019307302311062813, 0.01913171447813511, 0.01903018169105053, 0.11245345324277878, 0.018611691892147064, 0.10346616804599762, 0.018279792740941048, 0.01806708797812462, 0.017885584384202957, 0.017648018896579742, 0.017430169507861137, 0.11108216643333435, 0.016953647136688232, 0.01673359051346779, 0.27049657702445984, 0.016441013664007187, 0.10707906633615494, 0.2734295427799225, 0.10729844868183136, 0.10011719912290573, 0.016566196456551552, 0.11219321936368942, 0.01664917543530464, 0.016664085909724236, 0.277680367231369, 0.108368419110775, 0.016879012808203697, 0.2767581641674042, 0.017170336097478867, 0.01730753667652607, 0.28255921602249146, 0.017693590372800827, 0.017922403290867805, 0.09672757238149643, 0.27974727749824524, 0.10558373481035233, 0.10061372816562653, 0.09858368337154388, 0.019167402759194374, 0.01930902525782585, 0.019407086074352264, 0.09968994557857513, 0.019484538584947586, 0.10250484198331833, 0.01947866752743721, 0.10221199691295624, 0.019410189241170883, 0.019289996474981308, 0.01918283849954605, 0.019052162766456604, 0.01887240819633007, 0.01873345673084259, 0.09990116208791733, 0.09677717834711075, 0.018149932846426964, 0.017982155084609985, 0.01777561940252781, 0.10540566593408585, 0.10104169696569443, 0.28408604860305786, 0.017243562266230583, 0.017183655872941017, 0.10196700692176819, 0.28009483218193054, 0.01720789633691311, 0.27161335945129395, 0.017458952963352203, 0.017637569457292557, 0.09831424802541733, 0.01782182976603508, 0.017893793061375618, 0.11064895242452621, 0.017911743372678757, 0.017888827249407768, 0.10672137141227722, 0.01778654381632805, 0.017727570608258247, 0.5247141718864441, 0.2799336016178131, 0.28151699900627136, 0.018831869587302208, 0.485098659992218, 0.02011818252503872, 0.02079589292407036, 0.02143228054046631, 0.021926386281847954, 0.022324983030557632, 0.25981757044792175, 0.023131398484110832, 0.10528247803449631, 0.2522142827510834, 0.44885551929473877, 0.02506663091480732, 0.10756485164165497, 0.10857106745243073, 0.026851199567317963, 0.10496186465024948, 0.027663951739668846, 0.027969805523753166, 0.028245944529771805, 0.10692615807056427, 0.0285800751298666, 0.028451938182115555, 0.028368627652525902, 0.1107335239648819, 0.10135383903980255, 0.027958588674664497, 0.02778026834130287, 0.10746033489704132, 0.10531485080718994, 0.10814425349235535, 0.10624545812606812, 0.10801070183515549, 0.10105769336223602, 0.02641461230814457, 0.10153297334909439, 0.025993837043642998, 0.10741019248962402, 0.10536893457174301, 0.0253010131418705, 0.025077251717448235, 0.10528752207756042, 0.02452031522989273, 0.024239685386419296, 0.02394895814359188, 0.023634659126400948, 0.023286709561944008, 0.10440821945667267, 0.022671157494187355, 0.24735258519649506, 0.02215670980513096, 0.02199706993997097, 0.106819286942482, 0.11028558015823364, 0.10359770804643631, 0.2559240162372589, 0.4678614139556885, 0.021631896495819092, 0.021928442642092705, 0.10421071201562881, 0.022359611466526985, 0.10375865548849106, 0.022639628499746323, 0.022667061537504196, 0.02267799898982048, 0.10806398093700409, 0.02259446494281292, 0.022517669945955276, 0.02239624224603176, 0.10804904997348785, 0.02210521697998047, 0.021938253194093704, 0.0217419620603323, 0.021520301699638367, 0.021281789988279343, 0.10568272322416306, 0.020792199298739433, 0.020571330562233925, 0.020300712436437607, 0.020030632615089417, 0.1093997061252594, 0.10421673953533173, 0.019278626888990402, 0.019042301923036575, 0.1042441800236702, 0.018571408465504646, 0.1066323071718216, 0.01812226139008999, 0.1070326566696167, 0.017710935324430466, 0.2747860550880432, 0.10452590137720108, 0.017435576766729355, 0.10343705862760544, 0.10453051328659058, 0.2748424708843231, 0.01734420657157898, 0.27303287386894226, 0.017606209963560104, 0.017731700092554092, 0.27148154377937317, 0.01805727556347847, 0.267545610666275, 0.01852474920451641, 0.10573465377092361, 0.26654893159866333, 0.10678812116384506, 0.019610652700066566, 0.10964416712522507, 0.02006908319890499, 0.020207364112138748, 0.10355545580387115, 0.10020636767148972, 0.10044560581445694, 0.10722345858812332, 0.10376579314470291, 0.10844685137271881, 0.10415814071893692, 0.09971693903207779, 0.020724479109048843, 0.10622909665107727, 0.020682014524936676, 0.020624246448278427, 0.1029435396194458, 0.020441485568881035, 0.10357936471700668, 0.020222945138812065, 0.1008237972855568, 0.2606992721557617, 0.25488999485969543, 0.020155688747763634, 0.020266082137823105, 0.09921854734420776, 0.020364336669445038, 0.10647021234035492, 0.020390689373016357, 0.02033887803554535, 0.02027777209877968, 0.10679773986339569, 0.10892342776060104, 0.10826820135116577, 0.10355037450790405, 0.10108663141727448, 0.10530717670917511, 0.1037590354681015, 0.273061603307724, 0.10907682776451111, 0.01979082077741623, 0.10116152465343475, 0.019847778603434563, 0.10899025946855545, 0.2722003161907196, 0.01994556002318859, 0.02002689242362976, 0.10239338129758835, 0.10830965638160706, 0.1058816909790039, 0.02011151984333992, 0.10368242114782333, 0.10563553124666214, 0.020037973299622536, 0.253482460975647, 0.48689040541648865, 0.020495591685175896, 0.020844904705882072, 0.021142536774277687, 0.021357327699661255, 0.10240389406681061, 0.10517764836549759, 0.1037474200129509, 0.10319608449935913, 0.021965520456433296, 0.1089668720960617, 0.10454796999692917, 0.022034136578440666, 0.10395056009292603, 0.021973632276058197, 0.10375377535820007, 0.10488990694284439, 0.021778443828225136, 0.10104433447122574, 0.10456792265176773, 0.021489575505256653, 0.021362710744142532, 0.021211884915828705, 0.10418745130300522, 0.020885277539491653, 0.1035681888461113, 0.02054130658507347, 0.10516166687011719, 0.10463499277830124, 0.10777392983436584, 0.10344187915325165, 0.01974889636039734, 0.019602956250309944, 0.019429828971624374, 0.1049252599477768, 0.01907694898545742, 0.018882306292653084, 0.10687605291604996, 0.018505923449993134, 0.018308766186237335, 0.10043036192655563, 0.10344130545854568, 0.01776711829006672, 0.017585137858986855, 0.10143333673477173, 0.017236361280083656, 0.01705043576657772, 0.016866793856024742, 0.10386622697114944, 0.016505591571331024, 0.10475630313158035, 0.10401512682437897, 0.10375145822763443, 0.015886668115854263, 0.10489547997713089, 0.10856446623802185, 0.015531780198216438, 0.015420795418322086, 0.10748952627182007, 0.015174099244177341, 0.015043763443827629, 0.014914174564182758, 0.0993831679224968, 0.01464714203029871, 0.014505190774798393, 0.014363732188940048, 0.10329804569482803, 0.01408600527793169, 0.10682989656925201, 0.013831076212227345, 0.10762953758239746, 0.013592520728707314, 0.10465136170387268, 0.013383752666413784, 0.013271001167595387, 0.013155253604054451, 0.013035978190600872, 0.012900954112410545, 0.0127715477719903, 0.10362966358661652, 0.1059504970908165, 0.10358753800392151, 0.1092764213681221, 0.012265331111848354, 0.10445957630872726, 0.10709059238433838, 0.012091576121747494, 0.012032805941998959, 0.10688205808401108, 0.011909245513379574, 0.01184158492833376, 0.30349016189575195, 0.10556800663471222, 0.10596372932195663, 0.1043848767876625, 0.012025881558656693, 0.1049976721405983, 0.10678606480360031, 0.012168186716735363, 0.012193468399345875, 0.012199249118566513, 0.012186940759420395, 0.012159548699855804, 0.10207846015691757, 0.012089802883565426, 0.012048245407640934, 0.10160800069570541, 0.2982921898365021, 0.012033511884510517, 0.10401241481304169, 0.10279171168804169, 0.10715974122285843, 0.012293366715312004, 0.2961181402206421, 0.012495128437876701, 0.10722365230321884, 0.3005675971508026, 0.012998408637940884, 0.01321047730743885, 0.013372227549552917, 0.013502215966582298, 0.10070016235113144, 0.013687878847122192, 0.013753315433859825, 0.013792544603347778, 0.1092781201004982, 0.28580376505851746, 0.01396949589252472, 0.10590434074401855, 0.014170607551932335, 0.014243176206946373, 0.28727298974990845, 0.28800421953201294, 0.10700276494026184, 0.10719615966081619, 0.015237102285027504, 0.015434961766004562, 0.01558940764516592, 0.01570226438343525, 0.10274124145507812, 0.26912474632263184, 0.10694082826375961, 0.016221078112721443, 0.01635810174047947, 0.016449473798274994, 0.01650945469737053, 0.016546092927455902, 0.016538001596927643, 0.0165094044059515, 0.016455717384815216, 0.10611462593078613, 0.10835624486207962, 0.016258127987384796, 0.10504236072301865, 0.01611914485692978, 0.016042042523622513, 0.01593843474984169, 0.10327349603176117, 0.11033377796411514, 0.01563175767660141, 0.015529889613389969, 0.015411997213959694, 0.015297794714570045, 0.10396526008844376, 0.01502562128007412, 0.014904143288731575, 0.10101592540740967, 0.01463646162301302, 0.09926225244998932, 0.014396569691598415, 0.014279658906161785, 0.10398168116807938, 0.28658732771873474, 0.014056387357413769, 0.014064626768231392, 0.01404587272554636, 0.10502669215202332, 0.013971549458801746, 0.013924074359238148, 0.013862649910151958, 0.09762366116046906, 0.2881624102592468, 0.013787374831736088, 0.013829647563397884, 0.29356956481933594, 0.10445711761713028, 0.014116852544248104, 0.28796374797821045, 0.014441265724599361, 0.10470710694789886, 0.01476811058819294, 0.1074010506272316, 0.015013986267149448, 0.015094147995114326, 0.015156608074903488, 0.015178787522017956, 0.28450867533683777, 0.27981582283973694, 0.1044858768582344, 0.10283109545707703, 0.015972277149558067, 0.5306418538093567, 0.016583498567342758, 0.016981830820441246, 0.017316315323114395, 0.017594818025827408, 0.01781315542757511, 0.1015477403998375, 0.018158210441470146, 0.09594019502401352, 0.10231544822454453, 0.10595988482236862, 0.10719956457614899, 0.018613943830132484, 0.26565706729888916, 0.01881135255098343, 0.10403598099946976, 0.019026191905140877, 0.10719528049230576, 0.019144635647535324, 0.10141966491937637, 0.019159723073244095, 0.10174866020679474, 0.019147023558616638, 0.019063429906964302, 0.01899612694978714, 0.10210191458463669, 0.10564635694026947, 0.01871616207063198, 0.01863447017967701, 0.10572676360607147, 0.01837760955095291, 0.0182503554970026, 0.26380613446235657, 0.01810768060386181, 0.10284299403429031, 0.10101892799139023, 0.018010463565587997, 0.017942186444997787, 0.017872871831059456, 0.01777396723628044, 0.01764800399541855, 0.10734165459871292, 0.01740993559360504, 0.017278168350458145, 0.10322359204292297, 0.016996130347251892, 0.01687285304069519, 0.01668979786336422, 0.10808990895748138, 0.102730892598629, 0.10217149555683136, 0.01616382971405983, 0.10349059849977493, 0.01592143438756466, 0.10494004189968109, 0.015689190477132797, 0.10523391515016556, 0.015471922233700752, 0.28062620759010315, 0.015375887043774128, 0.10565653443336487, 0.015373464673757553, 0.015345883555710316, 0.015298095531761646, 0.2853424549102783, 0.015301805920898914, 0.01534255687147379, 0.10454162210226059, 0.01536291278898716, 0.01535086426883936, 0.015320510603487492, 0.015267521142959595, 0.10547395795583725, 0.015145299024879932, 0.1039198562502861, 0.10419608652591705, 0.014947491697967052, 0.10792896151542664, 0.01481364481151104, 0.5452737808227539, 0.014985131099820137, 0.10806234925985336, 0.0973651334643364, 0.11010235548019409, 0.015671949833631516, 0.015773830935359, 0.015852035954594612, 0.10847433656454086, 0.01595534011721611, 0.1100630834698677, 0.015988502651453018, 0.10655862838029861, 0.0159857627004385, 0.01595870964229107, 0.015907475724816322, 0.015852097421884537, 0.015762178227305412, 0.01566706970334053, 0.09789526462554932, 0.015456831082701683, 0.1071777492761612, 0.015274413861334324, 0.10472827404737473, 0.015062130987644196, 0.014966543763875961, 0.10440279543399811, 0.5419822931289673, 0.10832500457763672, 0.1083398088812828, 0.0988638699054718, 0.27362504601478577, 0.015753131359815598, 0.015960441902279854, 0.016139280050992966, 0.2776114046573639, 0.108669713139534, 0.09889665991067886, 0.26904046535491943, 0.01723231002688408, 0.10408060997724533, 0.10214771330356598, 0.10101278126239777, 0.018078310415148735, 0.10552099347114563, 0.271336168050766, 0.10739631205797195, 0.01878349483013153, 0.2598651945590973, 0.0192007627338171, 0.019413406029343605, 0.10266059637069702, 0.019718606024980545, 0.2617427110671997, 0.020016200840473175, 0.020170364528894424, 0.020275821909308434, 0.020343750715255737, 0.10578728467226028, 0.1045573502779007, 0.020404929295182228, 0.020382661372423172, 0.26301923394203186, 0.10195638239383698, 0.02047014981508255, 0.020492371171712875, 0.10601738095283508, 0.10735896974802017, 0.020468665286898613, 0.10152867436408997, 0.09820852428674698, 0.25268593430519104, 0.2563014626502991, 0.02057906612753868, 0.020723041146993637, 0.02080749347805977, 0.09691977500915527, 0.020886046811938286, 0.020895903930068016, 0.10380003601312637, 0.020836053416132927, 0.2629311680793762, 0.020823871716856956, 0.24991782009601593, 0.020984210073947906, 0.10316083580255508, 0.021152202039957047, 0.10199188441038132, 0.10425230115652084, 0.021230973303318024, 0.021211080253124237, 0.02116815187036991, 0.1044500544667244, 0.10108921676874161, 0.02094499208033085, 0.46500205993652344, 0.021068410947918892, 0.10454078763723373, 0.02135731466114521, 0.021458808332681656, 0.021504947915673256, 0.021528109908103943, 0.10853049159049988, 0.09783443808555603, 0.09966273605823517, 0.2568591237068176, 0.021512433886528015, 0.021571790799498558, 0.021568940952420235, 0.10127085447311401, 0.10428531467914581, 0.021469302475452423, 0.2618499994277954, 0.10447239130735397, 0.021516211330890656, 0.10099083185195923, 0.021541662514209747, 0.02151118777692318, 0.02144019491970539, 0.2568216025829315, 0.26864197850227356, 0.021559905260801315, 0.02162775956094265, 0.25118714570999146, 0.10767630487680435, 0.022006530314683914, 0.022106893360614777, 0.1028132513165474, 0.02219277434051037, 0.02217852883040905, 0.022132933139801025, 0.10136540234088898, 0.022026972845196724, 0.1053902879357338, 0.1015855073928833, 0.10837914794683456, 0.021672895178198814, 0.10986600816249847, 0.10503916442394257, 0.021368516609072685, 0.26189300417900085, 0.021266024559736252, 0.021252330392599106, 0.10408464819192886, 0.021131593734025955, 0.10347462445497513, 0.0209793783724308, 0.10677240788936615, 0.10588161647319794, 0.10055367648601532, 0.10629794746637344, 0.10721147805452347, 0.020475296303629875, 0.10190722346305847, 0.020289380103349686, 0.02017434500157833, 0.020045002922415733, 0.11072161048650742, 0.2638144791126251, 0.01976599544286728, 0.11034773290157318, 0.019689111039042473, 0.019636284559965134, 0.01955188252031803, 0.10125073045492172, 0.098856121301651, 0.01927470602095127, 0.10062152147293091, 0.01907302998006344, 0.0189456045627594, 0.2712213397026062, 0.27359405159950256, 0.10683141648769379, 0.019028590992093086, 0.26033633947372437, 0.10292515158653259, 0.01940111257135868, 0.019504941999912262, 0.10640978068113327, 0.019609395414590836, 0.019642310217022896, 0.10478696972131729, 0.019633689895272255, 0.019586266949772835, 0.019516341388225555, 0.019436506554484367, 0.10788773745298386, 0.10844674706459045, 0.01914602890610695, 0.10636244714260101, 0.4921707212924957, 0.10639718174934387, 0.09980984032154083, 0.01947818323969841, 0.01958954706788063, 0.2714273929595947, 0.01984182931482792, 0.019981849938631058, 0.020071033388376236, 0.10707547515630722, 0.020166559144854546, 0.10670312494039536, 0.020181531086564064, 0.020165907219052315, 0.020121583715081215, 0.10041234642267227, 0.10079634934663773, 0.10728508234024048, 0.10483291000127792, 0.1015675887465477, 0.10043409466743469, 0.019681677222251892, 0.26499029994010925, 0.01965651847422123, 0.2685587406158447, 0.10322228819131851, 0.01989622227847576, 0.10627292841672897, 0.1057722195982933, 0.020066745579242706, 0.10117673128843307, 0.0200809258967638, 0.020061729475855827, 0.10278865694999695, 0.019973905757069588, 0.019904688000679016, 0.019812745973467827]\n",
            "Val loss 0.07589786184651781\n",
            "Val auc roc 0.5033068783068783\n",
            "Saved model state dict for epoch 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61af8ce243c7424cb881612bda138989",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1770.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.0795\n",
            "Train Losses : [0.019712060689926147, 0.019582493230700493, 0.01944112777709961, 0.019287239760160446, 0.01913059502840042, 0.1046365350484848, 0.0187947079539299, 0.018635541200637817, 0.26081880927085876, 0.018412461504340172, 0.09819334000349045, 0.018278708681464195, 0.01818748377263546, 0.27131807804107666, 0.018111756071448326, 0.01809905841946602, 0.018071280792355537, 0.1016567051410675, 0.017967967316508293, 0.01790061965584755, 0.09893722832202911, 0.017743928357958794, 0.017652399837970734, 0.017542265355587006, 0.10350830107927322, 0.017317581921815872, 0.017188232392072678, 0.01706138253211975, 0.0169331394135952, 0.01678060181438923, 0.016627201810479164, 0.1018487811088562, 0.10215956717729568, 0.016208890825510025, 0.016072431579232216, 0.015930401161313057, 0.10042378306388855, 0.28257831931114197, 0.015656352043151855, 0.2838406562805176, 0.015713432803750038, 0.01576579362154007, 0.2869648039340973, 0.10309253633022308, 0.0160360224545002, 0.016119757667183876, 0.016182569786906242, 0.10838685929775238, 0.10209689289331436, 0.016263337805867195, 0.016277778893709183, 0.10401362180709839, 0.10267329216003418, 0.01623454876244068, 0.10822907835245132, 0.01617548055946827, 0.281881183385849, 0.016198772937059402, 0.10346339643001556, 0.01627645269036293, 0.01628703437745571, 0.10557423532009125, 0.10386291146278381, 0.2758994996547699, 0.10347485542297363, 0.2679254412651062, 0.01663687638938427, 0.28668296337127686, 0.01703082211315632, 0.01722100004553795, 0.10573820769786835, 0.10725703090429306, 0.2687453329563141, 0.01784367859363556, 0.018011918291449547, 0.1027737557888031, 0.01826466992497444, 0.10094286501407623, 0.018420763313770294, 0.018465567380189896, 0.018476607277989388, 0.018459191545844078, 0.10505769401788712, 0.018393313512206078, 0.018343709409236908, 0.10167580097913742, 0.10711102932691574, 0.018143540248274803, 0.10046152025461197, 0.2721458673477173, 0.018042420968413353, 0.018064040690660477, 0.09976588934659958, 0.1037069708108902, 0.01801847666501999, 0.10532467812299728, 0.01796223409473896, 0.49866151809692383, 0.49227288365364075, 0.10186301171779633, 0.018975302577018738, 0.10303734987974167, 0.019640035927295685, 0.10340897738933563, 0.10691909492015839, 0.020333340391516685, 0.10401900857686996, 0.02064417116343975, 0.10099615901708603, 0.10137875378131866, 0.10458296537399292, 0.020966818556189537, 0.24922066926956177, 0.021129285916686058, 0.021223850548267365, 0.2603079676628113, 0.1020771712064743, 0.10404238849878311, 0.10846326500177383, 0.10036294907331467, 0.10883765667676926, 0.11040414124727249, 0.021931439638137817, 0.10563622415065765, 0.021980326622724533, 0.0219582412391901, 0.1082601547241211, 0.02187843807041645, 0.10860306769609451, 0.02173423394560814, 0.021632971242070198, 0.02153610624372959, 0.2501230537891388, 0.021387450397014618, 0.021354833617806435, 0.021267704665660858, 0.02117842249572277, 0.021080508828163147, 0.02096368372440338, 0.26056957244873047, 0.10422664135694504, 0.473483681678772, 0.020972812548279762, 0.021157288923859596, 0.021294524893164635, 0.2560198903083801, 0.10490910708904266, 0.10758820921182632, 0.10183417052030563, 0.021982455626130104, 0.022061195224523544, 0.02210385352373123, 0.10378289967775345, 0.10454356670379639, 0.022128326818346977, 0.022099265828728676, 0.24359367787837982, 0.022077467292547226, 0.022079631686210632, 0.09863898158073425, 0.022045182064175606, 0.021989481523633003, 0.10613075643777847, 0.02184581570327282, 0.10373789817094803, 0.10385685414075851, 0.09984304755926132, 0.10376317799091339, 0.02142990566790104, 0.2540157437324524, 0.09790950268507004, 0.021362829953432083, 0.021336479112505913, 0.021286379545927048, 0.02121017314493656, 0.02111276425421238, 0.02101265639066696, 0.26135051250457764, 0.020861798897385597, 0.02082384005188942, 0.10907107591629028, 0.10157935321331024, 0.02063526213169098, 0.020569920539855957, 0.26802271604537964, 0.4836506247520447, 0.2480802685022354, 0.0986967459321022, 0.021348675712943077, 0.10779516398906708, 0.10417409986257553, 0.021964680403470993, 0.10520465672016144, 0.25350379943847656, 0.02243933454155922, 0.10172989964485168, 0.10272593051195145, 0.022832315415143967, 0.022930053994059563, 0.10545064508914948, 0.022982552647590637, 0.02296912856400013, 0.10479447990655899, 0.10410753637552261, 0.022871576249599457, 0.10309281200170517, 0.022745927795767784, 0.022661695256829262, 0.022564278915524483, 0.0996842309832573, 0.02232326567173004, 0.2604050934314728, 0.10615286976099014, 0.0221321489661932, 0.02207418717443943, 0.1075906828045845, 0.10199728608131409, 0.021854983642697334, 0.0980454534292221, 0.1010250449180603, 0.1012023463845253, 0.021541276946663857, 0.02143489010632038, 0.021342163905501366, 0.09685726463794708, 0.10191424936056137, 0.02098105289041996, 0.02084246464073658, 0.10500378161668777, 0.10396751761436462, 0.020463189110159874, 0.10298389196395874, 0.10468430072069168, 0.1004289984703064, 0.25312861800193787, 0.020018145442008972, 0.10834810137748718, 0.01998382993042469, 0.019938498735427856, 0.2730253040790558, 0.2674751877784729, 0.020031757652759552, 0.02011381834745407, 0.10758703202009201, 0.020216569304466248, 0.0202439334243536, 0.02021145261824131, 0.24984116852283478, 0.26366686820983887, 0.1034126803278923, 0.0205210093408823, 0.10113680362701416, 0.02068510837852955, 0.10126352310180664, 0.02075996622443199, 0.1072208434343338, 0.020783517509698868, 0.020761612802743912, 0.02071467414498329, 0.2630285620689392, 0.46797019243240356, 0.10456359386444092, 0.10377660393714905, 0.10266036540269852, 0.02157069742679596, 0.021720198914408684, 0.021826963871717453, 0.021886032074689865, 0.1002931222319603, 0.021955356001853943, 0.02193519100546837, 0.021900402382016182, 0.021837832406163216, 0.2502840757369995, 0.021799959242343903, 0.10387124121189117, 0.2556363642215729, 0.2589234411716461, 0.022011790424585342, 0.022118452936410904, 0.09942633658647537, 0.10822807997465134, 0.1041363850235939, 0.1027757003903389, 0.10481464117765427, 0.022420059889554977, 0.02241959609091282, 0.022401440888643265, 0.022332442924380302, 0.022273004055023193, 0.25018370151519775, 0.10584013909101486, 0.022160600870847702, 0.022132355719804764, 0.10448674857616425, 0.25833699107170105, 0.022074434906244278, 0.10587572306394577, 0.0220849197357893, 0.022065185010433197, 0.10018430650234222, 0.021979009732604027, 0.021897446364164352, 0.1044362410902977, 0.021733039990067482, 0.10534285753965378, 0.1090327799320221, 0.02145230397582054, 0.10261647403240204, 0.021267065778374672, 0.021138707175850868, 0.021018339321017265, 0.10667145997285843, 0.25929367542266846, 0.1062910333275795, 0.020721539855003357, 0.10307174921035767, 0.02062271535396576, 0.10675457119941711, 0.10212697088718414, 0.2594205439090729, 0.020476870238780975, 0.02048790268599987, 0.2565959692001343, 0.020515188574790955, 0.020567594096064568, 0.020571663975715637, 0.25512704253196716, 0.020640617236495018, 0.020670855417847633, 0.02068883366882801, 0.10415644198656082, 0.020673219114542007, 0.2585512697696686, 0.10761911422014236, 0.020724056288599968, 0.2626051902770996, 0.02085539698600769, 0.11059870570898056, 0.020984889939427376, 0.02100103721022606, 0.02099853940308094, 0.10585025697946548, 0.11010108143091202, 0.02091117948293686, 0.10305013507604599, 0.25257423520088196, 0.1076807901263237, 0.020906461402773857, 0.10487440228462219, 0.09986221045255661, 0.02092982828617096, 0.10139919817447662, 0.020901011303067207, 0.02083405666053295, 0.10297807306051254, 0.10710310190916061, 0.020658455789089203, 0.10205815732479095, 0.020509343594312668, 0.02041487768292427, 0.10188272595405579, 0.020223522558808327, 0.020145168527960777, 0.020010288804769516, 0.10888136923313141, 0.0197598859667778, 0.019642597064375877, 0.019508596509695053, 0.10564582794904709, 0.01924821548163891, 0.10253318399190903, 0.10505940765142441, 0.018864594399929047, 0.2648238241672516, 0.018708564341068268, 0.018678460270166397, 0.102201446890831, 0.2579513490200043, 0.106119304895401, 0.1032872125506401, 0.018661051988601685, 0.0985010415315628, 0.26126113533973694, 0.018759839236736298, 0.01881602220237255, 0.1040530800819397, 0.10758119076490402, 0.018879391252994537, 0.018882090225815773, 0.10961522907018661, 0.01883551850914955, 0.01879531517624855, 0.018736690282821655, 0.10923382639884949, 0.01861841045320034, 0.10360246896743774, 0.10835139453411102, 0.01841450110077858, 0.26547396183013916, 0.018360283225774765, 0.10333984345197678, 0.01832735538482666, 0.1076643317937851, 0.018289392814040184, 0.10387349128723145, 0.018200663849711418, 0.018157243728637695, 0.018079517409205437, 0.018008621409535408, 0.017924627289175987, 0.017817780375480652, 0.10643628984689713, 0.01761120930314064, 0.0983910784125328, 0.01743275299668312, 0.10605697333812714, 0.10317548364400864, 0.0171537846326828, 0.0170609001070261, 0.2730494737625122, 0.016955647617578506, 0.016918474808335304, 0.016902174800634384, 0.016843462362885475, 0.10781402885913849, 0.2753053307533264, 0.10404376685619354, 0.2755356431007385, 0.016880324110388756, 0.016967765986919403, 0.2697823941707611, 0.017152054235339165, 0.10451754927635193, 0.10292915999889374, 0.27621379494667053, 0.017575901001691818, 0.01769716665148735, 0.10097698122262955, 0.10599134117364883, 0.2664777636528015, 0.10167429596185684, 0.0182306207716465, 0.018322115764021873, 0.1050269827246666, 0.01846994087100029, 0.01850765570998192, 0.10410335659980774, 0.10686670243740082, 0.10345327109098434, 0.018554113805294037, 0.10442797839641571, 0.018534447997808456, 0.018515903502702713, 0.018471350893378258, 0.27504175901412964, 0.1058875322341919, 0.10372193157672882, 0.1081453189253807, 0.01850610412657261, 0.1050533726811409, 0.018501972779631615, 0.10675658285617828, 0.10613667964935303, 0.10902872681617737, 0.10381004214286804, 0.10023508965969086, 0.10543009638786316, 0.018373344093561172, 0.10159683227539062, 0.10626913607120514, 0.10060504078865051, 0.01826643757522106, 0.10571666061878204, 0.01819978654384613, 0.018137993291020393, 0.0180724635720253, 0.01800708845257759, 0.017923938110470772, 0.017830057069659233, 0.01771601475775242, 0.27586135268211365, 0.017595669254660606, 0.10240598767995834, 0.10463381558656693, 0.10554514080286026, 0.10641279816627502, 0.5109909772872925, 0.017618510872125626, 0.01776808686554432, 0.017880754545331, 0.01797729916870594, 0.018030060455203056, 0.018054287880659103, 0.01807519420981407, 0.018057629466056824, 0.2667821943759918, 0.018099550157785416, 0.10268884152173996, 0.018168123438954353, 0.4922770857810974, 0.018378116190433502, 0.018525484949350357, 0.018651530146598816, 0.018744630739092827, 0.1020636335015297, 0.018883246928453445, 0.018921617418527603, 0.018937930464744568, 0.018935350701212883, 0.01890808902680874, 0.01887051947414875, 0.262801855802536, 0.018838023766875267, 0.26075294613838196, 0.018926389515399933, 0.018989060074090958, 0.01900194026529789, 0.09833003580570221, 0.10503454506397247, 0.019017374143004417, 0.10059629380702972, 0.018996143713593483, 0.10700955241918564, 0.018940463662147522, 0.10480088740587234, 0.018842624500393867, 0.09933042526245117, 0.018738694489002228, 0.01868310570716858, 0.01859935000538826, 0.018520480021834373, 0.09780075401067734, 0.018334146589040756, 0.018246080726385117, 0.10671084374189377, 0.1071917712688446, 0.10410234332084656, 0.10810860991477966, 0.2699005901813507, 0.017853016033768654, 0.09877000749111176, 0.017843397334218025, 0.017827872186899185, 0.01779911480844021, 0.017738547176122665, 0.017688799649477005, 0.017600269988179207, 0.017529048025608063, 0.2647615969181061, 0.2680530548095703, 0.017512496560811996, 0.017535662278532982, 0.01756024733185768, 0.01758112944662571, 0.01757240854203701, 0.2596699893474579, 0.017580047249794006, 0.09935732930898666, 0.10692863166332245, 0.017650265246629715, 0.10687030106782913, 0.10128170251846313, 0.01765165477991104, 0.09977568686008453, 0.10711938887834549, 0.10495681315660477, 0.01761339232325554, 0.01758560538291931, 0.017549531534314156, 0.09949631243944168, 0.1064612939953804, 0.01741914078593254, 0.10830269008874893, 0.1049303412437439, 0.017271531745791435, 0.017219819128513336, 0.10973338782787323, 0.10466243326663971, 0.017045335844159126, 0.11204755306243896, 0.016928182914853096, 0.10398049652576447, 0.510581910610199, 0.5083773136138916, 0.0172340739518404, 0.01748776249587536, 0.017721643671393394, 0.017887502908706665, 0.01803167164325714, 0.10773709416389465, 0.10250917077064514, 0.018332764506340027, 0.01840987242758274, 0.01845344342291355, 0.01846420206129551, 0.1063724234700203, 0.10176848620176315, 0.018484706059098244, 0.10508202016353607, 0.10201490670442581, 0.2656427025794983, 0.263395756483078, 0.018628820776939392, 0.018706247210502625, 0.018770107999444008, 0.018820667639374733, 0.018842164427042007, 0.018848532810807228, 0.2764066755771637, 0.018893660977482796, 0.10629234462976456, 0.01894555799663067, 0.018955551087856293, 0.10706588625907898, 0.0189475417137146, 0.26072853803634644, 0.10175860673189163, 0.019009193405508995, 0.01903722994029522, 0.10914132744073868, 0.019027676433324814, 0.10883474349975586, 0.10143139213323593, 0.018977893516421318, 0.018939660862088203, 0.018894443288445473, 0.018839465454220772, 0.018778372555971146, 0.018687861040234566, 0.10178932547569275, 0.018536413088440895, 0.264938622713089, 0.10675457119941711, 0.01844460517168045, 0.10077414661645889, 0.10627781599760056, 0.018375802785158157, 0.09911329299211502, 0.018313799053430557, 0.1087309718132019, 0.018229670822620392, 0.018160883337259293, 0.018099524080753326, 0.018033111467957497, 0.01795450784265995, 0.017855843529105186, 0.01777496561408043, 0.01766713336110115, 0.10488405078649521, 0.1055535227060318, 0.017396781593561172, 0.10441602766513824, 0.017227280884981155, 0.2728371322154999, 0.01713111437857151, 0.017110150307416916, 0.28563883900642395, 0.01709970459342003, 0.10250531136989594, 0.1032283678650856, 0.01716000959277153, 0.10113032907247543, 0.1018713116645813, 0.10837498307228088, 0.017158765345811844, 0.017143243923783302, 0.017109962180256844, 0.017081033438444138, 0.10504329204559326, 0.01700112782418728, 0.10558295994997025, 0.1021694540977478, 0.016862506046891212, 0.016813088208436966, 0.2809704542160034, 0.01675601117312908, 0.016745658591389656, 0.01673794351518154, 0.01670779474079609, 0.016669809818267822, 0.09910400956869125, 0.01657225750386715, 0.10168711841106415, 0.09833335876464844, 0.016446787863969803, 0.01639288291335106, 0.10761629790067673, 0.1064196303486824, 0.01625213772058487, 0.09990187734365463, 0.10808524489402771, 0.01609620824456215, 0.016049550846219063, 0.015992196276783943, 0.09703294187784195, 0.015890341252088547, 0.10625161975622177, 0.015768352895975113, 0.015701476484537125, 0.01563551463186741, 0.0988611951470375, 0.10981833934783936, 0.01544150896370411, 0.10351096093654633, 0.015320729464292526, 0.0993536040186882, 0.015221060253679752, 0.09867081046104431, 0.015108704566955566, 0.10200461000204086, 0.09967652708292007, 0.09849397838115692, 0.10683440417051315, 0.2899477481842041, 0.01494207326322794, 0.10605066269636154, 0.01498354971408844, 0.014987732283771038, 0.10749515146017075, 0.10302779823541641, 0.014984838664531708, 0.01496783085167408, 0.10979729890823364, 0.10257536917924881, 0.014916439540684223, 0.01490050833672285, 0.014870435930788517, 0.014830322936177254, 0.014768521301448345, 0.10234276950359344, 0.10861272364854813, 0.103189617395401, 0.10811268538236618, 0.2877224087715149, 0.01461759302765131, 0.10213165730237961, 0.27895382046699524, 0.014751367270946503, 0.28485897183418274, 0.10951072722673416, 0.10033497959375381, 0.015128022991120815, 0.015216521918773651, 0.01527253445237875, 0.015316162258386612, 0.015360317192971706, 0.015359992161393166, 0.09948171675205231, 0.09934865683317184, 0.015368565917015076, 0.10615922510623932, 0.1080092191696167, 0.01535257138311863, 0.015345921739935875, 0.01531553827226162, 0.11004775762557983, 0.10598942637443542, 0.015236846171319485, 0.01519208773970604, 0.01515122503042221, 0.10968068987131119, 0.10855411738157272, 0.10641453415155411, 0.014995933510363102, 0.014967912808060646, 0.014916762709617615, 0.09844996780157089, 0.014824909158051014, 0.014770184643566608, 0.2966236174106598, 0.10619719326496124, 0.014739224687218666, 0.014747831970453262, 0.014734175056219101, 0.014711637049913406, 0.10624668002128601, 0.1024545207619667, 0.014637297950685024, 0.27440309524536133, 0.01464506983757019, 0.10416603833436966, 0.01468475628644228, 0.014697879552841187, 0.10361040383577347, 0.014700734987854958, 0.2832232713699341, 0.10041940957307816, 0.10446041822433472, 0.014825014397501945, 0.014849224127829075, 0.014865977689623833, 0.01487799733877182, 0.1037132740020752, 0.1071208268404007, 0.014861766248941422, 0.014836560934782028, 0.2909903824329376, 0.014857761561870575, 0.01487978920340538, 0.10689882189035416, 0.541361391544342, 0.10687123984098434, 0.10498055815696716, 0.01533510535955429, 0.015446240082383156, 0.5320200324058533, 0.015753351151943207, 0.10712236166000366, 0.09971974045038223, 0.10779913514852524, 0.10398422926664352, 0.016552265733480453, 0.104175865650177, 0.10502038896083832, 0.102439284324646, 0.016927767544984818, 0.016986211761832237, 0.017031492665410042, 0.2824021875858307, 0.10509178042411804, 0.017208514735102654, 0.2626191973686218, 0.017370548099279404, 0.01746300607919693, 0.01751764304935932, 0.09906908869743347, 0.01760147511959076, 0.017612559720873833, 0.2762092053890228, 0.01768944039940834, 0.01772315613925457, 0.26034626364707947, 0.10438445210456848, 0.01790550723671913, 0.01796090602874756, 0.017987489700317383, 0.01800191029906273, 0.10267568379640579, 0.018014514818787575, 0.01799246296286583, 0.017987387254834175, 0.10688471049070358, 0.49256324768066406, 0.2753579914569855, 0.09875188022851944, 0.10913313925266266, 0.49414512515068054, 0.10476027429103851, 0.018935924395918846, 0.019138019531965256, 0.10463199019432068, 0.27050670981407166, 0.01964111067354679, 0.0992019921541214, 0.019927142187952995, 0.020029915496706963, 0.10667091608047485, 0.020194999873638153, 0.10436420887708664, 0.10101566463708878, 0.10287463665008545, 0.02036358416080475, 0.020386235788464546, 0.1001020297408104, 0.02038871869444847, 0.020384889096021652, 0.020353518426418304, 0.10542362183332443, 0.10476763546466827, 0.2622276544570923, 0.10711019486188889, 0.10673128068447113, 0.02030203491449356, 0.10822034627199173, 0.020290136337280273, 0.020283490419387817, 0.10431525111198425, 0.020228112116456032, 0.020185889676213264, 0.10502345114946365, 0.10147763043642044, 0.020052162930369377, 0.019996754825115204, 0.104291632771492, 0.019865000620484352, 0.10768356174230576, 0.09977156668901443, 0.10288065671920776, 0.019640738144516945, 0.019591396674513817, 0.10603546351194382, 0.019464503973722458, 0.019393278285861015, 0.10542342811822891, 0.2653747797012329, 0.019263632595539093, 0.019238082692027092, 0.1078302264213562, 0.019178304821252823, 0.01913781277835369, 0.019086621701717377, 0.019024457782506943, 0.018963465467095375, 0.01890242099761963, 0.0188217144459486, 0.018738485872745514, 0.018661504611372948, 0.018565813079476357, 0.2742566466331482, 0.2625635266304016, 0.0184673760086298, 0.10583959519863129, 0.018485689535737038, 0.10785746574401855, 0.10285346210002899, 0.26264098286628723, 0.01851966418325901, 0.10696115344762802, 0.10157567262649536, 0.018596963956952095, 0.10133156925439835, 0.01861991174519062, 0.01860087178647518, 0.2706091105937958, 0.01861763373017311, 0.018650906160473824, 0.018638363108038902, 0.018643861636519432, 0.01862812601029873, 0.01859941892325878, 0.01855117827653885, 0.018504809588193893, 0.10053795576095581, 0.10478957742452621, 0.2618001103401184, 0.2679958641529083, 0.018427662551403046, 0.0998750552535057, 0.018501712009310722, 0.104149729013443, 0.1065833792090416, 0.10319823771715164, 0.018579572439193726, 0.10183171182870865, 0.018580080941319466, 0.018567977473139763, 0.018541602417826653, 0.10590501874685287, 0.01848156377673149, 0.01843526028096676, 0.018395885825157166, 0.018353674560785294, 0.018289022147655487, 0.10630597174167633, 0.10548266768455505, 0.10261925309896469, 0.10767006874084473, 0.018024291843175888, 0.10483339428901672, 0.017939548939466476, 0.26364511251449585, 0.10620301961898804, 0.017881227657198906, 0.01787327602505684, 0.10748577862977982, 0.2700707018375397, 0.017873382195830345, 0.017886649817228317, 0.09884621202945709, 0.2675694227218628, 0.2722465693950653, 0.018051382154226303, 0.10670045763254166, 0.018185967579483986, 0.018243322148919106, 0.10933193564414978, 0.10619969666004181, 0.018328536301851273, 0.018338710069656372, 0.259356290102005, 0.018376246094703674, 0.2736551761627197, 0.09935619682073593, 0.018554316833615303, 0.09750527888536453, 0.018650969490408897, 0.10775608569383621, 0.1032504066824913, 0.10223805904388428, 0.018731839954853058, 0.100324347615242, 0.01875971257686615, 0.26866233348846436, 0.2677299380302429, 0.10652247816324234, 0.10107050091028214, 0.10588149726390839, 0.019070390611886978, 0.019124554470181465, 0.0191422700881958, 0.019141973927617073, 0.09882625937461853, 0.01915285363793373, 0.10306338220834732, 0.10322389751672745, 0.01910138502717018, 0.019093038514256477, 0.262204647064209, 0.01908501423895359, 0.019079212099313736, 0.019075259566307068, 0.10969187319278717, 0.019028663635253906, 0.10496384650468826, 0.018978988751769066, 0.01895202323794365, 0.018907001242041588, 0.10023659467697144, 0.10062529146671295, 0.018777785822749138, 0.2726079821586609, 0.10760542005300522, 0.01874471642076969, 0.018719756975769997, 0.5015891790390015, 0.10101190954446793, 0.10786014795303345, 0.10533236712217331, 0.26456940174102783, 0.019160473719239235, 0.09682298451662064, 0.10377473384141922, 0.10873055458068848, 0.1066167801618576, 0.01950453780591488, 0.10950657725334167, 0.10756035149097443, 0.019593698903918266, 0.10342035442590714, 0.10575176775455475, 0.01964966021478176, 0.10373569279909134, 0.019630741328001022, 0.2587227523326874, 0.10649304836988449, 0.10216810554265976, 0.01970704086124897, 0.01970561593770981, 0.019713906571269035, 0.47385144233703613, 0.10483352094888687, 0.10323037207126617, 0.019968092441558838, 0.020023459568619728, 0.1062040850520134, 0.10486764460802078, 0.02011648751795292, 0.10657420009374619, 0.2563828229904175, 0.02020639181137085, 0.10360851883888245, 0.1048612967133522, 0.25783416628837585, 0.020373370498418808, 0.020425062626600266, 0.10557626932859421, 0.10652729123830795, 0.10566414892673492, 0.020542722195386887, 0.020545752719044685, 0.09801500290632248, 0.2672039568424225, 0.02056332491338253, 0.020596420392394066, 0.020585408434271812, 0.02059880457818508, 0.020578688010573387, 0.02054356224834919, 0.020524432882666588, 0.020473221316933632, 0.020421359688043594, 0.10456132143735886, 0.020310640335083008, 0.1041356697678566, 0.020199451595544815, 0.020153019577264786, 0.10700567066669464, 0.02003392018377781, 0.019977398216724396, 0.019896704703569412, 0.10109567642211914, 0.01976035349071026, 0.019698308780789375, 0.10139807313680649, 0.019556982442736626, 0.019492339342832565, 0.09835803508758545, 0.019365541636943817, 0.2606452405452728, 0.019274407997727394, 0.0971631333231926, 0.10795202106237411, 0.10588626563549042, 0.10697827488183975, 0.019140878692269325, 0.10615552216768265, 0.019084038212895393, 0.10149391740560532, 0.019022006541490555, 0.01898733153939247, 0.01894516684114933, 0.018883204087615013, 0.018840262666344643, 0.1048484519124031, 0.01872442476451397, 0.10485553741455078, 0.018614277243614197, 0.018551580607891083, 0.01849629543721676, 0.018438195809721947, 0.2711533010005951, 0.018356025218963623, 0.018327752128243446, 0.018296193331480026, 0.01825796253979206, 0.018217507749795914, 0.10396800935268402, 0.10010144114494324, 0.10668551176786423, 0.018043899908661842, 0.01801406964659691, 0.2771815061569214, 0.01794462651014328, 0.017948433756828308, 0.01792641170322895, 0.017903143540024757, 0.017861243337392807, 0.5017490386962891, 0.100702203810215, 0.2669452130794525, 0.504340648651123, 0.01821574568748474, 0.018371572718024254, 0.018496224656701088, 0.01858671009540558, 0.018670083954930305, 0.01874675415456295, 0.018790284171700478, 0.09919904917478561, 0.018868176266551018, 0.018888752907514572, 0.10065558552742004, 0.10781946033239365, 0.10210201144218445, 0.1088489219546318, 0.018928326666355133, 0.27817872166633606, 0.2576310634613037, 0.019031226634979248, 0.01907394640147686, 0.2616027593612671, 0.4994544982910156, 0.019333794713020325, 0.1032409816980362, 0.10486716777086258, 0.10517380386590958, 0.019759798422455788, 0.26834726333618164, 0.2587277591228485, 0.020071526989340782, 0.020162368193268776, 0.020263181999325752, 0.020325694233179092, 0.020380869507789612, 0.2637206017971039, 0.020468667149543762, 0.1050192192196846, 0.02056807652115822, 0.10875997692346573, 0.020635109394788742, 0.10368591547012329, 0.020661236718297005, 0.02065260335803032, 0.4779479205608368, 0.10491371154785156, 0.09984906762838364, 0.09978600591421127, 0.2616572678089142, 0.2533344626426697, 0.021107841283082962, 0.10719604790210724, 0.02127552404999733, 0.021344348788261414, 0.021389156579971313, 0.021426720544695854, 0.1025109738111496, 0.10478299856185913, 0.021457703784108162, 0.10444421321153641, 0.021474748849868774, 0.10578298568725586, 0.10265196859836578, 0.09746722877025604, 0.021432971581816673, 0.09998173266649246, 0.0213946346193552, 0.2559731900691986, 0.10846840590238571, 0.25773105025291443, 0.021428661420941353, 0.10696084052324295, 0.021457163617014885, 0.021467382088303566, 0.021468207240104675, 0.26159170269966125, 0.10664504766464233, 0.021502219140529633, 0.1049935594201088, 0.1030406653881073, 0.10106121748685837, 0.021536804735660553, 0.021522026509046555, 0.021506672725081444, 0.10849642008543015, 0.021456507965922356, 0.1032770425081253, 0.10541809350252151, 0.021367983892560005, 0.02134818583726883, 0.10638489574193954, 0.02127489447593689, 0.1072244867682457, 0.02118135429918766, 0.021152323111891747, 0.021106230095028877, 0.021047931164503098, 0.10973937809467316, 0.10491187125444412, 0.2562713623046875, 0.10842331498861313, 0.020874615758657455, 0.020845068618655205, 0.020820487290620804, 0.25826770067214966, 0.020795905962586403, 0.020786864683032036, 0.26121023297309875, 0.10507828742265701, 0.020809758454561234, 0.020817024633288383, 0.020805949345231056, 0.020790912210941315, 0.2513914704322815, 0.02078206092119217, 0.10439150035381317, 0.10437315702438354, 0.02078561671078205, 0.020789049565792084, 0.10055515170097351, 0.10746143013238907, 0.10738935321569443, 0.020723408088088036, 0.10786639899015427, 0.10367900133132935, 0.020661357790231705, 0.1052340641617775, 0.10124072432518005, 0.1056206151843071, 0.020572688430547714, 0.10540756583213806, 0.02051619626581669, 0.10618054866790771, 0.020463639870285988, 0.02042846940457821, 0.020392851904034615, 0.020343998447060585, 0.10262931138277054, 0.020261945202946663, 0.020212508738040924, 0.020168181508779526, 0.10572396963834763, 0.10497695952653885, 0.020022600889205933, 0.019971899688243866, 0.019935820251703262, 0.019882887601852417, 0.019824741408228874, 0.09914941340684891, 0.019715899601578712, 0.019669614732265472, 0.10591136664152145, 0.10315298289060593, 0.10158985108137131, 0.019473066553473473, 0.10496265441179276, 0.10026977956295013, 0.01935589499771595, 0.10057327151298523, 0.10600600391626358, 0.10443343222141266, 0.01922723837196827, 0.10240289568901062, 0.1033448725938797, 0.25960275530815125, 0.019131403416395187, 0.019130026921629906, 0.10689730942249298, 0.2662215530872345, 0.019125618040561676, 0.1062043309211731, 0.019131261855363846, 0.10585679113864899, 0.019137825816869736, 0.019125305116176605, 0.019120031967759132, 0.10518419742584229, 0.10857554525136948, 0.019060121849179268, 0.10363484174013138, 0.01902620494365692, 0.10728887468576431, 0.018979720771312714, 0.018953360617160797, 0.01891997456550598, 0.10405968874692917, 0.26389792561531067, 0.10148435831069946, 0.09943141788244247, 0.10619651526212692, 0.018855061382055283, 0.1012076810002327, 0.018845314159989357, 0.10567155480384827, 0.10475960373878479, 0.10677632689476013, 0.1081913635134697, 0.2604835331439972, 0.018800407648086548, 0.10863521695137024, 0.01882137544453144, 0.10523559898138046, 0.0188294630497694, 0.01883040927350521, 0.10454777628183365, 0.10666193813085556, 0.01879696175456047, 0.10192165523767471, 0.0187606792896986, 0.10113689303398132, 0.018720049411058426, 0.01870022341609001, 0.7669652104377747, 0.26577624678611755, 0.1070936843752861, 0.10148970037698746, 0.019038157537579536, 0.019096383824944496, 0.019153082743287086, 0.09933457523584366, 0.019243936985731125, 0.019258785992860794, 0.019290238618850708, 0.0192953459918499, 0.019304826855659485, 0.019303686916828156, 0.10208117216825485, 0.0192795991897583, 0.019267253577709198, 0.1080482080578804, 0.01923350989818573, 0.019220495596528053, 0.26321926712989807, 0.0191875621676445, 0.10412510484457016, 0.019180145114660263, 0.10171554982662201, 0.019159024581313133, 0.10625060647726059, 0.10314054787158966, 0.01912933774292469, 0.019108526408672333, 0.019086766988039017, 0.019074764102697372, 0.10540896654129028, 0.10263413190841675, 0.018995216116309166, 0.09951372444629669, 0.10448789596557617, 0.018916606903076172, 0.10545707494020462, 0.018874147906899452, 0.10332874208688736, 0.01883847266435623, 0.01880764774978161, 0.0187703687697649, 0.263802707195282, 0.27165505290031433, 0.10841715335845947, 0.018772857263684273, 0.1031251773238182, 0.018777912482619286, 0.27233651280403137, 0.10534141212701797, 0.09941878169775009, 0.018837125971913338, 0.018860992044210434, 0.2711898386478424, 0.018884366378188133, 0.01890582963824272, 0.01890663057565689, 0.10480035096406937, 0.01891624741256237, 0.10717442631721497, 0.10584194958209991, 0.018912186846137047, 0.01890093833208084, 0.018889062106609344, 0.10659755021333694, 0.10156893730163574, 0.09888604283332825, 0.018835674971342087, 0.018828928470611572, 0.018805526196956635, 0.018780408427119255, 0.26809799671173096, 0.018754037097096443, 0.01874593086540699, 0.018745433539152145, 0.018725480884313583, 0.10675471276044846, 0.018690893426537514, 0.10115119814872742, 0.10458456724882126, 0.01863151788711548, 0.1068507507443428, 0.10576914995908737, 0.01858106628060341, 0.018557727336883545, 0.10551637411117554, 0.018527712672948837, 0.018504617735743523, 0.01848142221570015, 0.018445240333676338, 0.1077640950679779, 0.10071046650409698, 0.26879122853279114, 0.10333514213562012, 0.4932117760181427, 0.018422693014144897, 0.10595088452100754, 0.01849295012652874, 0.10439353436231613, 0.018536880612373352, 0.018546441569924355, 0.09833253920078278, 0.01857003942131996, 0.01857314445078373, 0.10152696818113327, 0.018577920272946358, 0.27157357335090637, 0.018581222742795944, 0.018593348562717438, 0.018583737313747406, 0.018588189035654068, 0.1069968193769455, 0.01857437752187252, 0.10640721768140793, 0.10160886496305466, 0.10219426453113556, 0.018552249297499657, 0.018535416573286057, 0.10284943878650665, 0.2644384801387787, 0.018512725830078125, 0.10504014045000076, 0.018519116565585136, 0.10513122379779816, 0.018507681787014008, 0.018500162288546562, 0.018484942615032196, 0.10368549078702927, 0.2623533606529236, 0.01847122423350811, 0.018476825207471848, 0.1045737937092781, 0.09929291903972626, 0.10236706584692001, 0.018467487767338753, 0.018460741266608238, 0.10135228931903839, 0.10523021966218948, 0.10086725652217865, 0.018423905596137047, 0.018422001972794533, 0.10356578975915909, 0.018391847610473633, 0.26654747128486633, 0.01837366260588169, 0.018376009538769722, 0.10133182257413864, 0.01836490072309971, 0.01836041361093521, 0.01834995672106743, 0.018336661159992218, 0.018319571390748024, 0.10432934761047363, 0.018289247527718544, 0.10711634904146194, 0.10785660147666931, 0.01824142038822174, 0.10075364261865616, 0.10428536683320999, 0.10433726757764816, 0.26740774512290955, 0.10578904300928116, 0.10586225241422653, 0.01818908378481865, 0.2685876488685608, 0.27262184023857117, 0.0994504764676094, 0.018258729949593544, 0.018273858353495598, 0.01828816346824169, 0.26864057779312134, 0.018316950649023056, 0.01834208332002163, 0.018354514613747597, 0.018353017047047615, 0.2599560022354126, 0.018377522006630898, 0.018391847610473633, 0.27112647891044617, 0.10478091239929199, 0.018429234623908997, 0.018441874533891678, 0.01844572089612484, 0.10042992979288101, 0.01845637708902359, 0.018455175682902336, 0.1079355850815773, 0.01845403201878071, 0.2720605134963989, 0.018455734476447105, 0.10269565880298615, 0.018465273082256317, 0.018469348549842834, 0.018463440239429474, 0.018452046439051628, 0.10572835803031921, 0.10272243618965149, 0.49774372577667236, 0.10053621232509613, 0.10714437812566757, 0.01850164122879505, 0.10345868021249771, 0.018538180738687515, 0.01854133792221546, 0.1081104576587677, 0.09913922101259232, 0.10582096129655838, 0.10612411797046661, 0.018571672961115837, 0.0185607448220253, 0.10104504972696304, 0.018566183745861053, 0.01856095716357231, 0.26366978883743286, 0.018557297065854073, 0.10397233814001083, 0.01855919510126114, 0.018556907773017883, 0.01854856312274933, 0.01853969134390354, 0.018537908792495728, 0.10480049252510071, 0.09814167022705078, 0.018516940996050835, 0.018498491495847702, 0.01847699284553528, 0.27106988430023193, 0.10020721703767776, 0.10605095326900482, 0.018461206927895546, 0.018468689173460007, 0.018458226695656776, 0.01844988577067852, 0.018440619111061096, 0.018429912626743317, 0.10611387342214584, 0.10284300148487091, 0.018392696976661682, 0.018384583294391632, 0.018365686759352684, 0.018353542312979698, 0.2588418424129486, 0.01833752728998661, 0.01832754723727703, 0.10418326407670975, 0.10413526743650436, 0.018315458670258522, 0.018299030140042305, 0.10694172233343124, 0.018287381157279015, 0.10259745270013809, 0.10229799896478653, 0.018257349729537964, 0.018260246142745018, 0.018242912366986275, 0.018232567235827446, 0.10669160634279251, 0.01821121945977211, 0.27105873823165894, 0.018193406984210014, 0.01819152571260929, 0.10061377286911011, 0.09995321184396744, 0.01817186176776886, 0.018159912899136543, 0.10499223321676254, 0.018153367564082146, 0.10557013005018234, 0.10262677818536758, 0.10400751233100891, 0.10044059157371521, 0.018114270642399788, 0.10435649752616882, 0.0181047972291708, 0.018099751323461533, 0.018095917999744415, 0.018080024048686028, 0.10525919497013092, 0.0180608332157135, 0.018048975616693497, 0.018040383234620094, 0.2671845257282257, 0.0180288627743721, 0.10546186566352844, 0.2745419442653656, 0.01803000457584858, 0.10714589804410934, 0.01802181452512741, 0.018022140488028526, 0.10565639287233353, 0.2624800205230713, 0.10584680736064911, 0.018028499558568, 0.01802963577210903, 0.10561008751392365, 0.10244303941726685, 0.018040062859654427, 0.018034186214208603, 0.018031606450676918, 0.018031472340226173, 0.2641400396823883, 0.018023358657956123, 0.10484280437231064, 0.018038231879472733, 0.018023468554019928, 0.01802646741271019, 0.10377025604248047, 0.2711386978626251, 0.018019137904047966, 0.265348881483078, 0.018034862354397774, 0.018038203939795494, 0.10370545089244843, 0.018040712922811508, 0.018044013530015945, 0.018036870285868645, 0.10301771014928818, 0.01803533546626568, 0.2645214796066284, 0.10263164341449738, 0.10171913355588913, 0.1053696796298027, 0.01804378069937229, 0.10269657522439957, 0.1040787622332573, 0.10632874816656113, 0.01805746555328369, 0.10563040524721146, 0.10238489508628845, 0.01804686151444912, 0.10353633016347885, 0.018049370497465134, 0.10401862859725952, 0.1059982106089592, 0.1034172847867012, 0.01804182678461075, 0.018043166026473045, 0.10116149485111237, 0.018029963597655296, 0.018022380769252777, 0.09900188446044922, 0.2687022387981415, 0.10743284970521927, 0.018009744584560394, 0.10408739000558853, 0.1069016233086586, 0.01801341585814953, 0.10650602728128433, 0.018019506707787514, 0.10142364352941513, 0.018009377643465996, 0.01800362952053547, 0.10085811465978622, 0.018003590404987335, 0.27394741773605347, 0.10814909636974335, 0.017992790788412094, 0.10435451567173004, 0.017995350062847137, 0.01799275167286396, 0.018001001328229904, 0.10066933184862137, 0.26211658120155334, 0.10049183666706085, 0.017999647185206413, 0.2714555263519287, 0.10698367655277252, 0.1016668975353241, 0.1074603870511055, 0.10291142016649246, 0.018015872687101364, 0.27285805344581604, 0.018019698560237885, 0.018016453832387924, 0.018018092960119247, 0.018020417541265488, 0.018022941425442696, 0.018019050359725952, 0.10531461238861084, 0.01801188848912716, 0.018015585839748383, 0.018017539754509926, 0.018010003492236137, 0.10444242507219315, 0.10465571284294128, 0.2637185752391815, 0.10094933956861496, 0.10729407519102097, 0.01801084168255329, 0.01800268329679966, 0.1018744483590126, 0.26830604672431946, 0.10274066776037216, 0.10001648962497711, 0.0180166307836771, 0.018006928265094757, 0.10063651949167252, 0.10589784383773804, 0.018012279644608498, 0.018018286675214767, 0.01800772175192833, 0.018007293343544006, 0.26908454298973083, 0.01801259256899357, 0.018010325729846954, 0.10605668276548386, 0.27319589257240295, 0.10317716747522354, 0.018012048676609993, 0.10799960047006607, 0.01800522953271866, 0.10141105949878693, 0.10695608705282211, 0.018020588904619217, 0.018006395548582077, 0.018009070307016373]\n",
            "Val loss 0.07773662821015939\n",
            "Val auc roc 0.5\n",
            "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch     3: reducing learning rate of group 1 to 1.0000e-04.\n",
            "Saved model state dict for epoch 2 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFm0nuBLjo-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d069d8b1-e1c8-4227-c3b5-3f22f88cf432"
      },
      "source": [
        "model = MultiModalBertClf(no_of_classes, bert_tokenizer)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('./model_state_dict.pth'))\n",
        "    print('Loaded previous model state successfully!')\n",
        "except:\n",
        "    print('Starting fresh! Previous model state dict load unsuccessful')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded previous model state successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXL1gy1tRZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc5diJj175Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), './model_'+col_name+'_'+str(datetime.datetime.now())+'.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMm6SH297H5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_submission_data = pd.read_csv('./final_test3_unpreprocessed.csv')\n",
        "test_submission_dataset=SubmissionDataset(test_submission_data, './test_images', img_transformations, bert_tokenizer, vocab)\n",
        "test_submission_dataloader=torch.utils.data.DataLoader(test_submission_dataset, batch_size=4, collate_fn=collate_function_for_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y9PDREj1A1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357c2392-bc11-45e1-f848-c50db680606f"
      },
      "source": [
        "len(test_submission_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ez1sufJ7oqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, tweet_ids = model_predict(test_submission_dataloader, model, chosen_criteria, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOclNQGRFWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "    predictions[i]=(predictions[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnJHqglG5s0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKcQfDh7NCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "437c1435-63d2-418e-e4de-ff1e629b5b9c"
      },
      "source": [
        "tids = []\n",
        "for i in range(len(tweet_ids)):\n",
        "    tids+=[[str(tweet_ids[i][0])]]\n",
        "tids_arr = np.array(tids)\n",
        "tids_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1995, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QGf7qcW897U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TweetIds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWDbQnT4yfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tweet_ids = np.array(tweet_ids).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo4r_mE56ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(tweet_ids.shape[0]):\n",
        "#     tweet_ids[i][0]=str(tweet_ids[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ8IOaG62RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(tweet_ids[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5X5Pmb1geu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df = pd.DataFrame(np.concatenate((tids_arr, predictions), axis=1), columns=['TweetId', col_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHbyBTW5A2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "7923ecbb-7f8e-4f8c-d336-447f315eae5e"
      },
      "source": [
        "submit_df[submit_df[col_name]==0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetId</th>\n",
              "      <th>Generalized_Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [TweetId, Generalized_Hate]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQemOi-I6K0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit_df.to_csv(col_name+' '+str(datetime.datetime.now())+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQt3drOM94rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec32c9b8-06be-4aa3-b409-dac1a64be2f3"
      },
      "source": [
        "str(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-07-28 11:14:53.792686'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mSTypu-_r5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}